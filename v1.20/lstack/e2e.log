I0521 08:01:02.750249      25 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-940571618
I0521 08:01:02.750281      25 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0521 08:01:02.750385      25 e2e.go:129] Starting e2e run "a0a8bd7c-5509-447c-ab5c-7ef292a7d5d2" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1621584060 - Will randomize all specs
Will run 311 of 5668 specs

May 21 08:01:02.768: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:01:02.771: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 21 08:01:02.794: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 21 08:01:02.943: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 21 08:01:02.943: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
May 21 08:01:02.943: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 21 08:01:03.060: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 21 08:01:03.060: INFO: e2e test version: v1.20.7
May 21 08:01:03.062: INFO: kube-apiserver version: v1.20.7
May 21 08:01:03.062: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:01:03.132: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:01:03.132: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename discovery
May 21 08:01:03.235: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
May 21 08:01:03.248: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:01:05.362: INFO: Checking APIGroup: apiregistration.k8s.io
May 21 08:01:05.364: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 21 08:01:05.364: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.364: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 21 08:01:05.364: INFO: Checking APIGroup: apps
May 21 08:01:05.365: INFO: PreferredVersion.GroupVersion: apps/v1
May 21 08:01:05.365: INFO: Versions found [{apps/v1 v1}]
May 21 08:01:05.365: INFO: apps/v1 matches apps/v1
May 21 08:01:05.365: INFO: Checking APIGroup: events.k8s.io
May 21 08:01:05.367: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 21 08:01:05.367: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.367: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 21 08:01:05.367: INFO: Checking APIGroup: authentication.k8s.io
May 21 08:01:05.368: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 21 08:01:05.368: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.368: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 21 08:01:05.368: INFO: Checking APIGroup: authorization.k8s.io
May 21 08:01:05.369: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 21 08:01:05.369: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.369: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 21 08:01:05.369: INFO: Checking APIGroup: autoscaling
May 21 08:01:05.371: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 21 08:01:05.371: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 21 08:01:05.371: INFO: autoscaling/v1 matches autoscaling/v1
May 21 08:01:05.371: INFO: Checking APIGroup: batch
May 21 08:01:05.372: INFO: PreferredVersion.GroupVersion: batch/v1
May 21 08:01:05.372: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 21 08:01:05.372: INFO: batch/v1 matches batch/v1
May 21 08:01:05.372: INFO: Checking APIGroup: certificates.k8s.io
May 21 08:01:05.373: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 21 08:01:05.373: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.373: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 21 08:01:05.373: INFO: Checking APIGroup: networking.k8s.io
May 21 08:01:05.374: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 21 08:01:05.374: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.374: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 21 08:01:05.374: INFO: Checking APIGroup: extensions
May 21 08:01:05.376: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 21 08:01:05.376: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 21 08:01:05.376: INFO: extensions/v1beta1 matches extensions/v1beta1
May 21 08:01:05.376: INFO: Checking APIGroup: policy
May 21 08:01:05.377: INFO: PreferredVersion.GroupVersion: policy/v1beta1
May 21 08:01:05.377: INFO: Versions found [{policy/v1beta1 v1beta1}]
May 21 08:01:05.377: INFO: policy/v1beta1 matches policy/v1beta1
May 21 08:01:05.377: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 21 08:01:05.378: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 21 08:01:05.378: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.378: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 21 08:01:05.378: INFO: Checking APIGroup: storage.k8s.io
May 21 08:01:05.380: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 21 08:01:05.380: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.380: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 21 08:01:05.380: INFO: Checking APIGroup: admissionregistration.k8s.io
May 21 08:01:05.381: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 21 08:01:05.381: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.381: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 21 08:01:05.381: INFO: Checking APIGroup: apiextensions.k8s.io
May 21 08:01:05.382: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 21 08:01:05.382: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.382: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 21 08:01:05.382: INFO: Checking APIGroup: scheduling.k8s.io
May 21 08:01:05.383: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 21 08:01:05.383: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.383: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 21 08:01:05.383: INFO: Checking APIGroup: coordination.k8s.io
May 21 08:01:05.385: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 21 08:01:05.385: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.385: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 21 08:01:05.385: INFO: Checking APIGroup: node.k8s.io
May 21 08:01:05.386: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 21 08:01:05.386: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.386: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 21 08:01:05.386: INFO: Checking APIGroup: discovery.k8s.io
May 21 08:01:05.387: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
May 21 08:01:05.387: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.387: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
May 21 08:01:05.387: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 21 08:01:05.388: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
May 21 08:01:05.388: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May 21 08:01:05.388: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
May 21 08:01:05.388: INFO: Checking APIGroup: crd.projectcalico.org
May 21 08:01:05.389: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May 21 08:01:05.389: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May 21 08:01:05.389: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
May 21 08:01:05.389: INFO: Checking APIGroup: operator.tigera.io
May 21 08:01:05.391: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
May 21 08:01:05.391: INFO: Versions found [{operator.tigera.io/v1 v1}]
May 21 08:01:05.391: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:01:05.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5614" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":1,"skipped":37,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:01:05.442: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5036.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5036.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 08:01:53.782: INFO: DNS probes using dns-5036/dns-test-69a3553e-5570-4da3-bd9b-54b882540c8d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:01:53.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5036" for this suite.

• [SLOW TEST:48.485 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":2,"skipped":73,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:01:53.928: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:01:56.330: INFO: Deleting pod "var-expansion-af78f588-809c-401d-b35d-3e77a88fb1aa" in namespace "var-expansion-4344"
May 21 08:01:56.338: INFO: Wait up to 5m0s for pod "var-expansion-af78f588-809c-401d-b35d-3e77a88fb1aa" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:02:00.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4344" for this suite.

• [SLOW TEST:6.440 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":3,"skipped":85,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:02:00.368: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:02:07.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2606" for this suite.

• [SLOW TEST:7.224 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":4,"skipped":123,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:02:07.593: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-rf4t
STEP: Creating a pod to test atomic-volume-subpath
May 21 08:02:07.808: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rf4t" in namespace "subpath-7709" to be "Succeeded or Failed"
May 21 08:02:07.866: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Pending", Reason="", readiness=false. Elapsed: 58.412767ms
May 21 08:02:09.879: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07150179s
May 21 08:02:11.890: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Running", Reason="", readiness=true. Elapsed: 4.081830805s
May 21 08:02:13.900: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Running", Reason="", readiness=true. Elapsed: 6.091716632s
May 21 08:02:15.915: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Running", Reason="", readiness=true. Elapsed: 8.106989901s
May 21 08:02:17.929: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Running", Reason="", readiness=true. Elapsed: 10.120676031s
May 21 08:02:19.938: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Running", Reason="", readiness=true. Elapsed: 12.129847032s
May 21 08:02:21.946: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Running", Reason="", readiness=true. Elapsed: 14.13801824s
May 21 08:02:23.953: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Running", Reason="", readiness=true. Elapsed: 16.14485064s
May 21 08:02:25.969: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Running", Reason="", readiness=true. Elapsed: 18.160770628s
May 21 08:02:28.001: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Running", Reason="", readiness=true. Elapsed: 20.192916948s
May 21 08:02:30.007: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Running", Reason="", readiness=true. Elapsed: 22.198745648s
May 21 08:02:32.013: INFO: Pod "pod-subpath-test-configmap-rf4t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.205060531s
STEP: Saw pod success
May 21 08:02:32.013: INFO: Pod "pod-subpath-test-configmap-rf4t" satisfied condition "Succeeded or Failed"
May 21 08:02:32.017: INFO: Trying to get logs from node k8s-03 pod pod-subpath-test-configmap-rf4t container test-container-subpath-configmap-rf4t: <nil>
STEP: delete the pod
May 21 08:02:32.150: INFO: Waiting for pod pod-subpath-test-configmap-rf4t to disappear
May 21 08:02:32.155: INFO: Pod pod-subpath-test-configmap-rf4t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rf4t
May 21 08:02:32.155: INFO: Deleting pod "pod-subpath-test-configmap-rf4t" in namespace "subpath-7709"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:02:32.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7709" for this suite.

• [SLOW TEST:24.580 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":5,"skipped":132,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:02:32.174: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 08:02:32.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-196d5f05-a7c2-46e1-9d96-0fb8d37e40b0" in namespace "projected-2484" to be "Succeeded or Failed"
May 21 08:02:32.482: INFO: Pod "downwardapi-volume-196d5f05-a7c2-46e1-9d96-0fb8d37e40b0": Phase="Pending", Reason="", readiness=false. Elapsed: 127.179206ms
May 21 08:02:34.487: INFO: Pod "downwardapi-volume-196d5f05-a7c2-46e1-9d96-0fb8d37e40b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132515311s
May 21 08:02:36.501: INFO: Pod "downwardapi-volume-196d5f05-a7c2-46e1-9d96-0fb8d37e40b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.145760253s
STEP: Saw pod success
May 21 08:02:36.501: INFO: Pod "downwardapi-volume-196d5f05-a7c2-46e1-9d96-0fb8d37e40b0" satisfied condition "Succeeded or Failed"
May 21 08:02:36.504: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-196d5f05-a7c2-46e1-9d96-0fb8d37e40b0 container client-container: <nil>
STEP: delete the pod
May 21 08:02:36.583: INFO: Waiting for pod downwardapi-volume-196d5f05-a7c2-46e1-9d96-0fb8d37e40b0 to disappear
May 21 08:02:36.626: INFO: Pod downwardapi-volume-196d5f05-a7c2-46e1-9d96-0fb8d37e40b0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:02:36.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2484" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":140,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:02:36.640: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-a3bfe4b2-5c08-44d5-9407-63ba321e27d8
STEP: Creating a pod to test consume secrets
May 21 08:02:37.010: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-08e69c19-ebd7-4abe-9917-0c99ac9e6e30" in namespace "projected-3428" to be "Succeeded or Failed"
May 21 08:02:37.073: INFO: Pod "pod-projected-secrets-08e69c19-ebd7-4abe-9917-0c99ac9e6e30": Phase="Pending", Reason="", readiness=false. Elapsed: 62.72247ms
May 21 08:02:39.079: INFO: Pod "pod-projected-secrets-08e69c19-ebd7-4abe-9917-0c99ac9e6e30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068877982s
May 21 08:02:41.084: INFO: Pod "pod-projected-secrets-08e69c19-ebd7-4abe-9917-0c99ac9e6e30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073987723s
STEP: Saw pod success
May 21 08:02:41.084: INFO: Pod "pod-projected-secrets-08e69c19-ebd7-4abe-9917-0c99ac9e6e30" satisfied condition "Succeeded or Failed"
May 21 08:02:41.088: INFO: Trying to get logs from node k8s-03 pod pod-projected-secrets-08e69c19-ebd7-4abe-9917-0c99ac9e6e30 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 08:02:41.166: INFO: Waiting for pod pod-projected-secrets-08e69c19-ebd7-4abe-9917-0c99ac9e6e30 to disappear
May 21 08:02:41.197: INFO: Pod pod-projected-secrets-08e69c19-ebd7-4abe-9917-0c99ac9e6e30 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:02:41.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3428" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":7,"skipped":156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:02:41.210: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:02:41.336: INFO: Creating ReplicaSet my-hostname-basic-f1a75de0-20cc-434a-83bf-297a0a1411b8
May 21 08:02:41.347: INFO: Pod name my-hostname-basic-f1a75de0-20cc-434a-83bf-297a0a1411b8: Found 0 pods out of 1
May 21 08:02:46.363: INFO: Pod name my-hostname-basic-f1a75de0-20cc-434a-83bf-297a0a1411b8: Found 1 pods out of 1
May 21 08:02:46.363: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f1a75de0-20cc-434a-83bf-297a0a1411b8" is running
May 21 08:02:46.367: INFO: Pod "my-hostname-basic-f1a75de0-20cc-434a-83bf-297a0a1411b8-bjt88" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 08:02:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 08:02:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 08:02:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 08:02:41 +0000 UTC Reason: Message:}])
May 21 08:02:46.368: INFO: Trying to dial the pod
May 21 08:02:51.384: INFO: Controller my-hostname-basic-f1a75de0-20cc-434a-83bf-297a0a1411b8: Got expected result from replica 1 [my-hostname-basic-f1a75de0-20cc-434a-83bf-297a0a1411b8-bjt88]: "my-hostname-basic-f1a75de0-20cc-434a-83bf-297a0a1411b8-bjt88", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:02:51.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2315" for this suite.

• [SLOW TEST:10.188 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":8,"skipped":223,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:02:51.398: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-4d1088d3-2b20-403e-ba9f-a838cb2512ad
STEP: Creating configMap with name cm-test-opt-upd-5c2c981b-0521-451d-91ef-b0d9a9041cd9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4d1088d3-2b20-403e-ba9f-a838cb2512ad
STEP: Updating configmap cm-test-opt-upd-5c2c981b-0521-451d-91ef-b0d9a9041cd9
STEP: Creating configMap with name cm-test-opt-create-66a1c71c-7798-40b3-b105-16bd506b54f4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:04:06.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5618" for this suite.

• [SLOW TEST:74.923 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":9,"skipped":227,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:04:06.321: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 21 08:04:06.437: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:04:11.390: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:04:35.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-472" for this suite.

• [SLOW TEST:29.369 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":10,"skipped":227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:04:35.691: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:04:36.009: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a95c919a-305b-478c-a4d7-373d7d2517b8" in namespace "security-context-test-2522" to be "Succeeded or Failed"
May 21 08:04:36.014: INFO: Pod "busybox-user-65534-a95c919a-305b-478c-a4d7-373d7d2517b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.360067ms
May 21 08:04:38.027: INFO: Pod "busybox-user-65534-a95c919a-305b-478c-a4d7-373d7d2517b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018472657s
May 21 08:04:40.063: INFO: Pod "busybox-user-65534-a95c919a-305b-478c-a4d7-373d7d2517b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054106569s
May 21 08:04:40.063: INFO: Pod "busybox-user-65534-a95c919a-305b-478c-a4d7-373d7d2517b8" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:04:40.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2522" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":11,"skipped":272,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:04:40.160: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 08:04:41.915: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 08:04:43.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181082, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181082, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181082, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181081, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:04:45.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181082, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181082, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181082, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181081, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 08:04:49.108: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 21 08:04:49.142: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:04:49.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6920" for this suite.
STEP: Destroying namespace "webhook-6920-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:9.224 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":12,"skipped":287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:04:49.385: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:05:06.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6625" for this suite.

• [SLOW TEST:17.427 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":13,"skipped":347,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:05:06.812: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 21 08:05:07.032: INFO: Waiting up to 5m0s for pod "pod-acd98f1f-6d3f-4801-b24a-01e89f32fe06" in namespace "emptydir-4606" to be "Succeeded or Failed"
May 21 08:05:07.110: INFO: Pod "pod-acd98f1f-6d3f-4801-b24a-01e89f32fe06": Phase="Pending", Reason="", readiness=false. Elapsed: 77.882142ms
May 21 08:05:09.162: INFO: Pod "pod-acd98f1f-6d3f-4801-b24a-01e89f32fe06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130034855s
May 21 08:05:11.255: INFO: Pod "pod-acd98f1f-6d3f-4801-b24a-01e89f32fe06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.223137262s
STEP: Saw pod success
May 21 08:05:11.255: INFO: Pod "pod-acd98f1f-6d3f-4801-b24a-01e89f32fe06" satisfied condition "Succeeded or Failed"
May 21 08:05:11.259: INFO: Trying to get logs from node k8s-03 pod pod-acd98f1f-6d3f-4801-b24a-01e89f32fe06 container test-container: <nil>
STEP: delete the pod
May 21 08:05:11.439: INFO: Waiting for pod pod-acd98f1f-6d3f-4801-b24a-01e89f32fe06 to disappear
May 21 08:05:11.450: INFO: Pod pod-acd98f1f-6d3f-4801-b24a-01e89f32fe06 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:05:11.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4606" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":359,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:05:11.472: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 21 08:05:11.588: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 08:06:11.642: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:06:11.676: INFO: Starting informer...
STEP: Starting pods...
May 21 08:06:11.975: INFO: Pod1 is running on k8s-03. Tainting Node
May 21 08:06:16.204: INFO: Pod2 is running on k8s-03. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 21 08:06:30.233: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 21 08:06:50.208: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:06:50.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1467" for this suite.

• [SLOW TEST:98.814 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":15,"skipped":375,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:06:50.286: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 08:06:50.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-937dfecf-d1b3-4359-a7ac-602b0eb4decf" in namespace "downward-api-8674" to be "Succeeded or Failed"
May 21 08:06:50.497: INFO: Pod "downwardapi-volume-937dfecf-d1b3-4359-a7ac-602b0eb4decf": Phase="Pending", Reason="", readiness=false. Elapsed: 51.841162ms
May 21 08:06:52.502: INFO: Pod "downwardapi-volume-937dfecf-d1b3-4359-a7ac-602b0eb4decf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0573761s
May 21 08:06:54.516: INFO: Pod "downwardapi-volume-937dfecf-d1b3-4359-a7ac-602b0eb4decf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070802467s
STEP: Saw pod success
May 21 08:06:54.516: INFO: Pod "downwardapi-volume-937dfecf-d1b3-4359-a7ac-602b0eb4decf" satisfied condition "Succeeded or Failed"
May 21 08:06:54.520: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-937dfecf-d1b3-4359-a7ac-602b0eb4decf container client-container: <nil>
STEP: delete the pod
May 21 08:06:54.583: INFO: Waiting for pod downwardapi-volume-937dfecf-d1b3-4359-a7ac-602b0eb4decf to disappear
May 21 08:06:54.591: INFO: Pod downwardapi-volume-937dfecf-d1b3-4359-a7ac-602b0eb4decf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:06:54.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8674" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":380,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:06:54.603: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2901
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 21 08:06:54.788: INFO: Found 0 stateful pods, waiting for 3
May 21 08:07:04.797: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 08:07:04.797: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 08:07:04.797: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 21 08:07:14.797: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 08:07:14.797: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 08:07:14.797: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 21 08:07:14.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-2901 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 08:07:18.356: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 08:07:18.357: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 08:07:18.357: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 21 08:07:28.465: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 21 08:07:38.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-2901 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 08:07:38.986: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 08:07:38.986: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 08:07:38.986: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 08:07:49.034: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:07:49.034: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:07:49.034: INFO: Waiting for Pod statefulset-2901/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:07:59.096: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:07:59.096: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:07:59.096: INFO: Waiting for Pod statefulset-2901/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:08:09.903: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:08:09.903: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:08:09.903: INFO: Waiting for Pod statefulset-2901/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:08:19.049: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:08:19.049: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:08:19.049: INFO: Waiting for Pod statefulset-2901/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:08:29.053: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:08:29.053: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:08:29.053: INFO: Waiting for Pod statefulset-2901/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:08:39.051: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:08:39.051: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:08:49.052: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:08:49.052: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:09:05.042: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:09:05.042: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:09:09.047: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:09:09.047: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 08:09:19.050: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:09:19.050: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
May 21 08:09:29.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-2901 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 08:09:29.419: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 08:09:29.419: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 08:09:29.419: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 08:09:39.482: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 21 08:09:49.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-2901 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 08:09:49.924: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 08:09:49.924: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 08:09:49.924: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 08:09:59.954: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:09:59.954: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 21 08:09:59.954: INFO: Waiting for Pod statefulset-2901/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 21 08:09:59.954: INFO: Waiting for Pod statefulset-2901/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 21 08:10:09.966: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
May 21 08:10:09.966: INFO: Waiting for Pod statefulset-2901/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 21 08:10:09.966: INFO: Waiting for Pod statefulset-2901/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 21 08:10:19.984: INFO: Waiting for StatefulSet statefulset-2901/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 08:10:29.969: INFO: Deleting all statefulset in ns statefulset-2901
May 21 08:10:29.973: INFO: Scaling statefulset ss2 to 0
May 21 08:10:50.061: INFO: Waiting for statefulset status.replicas updated to 0
May 21 08:10:50.064: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:10:50.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2901" for this suite.

• [SLOW TEST:235.502 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":17,"skipped":384,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:10:50.106: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:10:50.432: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:10:57.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1411" for this suite.

• [SLOW TEST:7.017 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":18,"skipped":400,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:10:57.123: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 08:10:58.795: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 08:11:00.810: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181458, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181458, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181458, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757181458, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 08:11:03.861: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:11:04.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5246" for this suite.
STEP: Destroying namespace "webhook-5246-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.466 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":19,"skipped":418,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:11:04.590: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:11:04.779: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 08:11:10.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-8759 --namespace=crd-publish-openapi-8759 create -f -'
May 21 08:11:14.166: INFO: stderr: ""
May 21 08:11:14.166: INFO: stdout: "e2e-test-crd-publish-openapi-7087-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 21 08:11:14.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-8759 --namespace=crd-publish-openapi-8759 delete e2e-test-crd-publish-openapi-7087-crds test-cr'
May 21 08:11:14.378: INFO: stderr: ""
May 21 08:11:14.378: INFO: stdout: "e2e-test-crd-publish-openapi-7087-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 21 08:11:14.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-8759 --namespace=crd-publish-openapi-8759 apply -f -'
May 21 08:11:15.244: INFO: stderr: ""
May 21 08:11:15.244: INFO: stdout: "e2e-test-crd-publish-openapi-7087-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 21 08:11:15.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-8759 --namespace=crd-publish-openapi-8759 delete e2e-test-crd-publish-openapi-7087-crds test-cr'
May 21 08:11:15.425: INFO: stderr: ""
May 21 08:11:15.425: INFO: stdout: "e2e-test-crd-publish-openapi-7087-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 21 08:11:15.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-8759 explain e2e-test-crd-publish-openapi-7087-crds'
May 21 08:11:16.440: INFO: stderr: ""
May 21 08:11:16.440: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7087-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:11:22.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8759" for this suite.

• [SLOW TEST:17.445 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":20,"skipped":422,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:11:22.035: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:11:22.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-477" for this suite.
STEP: Destroying namespace "nspatchtest-603ad920-5d1a-4bab-ade5-e344b4ddf42a-1816" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":21,"skipped":440,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:11:22.576: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-ebf788cb-f12d-4717-9bf4-825c66a5e13a in namespace container-probe-7480
May 21 08:11:26.733: INFO: Started pod liveness-ebf788cb-f12d-4717-9bf4-825c66a5e13a in namespace container-probe-7480
STEP: checking the pod's current state and verifying that restartCount is present
May 21 08:11:26.737: INFO: Initial restart count of pod liveness-ebf788cb-f12d-4717-9bf4-825c66a5e13a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:15:26.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7480" for this suite.

• [SLOW TEST:244.291 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:15:26.868: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 21 08:15:27.101: INFO: Waiting up to 5m0s for pod "pod-6e4871fa-41c3-4ad4-bfb8-cab29c5e2a2d" in namespace "emptydir-669" to be "Succeeded or Failed"
May 21 08:15:27.133: INFO: Pod "pod-6e4871fa-41c3-4ad4-bfb8-cab29c5e2a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 31.525147ms
May 21 08:15:29.139: INFO: Pod "pod-6e4871fa-41c3-4ad4-bfb8-cab29c5e2a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037416136s
May 21 08:15:31.264: INFO: Pod "pod-6e4871fa-41c3-4ad4-bfb8-cab29c5e2a2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.162396775s
STEP: Saw pod success
May 21 08:15:31.264: INFO: Pod "pod-6e4871fa-41c3-4ad4-bfb8-cab29c5e2a2d" satisfied condition "Succeeded or Failed"
May 21 08:15:31.270: INFO: Trying to get logs from node k8s-03 pod pod-6e4871fa-41c3-4ad4-bfb8-cab29c5e2a2d container test-container: <nil>
STEP: delete the pod
May 21 08:15:31.426: INFO: Waiting for pod pod-6e4871fa-41c3-4ad4-bfb8-cab29c5e2a2d to disappear
May 21 08:15:31.434: INFO: Pod pod-6e4871fa-41c3-4ad4-bfb8-cab29c5e2a2d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:15:31.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-669" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":478,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:15:31.447: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0521 08:15:42.347489      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 08:16:44.372: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 21 08:16:44.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hc4q" in namespace "gc-654"
May 21 08:16:44.455: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zpcq" in namespace "gc-654"
May 21 08:16:44.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-68h8k" in namespace "gc-654"
May 21 08:16:44.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jg95" in namespace "gc-654"
May 21 08:16:44.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-98b6k" in namespace "gc-654"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:16:44.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-654" for this suite.

• [SLOW TEST:73.563 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":24,"skipped":488,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:16:45.011: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8343.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8343.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 08:16:51.644: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:51.650: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:51.659: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:51.697: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:51.750: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:51.759: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:51.767: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:51.775: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:51.792: INFO: Lookups using dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local]

May 21 08:16:56.798: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:56.802: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:56.806: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:56.811: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:56.824: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:56.830: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:56.834: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:56.838: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:16:56.847: INFO: Lookups using dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local]

May 21 08:17:01.798: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:01.802: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:01.807: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:01.812: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:01.827: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:01.832: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:01.837: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:01.841: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:01.850: INFO: Lookups using dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local]

May 21 08:17:06.799: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:06.803: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:06.808: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:06.812: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:06.826: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:06.830: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:06.835: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:06.839: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:06.849: INFO: Lookups using dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local]

May 21 08:17:11.799: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:11.803: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:11.808: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:11.812: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:11.826: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:11.831: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:11.835: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:11.840: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:11.849: INFO: Lookups using dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local]

May 21 08:17:16.799: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:16.804: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:16.808: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:16.812: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:16.826: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:16.831: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:16.836: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:16.840: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local from pod dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0: the server could not find the requested resource (get pods dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0)
May 21 08:17:16.849: INFO: Lookups using dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8343.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8343.svc.cluster.local jessie_udp@dns-test-service-2.dns-8343.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8343.svc.cluster.local]

May 21 08:17:21.850: INFO: DNS probes using dns-8343/dns-test-8e68d661-a970-449b-ad7d-20ba97d0cee0 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:17:22.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8343" for this suite.

• [SLOW TEST:37.389 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":25,"skipped":520,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:17:22.401: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 21 08:17:22.618: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 08:17:22.629: INFO: Waiting for terminating namespaces to be deleted...
May 21 08:17:22.638: INFO: 
Logging pods the apiserver thinks is on node k8s-02 before test
May 21 08:17:22.649: INFO: calico-node-gpcrh from calico-system started at 2021-05-21 05:44:44 +0000 UTC (1 container statuses recorded)
May 21 08:17:22.650: INFO: 	Container calico-node ready: true, restart count 0
May 21 08:17:22.650: INFO: calico-typha-7578696db5-f4hhm from calico-system started at 2021-05-21 05:45:42 +0000 UTC (1 container statuses recorded)
May 21 08:17:22.650: INFO: 	Container calico-typha ready: true, restart count 0
May 21 08:17:22.650: INFO: kube-proxy-kl6jm from kube-system started at 2021-05-21 05:44:44 +0000 UTC (1 container statuses recorded)
May 21 08:17:22.650: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 08:17:22.650: INFO: sonobuoy-e2e-job-4762d948ca704da5 from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 08:17:22.650: INFO: 	Container e2e ready: true, restart count 0
May 21 08:17:22.650: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 08:17:22.650: INFO: sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-7h98l from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 08:17:22.650: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 08:17:22.650: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 08:17:22.650: INFO: 
Logging pods the apiserver thinks is on node k8s-03 before test
May 21 08:17:22.673: INFO: calico-node-s987r from calico-system started at 2021-05-21 05:45:50 +0000 UTC (1 container statuses recorded)
May 21 08:17:22.673: INFO: 	Container calico-node ready: true, restart count 0
May 21 08:17:22.673: INFO: calico-typha-7578696db5-qh5gh from calico-system started at 2021-05-21 05:47:49 +0000 UTC (1 container statuses recorded)
May 21 08:17:22.673: INFO: 	Container calico-typha ready: true, restart count 0
May 21 08:17:22.673: INFO: kube-proxy-7n7r8 from kube-system started at 2021-05-21 05:45:50 +0000 UTC (1 container statuses recorded)
May 21 08:17:22.673: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 08:17:22.673: INFO: sonobuoy from sonobuoy started at 2021-05-21 08:00:13 +0000 UTC (1 container statuses recorded)
May 21 08:17:22.673: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 08:17:22.673: INFO: sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-p7f4t from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 08:17:22.673: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 08:17:22.673: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node k8s-02
STEP: verifying the node has the label node k8s-03
May 21 08:17:22.965: INFO: Pod calico-node-gpcrh requesting resource cpu=0m on Node k8s-02
May 21 08:17:22.965: INFO: Pod calico-node-s987r requesting resource cpu=0m on Node k8s-03
May 21 08:17:22.965: INFO: Pod calico-typha-7578696db5-f4hhm requesting resource cpu=0m on Node k8s-02
May 21 08:17:22.965: INFO: Pod calico-typha-7578696db5-qh5gh requesting resource cpu=0m on Node k8s-03
May 21 08:17:22.965: INFO: Pod kube-proxy-7n7r8 requesting resource cpu=0m on Node k8s-03
May 21 08:17:22.965: INFO: Pod kube-proxy-kl6jm requesting resource cpu=0m on Node k8s-02
May 21 08:17:22.965: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-03
May 21 08:17:22.965: INFO: Pod sonobuoy-e2e-job-4762d948ca704da5 requesting resource cpu=0m on Node k8s-02
May 21 08:17:22.965: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-7h98l requesting resource cpu=0m on Node k8s-02
May 21 08:17:22.965: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-p7f4t requesting resource cpu=0m on Node k8s-03
STEP: Starting Pods to consume most of the cluster CPU.
May 21 08:17:22.965: INFO: Creating a pod which consumes cpu=2800m on Node k8s-02
May 21 08:17:22.975: INFO: Creating a pod which consumes cpu=2800m on Node k8s-03
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5d53bc-88da-4796-ac3f-5d69470029fd.16810701543579ab], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5931/filler-pod-7f5d53bc-88da-4796-ac3f-5d69470029fd to k8s-03]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5d53bc-88da-4796-ac3f-5d69470029fd.16810701d3355f97], Reason = [Pulled], Message = [Container image "registry.cn-hangzhou.aliyuncs.com/ks8gcr/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5d53bc-88da-4796-ac3f-5d69470029fd.16810701f758065b], Reason = [Created], Message = [Created container filler-pod-7f5d53bc-88da-4796-ac3f-5d69470029fd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5d53bc-88da-4796-ac3f-5d69470029fd.1681070205a6ba72], Reason = [Started], Message = [Started container filler-pod-7f5d53bc-88da-4796-ac3f-5d69470029fd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b56afb57-84b8-41df-80b6-10e2226cd80d.168107014fdc49d7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5931/filler-pod-b56afb57-84b8-41df-80b6-10e2226cd80d to k8s-02]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b56afb57-84b8-41df-80b6-10e2226cd80d.16810701d20cb621], Reason = [Pulled], Message = [Container image "registry.cn-hangzhou.aliyuncs.com/ks8gcr/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b56afb57-84b8-41df-80b6-10e2226cd80d.16810701f51ffbe3], Reason = [Created], Message = [Created container filler-pod-b56afb57-84b8-41df-80b6-10e2226cd80d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b56afb57-84b8-41df-80b6-10e2226cd80d.1681070203d11d42], Reason = [Started], Message = [Started container filler-pod-b56afb57-84b8-41df-80b6-10e2226cd80d]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168107024463e641], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168107024ab6eccb], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node k8s-02
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-03
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:17:28.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5931" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:5.885 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":26,"skipped":524,"failed":0}
SSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:17:28.286: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 21 08:17:28.547: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 21 08:17:28.586: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:17:28.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5268" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":27,"skipped":532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:17:28.696: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 21 08:17:28.928: INFO: Waiting up to 5m0s for pod "pod-17599825-d16f-4d5c-8183-8e8e0543a20c" in namespace "emptydir-9935" to be "Succeeded or Failed"
May 21 08:17:28.997: INFO: Pod "pod-17599825-d16f-4d5c-8183-8e8e0543a20c": Phase="Pending", Reason="", readiness=false. Elapsed: 68.450463ms
May 21 08:17:31.008: INFO: Pod "pod-17599825-d16f-4d5c-8183-8e8e0543a20c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079846505s
May 21 08:17:33.020: INFO: Pod "pod-17599825-d16f-4d5c-8183-8e8e0543a20c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.091880085s
STEP: Saw pod success
May 21 08:17:33.020: INFO: Pod "pod-17599825-d16f-4d5c-8183-8e8e0543a20c" satisfied condition "Succeeded or Failed"
May 21 08:17:33.025: INFO: Trying to get logs from node k8s-03 pod pod-17599825-d16f-4d5c-8183-8e8e0543a20c container test-container: <nil>
STEP: delete the pod
May 21 08:17:33.138: INFO: Waiting for pod pod-17599825-d16f-4d5c-8183-8e8e0543a20c to disappear
May 21 08:17:33.143: INFO: Pod pod-17599825-d16f-4d5c-8183-8e8e0543a20c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:17:33.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9935" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":28,"skipped":563,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:17:33.158: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 21 08:17:33.327: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 08:17:33.345: INFO: Waiting for terminating namespaces to be deleted...
May 21 08:17:33.351: INFO: 
Logging pods the apiserver thinks is on node k8s-02 before test
May 21 08:17:33.373: INFO: calico-node-gpcrh from calico-system started at 2021-05-21 05:44:44 +0000 UTC (1 container statuses recorded)
May 21 08:17:33.373: INFO: 	Container calico-node ready: true, restart count 0
May 21 08:17:33.373: INFO: calico-typha-7578696db5-f4hhm from calico-system started at 2021-05-21 05:45:42 +0000 UTC (1 container statuses recorded)
May 21 08:17:33.373: INFO: 	Container calico-typha ready: true, restart count 0
May 21 08:17:33.373: INFO: kube-proxy-kl6jm from kube-system started at 2021-05-21 05:44:44 +0000 UTC (1 container statuses recorded)
May 21 08:17:33.373: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 08:17:33.373: INFO: filler-pod-b56afb57-84b8-41df-80b6-10e2226cd80d from sched-pred-5931 started at 2021-05-21 08:17:23 +0000 UTC (1 container statuses recorded)
May 21 08:17:33.373: INFO: 	Container filler-pod-b56afb57-84b8-41df-80b6-10e2226cd80d ready: true, restart count 0
May 21 08:17:33.373: INFO: sonobuoy-e2e-job-4762d948ca704da5 from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 08:17:33.373: INFO: 	Container e2e ready: true, restart count 0
May 21 08:17:33.373: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 08:17:33.373: INFO: sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-7h98l from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 08:17:33.373: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 08:17:33.373: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 08:17:33.373: INFO: 
Logging pods the apiserver thinks is on node k8s-03 before test
May 21 08:17:33.408: INFO: calico-node-s987r from calico-system started at 2021-05-21 05:45:50 +0000 UTC (1 container statuses recorded)
May 21 08:17:33.408: INFO: 	Container calico-node ready: true, restart count 0
May 21 08:17:33.408: INFO: calico-typha-7578696db5-qh5gh from calico-system started at 2021-05-21 05:47:49 +0000 UTC (1 container statuses recorded)
May 21 08:17:33.408: INFO: 	Container calico-typha ready: true, restart count 0
May 21 08:17:33.408: INFO: kube-proxy-7n7r8 from kube-system started at 2021-05-21 05:45:50 +0000 UTC (1 container statuses recorded)
May 21 08:17:33.408: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 08:17:33.408: INFO: filler-pod-7f5d53bc-88da-4796-ac3f-5d69470029fd from sched-pred-5931 started at 2021-05-21 08:17:23 +0000 UTC (1 container statuses recorded)
May 21 08:17:33.408: INFO: 	Container filler-pod-7f5d53bc-88da-4796-ac3f-5d69470029fd ready: true, restart count 0
May 21 08:17:33.408: INFO: sonobuoy from sonobuoy started at 2021-05-21 08:00:13 +0000 UTC (1 container statuses recorded)
May 21 08:17:33.408: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 08:17:33.408: INFO: sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-p7f4t from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 08:17:33.408: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 08:17:33.408: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6b2b87da-1af9-4a13-874d-38736e04d0ee 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 8.16.0.95 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-6b2b87da-1af9-4a13-874d-38736e04d0ee off the node k8s-03
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6b2b87da-1af9-4a13-874d-38736e04d0ee
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:22:41.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2419" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:308.816 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":29,"skipped":564,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:22:41.974: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-2991
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2991 to expose endpoints map[]
May 21 08:22:42.176: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 21 08:22:43.193: INFO: successfully validated that service multi-endpoint-test in namespace services-2991 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2991
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2991 to expose endpoints map[pod1:[100]]
May 21 08:22:46.372: INFO: successfully validated that service multi-endpoint-test in namespace services-2991 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2991
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2991 to expose endpoints map[pod1:[100] pod2:[101]]
May 21 08:22:50.498: INFO: successfully validated that service multi-endpoint-test in namespace services-2991 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-2991
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2991 to expose endpoints map[pod2:[101]]
May 21 08:22:50.797: INFO: successfully validated that service multi-endpoint-test in namespace services-2991 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2991
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2991 to expose endpoints map[]
May 21 08:22:51.021: INFO: successfully validated that service multi-endpoint-test in namespace services-2991 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:22:51.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2991" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:9.292 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":30,"skipped":575,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:22:51.267: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2383
STEP: creating service affinity-clusterip in namespace services-2383
STEP: creating replication controller affinity-clusterip in namespace services-2383
I0521 08:22:51.615170      25 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-2383, replica count: 3
I0521 08:22:54.665938      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:22:57.666190      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 08:22:57.684: INFO: Creating new exec pod
May 21 08:23:02.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2383 exec execpod-affinity7hz2r -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
May 21 08:23:05.229: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 21 08:23:05.229: INFO: stdout: ""
May 21 08:23:05.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2383 exec execpod-affinity7hz2r -- /bin/sh -x -c nc -zv -t -w 2 10.96.14.173 80'
May 21 08:23:05.575: INFO: stderr: "+ nc -zv -t -w 2 10.96.14.173 80\nConnection to 10.96.14.173 80 port [tcp/http] succeeded!\n"
May 21 08:23:05.575: INFO: stdout: ""
May 21 08:23:05.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2383 exec execpod-affinity7hz2r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.14.173:80/ ; done'
May 21 08:23:06.085: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.14.173:80/\n"
May 21 08:23:06.086: INFO: stdout: "\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5\naffinity-clusterip-9vzv5"
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Received response from host: affinity-clusterip-9vzv5
May 21 08:23:06.086: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2383, will wait for the garbage collector to delete the pods
May 21 08:23:06.369: INFO: Deleting ReplicationController affinity-clusterip took: 67.152294ms
May 21 08:23:07.069: INFO: Terminating ReplicationController affinity-clusterip pods took: 700.341943ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:23:50.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2383" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:59.198 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":31,"skipped":588,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:23:50.465: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 08:23:50.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdde6fdc-e5a0-4d79-9a1b-24ad8707e5a2" in namespace "downward-api-6044" to be "Succeeded or Failed"
May 21 08:23:50.638: INFO: Pod "downwardapi-volume-fdde6fdc-e5a0-4d79-9a1b-24ad8707e5a2": Phase="Pending", Reason="", readiness=false. Elapsed: 33.038112ms
May 21 08:23:52.654: INFO: Pod "downwardapi-volume-fdde6fdc-e5a0-4d79-9a1b-24ad8707e5a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049104361s
May 21 08:23:54.660: INFO: Pod "downwardapi-volume-fdde6fdc-e5a0-4d79-9a1b-24ad8707e5a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055095984s
STEP: Saw pod success
May 21 08:23:54.660: INFO: Pod "downwardapi-volume-fdde6fdc-e5a0-4d79-9a1b-24ad8707e5a2" satisfied condition "Succeeded or Failed"
May 21 08:23:54.664: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-fdde6fdc-e5a0-4d79-9a1b-24ad8707e5a2 container client-container: <nil>
STEP: delete the pod
May 21 08:23:54.752: INFO: Waiting for pod downwardapi-volume-fdde6fdc-e5a0-4d79-9a1b-24ad8707e5a2 to disappear
May 21 08:23:54.760: INFO: Pod downwardapi-volume-fdde6fdc-e5a0-4d79-9a1b-24ad8707e5a2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:23:54.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6044" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":32,"skipped":593,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:23:54.777: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:23:54.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1405" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":33,"skipped":610,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:23:54.930: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 21 08:23:55.069: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 21 08:24:17.549: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:24:22.837: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:24:46.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1346" for this suite.

• [SLOW TEST:52.027 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":34,"skipped":618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:24:46.958: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
May 21 08:24:47.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-8575 create -f -'
May 21 08:24:48.138: INFO: stderr: ""
May 21 08:24:48.138: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 21 08:24:48.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-8575 diff -f -'
May 21 08:24:49.696: INFO: rc: 1
May 21 08:24:49.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-8575 delete -f -'
May 21 08:24:49.903: INFO: stderr: ""
May 21 08:24:49.903: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:24:49.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8575" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":35,"skipped":643,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:24:49.917: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-533fad18-3051-4acc-a032-5b1a80267962
STEP: Creating a pod to test consume configMaps
May 21 08:24:50.103: INFO: Waiting up to 5m0s for pod "pod-configmaps-80219b55-4763-4eee-a142-093e2601cbb3" in namespace "configmap-5752" to be "Succeeded or Failed"
May 21 08:24:50.116: INFO: Pod "pod-configmaps-80219b55-4763-4eee-a142-093e2601cbb3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.277258ms
May 21 08:24:52.168: INFO: Pod "pod-configmaps-80219b55-4763-4eee-a142-093e2601cbb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065625187s
May 21 08:24:54.175: INFO: Pod "pod-configmaps-80219b55-4763-4eee-a142-093e2601cbb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072064977s
STEP: Saw pod success
May 21 08:24:54.175: INFO: Pod "pod-configmaps-80219b55-4763-4eee-a142-093e2601cbb3" satisfied condition "Succeeded or Failed"
May 21 08:24:54.179: INFO: Trying to get logs from node k8s-02 pod pod-configmaps-80219b55-4763-4eee-a142-093e2601cbb3 container configmap-volume-test: <nil>
STEP: delete the pod
May 21 08:24:54.246: INFO: Waiting for pod pod-configmaps-80219b55-4763-4eee-a142-093e2601cbb3 to disappear
May 21 08:24:54.250: INFO: Pod pod-configmaps-80219b55-4763-4eee-a142-093e2601cbb3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:24:54.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5752" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:24:54.265: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-09701cae-9909-4061-aab4-48b8c880248b in namespace container-probe-1486
May 21 08:24:58.530: INFO: Started pod test-webserver-09701cae-9909-4061-aab4-48b8c880248b in namespace container-probe-1486
STEP: checking the pod's current state and verifying that restartCount is present
May 21 08:24:58.535: INFO: Initial restart count of pod test-webserver-09701cae-9909-4061-aab4-48b8c880248b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:28:59.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1486" for this suite.

• [SLOW TEST:244.969 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":37,"skipped":719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:28:59.235: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:29:03.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9721" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":38,"skipped":751,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:29:03.513: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 21 08:29:03.706: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:29:15.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-444" for this suite.

• [SLOW TEST:12.495 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":39,"skipped":787,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:29:16.008: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-3a55f791-1813-401c-80aa-66077f824eea
STEP: Creating a pod to test consume configMaps
May 21 08:29:16.405: INFO: Waiting up to 5m0s for pod "pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7" in namespace "configmap-5961" to be "Succeeded or Failed"
May 21 08:29:16.440: INFO: Pod "pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7": Phase="Pending", Reason="", readiness=false. Elapsed: 35.188659ms
May 21 08:29:18.448: INFO: Pod "pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042825967s
May 21 08:29:20.454: INFO: Pod "pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048876195s
May 21 08:29:22.481: INFO: Pod "pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075812914s
May 21 08:29:24.488: INFO: Pod "pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083010015s
May 21 08:29:26.494: INFO: Pod "pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.088669607s
STEP: Saw pod success
May 21 08:29:26.494: INFO: Pod "pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7" satisfied condition "Succeeded or Failed"
May 21 08:29:26.498: INFO: Trying to get logs from node k8s-03 pod pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7 container agnhost-container: <nil>
STEP: delete the pod
May 21 08:29:26.604: INFO: Waiting for pod pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7 to disappear
May 21 08:29:26.611: INFO: Pod pod-configmaps-f08a4bc7-85bd-43eb-a98b-96795a64b2e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:29:26.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5961" for this suite.

• [SLOW TEST:10.614 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":789,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:29:26.623: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
May 21 08:29:26.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-7085 api-versions'
May 21 08:29:27.918: INFO: stderr: ""
May 21 08:29:27.918: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noperator.tigera.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:29:27.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7085" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":41,"skipped":802,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:29:28.226: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:29:30.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-538 version'
May 21 08:29:31.136: INFO: stderr: ""
May 21 08:29:31.137: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.7\", GitCommit:\"132a687512d7fb058d0f5890f07d4121b3f0a2e2\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T12:40:09Z\", GoVersion:\"go1.15.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.7\", GitCommit:\"132a687512d7fb058d0f5890f07d4121b3f0a2e2\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T12:32:49Z\", GoVersion:\"go1.15.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:29:31.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-538" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":42,"skipped":808,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:29:31.209: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:29:35.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7299" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":43,"skipped":810,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:29:35.609: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 08:29:35.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3aea881-27a2-4bc9-852f-7e08252a728a" in namespace "downward-api-4700" to be "Succeeded or Failed"
May 21 08:29:35.874: INFO: Pod "downwardapi-volume-e3aea881-27a2-4bc9-852f-7e08252a728a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.561906ms
May 21 08:29:37.883: INFO: Pod "downwardapi-volume-e3aea881-27a2-4bc9-852f-7e08252a728a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030098221s
May 21 08:29:39.929: INFO: Pod "downwardapi-volume-e3aea881-27a2-4bc9-852f-7e08252a728a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076219317s
STEP: Saw pod success
May 21 08:29:39.929: INFO: Pod "downwardapi-volume-e3aea881-27a2-4bc9-852f-7e08252a728a" satisfied condition "Succeeded or Failed"
May 21 08:29:39.937: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-e3aea881-27a2-4bc9-852f-7e08252a728a container client-container: <nil>
STEP: delete the pod
May 21 08:29:40.061: INFO: Waiting for pod downwardapi-volume-e3aea881-27a2-4bc9-852f-7e08252a728a to disappear
May 21 08:29:40.068: INFO: Pod downwardapi-volume-e3aea881-27a2-4bc9-852f-7e08252a728a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:29:40.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4700" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":44,"skipped":817,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:29:40.083: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:29:40.276: INFO: Creating deployment "test-recreate-deployment"
May 21 08:29:40.285: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 21 08:29:40.358: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 21 08:29:42.369: INFO: Waiting deployment "test-recreate-deployment" to complete
May 21 08:29:42.373: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182580, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182580, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182580, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182580, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c9d49486\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:29:45.033: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 21 08:29:45.392: INFO: Updating deployment test-recreate-deployment
May 21 08:29:45.392: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 08:29:47.696: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8193  9949093a-9775-43ce-bddd-9e1f5e696196 29802 2 2021-05-21 08:29:40 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 08:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 08:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d6c9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-21 08:29:47 +0000 UTC,LastTransitionTime:2021-05-21 08:29:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-05-21 08:29:47 +0000 UTC,LastTransitionTime:2021-05-21 08:29:40 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 21 08:29:47.762: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-8193  67758d63-bb68-4ca0-a61b-cba8e9c83b42 29800 1 2021-05-21 08:29:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9949093a-9775-43ce-bddd-9e1f5e696196 0xc000d6ce20 0xc000d6ce21}] []  [{kube-controller-manager Update apps/v1 2021-05-21 08:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9949093a-9775-43ce-bddd-9e1f5e696196\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d6ce98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 08:29:47.762: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 21 08:29:47.762: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c9d49486  deployment-8193  d35e4f74-2417-4050-8aba-a3881f413169 29788 2 2021-05-21 08:29:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c9d49486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9949093a-9775-43ce-bddd-9e1f5e696196 0xc000d6cd1f 0xc000d6cd30}] []  [{kube-controller-manager Update apps/v1 2021-05-21 08:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9949093a-9775-43ce-bddd-9e1f5e696196\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9d49486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c9d49486] map[] [] []  []} {[] [] [{agnhost registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d6cda8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 08:29:47.770: INFO: Pod "test-recreate-deployment-f79dd4667-lmj9j" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-lmj9j test-recreate-deployment-f79dd4667- deployment-8193  bf3a8db5-14b8-4cdc-b010-d84561a9ac3a 29804 0 2021-05-21 08:29:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 67758d63-bb68-4ca0-a61b-cba8e9c83b42 0xc0009d6960 0xc0009d6961}] []  [{kube-controller-manager Update v1 2021-05-21 08:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67758d63-bb68-4ca0-a61b-cba8e9c83b42\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-46glg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-46glg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-46glg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:29:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:29:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:29:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:29:47.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8193" for this suite.

• [SLOW TEST:7.753 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":45,"skipped":836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:29:47.838: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:29:48.178: INFO: Create a RollingUpdate DaemonSet
May 21 08:29:48.195: INFO: Check that daemon pods launch on every node of the cluster
May 21 08:29:48.272: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 08:29:48.346: INFO: Number of nodes with available pods: 0
May 21 08:29:48.346: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:29:49.354: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 08:29:49.358: INFO: Number of nodes with available pods: 0
May 21 08:29:49.358: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:29:50.404: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 08:29:50.408: INFO: Number of nodes with available pods: 0
May 21 08:29:50.408: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:29:51.365: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 08:29:51.371: INFO: Number of nodes with available pods: 0
May 21 08:29:51.371: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:29:52.367: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 08:29:52.373: INFO: Number of nodes with available pods: 2
May 21 08:29:52.373: INFO: Number of running nodes: 2, number of available pods: 2
May 21 08:29:52.373: INFO: Update the DaemonSet to trigger a rollout
May 21 08:29:52.384: INFO: Updating DaemonSet daemon-set
May 21 08:29:56.443: INFO: Roll back the DaemonSet before rollout is complete
May 21 08:29:56.454: INFO: Updating DaemonSet daemon-set
May 21 08:29:56.454: INFO: Make sure DaemonSet rollback is complete
May 21 08:29:56.462: INFO: Wrong image for pod: daemon-set-5vznk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 08:29:56.462: INFO: Pod daemon-set-5vznk is not available
May 21 08:29:56.496: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 08:29:57.506: INFO: Wrong image for pod: daemon-set-5vznk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 08:29:57.507: INFO: Pod daemon-set-5vznk is not available
May 21 08:29:57.514: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 08:29:58.620: INFO: Wrong image for pod: daemon-set-5vznk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 08:29:58.620: INFO: Pod daemon-set-5vznk is not available
May 21 08:29:58.638: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 08:29:59.503: INFO: Pod daemon-set-mr4n9 is not available
May 21 08:29:59.515: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7800, will wait for the garbage collector to delete the pods
May 21 08:29:59.602: INFO: Deleting DaemonSet.extensions daemon-set took: 18.212665ms
May 21 08:30:00.503: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.287643ms
May 21 08:30:10.412: INFO: Number of nodes with available pods: 0
May 21 08:30:10.412: INFO: Number of running nodes: 0, number of available pods: 0
May 21 08:30:10.420: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30019"},"items":null}

May 21 08:30:10.461: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30019"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:30:10.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7800" for this suite.

• [SLOW TEST:22.651 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":46,"skipped":881,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:30:10.489: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 21 08:30:10.709: INFO: observed Pod pod-test in namespace pods-3442 in phase Pending conditions []
May 21 08:30:10.713: INFO: observed Pod pod-test in namespace pods-3442 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 08:30:10 +0000 UTC  }]
May 21 08:30:10.765: INFO: observed Pod pod-test in namespace pods-3442 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 08:30:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 08:30:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 08:30:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 08:30:10 +0000 UTC  }]
May 21 08:30:11.403: INFO: observed Pod pod-test in namespace pods-3442 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 08:30:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 08:30:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 08:30:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 08:30:10 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 21 08:30:13.996: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 21 08:30:14.182: INFO: observed event type ADDED
May 21 08:30:14.182: INFO: observed event type MODIFIED
May 21 08:30:14.182: INFO: observed event type MODIFIED
May 21 08:30:14.182: INFO: observed event type MODIFIED
May 21 08:30:14.183: INFO: observed event type MODIFIED
May 21 08:30:14.183: INFO: observed event type MODIFIED
May 21 08:30:14.183: INFO: observed event type MODIFIED
May 21 08:30:14.183: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:30:14.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3442" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":47,"skipped":884,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:30:14.203: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 21 08:30:18.530: INFO: &Pod{ObjectMeta:{send-events-276e9be4-435a-435a-b615-3d6de32d5288  events-6913  0a952f57-088d-4a7e-afb3-51669ea83c1a 30114 0 2021-05-21 08:30:14 +0000 UTC <nil> <nil> map[name:foo time:482001046] map[cni.projectcalico.org/podIP:10.100.165.211/32 cni.projectcalico.org/podIPs:10.100.165.211/32] [] []  [{e2e.test Update v1 2021-05-21 08:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 08:30:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 08:30:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.165.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx4ln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx4ln,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx4ln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:30:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:30:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:30:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:30:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:10.100.165.211,StartTime:2021-05-21 08:30:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 08:30:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21,ImageID:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost@sha256:e23f72f469927ead04b7bb2a5cedf8569906c38ced37a64767542e7d22831d45,ContainerID:containerd://517ffc26ae3b2974b76e913b3d35e148e7db59c94be9552c9e4655e65298db79,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.165.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 21 08:30:20.541: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 21 08:30:22.553: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:30:22.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6913" for this suite.

• [SLOW TEST:8.480 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":48,"skipped":892,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:30:22.683: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 08:30:22.885: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b79da951-0f37-4818-80e4-3ea8a8336aff" in namespace "downward-api-67" to be "Succeeded or Failed"
May 21 08:30:22.912: INFO: Pod "downwardapi-volume-b79da951-0f37-4818-80e4-3ea8a8336aff": Phase="Pending", Reason="", readiness=false. Elapsed: 26.582679ms
May 21 08:30:24.918: INFO: Pod "downwardapi-volume-b79da951-0f37-4818-80e4-3ea8a8336aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033076406s
May 21 08:30:26.925: INFO: Pod "downwardapi-volume-b79da951-0f37-4818-80e4-3ea8a8336aff": Phase="Running", Reason="", readiness=true. Elapsed: 4.039885919s
May 21 08:30:28.934: INFO: Pod "downwardapi-volume-b79da951-0f37-4818-80e4-3ea8a8336aff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049507503s
STEP: Saw pod success
May 21 08:30:28.935: INFO: Pod "downwardapi-volume-b79da951-0f37-4818-80e4-3ea8a8336aff" satisfied condition "Succeeded or Failed"
May 21 08:30:28.939: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-b79da951-0f37-4818-80e4-3ea8a8336aff container client-container: <nil>
STEP: delete the pod
May 21 08:30:29.240: INFO: Waiting for pod downwardapi-volume-b79da951-0f37-4818-80e4-3ea8a8336aff to disappear
May 21 08:30:29.287: INFO: Pod downwardapi-volume-b79da951-0f37-4818-80e4-3ea8a8336aff no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:30:29.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-67" for this suite.

• [SLOW TEST:6.620 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":49,"skipped":911,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:30:29.304: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
May 21 08:30:30.363: INFO: created pod pod-service-account-defaultsa
May 21 08:30:30.363: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 21 08:30:30.501: INFO: created pod pod-service-account-mountsa
May 21 08:30:30.501: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 21 08:30:30.549: INFO: created pod pod-service-account-nomountsa
May 21 08:30:30.549: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 21 08:30:30.759: INFO: created pod pod-service-account-defaultsa-mountspec
May 21 08:30:30.759: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 21 08:30:31.105: INFO: created pod pod-service-account-mountsa-mountspec
May 21 08:30:31.105: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 21 08:30:31.159: INFO: created pod pod-service-account-nomountsa-mountspec
May 21 08:30:31.159: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 21 08:30:31.281: INFO: created pod pod-service-account-defaultsa-nomountspec
May 21 08:30:31.281: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 21 08:30:31.814: INFO: created pod pod-service-account-mountsa-nomountspec
May 21 08:30:31.814: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 21 08:30:31.856: INFO: created pod pod-service-account-nomountsa-nomountspec
May 21 08:30:31.856: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:30:31.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5212" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":50,"skipped":938,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:30:32.806: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8890
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8890
STEP: creating replication controller externalsvc in namespace services-8890
I0521 08:30:33.652356      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8890, replica count: 2
I0521 08:30:36.702920      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:30:39.703318      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:30:42.703677      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:30:45.704060      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:30:48.704298      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:30:51.704699      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 21 08:30:52.120: INFO: Creating new exec pod
May 21 08:30:56.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-8890 exec execpodswfpd -- /bin/sh -x -c nslookup clusterip-service.services-8890.svc.cluster.local'
May 21 08:30:56.719: INFO: stderr: "+ nslookup clusterip-service.services-8890.svc.cluster.local\n"
May 21 08:30:56.719: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-8890.svc.cluster.local\tcanonical name = externalsvc.services-8890.svc.cluster.local.\nName:\texternalsvc.services-8890.svc.cluster.local\nAddress: 10.96.173.159\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8890, will wait for the garbage collector to delete the pods
May 21 08:30:56.840: INFO: Deleting ReplicationController externalsvc took: 64.770284ms
May 21 08:30:57.540: INFO: Terminating ReplicationController externalsvc pods took: 700.498582ms
May 21 08:31:10.283: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:31:10.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8890" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:37.646 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":51,"skipped":956,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:31:10.453: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:31:10.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8187" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":52,"skipped":966,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:31:10.688: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 08:31:12.310: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 08:31:14.579: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182672, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182672, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182672, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182672, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 08:31:17.794: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:31:17.801: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:31:19.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6175" for this suite.
STEP: Destroying namespace "webhook-6175-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.928 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":53,"skipped":980,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:31:19.617: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 08:31:19.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3e64476-b9f3-4a69-b90f-f04665ac51ea" in namespace "projected-8548" to be "Succeeded or Failed"
May 21 08:31:19.785: INFO: Pod "downwardapi-volume-f3e64476-b9f3-4a69-b90f-f04665ac51ea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.85363ms
May 21 08:31:21.792: INFO: Pod "downwardapi-volume-f3e64476-b9f3-4a69-b90f-f04665ac51ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01852767s
May 21 08:31:23.816: INFO: Pod "downwardapi-volume-f3e64476-b9f3-4a69-b90f-f04665ac51ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042299568s
STEP: Saw pod success
May 21 08:31:23.816: INFO: Pod "downwardapi-volume-f3e64476-b9f3-4a69-b90f-f04665ac51ea" satisfied condition "Succeeded or Failed"
May 21 08:31:23.821: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-f3e64476-b9f3-4a69-b90f-f04665ac51ea container client-container: <nil>
STEP: delete the pod
May 21 08:31:23.952: INFO: Waiting for pod downwardapi-volume-f3e64476-b9f3-4a69-b90f-f04665ac51ea to disappear
May 21 08:31:23.964: INFO: Pod downwardapi-volume-f3e64476-b9f3-4a69-b90f-f04665ac51ea no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:31:23.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8548" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:31:23.987: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:31:24.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4619" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":55,"skipped":1064,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:31:24.494: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-9ae89ee1-3bce-4510-a602-1f60c4d1f968
STEP: Creating a pod to test consume secrets
May 21 08:31:24.817: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9ce355b4-714c-40b0-9836-091461771ec2" in namespace "projected-7733" to be "Succeeded or Failed"
May 21 08:31:24.823: INFO: Pod "pod-projected-secrets-9ce355b4-714c-40b0-9836-091461771ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.981407ms
May 21 08:31:26.964: INFO: Pod "pod-projected-secrets-9ce355b4-714c-40b0-9836-091461771ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.146858029s
May 21 08:31:28.969: INFO: Pod "pod-projected-secrets-9ce355b4-714c-40b0-9836-091461771ec2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.151938038s
STEP: Saw pod success
May 21 08:31:28.969: INFO: Pod "pod-projected-secrets-9ce355b4-714c-40b0-9836-091461771ec2" satisfied condition "Succeeded or Failed"
May 21 08:31:28.973: INFO: Trying to get logs from node k8s-03 pod pod-projected-secrets-9ce355b4-714c-40b0-9836-091461771ec2 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 08:31:29.103: INFO: Waiting for pod pod-projected-secrets-9ce355b4-714c-40b0-9836-091461771ec2 to disappear
May 21 08:31:29.112: INFO: Pod pod-projected-secrets-9ce355b4-714c-40b0-9836-091461771ec2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:31:29.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7733" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":1065,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:31:29.132: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 08:31:33.497: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:31:33.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-747" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":57,"skipped":1091,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:31:33.813: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:31:50.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-277" for this suite.

• [SLOW TEST:16.445 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":58,"skipped":1093,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:31:50.258: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-35e30019-d92c-46c0-b029-c175e55b529e
STEP: Creating a pod to test consume configMaps
May 21 08:31:50.482: INFO: Waiting up to 5m0s for pod "pod-configmaps-dbeb025d-6343-4c3c-877c-9c00d138b16e" in namespace "configmap-2834" to be "Succeeded or Failed"
May 21 08:31:50.489: INFO: Pod "pod-configmaps-dbeb025d-6343-4c3c-877c-9c00d138b16e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.726149ms
May 21 08:31:52.497: INFO: Pod "pod-configmaps-dbeb025d-6343-4c3c-877c-9c00d138b16e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014468231s
May 21 08:31:54.508: INFO: Pod "pod-configmaps-dbeb025d-6343-4c3c-877c-9c00d138b16e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025445156s
STEP: Saw pod success
May 21 08:31:54.508: INFO: Pod "pod-configmaps-dbeb025d-6343-4c3c-877c-9c00d138b16e" satisfied condition "Succeeded or Failed"
May 21 08:31:54.520: INFO: Trying to get logs from node k8s-03 pod pod-configmaps-dbeb025d-6343-4c3c-877c-9c00d138b16e container agnhost-container: <nil>
STEP: delete the pod
May 21 08:31:54.647: INFO: Waiting for pod pod-configmaps-dbeb025d-6343-4c3c-877c-9c00d138b16e to disappear
May 21 08:31:54.656: INFO: Pod pod-configmaps-dbeb025d-6343-4c3c-877c-9c00d138b16e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:31:54.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2834" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":1099,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:31:54.793: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:31:55.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-339" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":60,"skipped":1104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:31:55.156: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3040
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 08:31:55.280: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 08:31:55.439: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 08:31:57.445: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 08:31:59.446: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 08:32:01.452: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:32:03.449: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:32:05.447: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:32:07.456: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:32:09.445: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:32:11.452: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:32:13.457: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:32:15.446: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:32:17.451: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:32:19.449: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 08:32:19.453: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 21 08:32:23.522: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 21 08:32:23.522: INFO: Breadth first check of 10.100.179.30 on host 8.16.0.94...
May 21 08:32:23.527: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.165.226:9080/dial?request=hostname&protocol=udp&host=10.100.179.30&port=8081&tries=1'] Namespace:pod-network-test-3040 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 08:32:23.527: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:32:23.698: INFO: Waiting for responses: map[]
May 21 08:32:23.698: INFO: reached 10.100.179.30 after 0/1 tries
May 21 08:32:23.698: INFO: Breadth first check of 10.100.165.225 on host 8.16.0.95...
May 21 08:32:23.704: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.165.226:9080/dial?request=hostname&protocol=udp&host=10.100.165.225&port=8081&tries=1'] Namespace:pod-network-test-3040 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 08:32:23.705: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:32:23.885: INFO: Waiting for responses: map[]
May 21 08:32:23.886: INFO: reached 10.100.165.225 after 0/1 tries
May 21 08:32:23.886: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:32:23.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3040" for this suite.

• [SLOW TEST:28.777 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":61,"skipped":1127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:32:23.935: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 21 08:32:25.446: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 21 08:32:27.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182745, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182745, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182745, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182745, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5df4db4495\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:32:29.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182745, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182745, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182745, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757182745, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5df4db4495\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 08:32:32.586: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:32:32.592: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:32:33.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2447" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.294 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":62,"skipped":1163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:32:34.231: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 21 08:32:34.321: INFO: Waiting up to 5m0s for pod "downward-api-5bc05158-34cc-4c13-8f7a-32ffb7497b2c" in namespace "downward-api-7277" to be "Succeeded or Failed"
May 21 08:32:34.384: INFO: Pod "downward-api-5bc05158-34cc-4c13-8f7a-32ffb7497b2c": Phase="Pending", Reason="", readiness=false. Elapsed: 62.838963ms
May 21 08:32:36.391: INFO: Pod "downward-api-5bc05158-34cc-4c13-8f7a-32ffb7497b2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069908295s
May 21 08:32:38.394: INFO: Pod "downward-api-5bc05158-34cc-4c13-8f7a-32ffb7497b2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072769348s
STEP: Saw pod success
May 21 08:32:38.394: INFO: Pod "downward-api-5bc05158-34cc-4c13-8f7a-32ffb7497b2c" satisfied condition "Succeeded or Failed"
May 21 08:32:38.396: INFO: Trying to get logs from node k8s-02 pod downward-api-5bc05158-34cc-4c13-8f7a-32ffb7497b2c container dapi-container: <nil>
STEP: delete the pod
May 21 08:32:38.460: INFO: Waiting for pod downward-api-5bc05158-34cc-4c13-8f7a-32ffb7497b2c to disappear
May 21 08:32:38.481: INFO: Pod downward-api-5bc05158-34cc-4c13-8f7a-32ffb7497b2c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:32:38.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7277" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":1264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:32:38.490: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 21 08:32:38.749: INFO: Waiting up to 5m0s for pod "pod-8107ac7e-c09e-47c2-a627-2ba48944faea" in namespace "emptydir-509" to be "Succeeded or Failed"
May 21 08:32:38.760: INFO: Pod "pod-8107ac7e-c09e-47c2-a627-2ba48944faea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.559187ms
May 21 08:32:40.768: INFO: Pod "pod-8107ac7e-c09e-47c2-a627-2ba48944faea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018219931s
May 21 08:32:42.774: INFO: Pod "pod-8107ac7e-c09e-47c2-a627-2ba48944faea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02488048s
STEP: Saw pod success
May 21 08:32:42.774: INFO: Pod "pod-8107ac7e-c09e-47c2-a627-2ba48944faea" satisfied condition "Succeeded or Failed"
May 21 08:32:42.778: INFO: Trying to get logs from node k8s-03 pod pod-8107ac7e-c09e-47c2-a627-2ba48944faea container test-container: <nil>
STEP: delete the pod
May 21 08:32:42.831: INFO: Waiting for pod pod-8107ac7e-c09e-47c2-a627-2ba48944faea to disappear
May 21 08:32:42.841: INFO: Pod pod-8107ac7e-c09e-47c2-a627-2ba48944faea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:32:42.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-509" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":64,"skipped":1304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:32:42.854: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 21 08:32:43.171: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3584  c008584f-48c8-4626-beb6-b32930182a70 31316 0 2021-05-21 08:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 08:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 08:32:43.172: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3584  c008584f-48c8-4626-beb6-b32930182a70 31317 0 2021-05-21 08:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 08:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 21 08:32:43.220: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3584  c008584f-48c8-4626-beb6-b32930182a70 31318 0 2021-05-21 08:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 08:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 08:32:43.220: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3584  c008584f-48c8-4626-beb6-b32930182a70 31319 0 2021-05-21 08:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 08:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:32:43.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3584" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":65,"skipped":1329,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:32:43.306: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:32:43.480: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:32:44.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1946" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":66,"skipped":1333,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:32:44.205: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
May 21 08:32:44.367: INFO: created test-pod-1
May 21 08:32:44.377: INFO: created test-pod-2
May 21 08:32:44.436: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:32:44.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6962" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":67,"skipped":1342,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:32:44.972: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 21 08:32:45.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-796 create -f -'
May 21 08:32:46.537: INFO: stderr: ""
May 21 08:32:46.537: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 08:32:47.598: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:32:47.598: INFO: Found 0 / 1
May 21 08:32:48.581: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:32:48.581: INFO: Found 0 / 1
May 21 08:32:49.560: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:32:49.560: INFO: Found 0 / 1
May 21 08:32:50.570: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:32:50.570: INFO: Found 1 / 1
May 21 08:32:50.570: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 21 08:32:50.574: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:32:50.574: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 08:32:50.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-796 patch pod agnhost-primary-25rgw -p {"metadata":{"annotations":{"x":"y"}}}'
May 21 08:32:50.754: INFO: stderr: ""
May 21 08:32:50.754: INFO: stdout: "pod/agnhost-primary-25rgw patched\n"
STEP: checking annotations
May 21 08:32:50.838: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:32:50.838: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:32:50.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-796" for this suite.

• [SLOW TEST:5.879 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1466
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":68,"skipped":1346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:32:50.851: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-cd51ea2d-e1f5-4f87-982a-f371c68b8992
STEP: Creating a pod to test consume secrets
May 21 08:32:51.029: INFO: Waiting up to 5m0s for pod "pod-secrets-9db83459-6a5c-47ff-88cb-345b539dc475" in namespace "secrets-4975" to be "Succeeded or Failed"
May 21 08:32:51.176: INFO: Pod "pod-secrets-9db83459-6a5c-47ff-88cb-345b539dc475": Phase="Pending", Reason="", readiness=false. Elapsed: 147.39214ms
May 21 08:32:53.185: INFO: Pod "pod-secrets-9db83459-6a5c-47ff-88cb-345b539dc475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.156224056s
May 21 08:32:55.193: INFO: Pod "pod-secrets-9db83459-6a5c-47ff-88cb-345b539dc475": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.163961242s
STEP: Saw pod success
May 21 08:32:55.193: INFO: Pod "pod-secrets-9db83459-6a5c-47ff-88cb-345b539dc475" satisfied condition "Succeeded or Failed"
May 21 08:32:55.197: INFO: Trying to get logs from node k8s-03 pod pod-secrets-9db83459-6a5c-47ff-88cb-345b539dc475 container secret-volume-test: <nil>
STEP: delete the pod
May 21 08:32:55.335: INFO: Waiting for pod pod-secrets-9db83459-6a5c-47ff-88cb-345b539dc475 to disappear
May 21 08:32:55.339: INFO: Pod pod-secrets-9db83459-6a5c-47ff-88cb-345b539dc475 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:32:55.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4975" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":1370,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:32:55.353: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0521 08:33:05.542689      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 08:34:07.569: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:34:07.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8766" for this suite.

• [SLOW TEST:72.245 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":70,"skipped":1377,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:34:07.598: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 08:34:07.806: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2fc0b5ad-d741-4b04-a35d-c10b096cd6bc" in namespace "projected-4727" to be "Succeeded or Failed"
May 21 08:34:07.871: INFO: Pod "downwardapi-volume-2fc0b5ad-d741-4b04-a35d-c10b096cd6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 65.040996ms
May 21 08:34:09.883: INFO: Pod "downwardapi-volume-2fc0b5ad-d741-4b04-a35d-c10b096cd6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07675767s
May 21 08:34:11.930: INFO: Pod "downwardapi-volume-2fc0b5ad-d741-4b04-a35d-c10b096cd6bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.123567354s
STEP: Saw pod success
May 21 08:34:11.930: INFO: Pod "downwardapi-volume-2fc0b5ad-d741-4b04-a35d-c10b096cd6bc" satisfied condition "Succeeded or Failed"
May 21 08:34:11.938: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-2fc0b5ad-d741-4b04-a35d-c10b096cd6bc container client-container: <nil>
STEP: delete the pod
May 21 08:34:12.040: INFO: Waiting for pod downwardapi-volume-2fc0b5ad-d741-4b04-a35d-c10b096cd6bc to disappear
May 21 08:34:12.067: INFO: Pod downwardapi-volume-2fc0b5ad-d741-4b04-a35d-c10b096cd6bc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:34:12.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4727" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":71,"skipped":1379,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:34:12.081: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-z5hh
STEP: Creating a pod to test atomic-volume-subpath
May 21 08:34:12.346: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-z5hh" in namespace "subpath-5744" to be "Succeeded or Failed"
May 21 08:34:12.359: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Pending", Reason="", readiness=false. Elapsed: 13.051576ms
May 21 08:34:14.365: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019554782s
May 21 08:34:16.383: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Running", Reason="", readiness=true. Elapsed: 4.036663904s
May 21 08:34:18.390: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Running", Reason="", readiness=true. Elapsed: 6.044455979s
May 21 08:34:20.397: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Running", Reason="", readiness=true. Elapsed: 8.050968582s
May 21 08:34:22.404: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Running", Reason="", readiness=true. Elapsed: 10.058020731s
May 21 08:34:24.411: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Running", Reason="", readiness=true. Elapsed: 12.06496674s
May 21 08:34:26.541: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Running", Reason="", readiness=true. Elapsed: 14.19544792s
May 21 08:34:28.549: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Running", Reason="", readiness=true. Elapsed: 16.203180827s
May 21 08:34:30.557: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Running", Reason="", readiness=true. Elapsed: 18.210671557s
May 21 08:34:32.564: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Running", Reason="", readiness=true. Elapsed: 20.218272186s
May 21 08:34:34.572: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Running", Reason="", readiness=true. Elapsed: 22.226448085s
May 21 08:34:36.623: INFO: Pod "pod-subpath-test-downwardapi-z5hh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.276940851s
STEP: Saw pod success
May 21 08:34:36.623: INFO: Pod "pod-subpath-test-downwardapi-z5hh" satisfied condition "Succeeded or Failed"
May 21 08:34:36.627: INFO: Trying to get logs from node k8s-03 pod pod-subpath-test-downwardapi-z5hh container test-container-subpath-downwardapi-z5hh: <nil>
STEP: delete the pod
May 21 08:34:36.707: INFO: Waiting for pod pod-subpath-test-downwardapi-z5hh to disappear
May 21 08:34:36.746: INFO: Pod pod-subpath-test-downwardapi-z5hh no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-z5hh
May 21 08:34:36.746: INFO: Deleting pod "pod-subpath-test-downwardapi-z5hh" in namespace "subpath-5744"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:34:36.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5744" for this suite.

• [SLOW TEST:24.682 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":72,"skipped":1400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:34:36.763: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 21 08:34:41.788: INFO: Successfully updated pod "annotationupdatec1958fe7-802e-4087-8717-33e0dbf71b35"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:34:45.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4922" for this suite.

• [SLOW TEST:9.083 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":73,"skipped":1449,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:34:45.847: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
May 21 08:36:46.768: INFO: Successfully updated pod "var-expansion-788a24b5-956f-47ab-b85b-62fab1dd7cfe"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 21 08:36:48.820: INFO: Deleting pod "var-expansion-788a24b5-956f-47ab-b85b-62fab1dd7cfe" in namespace "var-expansion-8164"
May 21 08:36:48.831: INFO: Wait up to 5m0s for pod "var-expansion-788a24b5-956f-47ab-b85b-62fab1dd7cfe" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:37:22.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8164" for this suite.

• [SLOW TEST:157.023 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":74,"skipped":1462,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:37:22.871: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7742
STEP: creating service affinity-clusterip-transition in namespace services-7742
STEP: creating replication controller affinity-clusterip-transition in namespace services-7742
I0521 08:37:23.048108      25 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-7742, replica count: 3
I0521 08:37:26.098416      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:37:29.098805      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 08:37:29.111: INFO: Creating new exec pod
May 21 08:37:34.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-7742 exec execpod-affinitygczw9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
May 21 08:37:36.741: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 21 08:37:36.741: INFO: stdout: ""
May 21 08:37:36.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-7742 exec execpod-affinitygczw9 -- /bin/sh -x -c nc -zv -t -w 2 10.96.61.135 80'
May 21 08:37:37.097: INFO: stderr: "+ nc -zv -t -w 2 10.96.61.135 80\nConnection to 10.96.61.135 80 port [tcp/http] succeeded!\n"
May 21 08:37:37.097: INFO: stdout: ""
May 21 08:37:37.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-7742 exec execpod-affinitygczw9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.61.135:80/ ; done'
May 21 08:37:37.607: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n"
May 21 08:37:37.607: INFO: stdout: "\naffinity-clusterip-transition-nrqkp\naffinity-clusterip-transition-mvlhj\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-nrqkp\naffinity-clusterip-transition-nrqkp\naffinity-clusterip-transition-mvlhj\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-mvlhj\naffinity-clusterip-transition-mvlhj\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-nrqkp\naffinity-clusterip-transition-mvlhj\naffinity-clusterip-transition-mvlhj"
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-nrqkp
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-mvlhj
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-nrqkp
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-nrqkp
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-mvlhj
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-mvlhj
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-mvlhj
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-nrqkp
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-mvlhj
May 21 08:37:37.607: INFO: Received response from host: affinity-clusterip-transition-mvlhj
May 21 08:37:37.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-7742 exec execpod-affinitygczw9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.61.135:80/ ; done'
May 21 08:37:38.159: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.61.135:80/\n"
May 21 08:37:38.159: INFO: stdout: "\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4\naffinity-clusterip-transition-8zzs4"
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Received response from host: affinity-clusterip-transition-8zzs4
May 21 08:37:38.159: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7742, will wait for the garbage collector to delete the pods
May 21 08:37:38.568: INFO: Deleting ReplicationController affinity-clusterip-transition took: 121.525725ms
May 21 08:37:39.268: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 700.308923ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:37:50.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7742" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:27.882 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":75,"skipped":1464,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:37:50.753: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 21 08:37:55.551: INFO: Successfully updated pod "annotationupdate5d32e2c4-ec45-444e-a219-eed0f669e6db"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:37:59.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4656" for this suite.

• [SLOW TEST:8.962 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1477,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:37:59.715: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 21 08:37:59.841: INFO: Waiting up to 5m0s for pod "pod-74a4879a-be06-46a7-b7ca-ed196fc7c99d" in namespace "emptydir-9779" to be "Succeeded or Failed"
May 21 08:37:59.949: INFO: Pod "pod-74a4879a-be06-46a7-b7ca-ed196fc7c99d": Phase="Pending", Reason="", readiness=false. Elapsed: 107.127396ms
May 21 08:38:01.967: INFO: Pod "pod-74a4879a-be06-46a7-b7ca-ed196fc7c99d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.125828648s
May 21 08:38:03.975: INFO: Pod "pod-74a4879a-be06-46a7-b7ca-ed196fc7c99d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.13331288s
STEP: Saw pod success
May 21 08:38:03.975: INFO: Pod "pod-74a4879a-be06-46a7-b7ca-ed196fc7c99d" satisfied condition "Succeeded or Failed"
May 21 08:38:03.979: INFO: Trying to get logs from node k8s-03 pod pod-74a4879a-be06-46a7-b7ca-ed196fc7c99d container test-container: <nil>
STEP: delete the pod
May 21 08:38:04.008: INFO: Waiting for pod pod-74a4879a-be06-46a7-b7ca-ed196fc7c99d to disappear
May 21 08:38:04.036: INFO: Pod pod-74a4879a-be06-46a7-b7ca-ed196fc7c99d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:38:04.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9779" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:38:04.057: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:38:04.303: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"35563e96-52af-49a5-9a61-4376e29090da", Controller:(*bool)(0xc001a26a8e), BlockOwnerDeletion:(*bool)(0xc001a26a8f)}}
May 21 08:38:04.339: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"09fa83ea-75bf-43d3-b800-375fcafc2b85", Controller:(*bool)(0xc003d91e86), BlockOwnerDeletion:(*bool)(0xc003d91e87)}}
May 21 08:38:04.373: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4c18b456-1f55-45f8-a086-8430c5bf5ce9", Controller:(*bool)(0xc001a26c96), BlockOwnerDeletion:(*bool)(0xc001a26c97)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:38:09.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7252" for this suite.

• [SLOW TEST:5.416 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":78,"skipped":1519,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:38:09.473: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-301d8b40-94c6-4ce2-a8a2-eb5153d79f85
STEP: Creating a pod to test consume secrets
May 21 08:38:09.913: INFO: Waiting up to 5m0s for pod "pod-secrets-5294954c-ee14-4ee6-8b9b-30cbdf19e611" in namespace "secrets-963" to be "Succeeded or Failed"
May 21 08:38:09.922: INFO: Pod "pod-secrets-5294954c-ee14-4ee6-8b9b-30cbdf19e611": Phase="Pending", Reason="", readiness=false. Elapsed: 9.12613ms
May 21 08:38:11.935: INFO: Pod "pod-secrets-5294954c-ee14-4ee6-8b9b-30cbdf19e611": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022488172s
May 21 08:38:13.943: INFO: Pod "pod-secrets-5294954c-ee14-4ee6-8b9b-30cbdf19e611": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030379246s
STEP: Saw pod success
May 21 08:38:13.943: INFO: Pod "pod-secrets-5294954c-ee14-4ee6-8b9b-30cbdf19e611" satisfied condition "Succeeded or Failed"
May 21 08:38:13.948: INFO: Trying to get logs from node k8s-03 pod pod-secrets-5294954c-ee14-4ee6-8b9b-30cbdf19e611 container secret-volume-test: <nil>
STEP: delete the pod
May 21 08:38:14.033: INFO: Waiting for pod pod-secrets-5294954c-ee14-4ee6-8b9b-30cbdf19e611 to disappear
May 21 08:38:14.041: INFO: Pod pod-secrets-5294954c-ee14-4ee6-8b9b-30cbdf19e611 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:38:14.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-963" for this suite.
STEP: Destroying namespace "secret-namespace-9425" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":79,"skipped":1536,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:38:14.070: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5621
May 21 08:38:18.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-5621 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 21 08:38:18.606: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 21 08:38:18.606: INFO: stdout: "iptables"
May 21 08:38:18.606: INFO: proxyMode: iptables
May 21 08:38:18.661: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 08:38:18.673: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-5621
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5621
I0521 08:38:18.786362      25 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5621, replica count: 3
I0521 08:38:21.837089      25 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:38:24.837561      25 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 08:38:24.850: INFO: Creating new exec pod
May 21 08:38:30.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-5621 exec execpod-affinitygx6pk -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
May 21 08:38:30.408: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 21 08:38:30.408: INFO: stdout: ""
May 21 08:38:30.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-5621 exec execpod-affinitygx6pk -- /bin/sh -x -c nc -zv -t -w 2 10.96.254.158 80'
May 21 08:38:30.802: INFO: stderr: "+ nc -zv -t -w 2 10.96.254.158 80\nConnection to 10.96.254.158 80 port [tcp/http] succeeded!\n"
May 21 08:38:30.802: INFO: stdout: ""
May 21 08:38:30.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-5621 exec execpod-affinitygx6pk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.254.158:80/ ; done'
May 21 08:38:31.315: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n"
May 21 08:38:31.315: INFO: stdout: "\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d\naffinity-clusterip-timeout-8kg4d"
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Received response from host: affinity-clusterip-timeout-8kg4d
May 21 08:38:31.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-5621 exec execpod-affinitygx6pk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.254.158:80/'
May 21 08:38:31.658: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n"
May 21 08:38:31.658: INFO: stdout: "affinity-clusterip-timeout-8kg4d"
May 21 08:38:51.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-5621 exec execpod-affinitygx6pk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.254.158:80/'
May 21 08:38:51.950: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n"
May 21 08:38:51.950: INFO: stdout: "affinity-clusterip-timeout-8kg4d"
May 21 08:39:11.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-5621 exec execpod-affinitygx6pk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.254.158:80/'
May 21 08:39:12.313: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.254.158:80/\n"
May 21 08:39:12.313: INFO: stdout: "affinity-clusterip-timeout-jvpsn"
May 21 08:39:12.313: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5621, will wait for the garbage collector to delete the pods
May 21 08:39:12.453: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 22.399979ms
May 21 08:39:13.053: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.454244ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:39:20.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5621" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:66.523 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":80,"skipped":1547,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:39:20.594: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4359
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4359
I0521 08:39:20.832210      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4359, replica count: 2
I0521 08:39:23.882909      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 08:39:26.883: INFO: Creating new exec pod
I0521 08:39:26.883210      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 08:39:31.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-4359 exec execpodlncfj -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 21 08:39:32.425: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 21 08:39:32.425: INFO: stdout: ""
May 21 08:39:32.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-4359 exec execpodlncfj -- /bin/sh -x -c nc -zv -t -w 2 10.96.61.23 80'
May 21 08:39:32.760: INFO: stderr: "+ nc -zv -t -w 2 10.96.61.23 80\nConnection to 10.96.61.23 80 port [tcp/http] succeeded!\n"
May 21 08:39:32.760: INFO: stdout: ""
May 21 08:39:32.760: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:39:32.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4359" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:12.293 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":81,"skipped":1563,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:39:32.888: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
May 21 08:39:33.031: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-546 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:39:33.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-546" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":82,"skipped":1579,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:39:33.197: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:39:46.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8873" for this suite.

• [SLOW TEST:13.639 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":83,"skipped":1592,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:39:46.837: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:39:47.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5470" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":84,"skipped":1599,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:39:47.186: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:39:51.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4452" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:39:51.622: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-408dfc7f-cbf9-42e4-89e8-0086dc5936db
May 21 08:39:51.811: INFO: Pod name my-hostname-basic-408dfc7f-cbf9-42e4-89e8-0086dc5936db: Found 0 pods out of 1
May 21 08:39:56.845: INFO: Pod name my-hostname-basic-408dfc7f-cbf9-42e4-89e8-0086dc5936db: Found 1 pods out of 1
May 21 08:39:56.845: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-408dfc7f-cbf9-42e4-89e8-0086dc5936db" are running
May 21 08:39:56.893: INFO: Pod "my-hostname-basic-408dfc7f-cbf9-42e4-89e8-0086dc5936db-nb6jm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 08:39:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 08:39:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 08:39:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 08:39:51 +0000 UTC Reason: Message:}])
May 21 08:39:56.894: INFO: Trying to dial the pod
May 21 08:40:01.907: INFO: Controller my-hostname-basic-408dfc7f-cbf9-42e4-89e8-0086dc5936db: Got expected result from replica 1 [my-hostname-basic-408dfc7f-cbf9-42e4-89e8-0086dc5936db-nb6jm]: "my-hostname-basic-408dfc7f-cbf9-42e4-89e8-0086dc5936db-nb6jm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:40:01.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7608" for this suite.

• [SLOW TEST:10.297 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":86,"skipped":1641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:40:01.921: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 08:40:03.276: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 08:40:05.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183203, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183203, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183203, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183203, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 08:40:08.372: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:40:08.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2600" for this suite.
STEP: Destroying namespace "webhook-2600-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.105 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":87,"skipped":1713,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:40:09.025: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 21 08:40:14.632: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:40:15.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1804" for this suite.

• [SLOW TEST:6.683 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":88,"skipped":1719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:40:15.709: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-8104/configmap-test-7e1cb594-1ac6-4c52-b57d-de66e620decd
STEP: Creating a pod to test consume configMaps
May 21 08:40:15.880: INFO: Waiting up to 5m0s for pod "pod-configmaps-8239ceeb-d218-4dde-af58-923968e3c307" in namespace "configmap-8104" to be "Succeeded or Failed"
May 21 08:40:15.884: INFO: Pod "pod-configmaps-8239ceeb-d218-4dde-af58-923968e3c307": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2618ms
May 21 08:40:17.891: INFO: Pod "pod-configmaps-8239ceeb-d218-4dde-af58-923968e3c307": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01092119s
May 21 08:40:19.899: INFO: Pod "pod-configmaps-8239ceeb-d218-4dde-af58-923968e3c307": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018968768s
STEP: Saw pod success
May 21 08:40:19.899: INFO: Pod "pod-configmaps-8239ceeb-d218-4dde-af58-923968e3c307" satisfied condition "Succeeded or Failed"
May 21 08:40:19.903: INFO: Trying to get logs from node k8s-03 pod pod-configmaps-8239ceeb-d218-4dde-af58-923968e3c307 container env-test: <nil>
STEP: delete the pod
May 21 08:40:20.004: INFO: Waiting for pod pod-configmaps-8239ceeb-d218-4dde-af58-923968e3c307 to disappear
May 21 08:40:20.025: INFO: Pod pod-configmaps-8239ceeb-d218-4dde-af58-923968e3c307 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:40:20.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8104" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":89,"skipped":1742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:40:20.038: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8562
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 08:40:20.336: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 08:40:20.653: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 08:40:22.702: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 08:40:24.661: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 08:40:26.662: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:40:28.664: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:40:30.661: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:40:32.665: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:40:34.665: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:40:36.660: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:40:38.664: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 08:40:38.672: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 21 08:40:42.770: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 21 08:40:42.770: INFO: Going to poll 10.100.179.32 on port 8081 at least 0 times, with a maximum of 34 tries before failing
May 21 08:40:42.776: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.179.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8562 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 08:40:42.776: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:40:43.933: INFO: Found all 1 expected endpoints: [netserver-0]
May 21 08:40:43.933: INFO: Going to poll 10.100.165.198 on port 8081 at least 0 times, with a maximum of 34 tries before failing
May 21 08:40:43.941: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.165.198 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8562 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 08:40:43.941: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:40:45.102: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:40:45.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8562" for this suite.

• [SLOW TEST:25.093 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1776,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:40:45.132: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 21 08:40:45.289: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:40:50.906: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:41:13.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3155" for this suite.

• [SLOW TEST:28.598 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":91,"skipped":1778,"failed":0}
SSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:41:13.730: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 08:41:14.069: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 21 08:41:14.074: INFO: starting watch
STEP: patching
STEP: updating
May 21 08:41:14.159: INFO: waiting for watch events with expected annotations
May 21 08:41:14.159: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:41:14.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2987" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":92,"skipped":1784,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:41:14.303: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-ecd7ee66-7445-4b5d-b20b-a36cd3361830
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:41:14.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6344" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":93,"skipped":1789,"failed":0}

------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:41:14.410: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-ceed87a2-854a-4c77-b112-ecc21437fac4
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:41:14.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7429" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":94,"skipped":1789,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:41:14.601: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:41:14.733: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2b2431cd-a802-4d2f-9c0a-c4007f50de9e" in namespace "security-context-test-2665" to be "Succeeded or Failed"
May 21 08:41:14.775: INFO: Pod "alpine-nnp-false-2b2431cd-a802-4d2f-9c0a-c4007f50de9e": Phase="Pending", Reason="", readiness=false. Elapsed: 42.462748ms
May 21 08:41:16.822: INFO: Pod "alpine-nnp-false-2b2431cd-a802-4d2f-9c0a-c4007f50de9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089025725s
May 21 08:41:18.898: INFO: Pod "alpine-nnp-false-2b2431cd-a802-4d2f-9c0a-c4007f50de9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16511023s
May 21 08:41:21.087: INFO: Pod "alpine-nnp-false-2b2431cd-a802-4d2f-9c0a-c4007f50de9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.353868412s
May 21 08:41:23.101: INFO: Pod "alpine-nnp-false-2b2431cd-a802-4d2f-9c0a-c4007f50de9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.368101432s
May 21 08:41:23.101: INFO: Pod "alpine-nnp-false-2b2431cd-a802-4d2f-9c0a-c4007f50de9e" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:41:23.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2665" for this suite.

• [SLOW TEST:8.550 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1862,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:41:23.151: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:41:23.319: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 21 08:41:23.368: INFO: Number of nodes with available pods: 0
May 21 08:41:23.368: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 21 08:41:23.455: INFO: Number of nodes with available pods: 0
May 21 08:41:23.455: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:24.462: INFO: Number of nodes with available pods: 0
May 21 08:41:24.462: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:25.462: INFO: Number of nodes with available pods: 0
May 21 08:41:25.462: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:26.464: INFO: Number of nodes with available pods: 0
May 21 08:41:26.465: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:27.464: INFO: Number of nodes with available pods: 1
May 21 08:41:27.464: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 21 08:41:27.537: INFO: Number of nodes with available pods: 1
May 21 08:41:27.537: INFO: Number of running nodes: 0, number of available pods: 1
May 21 08:41:28.552: INFO: Number of nodes with available pods: 0
May 21 08:41:28.552: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 21 08:41:28.644: INFO: Number of nodes with available pods: 0
May 21 08:41:28.644: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:29.651: INFO: Number of nodes with available pods: 0
May 21 08:41:29.651: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:30.666: INFO: Number of nodes with available pods: 0
May 21 08:41:30.666: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:31.654: INFO: Number of nodes with available pods: 0
May 21 08:41:31.654: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:32.656: INFO: Number of nodes with available pods: 0
May 21 08:41:32.656: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:33.654: INFO: Number of nodes with available pods: 0
May 21 08:41:33.654: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:34.649: INFO: Number of nodes with available pods: 0
May 21 08:41:34.649: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:35.654: INFO: Number of nodes with available pods: 0
May 21 08:41:35.654: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:36.660: INFO: Number of nodes with available pods: 0
May 21 08:41:36.660: INFO: Node k8s-02 is running more than one daemon pod
May 21 08:41:37.667: INFO: Number of nodes with available pods: 1
May 21 08:41:37.667: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8157, will wait for the garbage collector to delete the pods
May 21 08:41:37.744: INFO: Deleting DaemonSet.extensions daemon-set took: 8.747289ms
May 21 08:41:38.344: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.205168ms
May 21 08:41:44.152: INFO: Number of nodes with available pods: 0
May 21 08:41:44.152: INFO: Number of running nodes: 0, number of available pods: 0
May 21 08:41:44.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33933"},"items":null}

May 21 08:41:44.217: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33933"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:41:44.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8157" for this suite.

• [SLOW TEST:21.221 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":96,"skipped":1870,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:41:44.373: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8908
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8908
STEP: Creating statefulset with conflicting port in namespace statefulset-8908
STEP: Waiting until pod test-pod will start running in namespace statefulset-8908
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8908
May 21 08:41:50.682: INFO: Observed stateful pod in namespace: statefulset-8908, name: ss-0, uid: 44c32e93-0dfe-4047-99f2-12775274f004, status phase: Pending. Waiting for statefulset controller to delete.
May 21 08:41:51.268: INFO: Observed stateful pod in namespace: statefulset-8908, name: ss-0, uid: 44c32e93-0dfe-4047-99f2-12775274f004, status phase: Failed. Waiting for statefulset controller to delete.
May 21 08:41:51.353: INFO: Observed stateful pod in namespace: statefulset-8908, name: ss-0, uid: 44c32e93-0dfe-4047-99f2-12775274f004, status phase: Failed. Waiting for statefulset controller to delete.
May 21 08:41:51.405: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8908
STEP: Removing pod with conflicting port in namespace statefulset-8908
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8908 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 08:41:55.587: INFO: Deleting all statefulset in ns statefulset-8908
May 21 08:41:55.591: INFO: Scaling statefulset ss to 0
May 21 08:42:05.675: INFO: Waiting for statefulset status.replicas updated to 0
May 21 08:42:05.684: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:42:05.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8908" for this suite.

• [SLOW TEST:21.373 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":97,"skipped":1885,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:42:05.746: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8208
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 08:42:05.863: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 08:42:06.236: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 08:42:08.264: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 08:42:10.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:42:12.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:42:14.245: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:42:16.248: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:42:18.245: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:42:20.240: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:42:22.241: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:42:24.241: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:42:26.247: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 08:42:28.244: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 08:42:28.252: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 21 08:42:32.316: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 21 08:42:32.316: INFO: Breadth first check of 10.100.179.35 on host 8.16.0.94...
May 21 08:42:32.320: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.165.206:9080/dial?request=hostname&protocol=http&host=10.100.179.35&port=8080&tries=1'] Namespace:pod-network-test-8208 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 08:42:32.320: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:42:32.481: INFO: Waiting for responses: map[]
May 21 08:42:32.481: INFO: reached 10.100.179.35 after 0/1 tries
May 21 08:42:32.481: INFO: Breadth first check of 10.100.165.204 on host 8.16.0.95...
May 21 08:42:32.487: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.165.206:9080/dial?request=hostname&protocol=http&host=10.100.165.204&port=8080&tries=1'] Namespace:pod-network-test-8208 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 08:42:32.487: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 08:42:32.641: INFO: Waiting for responses: map[]
May 21 08:42:32.641: INFO: reached 10.100.165.204 after 0/1 tries
May 21 08:42:32.641: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:42:32.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8208" for this suite.

• [SLOW TEST:26.913 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":98,"skipped":1893,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:42:32.660: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 21 08:42:37.522: INFO: Successfully updated pod "pod-update-b066b89a-304d-45c4-8c58-67c859aa86b8"
STEP: verifying the updated pod is in kubernetes
May 21 08:42:37.585: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:42:37.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8494" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":99,"skipped":1897,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:42:37.602: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 08:42:37.810: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a0bbdc9-a2b6-408a-b987-36e095818643" in namespace "projected-2740" to be "Succeeded or Failed"
May 21 08:42:37.871: INFO: Pod "downwardapi-volume-2a0bbdc9-a2b6-408a-b987-36e095818643": Phase="Pending", Reason="", readiness=false. Elapsed: 59.979079ms
May 21 08:42:39.960: INFO: Pod "downwardapi-volume-2a0bbdc9-a2b6-408a-b987-36e095818643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.14965777s
May 21 08:42:41.968: INFO: Pod "downwardapi-volume-2a0bbdc9-a2b6-408a-b987-36e095818643": Phase="Running", Reason="", readiness=true. Elapsed: 4.15713456s
May 21 08:42:43.985: INFO: Pod "downwardapi-volume-2a0bbdc9-a2b6-408a-b987-36e095818643": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.174079701s
STEP: Saw pod success
May 21 08:42:43.985: INFO: Pod "downwardapi-volume-2a0bbdc9-a2b6-408a-b987-36e095818643" satisfied condition "Succeeded or Failed"
May 21 08:42:43.992: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-2a0bbdc9-a2b6-408a-b987-36e095818643 container client-container: <nil>
STEP: delete the pod
May 21 08:42:44.130: INFO: Waiting for pod downwardapi-volume-2a0bbdc9-a2b6-408a-b987-36e095818643 to disappear
May 21 08:42:44.136: INFO: Pod downwardapi-volume-2a0bbdc9-a2b6-408a-b987-36e095818643 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:42:44.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2740" for this suite.

• [SLOW TEST:6.552 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":100,"skipped":1900,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:42:44.155: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-d48638a8-0593-4609-a0ae-52c531e3bc85
STEP: Creating a pod to test consume secrets
May 21 08:42:44.357: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-51ca65a2-f7d0-43ff-b262-54768a31487e" in namespace "projected-2787" to be "Succeeded or Failed"
May 21 08:42:44.361: INFO: Pod "pod-projected-secrets-51ca65a2-f7d0-43ff-b262-54768a31487e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.197849ms
May 21 08:42:46.368: INFO: Pod "pod-projected-secrets-51ca65a2-f7d0-43ff-b262-54768a31487e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010841802s
May 21 08:42:48.380: INFO: Pod "pod-projected-secrets-51ca65a2-f7d0-43ff-b262-54768a31487e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022982327s
STEP: Saw pod success
May 21 08:42:48.380: INFO: Pod "pod-projected-secrets-51ca65a2-f7d0-43ff-b262-54768a31487e" satisfied condition "Succeeded or Failed"
May 21 08:42:48.383: INFO: Trying to get logs from node k8s-03 pod pod-projected-secrets-51ca65a2-f7d0-43ff-b262-54768a31487e container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 08:42:48.438: INFO: Waiting for pod pod-projected-secrets-51ca65a2-f7d0-43ff-b262-54768a31487e to disappear
May 21 08:42:48.475: INFO: Pod pod-projected-secrets-51ca65a2-f7d0-43ff-b262-54768a31487e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:42:48.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2787" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":101,"skipped":1905,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:42:48.492: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:42:52.852: INFO: Waiting up to 5m0s for pod "client-envvars-8483a5c5-ce97-4b66-9c90-3af60e3756bc" in namespace "pods-1038" to be "Succeeded or Failed"
May 21 08:42:52.862: INFO: Pod "client-envvars-8483a5c5-ce97-4b66-9c90-3af60e3756bc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.440198ms
May 21 08:42:54.869: INFO: Pod "client-envvars-8483a5c5-ce97-4b66-9c90-3af60e3756bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016175609s
May 21 08:42:56.874: INFO: Pod "client-envvars-8483a5c5-ce97-4b66-9c90-3af60e3756bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021193891s
STEP: Saw pod success
May 21 08:42:56.874: INFO: Pod "client-envvars-8483a5c5-ce97-4b66-9c90-3af60e3756bc" satisfied condition "Succeeded or Failed"
May 21 08:42:56.878: INFO: Trying to get logs from node k8s-03 pod client-envvars-8483a5c5-ce97-4b66-9c90-3af60e3756bc container env3cont: <nil>
STEP: delete the pod
May 21 08:42:56.933: INFO: Waiting for pod client-envvars-8483a5c5-ce97-4b66-9c90-3af60e3756bc to disappear
May 21 08:42:56.940: INFO: Pod client-envvars-8483a5c5-ce97-4b66-9c90-3af60e3756bc no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:42:56.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1038" for this suite.

• [SLOW TEST:8.461 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":1943,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:42:56.953: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:42:57.117: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:43:01.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3593" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":103,"skipped":1944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:43:01.245: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 08:43:05.540: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:43:05.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4780" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":104,"skipped":1968,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:43:05.625: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 21 08:43:05.743: INFO: Waiting up to 5m0s for pod "pod-9000e8c3-7562-4328-91f0-af9478aa6819" in namespace "emptydir-6179" to be "Succeeded or Failed"
May 21 08:43:05.780: INFO: Pod "pod-9000e8c3-7562-4328-91f0-af9478aa6819": Phase="Pending", Reason="", readiness=false. Elapsed: 37.226096ms
May 21 08:43:07.796: INFO: Pod "pod-9000e8c3-7562-4328-91f0-af9478aa6819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052823084s
May 21 08:43:09.803: INFO: Pod "pod-9000e8c3-7562-4328-91f0-af9478aa6819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060040932s
STEP: Saw pod success
May 21 08:43:09.803: INFO: Pod "pod-9000e8c3-7562-4328-91f0-af9478aa6819" satisfied condition "Succeeded or Failed"
May 21 08:43:09.807: INFO: Trying to get logs from node k8s-03 pod pod-9000e8c3-7562-4328-91f0-af9478aa6819 container test-container: <nil>
STEP: delete the pod
May 21 08:43:09.899: INFO: Waiting for pod pod-9000e8c3-7562-4328-91f0-af9478aa6819 to disappear
May 21 08:43:09.929: INFO: Pod pod-9000e8c3-7562-4328-91f0-af9478aa6819 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:43:09.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6179" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":105,"skipped":2019,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:43:10.035: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-3896
STEP: creating replication controller nodeport-test in namespace services-3896
I0521 08:43:10.556541      25 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-3896, replica count: 2
I0521 08:43:13.607136      25 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:43:16.607439      25 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 08:43:16.607: INFO: Creating new exec pod
May 21 08:43:21.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-3896 exec execpodt8dt7 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 21 08:43:22.089: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 21 08:43:22.089: INFO: stdout: ""
May 21 08:43:22.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-3896 exec execpodt8dt7 -- /bin/sh -x -c nc -zv -t -w 2 10.96.218.121 80'
May 21 08:43:22.427: INFO: stderr: "+ nc -zv -t -w 2 10.96.218.121 80\nConnection to 10.96.218.121 80 port [tcp/http] succeeded!\n"
May 21 08:43:22.427: INFO: stdout: ""
May 21 08:43:22.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-3896 exec execpodt8dt7 -- /bin/sh -x -c nc -zv -t -w 2 8.16.0.94 30061'
May 21 08:43:22.781: INFO: stderr: "+ nc -zv -t -w 2 8.16.0.94 30061\nConnection to 8.16.0.94 30061 port [tcp/30061] succeeded!\n"
May 21 08:43:22.782: INFO: stdout: ""
May 21 08:43:22.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-3896 exec execpodt8dt7 -- /bin/sh -x -c nc -zv -t -w 2 8.16.0.95 30061'
May 21 08:43:23.120: INFO: stderr: "+ nc -zv -t -w 2 8.16.0.95 30061\nConnection to 8.16.0.95 30061 port [tcp/30061] succeeded!\n"
May 21 08:43:23.120: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:43:23.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3896" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:13.102 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":106,"skipped":2028,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:43:23.138: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-5cee84ff-f3ee-4803-bc0e-bd6be9fd81a5
STEP: Creating secret with name secret-projected-all-test-volume-37777999-b4a1-4346-961d-c7be2024ffc7
STEP: Creating a pod to test Check all projections for projected volume plugin
May 21 08:43:23.515: INFO: Waiting up to 5m0s for pod "projected-volume-4d1cad65-25cc-43f1-b388-31bccdace0d1" in namespace "projected-6157" to be "Succeeded or Failed"
May 21 08:43:23.554: INFO: Pod "projected-volume-4d1cad65-25cc-43f1-b388-31bccdace0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 38.664726ms
May 21 08:43:25.562: INFO: Pod "projected-volume-4d1cad65-25cc-43f1-b388-31bccdace0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046408973s
May 21 08:43:27.604: INFO: Pod "projected-volume-4d1cad65-25cc-43f1-b388-31bccdace0d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088919638s
STEP: Saw pod success
May 21 08:43:27.604: INFO: Pod "projected-volume-4d1cad65-25cc-43f1-b388-31bccdace0d1" satisfied condition "Succeeded or Failed"
May 21 08:43:27.609: INFO: Trying to get logs from node k8s-02 pod projected-volume-4d1cad65-25cc-43f1-b388-31bccdace0d1 container projected-all-volume-test: <nil>
STEP: delete the pod
May 21 08:43:27.743: INFO: Waiting for pod projected-volume-4d1cad65-25cc-43f1-b388-31bccdace0d1 to disappear
May 21 08:43:27.761: INFO: Pod projected-volume-4d1cad65-25cc-43f1-b388-31bccdace0d1 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:43:27.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6157" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":107,"skipped":2047,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:43:27.774: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:43:27.984: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:43:29.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7939" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":108,"skipped":2048,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:43:29.130: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 21 08:43:29.446: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 08:43:29.505: INFO: Waiting for terminating namespaces to be deleted...
May 21 08:43:29.514: INFO: 
Logging pods the apiserver thinks is on node k8s-02 before test
May 21 08:43:29.528: INFO: calico-node-gpcrh from calico-system started at 2021-05-21 05:44:44 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.528: INFO: 	Container calico-node ready: true, restart count 0
May 21 08:43:29.528: INFO: calico-typha-7578696db5-f4hhm from calico-system started at 2021-05-21 05:45:42 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.528: INFO: 	Container calico-typha ready: true, restart count 0
May 21 08:43:29.528: INFO: kube-proxy-kl6jm from kube-system started at 2021-05-21 05:44:44 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.528: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 08:43:29.528: INFO: node-exporter-vff7w from lens-metrics started at 2021-05-21 08:29:05 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.528: INFO: 	Container node-exporter ready: true, restart count 0
May 21 08:43:29.528: INFO: prometheus-0 from lens-metrics started at 2021-05-21 08:29:05 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.528: INFO: 	Container prometheus ready: true, restart count 0
May 21 08:43:29.528: INFO: sonobuoy-e2e-job-4762d948ca704da5 from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 08:43:29.528: INFO: 	Container e2e ready: true, restart count 0
May 21 08:43:29.528: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 08:43:29.528: INFO: sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-7h98l from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 08:43:29.528: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 08:43:29.528: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 08:43:29.528: INFO: 
Logging pods the apiserver thinks is on node k8s-03 before test
May 21 08:43:29.548: INFO: calico-node-s987r from calico-system started at 2021-05-21 05:45:50 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.548: INFO: 	Container calico-node ready: true, restart count 0
May 21 08:43:29.548: INFO: calico-typha-7578696db5-qh5gh from calico-system started at 2021-05-21 05:47:49 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.548: INFO: 	Container calico-typha ready: true, restart count 0
May 21 08:43:29.548: INFO: kube-proxy-7n7r8 from kube-system started at 2021-05-21 05:45:50 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.548: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 08:43:29.548: INFO: kube-state-metrics-565db9f7c-zn72r from lens-metrics started at 2021-05-21 08:29:04 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.548: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 21 08:43:29.548: INFO: node-exporter-ldzgf from lens-metrics started at 2021-05-21 08:29:05 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.548: INFO: 	Container node-exporter ready: true, restart count 0
May 21 08:43:29.548: INFO: pod-logs-websocket-815170b8-1524-4e8e-aee4-c5f48750119c from pods-3593 started at 2021-05-21 08:42:57 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.548: INFO: 	Container main ready: true, restart count 0
May 21 08:43:29.548: INFO: sonobuoy from sonobuoy started at 2021-05-21 08:00:13 +0000 UTC (1 container statuses recorded)
May 21 08:43:29.548: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 08:43:29.548: INFO: sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-p7f4t from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 08:43:29.548: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 08:43:29.548: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1681086e1bfab695], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1681086e1d9812be], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:43:30.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-624" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":109,"skipped":2065,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:43:30.853: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:43:31.031: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Pending, waiting for it to be Running (with Ready = true)
May 21 08:43:33.055: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Pending, waiting for it to be Running (with Ready = true)
May 21 08:43:35.037: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Running (Ready = false)
May 21 08:43:37.038: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Running (Ready = false)
May 21 08:43:39.038: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Running (Ready = false)
May 21 08:43:41.038: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Running (Ready = false)
May 21 08:43:43.093: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Running (Ready = false)
May 21 08:43:45.037: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Running (Ready = false)
May 21 08:43:47.039: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Running (Ready = false)
May 21 08:43:49.037: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Running (Ready = false)
May 21 08:43:51.037: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Running (Ready = false)
May 21 08:43:53.041: INFO: The status of Pod test-webserver-343e537a-0cc8-4d22-adec-4d2587895ecd is Running (Ready = true)
May 21 08:43:53.045: INFO: Container started at 2021-05-21 08:43:33 +0000 UTC, pod became ready at 2021-05-21 08:43:51 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:43:53.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3315" for this suite.

• [SLOW TEST:22.218 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":2090,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:43:53.072: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6576 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6576;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6576 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6576;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6576.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6576.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6576.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6576.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6576.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6576.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6576.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6576.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6576.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6576.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6576.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 2.93.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.93.2_udp@PTR;check="$$(dig +tcp +noall +answer +search 2.93.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.93.2_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6576 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6576;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6576 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6576;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6576.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6576.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6576.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6576.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6576.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6576.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6576.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6576.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6576.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6576.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6576.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6576.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 2.93.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.93.2_udp@PTR;check="$$(dig +tcp +noall +answer +search 2.93.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.93.2_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 08:43:59.573: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.580: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.585: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.590: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.594: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.600: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.608: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.655: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.779: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.785: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.789: INFO: Unable to read jessie_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.794: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.799: INFO: Unable to read jessie_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.804: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.809: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.814: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:43:59.847: INFO: Lookups using dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6576 wheezy_tcp@dns-test-service.dns-6576 wheezy_udp@dns-test-service.dns-6576.svc wheezy_tcp@dns-test-service.dns-6576.svc wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6576 jessie_tcp@dns-test-service.dns-6576 jessie_udp@dns-test-service.dns-6576.svc jessie_tcp@dns-test-service.dns-6576.svc jessie_udp@_http._tcp.dns-test-service.dns-6576.svc jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc]

May 21 08:44:04.901: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.906: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.921: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.927: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.932: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.937: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.942: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.948: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.982: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.987: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.992: INFO: Unable to read jessie_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:04.996: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:05.002: INFO: Unable to read jessie_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:05.006: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:05.011: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:05.015: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:05.043: INFO: Lookups using dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6576 wheezy_tcp@dns-test-service.dns-6576 wheezy_udp@dns-test-service.dns-6576.svc wheezy_tcp@dns-test-service.dns-6576.svc wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6576 jessie_tcp@dns-test-service.dns-6576 jessie_udp@dns-test-service.dns-6576.svc jessie_tcp@dns-test-service.dns-6576.svc jessie_udp@_http._tcp.dns-test-service.dns-6576.svc jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc]

May 21 08:44:09.856: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.863: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.868: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.872: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.878: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.882: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.887: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.892: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.932: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.937: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.942: INFO: Unable to read jessie_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.947: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.952: INFO: Unable to read jessie_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.957: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.963: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.968: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:09.998: INFO: Lookups using dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6576 wheezy_tcp@dns-test-service.dns-6576 wheezy_udp@dns-test-service.dns-6576.svc wheezy_tcp@dns-test-service.dns-6576.svc wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6576 jessie_tcp@dns-test-service.dns-6576 jessie_udp@dns-test-service.dns-6576.svc jessie_tcp@dns-test-service.dns-6576.svc jessie_udp@_http._tcp.dns-test-service.dns-6576.svc jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc]

May 21 08:44:14.869: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.874: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.879: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.884: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.890: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.905: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.909: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.914: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.948: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.953: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.958: INFO: Unable to read jessie_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.962: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:14.966: INFO: Unable to read jessie_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:15.002: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:15.007: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:15.012: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:15.040: INFO: Lookups using dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6576 wheezy_tcp@dns-test-service.dns-6576 wheezy_udp@dns-test-service.dns-6576.svc wheezy_tcp@dns-test-service.dns-6576.svc wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6576 jessie_tcp@dns-test-service.dns-6576 jessie_udp@dns-test-service.dns-6576.svc jessie_tcp@dns-test-service.dns-6576.svc jessie_udp@_http._tcp.dns-test-service.dns-6576.svc jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc]

May 21 08:44:19.856: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.861: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.866: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.871: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.876: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.880: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.885: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.890: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.925: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.930: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.935: INFO: Unable to read jessie_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.939: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.944: INFO: Unable to read jessie_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.949: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.954: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.959: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:19.996: INFO: Lookups using dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6576 wheezy_tcp@dns-test-service.dns-6576 wheezy_udp@dns-test-service.dns-6576.svc wheezy_tcp@dns-test-service.dns-6576.svc wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6576 jessie_tcp@dns-test-service.dns-6576 jessie_udp@dns-test-service.dns-6576.svc jessie_tcp@dns-test-service.dns-6576.svc jessie_udp@_http._tcp.dns-test-service.dns-6576.svc jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc]

May 21 08:44:24.855: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.860: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.865: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.870: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.875: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.880: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.884: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.889: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.922: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.927: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.931: INFO: Unable to read jessie_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.936: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.940: INFO: Unable to read jessie_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.946: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.951: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:24.956: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:25.019: INFO: Lookups using dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6576 wheezy_tcp@dns-test-service.dns-6576 wheezy_udp@dns-test-service.dns-6576.svc wheezy_tcp@dns-test-service.dns-6576.svc wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6576 jessie_tcp@dns-test-service.dns-6576 jessie_udp@dns-test-service.dns-6576.svc jessie_tcp@dns-test-service.dns-6576.svc jessie_udp@_http._tcp.dns-test-service.dns-6576.svc jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc]

May 21 08:44:29.855: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.860: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.865: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.870: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.875: INFO: Unable to read wheezy_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.880: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.885: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.890: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.923: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.927: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.932: INFO: Unable to read jessie_udp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.936: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576 from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.941: INFO: Unable to read jessie_udp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.946: INFO: Unable to read jessie_tcp@dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.950: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:29.955: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc from pod dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38: the server could not find the requested resource (get pods dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38)
May 21 08:44:30.003: INFO: Lookups using dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6576 wheezy_tcp@dns-test-service.dns-6576 wheezy_udp@dns-test-service.dns-6576.svc wheezy_tcp@dns-test-service.dns-6576.svc wheezy_udp@_http._tcp.dns-test-service.dns-6576.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6576 jessie_tcp@dns-test-service.dns-6576 jessie_udp@dns-test-service.dns-6576.svc jessie_tcp@dns-test-service.dns-6576.svc jessie_udp@_http._tcp.dns-test-service.dns-6576.svc jessie_tcp@_http._tcp.dns-test-service.dns-6576.svc]

May 21 08:44:35.002: INFO: DNS probes using dns-6576/dns-test-4d4b55ec-e579-417b-bdb7-1626e4ebea38 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:44:35.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6576" for this suite.

• [SLOW TEST:42.546 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":111,"skipped":2092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:44:35.620: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:44:35.870: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 21 08:44:38.051: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:44:38.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4386" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":112,"skipped":2193,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:44:38.092: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 21 08:44:48.771: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 08:44:48.830: INFO: Pod pod-with-poststart-http-hook still exists
May 21 08:44:50.830: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 08:44:50.890: INFO: Pod pod-with-poststart-http-hook still exists
May 21 08:44:52.830: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 08:44:52.843: INFO: Pod pod-with-poststart-http-hook still exists
May 21 08:44:54.830: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 08:44:54.840: INFO: Pod pod-with-poststart-http-hook still exists
May 21 08:44:56.830: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 08:44:56.839: INFO: Pod pod-with-poststart-http-hook still exists
May 21 08:44:58.830: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 08:44:58.843: INFO: Pod pod-with-poststart-http-hook still exists
May 21 08:45:00.830: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 08:45:00.842: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:45:00.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7183" for this suite.

• [SLOW TEST:22.766 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":2202,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:45:00.859: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 21 08:45:00.978: INFO: Waiting up to 5m0s for pod "pod-485f5d41-16de-476b-b5a3-5835192d0ca2" in namespace "emptydir-6538" to be "Succeeded or Failed"
May 21 08:45:01.009: INFO: Pod "pod-485f5d41-16de-476b-b5a3-5835192d0ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 31.728285ms
May 21 08:45:03.108: INFO: Pod "pod-485f5d41-16de-476b-b5a3-5835192d0ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130690117s
May 21 08:45:05.117: INFO: Pod "pod-485f5d41-16de-476b-b5a3-5835192d0ca2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.139025999s
STEP: Saw pod success
May 21 08:45:05.117: INFO: Pod "pod-485f5d41-16de-476b-b5a3-5835192d0ca2" satisfied condition "Succeeded or Failed"
May 21 08:45:05.121: INFO: Trying to get logs from node k8s-03 pod pod-485f5d41-16de-476b-b5a3-5835192d0ca2 container test-container: <nil>
STEP: delete the pod
May 21 08:45:05.175: INFO: Waiting for pod pod-485f5d41-16de-476b-b5a3-5835192d0ca2 to disappear
May 21 08:45:05.234: INFO: Pod pod-485f5d41-16de-476b-b5a3-5835192d0ca2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:45:05.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6538" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":2209,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:45:05.276: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 21 08:45:05.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4600 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 21 08:45:05.702: INFO: stderr: ""
May 21 08:45:05.702: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 21 08:45:10.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4600 get pod e2e-test-httpd-pod -o json'
May 21 08:45:10.925: INFO: stderr: ""
May 21 08:45:10.925: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.100.165.225/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.165.225/32\"\n        },\n        \"creationTimestamp\": \"2021-05-21T08:45:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-21T08:45:05Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-21T08:45:06Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.100.165.225\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-21T08:45:08Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4600\",\n        \"resourceVersion\": \"35370\",\n        \"uid\": \"6b5d14c8-3f0f-4090-a7ec-eae8dc1e697a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-wk7kt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-03\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-wk7kt\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-wk7kt\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T08:45:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T08:45:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T08:45:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T08:45:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://53a2deaaa8467ef0f5d60971dcd9c2d56cef8e934ae08853c3972fdd48f03f90\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-21T08:45:08Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"8.16.0.95\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.165.225\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.165.225\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-21T08:45:05Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 21 08:45:10.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4600 replace -f -'
May 21 08:45:12.166: INFO: stderr: ""
May 21 08:45:12.166: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
May 21 08:45:12.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4600 delete pods e2e-test-httpd-pod'
May 21 08:45:14.998: INFO: stderr: ""
May 21 08:45:14.998: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:45:14.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4600" for this suite.

• [SLOW TEST:9.756 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":115,"skipped":2223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:45:15.033: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 21 08:45:15.181: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:45:48.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3961" for this suite.

• [SLOW TEST:33.872 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":116,"skipped":2268,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:45:48.906: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 21 08:45:49.094: INFO: Waiting up to 5m0s for pod "downward-api-b70b1341-bf7b-4504-9da8-8372db0ae6cd" in namespace "downward-api-1341" to be "Succeeded or Failed"
May 21 08:45:49.105: INFO: Pod "downward-api-b70b1341-bf7b-4504-9da8-8372db0ae6cd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.472268ms
May 21 08:45:51.114: INFO: Pod "downward-api-b70b1341-bf7b-4504-9da8-8372db0ae6cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019666827s
May 21 08:45:53.123: INFO: Pod "downward-api-b70b1341-bf7b-4504-9da8-8372db0ae6cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028866977s
STEP: Saw pod success
May 21 08:45:53.123: INFO: Pod "downward-api-b70b1341-bf7b-4504-9da8-8372db0ae6cd" satisfied condition "Succeeded or Failed"
May 21 08:45:53.128: INFO: Trying to get logs from node k8s-03 pod downward-api-b70b1341-bf7b-4504-9da8-8372db0ae6cd container dapi-container: <nil>
STEP: delete the pod
May 21 08:45:53.171: INFO: Waiting for pod downward-api-b70b1341-bf7b-4504-9da8-8372db0ae6cd to disappear
May 21 08:45:53.274: INFO: Pod downward-api-b70b1341-bf7b-4504-9da8-8372db0ae6cd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:45:53.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1341" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":2282,"failed":0}

------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:45:53.288: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:45:53.412: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-53654cf7-1ad7-48e2-917d-c094b3b290b8" in namespace "security-context-test-1285" to be "Succeeded or Failed"
May 21 08:45:53.449: INFO: Pod "busybox-readonly-false-53654cf7-1ad7-48e2-917d-c094b3b290b8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.863105ms
May 21 08:45:55.459: INFO: Pod "busybox-readonly-false-53654cf7-1ad7-48e2-917d-c094b3b290b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046782422s
May 21 08:45:57.536: INFO: Pod "busybox-readonly-false-53654cf7-1ad7-48e2-917d-c094b3b290b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.124026919s
May 21 08:45:57.536: INFO: Pod "busybox-readonly-false-53654cf7-1ad7-48e2-917d-c094b3b290b8" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:45:57.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1285" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":2282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:45:57.549: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 21 08:45:57.761: INFO: Waiting up to 5m0s for pod "pod-35907051-73dd-4dd1-855a-f82c474ac87e" in namespace "emptydir-3369" to be "Succeeded or Failed"
May 21 08:45:57.822: INFO: Pod "pod-35907051-73dd-4dd1-855a-f82c474ac87e": Phase="Pending", Reason="", readiness=false. Elapsed: 61.272533ms
May 21 08:45:59.836: INFO: Pod "pod-35907051-73dd-4dd1-855a-f82c474ac87e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074826202s
May 21 08:46:01.841: INFO: Pod "pod-35907051-73dd-4dd1-855a-f82c474ac87e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080283838s
STEP: Saw pod success
May 21 08:46:01.841: INFO: Pod "pod-35907051-73dd-4dd1-855a-f82c474ac87e" satisfied condition "Succeeded or Failed"
May 21 08:46:01.845: INFO: Trying to get logs from node k8s-03 pod pod-35907051-73dd-4dd1-855a-f82c474ac87e container test-container: <nil>
STEP: delete the pod
May 21 08:46:01.941: INFO: Waiting for pod pod-35907051-73dd-4dd1-855a-f82c474ac87e to disappear
May 21 08:46:01.948: INFO: Pod pod-35907051-73dd-4dd1-855a-f82c474ac87e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:46:01.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3369" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":2316,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:46:01.961: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 21 08:46:02.196: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4921  14f1423f-1cc0-40dd-b190-3a17fd702ab8 35611 0 2021-05-21 08:46:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 08:46:02.196: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4921  14f1423f-1cc0-40dd-b190-3a17fd702ab8 35611 0 2021-05-21 08:46:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 21 08:46:12.222: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4921  14f1423f-1cc0-40dd-b190-3a17fd702ab8 35654 0 2021-05-21 08:46:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 08:46:12.223: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4921  14f1423f-1cc0-40dd-b190-3a17fd702ab8 35654 0 2021-05-21 08:46:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 21 08:46:22.247: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4921  14f1423f-1cc0-40dd-b190-3a17fd702ab8 35674 0 2021-05-21 08:46:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 08:46:22.247: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4921  14f1423f-1cc0-40dd-b190-3a17fd702ab8 35674 0 2021-05-21 08:46:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 21 08:46:32.267: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4921  14f1423f-1cc0-40dd-b190-3a17fd702ab8 35695 0 2021-05-21 08:46:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 08:46:32.267: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4921  14f1423f-1cc0-40dd-b190-3a17fd702ab8 35695 0 2021-05-21 08:46:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 21 08:46:42.287: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4921  6227fee5-77dc-4189-bc5a-d60af97a4ee4 35715 0 2021-05-21 08:46:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 08:46:42.288: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4921  6227fee5-77dc-4189-bc5a-d60af97a4ee4 35715 0 2021-05-21 08:46:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 21 08:46:52.307: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4921  6227fee5-77dc-4189-bc5a-d60af97a4ee4 35735 0 2021-05-21 08:46:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 08:46:52.307: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4921  6227fee5-77dc-4189-bc5a-d60af97a4ee4 35735 0 2021-05-21 08:46:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 08:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:47:02.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4921" for this suite.

• [SLOW TEST:60.360 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":120,"skipped":2337,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:47:02.322: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-bcf28e51-d27f-4af7-9c9d-c161f8021abf
STEP: Creating a pod to test consume secrets
May 21 08:47:02.520: INFO: Waiting up to 5m0s for pod "pod-secrets-68312199-1c4c-4bec-bfc8-484c30c3a642" in namespace "secrets-6929" to be "Succeeded or Failed"
May 21 08:47:02.525: INFO: Pod "pod-secrets-68312199-1c4c-4bec-bfc8-484c30c3a642": Phase="Pending", Reason="", readiness=false. Elapsed: 5.608496ms
May 21 08:47:04.556: INFO: Pod "pod-secrets-68312199-1c4c-4bec-bfc8-484c30c3a642": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03657543s
May 21 08:47:06.588: INFO: Pod "pod-secrets-68312199-1c4c-4bec-bfc8-484c30c3a642": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068654085s
STEP: Saw pod success
May 21 08:47:06.589: INFO: Pod "pod-secrets-68312199-1c4c-4bec-bfc8-484c30c3a642" satisfied condition "Succeeded or Failed"
May 21 08:47:06.593: INFO: Trying to get logs from node k8s-03 pod pod-secrets-68312199-1c4c-4bec-bfc8-484c30c3a642 container secret-volume-test: <nil>
STEP: delete the pod
May 21 08:47:06.665: INFO: Waiting for pod pod-secrets-68312199-1c4c-4bec-bfc8-484c30c3a642 to disappear
May 21 08:47:06.684: INFO: Pod pod-secrets-68312199-1c4c-4bec-bfc8-484c30c3a642 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:47:06.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6929" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":121,"skipped":2358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:47:06.737: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
May 21 08:47:06.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-2415 create -f -'
May 21 08:47:08.008: INFO: stderr: ""
May 21 08:47:08.008: INFO: stdout: "pod/pause created\n"
May 21 08:47:08.008: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 21 08:47:08.008: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2415" to be "running and ready"
May 21 08:47:08.104: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 96.037068ms
May 21 08:47:10.113: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105078555s
May 21 08:47:12.151: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.142923262s
May 21 08:47:12.151: INFO: Pod "pause" satisfied condition "running and ready"
May 21 08:47:12.151: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
May 21 08:47:12.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-2415 label pods pause testing-label=testing-label-value'
May 21 08:47:12.356: INFO: stderr: ""
May 21 08:47:12.356: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 21 08:47:12.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-2415 get pod pause -L testing-label'
May 21 08:47:12.532: INFO: stderr: ""
May 21 08:47:12.532: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 21 08:47:12.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-2415 label pods pause testing-label-'
May 21 08:47:12.773: INFO: stderr: ""
May 21 08:47:12.773: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 21 08:47:12.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-2415 get pod pause -L testing-label'
May 21 08:47:12.961: INFO: stderr: ""
May 21 08:47:12.961: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
May 21 08:47:12.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-2415 delete --grace-period=0 --force -f -'
May 21 08:47:13.236: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 08:47:13.236: INFO: stdout: "pod \"pause\" force deleted\n"
May 21 08:47:13.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-2415 get rc,svc -l name=pause --no-headers'
May 21 08:47:13.431: INFO: stderr: "No resources found in kubectl-2415 namespace.\n"
May 21 08:47:13.432: INFO: stdout: ""
May 21 08:47:13.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-2415 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 08:47:13.625: INFO: stderr: ""
May 21 08:47:13.625: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:47:13.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2415" for this suite.

• [SLOW TEST:6.904 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":122,"skipped":2408,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:47:13.642: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 08:47:15.381: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 08:47:17.430: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183635, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183635, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183635, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183635, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 08:47:20.483: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:47:20.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-625" for this suite.
STEP: Destroying namespace "webhook-625-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.398 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":123,"skipped":2411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:47:21.041: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8060.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8060.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8060.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8060.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8060.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8060.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 08:47:27.324: INFO: DNS probes using dns-8060/dns-test-ba50ca13-a648-47be-b164-d9926d5764fa succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:47:27.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8060" for this suite.

• [SLOW TEST:6.529 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":124,"skipped":2467,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:47:27.571: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:47:27.705: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:47:31.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9344" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":125,"skipped":2495,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:47:31.959: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6503.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6503.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 08:47:38.379: INFO: DNS probes using dns-test-2b2320e6-d9a9-40fe-b01a-89c6129a995e succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6503.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6503.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 08:47:44.999: INFO: File wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local from pod  dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 08:47:45.003: INFO: File jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local from pod  dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 08:47:45.003: INFO: Lookups using dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 failed for: [wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local]

May 21 08:47:50.013: INFO: File wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local from pod  dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 08:47:50.019: INFO: File jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local from pod  dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 08:47:50.019: INFO: Lookups using dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 failed for: [wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local]

May 21 08:47:55.010: INFO: File wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local from pod  dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 08:47:55.015: INFO: File jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local from pod  dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 08:47:55.015: INFO: Lookups using dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 failed for: [wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local]

May 21 08:48:00.011: INFO: File wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local from pod  dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 08:48:00.015: INFO: File jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local from pod  dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 08:48:00.015: INFO: Lookups using dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 failed for: [wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local]

May 21 08:48:05.012: INFO: File wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local from pod  dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 08:48:05.017: INFO: File jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local from pod  dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 08:48:05.017: INFO: Lookups using dns-6503/dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 failed for: [wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local]

May 21 08:48:10.015: INFO: DNS probes using dns-test-00fd7cd5-8c65-4031-abed-7c2141c016b6 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6503.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6503.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6503.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6503.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 08:48:16.534: INFO: DNS probes using dns-test-17626e61-6769-412d-97fc-d697db74e9ca succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:48:16.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6503" for this suite.

• [SLOW TEST:45.086 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":126,"skipped":2502,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:48:17.047: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-9807/configmap-test-1ee0f533-b3ca-4fd3-a87b-25ddcfb10d49
STEP: Creating a pod to test consume configMaps
May 21 08:48:17.349: INFO: Waiting up to 5m0s for pod "pod-configmaps-8351d842-4c9a-46fd-97dc-20475bda2f78" in namespace "configmap-9807" to be "Succeeded or Failed"
May 21 08:48:17.379: INFO: Pod "pod-configmaps-8351d842-4c9a-46fd-97dc-20475bda2f78": Phase="Pending", Reason="", readiness=false. Elapsed: 30.655768ms
May 21 08:48:19.395: INFO: Pod "pod-configmaps-8351d842-4c9a-46fd-97dc-20475bda2f78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046114526s
May 21 08:48:21.400: INFO: Pod "pod-configmaps-8351d842-4c9a-46fd-97dc-20475bda2f78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051522612s
STEP: Saw pod success
May 21 08:48:21.400: INFO: Pod "pod-configmaps-8351d842-4c9a-46fd-97dc-20475bda2f78" satisfied condition "Succeeded or Failed"
May 21 08:48:21.404: INFO: Trying to get logs from node k8s-03 pod pod-configmaps-8351d842-4c9a-46fd-97dc-20475bda2f78 container env-test: <nil>
STEP: delete the pod
May 21 08:48:21.474: INFO: Waiting for pod pod-configmaps-8351d842-4c9a-46fd-97dc-20475bda2f78 to disappear
May 21 08:48:21.483: INFO: Pod pod-configmaps-8351d842-4c9a-46fd-97dc-20475bda2f78 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:48:21.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9807" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":127,"skipped":2550,"failed":0}
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:48:21.501: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:48:21.684: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9286
I0521 08:48:21.718344      25 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9286, replica count: 1
I0521 08:48:22.769384      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:48:23.769937      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:48:24.770383      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:48:25.770748      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 08:48:25.907: INFO: Created: latency-svc-j4cz4
May 21 08:48:25.941: INFO: Got endpoints: latency-svc-j4cz4 [69.99293ms]
May 21 08:48:26.057: INFO: Created: latency-svc-mldkj
May 21 08:48:26.104: INFO: Created: latency-svc-x879m
May 21 08:48:26.104: INFO: Got endpoints: latency-svc-mldkj [162.698807ms]
May 21 08:48:26.132: INFO: Got endpoints: latency-svc-x879m [191.287703ms]
May 21 08:48:26.206: INFO: Created: latency-svc-skzs2
May 21 08:48:26.381: INFO: Got endpoints: latency-svc-skzs2 [439.738626ms]
May 21 08:48:26.390: INFO: Created: latency-svc-qx8ls
May 21 08:48:26.445: INFO: Got endpoints: latency-svc-qx8ls [504.055389ms]
May 21 08:48:26.523: INFO: Created: latency-svc-87pj9
May 21 08:48:26.543: INFO: Got endpoints: latency-svc-87pj9 [601.517092ms]
May 21 08:48:26.577: INFO: Created: latency-svc-jddr6
May 21 08:48:26.622: INFO: Got endpoints: latency-svc-jddr6 [680.721259ms]
May 21 08:48:26.677: INFO: Created: latency-svc-4rnqx
May 21 08:48:26.865: INFO: Got endpoints: latency-svc-4rnqx [923.098954ms]
May 21 08:48:26.865: INFO: Created: latency-svc-cc8hc
May 21 08:48:26.925: INFO: Got endpoints: latency-svc-cc8hc [983.154767ms]
May 21 08:48:27.065: INFO: Created: latency-svc-qtg27
May 21 08:48:27.279: INFO: Got endpoints: latency-svc-qtg27 [1.337268504s]
May 21 08:48:27.340: INFO: Created: latency-svc-lj5z5
May 21 08:48:27.356: INFO: Got endpoints: latency-svc-lj5z5 [1.413692149s]
May 21 08:48:27.492: INFO: Created: latency-svc-q6wjk
May 21 08:48:27.504: INFO: Got endpoints: latency-svc-q6wjk [1.562487549s]
May 21 08:48:27.596: INFO: Created: latency-svc-bpl2g
May 21 08:48:27.605: INFO: Got endpoints: latency-svc-bpl2g [1.663265222s]
May 21 08:48:27.650: INFO: Created: latency-svc-p47cb
May 21 08:48:27.688: INFO: Got endpoints: latency-svc-p47cb [1.745855859s]
May 21 08:48:27.859: INFO: Created: latency-svc-h5plg
May 21 08:48:27.924: INFO: Got endpoints: latency-svc-h5plg [1.98148577s]
May 21 08:48:27.976: INFO: Created: latency-svc-lvmjd
May 21 08:48:27.987: INFO: Got endpoints: latency-svc-lvmjd [2.045216173s]
May 21 08:48:28.102: INFO: Created: latency-svc-9ch8q
May 21 08:48:28.114: INFO: Got endpoints: latency-svc-9ch8q [2.009688825s]
May 21 08:48:28.149: INFO: Created: latency-svc-mp7wc
May 21 08:48:28.237: INFO: Got endpoints: latency-svc-mp7wc [2.104501544s]
May 21 08:48:28.243: INFO: Created: latency-svc-72jwx
May 21 08:48:28.254: INFO: Got endpoints: latency-svc-72jwx [1.873153642s]
May 21 08:48:28.335: INFO: Created: latency-svc-nqhcr
May 21 08:48:28.410: INFO: Got endpoints: latency-svc-nqhcr [1.964748418s]
May 21 08:48:28.416: INFO: Created: latency-svc-25988
May 21 08:48:28.427: INFO: Got endpoints: latency-svc-25988 [1.884251124s]
May 21 08:48:28.460: INFO: Created: latency-svc-rcblz
May 21 08:48:28.469: INFO: Got endpoints: latency-svc-rcblz [1.847067478s]
May 21 08:48:28.573: INFO: Created: latency-svc-tp7dr
May 21 08:48:28.611: INFO: Created: latency-svc-m2kt8
May 21 08:48:28.613: INFO: Got endpoints: latency-svc-tp7dr [1.74782136s]
May 21 08:48:28.620: INFO: Got endpoints: latency-svc-m2kt8 [1.694992617s]
May 21 08:48:28.769: INFO: Created: latency-svc-xdxxj
May 21 08:48:28.774: INFO: Got endpoints: latency-svc-xdxxj [1.494950896s]
May 21 08:48:28.834: INFO: Created: latency-svc-g7zzl
May 21 08:48:28.890: INFO: Got endpoints: latency-svc-g7zzl [1.533974945s]
May 21 08:48:28.925: INFO: Created: latency-svc-jzgh5
May 21 08:48:28.935: INFO: Got endpoints: latency-svc-jzgh5 [1.430535498s]
May 21 08:48:28.961: INFO: Created: latency-svc-dtb4r
May 21 08:48:28.972: INFO: Got endpoints: latency-svc-dtb4r [1.366842559s]
May 21 08:48:29.228: INFO: Created: latency-svc-pkfrl
May 21 08:48:29.301: INFO: Got endpoints: latency-svc-pkfrl [1.612826734s]
May 21 08:48:29.419: INFO: Created: latency-svc-fnzqb
May 21 08:48:29.439: INFO: Got endpoints: latency-svc-fnzqb [1.515475075s]
May 21 08:48:29.600: INFO: Created: latency-svc-c952s
May 21 08:48:29.623: INFO: Got endpoints: latency-svc-c952s [1.635316998s]
May 21 08:48:29.746: INFO: Created: latency-svc-lqgsk
May 21 08:48:29.752: INFO: Got endpoints: latency-svc-lqgsk [1.637925913s]
May 21 08:48:29.786: INFO: Created: latency-svc-2kdxc
May 21 08:48:29.799: INFO: Got endpoints: latency-svc-2kdxc [1.562447125s]
May 21 08:48:29.893: INFO: Created: latency-svc-z5ww2
May 21 08:48:29.898: INFO: Got endpoints: latency-svc-z5ww2 [1.643781143s]
May 21 08:48:29.928: INFO: Created: latency-svc-76464
May 21 08:48:29.942: INFO: Got endpoints: latency-svc-76464 [1.531167839s]
May 21 08:48:30.000: INFO: Created: latency-svc-pjvrx
May 21 08:48:30.039: INFO: Created: latency-svc-hqzlt
May 21 08:48:30.040: INFO: Got endpoints: latency-svc-pjvrx [1.612891262s]
May 21 08:48:30.071: INFO: Got endpoints: latency-svc-hqzlt [1.601860712s]
May 21 08:48:30.168: INFO: Created: latency-svc-9qbln
May 21 08:48:30.176: INFO: Got endpoints: latency-svc-9qbln [1.562850317s]
May 21 08:48:30.201: INFO: Created: latency-svc-shcsq
May 21 08:48:30.211: INFO: Got endpoints: latency-svc-shcsq [1.591277654s]
May 21 08:48:30.260: INFO: Created: latency-svc-n5wdx
May 21 08:48:30.305: INFO: Got endpoints: latency-svc-n5wdx [1.531400933s]
May 21 08:48:30.311: INFO: Created: latency-svc-kvncd
May 21 08:48:30.334: INFO: Got endpoints: latency-svc-kvncd [1.444014048s]
May 21 08:48:30.361: INFO: Created: latency-svc-dkcnx
May 21 08:48:30.369: INFO: Got endpoints: latency-svc-dkcnx [1.433694351s]
May 21 08:48:30.531: INFO: Created: latency-svc-cxjh9
May 21 08:48:30.629: INFO: Got endpoints: latency-svc-cxjh9 [1.656947072s]
May 21 08:48:30.630: INFO: Created: latency-svc-8qf64
May 21 08:48:30.732: INFO: Got endpoints: latency-svc-8qf64 [1.430705145s]
May 21 08:48:30.780: INFO: Created: latency-svc-wgqqc
May 21 08:48:30.798: INFO: Got endpoints: latency-svc-wgqqc [1.359011702s]
May 21 08:48:30.921: INFO: Created: latency-svc-p76c4
May 21 08:48:30.995: INFO: Got endpoints: latency-svc-p76c4 [1.372125942s]
May 21 08:48:30.998: INFO: Created: latency-svc-jmq9b
May 21 08:48:31.091: INFO: Got endpoints: latency-svc-jmq9b [1.339118848s]
May 21 08:48:31.167: INFO: Created: latency-svc-dg4gj
May 21 08:48:31.246: INFO: Got endpoints: latency-svc-dg4gj [1.446082862s]
May 21 08:48:31.246: INFO: Created: latency-svc-5mnsm
May 21 08:48:31.255: INFO: Got endpoints: latency-svc-5mnsm [1.357060455s]
May 21 08:48:31.312: INFO: Created: latency-svc-n987z
May 21 08:48:31.323: INFO: Got endpoints: latency-svc-n987z [1.381123257s]
May 21 08:48:31.405: INFO: Created: latency-svc-q2x5v
May 21 08:48:31.437: INFO: Got endpoints: latency-svc-q2x5v [1.397138197s]
May 21 08:48:31.438: INFO: Created: latency-svc-8rs9q
May 21 08:48:31.462: INFO: Got endpoints: latency-svc-8rs9q [1.390065782s]
May 21 08:48:31.485: INFO: Created: latency-svc-9sz8s
May 21 08:48:31.497: INFO: Got endpoints: latency-svc-9sz8s [1.321512376s]
May 21 08:48:31.575: INFO: Created: latency-svc-9p446
May 21 08:48:31.588: INFO: Got endpoints: latency-svc-9p446 [1.376631705s]
May 21 08:48:31.617: INFO: Created: latency-svc-lgwtj
May 21 08:48:31.628: INFO: Got endpoints: latency-svc-lgwtj [1.322507588s]
May 21 08:48:31.705: INFO: Created: latency-svc-gqrf7
May 21 08:48:31.771: INFO: Created: latency-svc-gqvlt
May 21 08:48:31.771: INFO: Got endpoints: latency-svc-gqrf7 [1.437455734s]
May 21 08:48:31.864: INFO: Got endpoints: latency-svc-gqvlt [1.495614248s]
May 21 08:48:31.897: INFO: Created: latency-svc-xd5rl
May 21 08:48:31.997: INFO: Got endpoints: latency-svc-xd5rl [1.367339041s]
May 21 08:48:32.023: INFO: Created: latency-svc-gkrt2
May 21 08:48:32.031: INFO: Got endpoints: latency-svc-gkrt2 [1.299419947s]
May 21 08:48:32.157: INFO: Created: latency-svc-xc9b6
May 21 08:48:32.201: INFO: Created: latency-svc-5fb45
May 21 08:48:32.201: INFO: Got endpoints: latency-svc-xc9b6 [1.40283584s]
May 21 08:48:32.231: INFO: Got endpoints: latency-svc-5fb45 [1.236429901s]
May 21 08:48:32.279: INFO: Created: latency-svc-6r25p
May 21 08:48:32.314: INFO: Got endpoints: latency-svc-6r25p [1.222943072s]
May 21 08:48:32.358: INFO: Created: latency-svc-92f79
May 21 08:48:32.422: INFO: Created: latency-svc-h9jzq
May 21 08:48:32.423: INFO: Got endpoints: latency-svc-92f79 [1.176852103s]
May 21 08:48:32.456: INFO: Got endpoints: latency-svc-h9jzq [1.201243926s]
May 21 08:48:32.563: INFO: Created: latency-svc-rssj6
May 21 08:48:32.608: INFO: Created: latency-svc-9tc25
May 21 08:48:32.609: INFO: Got endpoints: latency-svc-rssj6 [1.285817752s]
May 21 08:48:32.641: INFO: Got endpoints: latency-svc-9tc25 [1.203174583s]
May 21 08:48:32.779: INFO: Created: latency-svc-f5kq6
May 21 08:48:32.897: INFO: Got endpoints: latency-svc-f5kq6 [1.435276144s]
May 21 08:48:32.905: INFO: Created: latency-svc-w5d7v
May 21 08:48:32.925: INFO: Got endpoints: latency-svc-w5d7v [1.42741973s]
May 21 08:48:32.979: INFO: Created: latency-svc-gzjvf
May 21 08:48:33.058: INFO: Got endpoints: latency-svc-gzjvf [1.469640084s]
May 21 08:48:33.272: INFO: Created: latency-svc-r9t5c
May 21 08:48:33.315: INFO: Got endpoints: latency-svc-r9t5c [1.687186804s]
May 21 08:48:33.348: INFO: Created: latency-svc-jlbhg
May 21 08:48:33.358: INFO: Got endpoints: latency-svc-jlbhg [1.586306926s]
May 21 08:48:33.450: INFO: Created: latency-svc-r8rhf
May 21 08:48:33.490: INFO: Created: latency-svc-qblx4
May 21 08:48:33.491: INFO: Got endpoints: latency-svc-r8rhf [1.626745928s]
May 21 08:48:33.525: INFO: Got endpoints: latency-svc-qblx4 [1.528250987s]
May 21 08:48:33.633: INFO: Created: latency-svc-tdcsg
May 21 08:48:33.673: INFO: Got endpoints: latency-svc-tdcsg [1.64240092s]
May 21 08:48:33.678: INFO: Created: latency-svc-dfpqp
May 21 08:48:33.743: INFO: Got endpoints: latency-svc-dfpqp [1.54135873s]
May 21 08:48:33.791: INFO: Created: latency-svc-zgpfn
May 21 08:48:33.800: INFO: Got endpoints: latency-svc-zgpfn [1.568328977s]
May 21 08:48:33.941: INFO: Created: latency-svc-q26th
May 21 08:48:33.990: INFO: Got endpoints: latency-svc-q26th [1.675920004s]
May 21 08:48:33.990: INFO: Created: latency-svc-xpbct
May 21 08:48:34.048: INFO: Got endpoints: latency-svc-xpbct [1.625152282s]
May 21 08:48:34.073: INFO: Created: latency-svc-2b7px
May 21 08:48:34.082: INFO: Got endpoints: latency-svc-2b7px [1.62501702s]
May 21 08:48:34.128: INFO: Created: latency-svc-blp7c
May 21 08:48:34.180: INFO: Got endpoints: latency-svc-blp7c [1.5709894s]
May 21 08:48:34.224: INFO: Created: latency-svc-9k7ss
May 21 08:48:34.266: INFO: Got endpoints: latency-svc-9k7ss [1.625068031s]
May 21 08:48:34.267: INFO: Created: latency-svc-kvhmw
May 21 08:48:34.332: INFO: Got endpoints: latency-svc-kvhmw [1.435501143s]
May 21 08:48:34.366: INFO: Created: latency-svc-cwn2l
May 21 08:48:34.401: INFO: Created: latency-svc-mnfrn
May 21 08:48:34.401: INFO: Got endpoints: latency-svc-cwn2l [1.47657256s]
May 21 08:48:34.445: INFO: Got endpoints: latency-svc-mnfrn [1.387403865s]
May 21 08:48:34.487: INFO: Created: latency-svc-b2xvf
May 21 08:48:34.515: INFO: Got endpoints: latency-svc-b2xvf [1.199747957s]
May 21 08:48:34.636: INFO: Created: latency-svc-mbqgv
May 21 08:48:34.679: INFO: Got endpoints: latency-svc-mbqgv [1.321058332s]
May 21 08:48:34.767: INFO: Created: latency-svc-mp74j
May 21 08:48:34.767: INFO: Got endpoints: latency-svc-mp74j [1.275903417s]
May 21 08:48:34.790: INFO: Created: latency-svc-qz989
May 21 08:48:34.870: INFO: Created: latency-svc-7gm8n
May 21 08:48:34.871: INFO: Got endpoints: latency-svc-qz989 [1.345514047s]
May 21 08:48:34.888: INFO: Got endpoints: latency-svc-7gm8n [1.214082396s]
May 21 08:48:34.942: INFO: Created: latency-svc-zhbwp
May 21 08:48:34.967: INFO: Got endpoints: latency-svc-zhbwp [1.224160651s]
May 21 08:48:35.072: INFO: Created: latency-svc-s7wrd
May 21 08:48:35.175: INFO: Created: latency-svc-q9cf8
May 21 08:48:35.176: INFO: Got endpoints: latency-svc-s7wrd [1.376010805s]
May 21 08:48:35.293: INFO: Got endpoints: latency-svc-q9cf8 [1.302762326s]
May 21 08:48:35.313: INFO: Created: latency-svc-wzzft
May 21 08:48:35.320: INFO: Got endpoints: latency-svc-wzzft [1.271750599s]
May 21 08:48:35.358: INFO: Created: latency-svc-c88gn
May 21 08:48:35.505: INFO: Created: latency-svc-w82xx
May 21 08:48:35.506: INFO: Got endpoints: latency-svc-c88gn [1.423555021s]
May 21 08:48:35.538: INFO: Got endpoints: latency-svc-w82xx [1.358642065s]
May 21 08:48:35.650: INFO: Created: latency-svc-bxlkd
May 21 08:48:35.662: INFO: Got endpoints: latency-svc-bxlkd [1.395956442s]
May 21 08:48:35.708: INFO: Created: latency-svc-sj8mr
May 21 08:48:35.720: INFO: Got endpoints: latency-svc-sj8mr [1.387259959s]
May 21 08:48:35.822: INFO: Created: latency-svc-5whqg
May 21 08:48:35.970: INFO: Got endpoints: latency-svc-5whqg [1.568491607s]
May 21 08:48:36.009: INFO: Created: latency-svc-jc7f2
May 21 08:48:36.019: INFO: Got endpoints: latency-svc-jc7f2 [1.573546083s]
May 21 08:48:36.041: INFO: Created: latency-svc-vmj5h
May 21 08:48:36.054: INFO: Got endpoints: latency-svc-vmj5h [1.538041978s]
May 21 08:48:36.147: INFO: Created: latency-svc-wmzmb
May 21 08:48:36.176: INFO: Got endpoints: latency-svc-wmzmb [1.497217028s]
May 21 08:48:36.182: INFO: Created: latency-svc-hk95p
May 21 08:48:36.196: INFO: Got endpoints: latency-svc-hk95p [1.428831157s]
May 21 08:48:36.221: INFO: Created: latency-svc-nrs99
May 21 08:48:36.229: INFO: Got endpoints: latency-svc-nrs99 [1.358298463s]
May 21 08:48:36.279: INFO: Created: latency-svc-mnqgq
May 21 08:48:36.313: INFO: Got endpoints: latency-svc-mnqgq [1.425461635s]
May 21 08:48:36.314: INFO: Created: latency-svc-cv5xq
May 21 08:48:36.337: INFO: Got endpoints: latency-svc-cv5xq [1.370056392s]
May 21 08:48:36.360: INFO: Created: latency-svc-67bcn
May 21 08:48:36.371: INFO: Got endpoints: latency-svc-67bcn [1.195546969s]
May 21 08:48:36.413: INFO: Created: latency-svc-9vggm
May 21 08:48:36.508: INFO: Got endpoints: latency-svc-9vggm [1.215621282s]
May 21 08:48:36.563: INFO: Created: latency-svc-zgpjp
May 21 08:48:36.593: INFO: Got endpoints: latency-svc-zgpjp [1.273036766s]
May 21 08:48:36.595: INFO: Created: latency-svc-qnvn6
May 21 08:48:36.604: INFO: Got endpoints: latency-svc-qnvn6 [1.098035546s]
May 21 08:48:36.662: INFO: Created: latency-svc-f5246
May 21 08:48:36.705: INFO: Got endpoints: latency-svc-f5246 [1.166652432s]
May 21 08:48:36.710: INFO: Created: latency-svc-rp7dn
May 21 08:48:36.728: INFO: Got endpoints: latency-svc-rp7dn [1.065946443s]
May 21 08:48:36.768: INFO: Created: latency-svc-bw26v
May 21 08:48:36.790: INFO: Got endpoints: latency-svc-bw26v [1.070330688s]
May 21 08:48:36.891: INFO: Created: latency-svc-f7wtw
May 21 08:48:36.953: INFO: Got endpoints: latency-svc-f7wtw [982.530342ms]
May 21 08:48:36.955: INFO: Created: latency-svc-pfg5z
May 21 08:48:36.991: INFO: Got endpoints: latency-svc-pfg5z [971.440638ms]
May 21 08:48:37.137: INFO: Created: latency-svc-hz4pt
May 21 08:48:37.154: INFO: Got endpoints: latency-svc-hz4pt [1.100830833s]
May 21 08:48:37.301: INFO: Created: latency-svc-966wr
May 21 08:48:37.314: INFO: Got endpoints: latency-svc-966wr [1.138246449s]
May 21 08:48:37.413: INFO: Created: latency-svc-xh6b5
May 21 08:48:37.451: INFO: Created: latency-svc-xxqqc
May 21 08:48:37.451: INFO: Got endpoints: latency-svc-xh6b5 [1.254592104s]
May 21 08:48:37.478: INFO: Got endpoints: latency-svc-xxqqc [1.249238586s]
May 21 08:48:37.504: INFO: Created: latency-svc-qmff7
May 21 08:48:37.511: INFO: Got endpoints: latency-svc-qmff7 [1.197633148s]
May 21 08:48:37.564: INFO: Created: latency-svc-6ls8b
May 21 08:48:37.596: INFO: Got endpoints: latency-svc-6ls8b [1.258500155s]
May 21 08:48:37.596: INFO: Created: latency-svc-b56wn
May 21 08:48:37.647: INFO: Got endpoints: latency-svc-b56wn [1.275854635s]
May 21 08:48:37.688: INFO: Created: latency-svc-w6zfw
May 21 08:48:37.716: INFO: Got endpoints: latency-svc-w6zfw [1.20717501s]
May 21 08:48:37.746: INFO: Created: latency-svc-t6nxh
May 21 08:48:37.774: INFO: Got endpoints: latency-svc-t6nxh [1.181586388s]
May 21 08:48:37.829: INFO: Created: latency-svc-hxzwh
May 21 08:48:37.870: INFO: Got endpoints: latency-svc-hxzwh [1.265452058s]
May 21 08:48:37.871: INFO: Created: latency-svc-f4fvg
May 21 08:48:37.896: INFO: Got endpoints: latency-svc-f4fvg [1.191089913s]
May 21 08:48:37.921: INFO: Created: latency-svc-pqdkq
May 21 08:48:37.993: INFO: Got endpoints: latency-svc-pqdkq [1.265506635s]
May 21 08:48:37.998: INFO: Created: latency-svc-kxh7d
May 21 08:48:38.038: INFO: Created: latency-svc-464rq
May 21 08:48:38.038: INFO: Got endpoints: latency-svc-kxh7d [1.247600043s]
May 21 08:48:38.078: INFO: Created: latency-svc-64tf9
May 21 08:48:38.079: INFO: Got endpoints: latency-svc-464rq [1.12627099s]
May 21 08:48:38.087: INFO: Got endpoints: latency-svc-64tf9 [1.096413713s]
May 21 08:48:38.154: INFO: Created: latency-svc-tf27l
May 21 08:48:38.183: INFO: Got endpoints: latency-svc-tf27l [1.028777345s]
May 21 08:48:38.247: INFO: Created: latency-svc-vls64
May 21 08:48:38.328: INFO: Got endpoints: latency-svc-vls64 [1.01342945s]
May 21 08:48:38.329: INFO: Created: latency-svc-gxhgc
May 21 08:48:38.368: INFO: Got endpoints: latency-svc-gxhgc [916.804115ms]
May 21 08:48:38.411: INFO: Created: latency-svc-zlxlk
May 21 08:48:38.463: INFO: Got endpoints: latency-svc-zlxlk [984.032681ms]
May 21 08:48:38.476: INFO: Created: latency-svc-h77cd
May 21 08:48:38.487: INFO: Got endpoints: latency-svc-h77cd [975.690308ms]
May 21 08:48:38.719: INFO: Created: latency-svc-p4sx4
May 21 08:48:38.777: INFO: Got endpoints: latency-svc-p4sx4 [1.181092495s]
May 21 08:48:38.780: INFO: Created: latency-svc-m4d2b
May 21 08:48:38.812: INFO: Got endpoints: latency-svc-m4d2b [1.164538667s]
May 21 08:48:38.897: INFO: Created: latency-svc-hlv7q
May 21 08:48:38.925: INFO: Got endpoints: latency-svc-hlv7q [1.208864398s]
May 21 08:48:39.041: INFO: Created: latency-svc-h2cd5
May 21 08:48:39.097: INFO: Created: latency-svc-zzrk8
May 21 08:48:39.101: INFO: Got endpoints: latency-svc-h2cd5 [1.326183313s]
May 21 08:48:39.133: INFO: Got endpoints: latency-svc-zzrk8 [1.263316045s]
May 21 08:48:39.233: INFO: Created: latency-svc-xxkdv
May 21 08:48:39.272: INFO: Got endpoints: latency-svc-xxkdv [1.37543562s]
May 21 08:48:39.273: INFO: Created: latency-svc-wz7mt
May 21 08:48:39.302: INFO: Got endpoints: latency-svc-wz7mt [1.308882098s]
May 21 08:48:39.330: INFO: Created: latency-svc-hm2pk
May 21 08:48:39.413: INFO: Got endpoints: latency-svc-hm2pk [1.375239983s]
May 21 08:48:39.437: INFO: Created: latency-svc-bg9v8
May 21 08:48:39.505: INFO: Got endpoints: latency-svc-bg9v8 [1.425957591s]
May 21 08:48:39.506: INFO: Created: latency-svc-2nmvz
May 21 08:48:39.548: INFO: Got endpoints: latency-svc-2nmvz [1.460455966s]
May 21 08:48:39.571: INFO: Created: latency-svc-zjnbx
May 21 08:48:39.599: INFO: Got endpoints: latency-svc-zjnbx [1.415913347s]
May 21 08:48:39.629: INFO: Created: latency-svc-7pxsp
May 21 08:48:39.731: INFO: Created: latency-svc-cjkbf
May 21 08:48:39.731: INFO: Got endpoints: latency-svc-7pxsp [1.403142864s]
May 21 08:48:39.813: INFO: Created: latency-svc-srk7b
May 21 08:48:39.814: INFO: Got endpoints: latency-svc-cjkbf [1.445806847s]
May 21 08:48:39.898: INFO: Created: latency-svc-hfsfq
May 21 08:48:39.899: INFO: Got endpoints: latency-svc-srk7b [1.436697448s]
May 21 08:48:39.980: INFO: Created: latency-svc-96xdn
May 21 08:48:39.980: INFO: Got endpoints: latency-svc-hfsfq [1.493599133s]
May 21 08:48:40.129: INFO: Got endpoints: latency-svc-96xdn [1.351770137s]
May 21 08:48:40.136: INFO: Created: latency-svc-99ttn
May 21 08:48:40.151: INFO: Got endpoints: latency-svc-99ttn [251.426476ms]
May 21 08:48:40.196: INFO: Created: latency-svc-5kkzh
May 21 08:48:40.273: INFO: Got endpoints: latency-svc-5kkzh [1.461037097s]
May 21 08:48:40.312: INFO: Created: latency-svc-kvfsv
May 21 08:48:40.329: INFO: Got endpoints: latency-svc-kvfsv [1.403884653s]
May 21 08:48:40.431: INFO: Created: latency-svc-v9224
May 21 08:48:40.455: INFO: Created: latency-svc-wj82f
May 21 08:48:40.455: INFO: Got endpoints: latency-svc-v9224 [1.354175071s]
May 21 08:48:40.479: INFO: Got endpoints: latency-svc-wj82f [1.346214929s]
May 21 08:48:40.508: INFO: Created: latency-svc-s66rh
May 21 08:48:40.572: INFO: Got endpoints: latency-svc-s66rh [1.300166433s]
May 21 08:48:40.577: INFO: Created: latency-svc-scxpv
May 21 08:48:40.588: INFO: Got endpoints: latency-svc-scxpv [1.285310958s]
May 21 08:48:40.619: INFO: Created: latency-svc-nxxp7
May 21 08:48:40.631: INFO: Got endpoints: latency-svc-nxxp7 [1.217289876s]
May 21 08:48:40.662: INFO: Created: latency-svc-gsrj8
May 21 08:48:40.672: INFO: Got endpoints: latency-svc-gsrj8 [1.167306214s]
May 21 08:48:40.727: INFO: Created: latency-svc-grjgj
May 21 08:48:40.795: INFO: Got endpoints: latency-svc-grjgj [1.247709671s]
May 21 08:48:40.797: INFO: Created: latency-svc-rjcv6
May 21 08:48:40.846: INFO: Got endpoints: latency-svc-rjcv6 [1.246749591s]
May 21 08:48:40.862: INFO: Created: latency-svc-zftds
May 21 08:48:40.871: INFO: Got endpoints: latency-svc-zftds [1.139433887s]
May 21 08:48:41.056: INFO: Created: latency-svc-xnwrl
May 21 08:48:41.120: INFO: Got endpoints: latency-svc-xnwrl [1.306168097s]
May 21 08:48:41.122: INFO: Created: latency-svc-8hb9f
May 21 08:48:41.215: INFO: Got endpoints: latency-svc-8hb9f [1.234462663s]
May 21 08:48:41.245: INFO: Created: latency-svc-jwwnq
May 21 08:48:41.382: INFO: Got endpoints: latency-svc-jwwnq [1.253416239s]
May 21 08:48:41.436: INFO: Created: latency-svc-mxcww
May 21 08:48:41.478: INFO: Got endpoints: latency-svc-mxcww [1.326666983s]
May 21 08:48:41.550: INFO: Created: latency-svc-gxm92
May 21 08:48:41.559: INFO: Got endpoints: latency-svc-gxm92 [1.285951624s]
May 21 08:48:41.681: INFO: Created: latency-svc-jgzm8
May 21 08:48:41.746: INFO: Got endpoints: latency-svc-jgzm8 [1.41752488s]
May 21 08:48:41.747: INFO: Created: latency-svc-tn5vf
May 21 08:48:41.879: INFO: Got endpoints: latency-svc-tn5vf [1.423800234s]
May 21 08:48:41.948: INFO: Created: latency-svc-phlhf
May 21 08:48:42.001: INFO: Got endpoints: latency-svc-phlhf [1.52125713s]
May 21 08:48:42.013: INFO: Created: latency-svc-kwqm6
May 21 08:48:42.048: INFO: Got endpoints: latency-svc-kwqm6 [1.475631756s]
May 21 08:48:42.187: INFO: Created: latency-svc-mvj5s
May 21 08:48:42.222: INFO: Created: latency-svc-zh4kz
May 21 08:48:42.223: INFO: Got endpoints: latency-svc-mvj5s [1.6349154s]
May 21 08:48:42.270: INFO: Got endpoints: latency-svc-zh4kz [1.639668992s]
May 21 08:48:42.317: INFO: Created: latency-svc-z4dvh
May 21 08:48:42.354: INFO: Got endpoints: latency-svc-z4dvh [1.681142998s]
May 21 08:48:42.379: INFO: Created: latency-svc-6jjtc
May 21 08:48:42.389: INFO: Got endpoints: latency-svc-6jjtc [1.593515079s]
May 21 08:48:42.463: INFO: Created: latency-svc-dsbhb
May 21 08:48:42.488: INFO: Got endpoints: latency-svc-dsbhb [1.641541442s]
May 21 08:48:42.489: INFO: Created: latency-svc-kr4kq
May 21 08:48:42.520: INFO: Got endpoints: latency-svc-kr4kq [1.649072319s]
May 21 08:48:42.550: INFO: Created: latency-svc-k5scf
May 21 08:48:42.554: INFO: Got endpoints: latency-svc-k5scf [1.433657797s]
May 21 08:48:42.672: INFO: Created: latency-svc-69mdt
May 21 08:48:42.899: INFO: Got endpoints: latency-svc-69mdt [1.683670023s]
May 21 08:48:42.900: INFO: Created: latency-svc-4mdsc
May 21 08:48:42.912: INFO: Got endpoints: latency-svc-4mdsc [1.529686152s]
May 21 08:48:43.107: INFO: Created: latency-svc-khtgd
May 21 08:48:43.246: INFO: Got endpoints: latency-svc-khtgd [1.768091533s]
May 21 08:48:43.260: INFO: Created: latency-svc-t6ps9
May 21 08:48:43.300: INFO: Got endpoints: latency-svc-t6ps9 [1.741082888s]
May 21 08:48:43.429: INFO: Created: latency-svc-qkpr2
May 21 08:48:43.440: INFO: Got endpoints: latency-svc-qkpr2 [1.693181838s]
May 21 08:48:43.479: INFO: Created: latency-svc-7ptsm
May 21 08:48:43.491: INFO: Got endpoints: latency-svc-7ptsm [1.611422101s]
May 21 08:48:43.547: INFO: Created: latency-svc-jvzlg
May 21 08:48:43.566: INFO: Got endpoints: latency-svc-jvzlg [1.564864395s]
May 21 08:48:43.637: INFO: Created: latency-svc-n94w7
May 21 08:48:43.689: INFO: Got endpoints: latency-svc-n94w7 [1.641282129s]
May 21 08:48:43.709: INFO: Created: latency-svc-zhs7w
May 21 08:48:43.733: INFO: Got endpoints: latency-svc-zhs7w [1.510408502s]
May 21 08:48:43.759: INFO: Created: latency-svc-vgnsw
May 21 08:48:43.766: INFO: Got endpoints: latency-svc-vgnsw [1.495162122s]
May 21 08:48:43.831: INFO: Created: latency-svc-5454h
May 21 08:48:43.889: INFO: Got endpoints: latency-svc-5454h [1.535342429s]
May 21 08:48:43.895: INFO: Created: latency-svc-pwtzd
May 21 08:48:43.965: INFO: Got endpoints: latency-svc-pwtzd [1.574999368s]
May 21 08:48:44.024: INFO: Created: latency-svc-q56rd
May 21 08:48:44.031: INFO: Got endpoints: latency-svc-q56rd [1.542948902s]
May 21 08:48:44.115: INFO: Created: latency-svc-h2jx6
May 21 08:48:44.160: INFO: Got endpoints: latency-svc-h2jx6 [1.640479696s]
May 21 08:48:44.161: INFO: Created: latency-svc-mbcpf
May 21 08:48:44.167: INFO: Got endpoints: latency-svc-mbcpf [1.612501652s]
May 21 08:48:44.194: INFO: Created: latency-svc-cmhz9
May 21 08:48:44.252: INFO: Created: latency-svc-rlpd8
May 21 08:48:44.252: INFO: Got endpoints: latency-svc-cmhz9 [1.353694892s]
May 21 08:48:44.261: INFO: Got endpoints: latency-svc-rlpd8 [1.348537085s]
May 21 08:48:44.294: INFO: Created: latency-svc-cnkwm
May 21 08:48:44.397: INFO: Created: latency-svc-28bks
May 21 08:48:44.398: INFO: Got endpoints: latency-svc-cnkwm [1.152564791s]
May 21 08:48:44.432: INFO: Got endpoints: latency-svc-28bks [1.130973902s]
May 21 08:48:44.469: INFO: Created: latency-svc-x8hgp
May 21 08:48:44.480: INFO: Got endpoints: latency-svc-x8hgp [1.04045294s]
May 21 08:48:44.537: INFO: Created: latency-svc-gqlwj
May 21 08:48:44.550: INFO: Got endpoints: latency-svc-gqlwj [1.059411896s]
May 21 08:48:44.586: INFO: Created: latency-svc-z9vr2
May 21 08:48:44.597: INFO: Got endpoints: latency-svc-z9vr2 [1.031015552s]
May 21 08:48:44.697: INFO: Created: latency-svc-9dg2g
May 21 08:48:44.730: INFO: Got endpoints: latency-svc-9dg2g [1.040866015s]
May 21 08:48:44.731: INFO: Latencies: [162.698807ms 191.287703ms 251.426476ms 439.738626ms 504.055389ms 601.517092ms 680.721259ms 916.804115ms 923.098954ms 971.440638ms 975.690308ms 982.530342ms 983.154767ms 984.032681ms 1.01342945s 1.028777345s 1.031015552s 1.04045294s 1.040866015s 1.059411896s 1.065946443s 1.070330688s 1.096413713s 1.098035546s 1.100830833s 1.12627099s 1.130973902s 1.138246449s 1.139433887s 1.152564791s 1.164538667s 1.166652432s 1.167306214s 1.176852103s 1.181092495s 1.181586388s 1.191089913s 1.195546969s 1.197633148s 1.199747957s 1.201243926s 1.203174583s 1.20717501s 1.208864398s 1.214082396s 1.215621282s 1.217289876s 1.222943072s 1.224160651s 1.234462663s 1.236429901s 1.246749591s 1.247600043s 1.247709671s 1.249238586s 1.253416239s 1.254592104s 1.258500155s 1.263316045s 1.265452058s 1.265506635s 1.271750599s 1.273036766s 1.275854635s 1.275903417s 1.285310958s 1.285817752s 1.285951624s 1.299419947s 1.300166433s 1.302762326s 1.306168097s 1.308882098s 1.321058332s 1.321512376s 1.322507588s 1.326183313s 1.326666983s 1.337268504s 1.339118848s 1.345514047s 1.346214929s 1.348537085s 1.351770137s 1.353694892s 1.354175071s 1.357060455s 1.358298463s 1.358642065s 1.359011702s 1.366842559s 1.367339041s 1.370056392s 1.372125942s 1.375239983s 1.37543562s 1.376010805s 1.376631705s 1.381123257s 1.387259959s 1.387403865s 1.390065782s 1.395956442s 1.397138197s 1.40283584s 1.403142864s 1.403884653s 1.413692149s 1.415913347s 1.41752488s 1.423555021s 1.423800234s 1.425461635s 1.425957591s 1.42741973s 1.428831157s 1.430535498s 1.430705145s 1.433657797s 1.433694351s 1.435276144s 1.435501143s 1.436697448s 1.437455734s 1.444014048s 1.445806847s 1.446082862s 1.460455966s 1.461037097s 1.469640084s 1.475631756s 1.47657256s 1.493599133s 1.494950896s 1.495162122s 1.495614248s 1.497217028s 1.510408502s 1.515475075s 1.52125713s 1.528250987s 1.529686152s 1.531167839s 1.531400933s 1.533974945s 1.535342429s 1.538041978s 1.54135873s 1.542948902s 1.562447125s 1.562487549s 1.562850317s 1.564864395s 1.568328977s 1.568491607s 1.5709894s 1.573546083s 1.574999368s 1.586306926s 1.591277654s 1.593515079s 1.601860712s 1.611422101s 1.612501652s 1.612826734s 1.612891262s 1.62501702s 1.625068031s 1.625152282s 1.626745928s 1.6349154s 1.635316998s 1.637925913s 1.639668992s 1.640479696s 1.641282129s 1.641541442s 1.64240092s 1.643781143s 1.649072319s 1.656947072s 1.663265222s 1.675920004s 1.681142998s 1.683670023s 1.687186804s 1.693181838s 1.694992617s 1.741082888s 1.745855859s 1.74782136s 1.768091533s 1.847067478s 1.873153642s 1.884251124s 1.964748418s 1.98148577s 2.009688825s 2.045216173s 2.104501544s]
May 21 08:48:44.731: INFO: 50 %ile: 1.387403865s
May 21 08:48:44.731: INFO: 90 %ile: 1.656947072s
May 21 08:48:44.731: INFO: 99 %ile: 2.045216173s
May 21 08:48:44.731: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:48:44.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9286" for this suite.

• [SLOW TEST:23.263 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":128,"skipped":2556,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:48:44.766: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
May 21 08:48:44.937: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
May 21 08:48:45.970: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 21 08:48:48.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183725, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bd464c496\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:48:50.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183725, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bd464c496\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:48:52.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183725, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bd464c496\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:48:54.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183725, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bd464c496\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:48:57.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183725, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bd464c496\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:48:58.693: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183725, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bd464c496\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:49:00.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183726, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183725, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bd464c496\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:49:11.487: INFO: Waited 9.311164838s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:49:13.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1530" for this suite.

• [SLOW TEST:28.550 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":129,"skipped":2580,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:49:13.316: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 08:49:14.740: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 08:49:16.908: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183754, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183754, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183754, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183754, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:49:18.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183754, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183754, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183754, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183754, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 08:49:22.072: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:49:22.109: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4981-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:49:23.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5981" for this suite.
STEP: Destroying namespace "webhook-5981-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:11.245 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":130,"skipped":2582,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:49:24.561: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3578
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3578
STEP: creating replication controller externalsvc in namespace services-3578
I0521 08:49:25.260122      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3578, replica count: 2
I0521 08:49:28.310849      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 08:49:31.311238      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 21 08:49:31.587: INFO: Creating new exec pod
May 21 08:49:35.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-3578 exec execpodr2k68 -- /bin/sh -x -c nslookup nodeport-service.services-3578.svc.cluster.local'
May 21 08:49:39.319: INFO: stderr: "+ nslookup nodeport-service.services-3578.svc.cluster.local\n"
May 21 08:49:39.319: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3578.svc.cluster.local\tcanonical name = externalsvc.services-3578.svc.cluster.local.\nName:\texternalsvc.services-3578.svc.cluster.local\nAddress: 10.96.37.100\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3578, will wait for the garbage collector to delete the pods
May 21 08:49:39.408: INFO: Deleting ReplicationController externalsvc took: 31.321693ms
May 21 08:49:39.508: INFO: Terminating ReplicationController externalsvc pods took: 100.428399ms
May 21 08:49:50.463: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:49:50.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3578" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:26.030 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":131,"skipped":2602,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:49:50.592: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 08:49:50.760: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d42ce05a-62f9-4ad5-9f29-85b79df8a67d" in namespace "projected-8858" to be "Succeeded or Failed"
May 21 08:49:50.800: INFO: Pod "downwardapi-volume-d42ce05a-62f9-4ad5-9f29-85b79df8a67d": Phase="Pending", Reason="", readiness=false. Elapsed: 39.267141ms
May 21 08:49:52.813: INFO: Pod "downwardapi-volume-d42ce05a-62f9-4ad5-9f29-85b79df8a67d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052592118s
May 21 08:49:54.850: INFO: Pod "downwardapi-volume-d42ce05a-62f9-4ad5-9f29-85b79df8a67d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.090019847s
STEP: Saw pod success
May 21 08:49:54.850: INFO: Pod "downwardapi-volume-d42ce05a-62f9-4ad5-9f29-85b79df8a67d" satisfied condition "Succeeded or Failed"
May 21 08:49:54.956: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-d42ce05a-62f9-4ad5-9f29-85b79df8a67d container client-container: <nil>
STEP: delete the pod
May 21 08:49:55.035: INFO: Waiting for pod downwardapi-volume-d42ce05a-62f9-4ad5-9f29-85b79df8a67d to disappear
May 21 08:49:55.120: INFO: Pod downwardapi-volume-d42ce05a-62f9-4ad5-9f29-85b79df8a67d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:49:55.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8858" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":132,"skipped":2603,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:49:55.146: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:50:06.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9600" for this suite.

• [SLOW TEST:11.517 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":133,"skipped":2620,"failed":0}
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:50:06.663: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
May 21 08:50:06.821: INFO: created test-event-1
May 21 08:50:06.827: INFO: created test-event-2
May 21 08:50:06.843: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 21 08:50:06.851: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 21 08:50:06.889: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:50:06.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2118" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":134,"skipped":2620,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:50:06.931: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:50:38.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8615" for this suite.

• [SLOW TEST:31.594 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":135,"skipped":2633,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:50:38.527: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
May 21 08:50:38.791: INFO: Waiting up to 5m0s for pod "var-expansion-f9171527-132d-4896-a8b9-92e3d7efdd6b" in namespace "var-expansion-2297" to be "Succeeded or Failed"
May 21 08:50:38.799: INFO: Pod "var-expansion-f9171527-132d-4896-a8b9-92e3d7efdd6b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0249ms
May 21 08:50:40.815: INFO: Pod "var-expansion-f9171527-132d-4896-a8b9-92e3d7efdd6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024398613s
May 21 08:50:42.947: INFO: Pod "var-expansion-f9171527-132d-4896-a8b9-92e3d7efdd6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.156833819s
STEP: Saw pod success
May 21 08:50:42.948: INFO: Pod "var-expansion-f9171527-132d-4896-a8b9-92e3d7efdd6b" satisfied condition "Succeeded or Failed"
May 21 08:50:42.956: INFO: Trying to get logs from node k8s-03 pod var-expansion-f9171527-132d-4896-a8b9-92e3d7efdd6b container dapi-container: <nil>
STEP: delete the pod
May 21 08:50:43.018: INFO: Waiting for pod var-expansion-f9171527-132d-4896-a8b9-92e3d7efdd6b to disappear
May 21 08:50:43.023: INFO: Pod var-expansion-f9171527-132d-4896-a8b9-92e3d7efdd6b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:50:43.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2297" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2693,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:50:43.038: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:50:43.316: INFO: Creating deployment "webserver-deployment"
May 21 08:50:43.322: INFO: Waiting for observed generation 1
May 21 08:50:45.343: INFO: Waiting for all required pods to come up
May 21 08:50:45.355: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 21 08:50:55.385: INFO: Waiting for deployment "webserver-deployment" to complete
May 21 08:50:55.397: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 21 08:50:55.420: INFO: Updating deployment webserver-deployment
May 21 08:50:55.420: INFO: Waiting for observed generation 2
May 21 08:50:57.608: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 21 08:50:57.616: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 21 08:50:57.651: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 21 08:50:58.088: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 21 08:50:58.088: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 21 08:50:58.095: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 21 08:50:58.146: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 21 08:50:58.146: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 21 08:50:58.179: INFO: Updating deployment webserver-deployment
May 21 08:50:58.179: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 21 08:50:58.493: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 21 08:51:00.654: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 08:51:00.985: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1088  6b8a458b-c6de-4952-8af9-920ce453509c 39562 3 2021-05-21 08:50:43 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 08:50:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 08:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008405d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-21 08:50:58 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-21 08:50:59 +0000 UTC,LastTransitionTime:2021-05-21 08:50:43 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 21 08:51:01.184: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1088  431aefd5-93c2-4489-a262-d2bb629f7977 39560 3 2021-05-21 08:50:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6b8a458b-c6de-4952-8af9-920ce453509c 0xc00309a5b7 0xc00309a5b8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 08:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b8a458b-c6de-4952-8af9-920ce453509c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00309a638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 08:51:01.184: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 21 08:51:01.188: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-1088  41a750db-90d1-4059-a77e-dc8d0e839f4d 39543 3 2021-05-21 08:50:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6b8a458b-c6de-4952-8af9-920ce453509c 0xc00309a697 0xc00309a698}] []  [{kube-controller-manager Update apps/v1 2021-05-21 08:50:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b8a458b-c6de-4952-8af9-920ce453509c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00309a708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 21 08:51:01.295: INFO: Pod "webserver-deployment-795d758f88-4dlwh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4dlwh webserver-deployment-795d758f88- deployment-1088  4445f1e9-860a-43a2-8291-32b82faefcf9 39566 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309acc7 0xc00309acc8}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.295: INFO: Pod "webserver-deployment-795d758f88-5jdq6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5jdq6 webserver-deployment-795d758f88- deployment-1088  c54afacc-57b4-4090-bd38-1e5edf7ef66b 39449 0 2021-05-21 08:50:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.179.41/32 cni.projectcalico.org/podIPs:10.100.179.41/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309ae90 0xc00309ae91}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 08:50:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.296: INFO: Pod "webserver-deployment-795d758f88-b6ztp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-b6ztp webserver-deployment-795d758f88- deployment-1088  6bbc187c-ed9c-41ad-93f0-0755ce761f63 39601 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309b050 0xc00309b051}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:51:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.296: INFO: Pod "webserver-deployment-795d758f88-g5hw4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-g5hw4 webserver-deployment-795d758f88- deployment-1088  33e23545-b1a9-480f-a966-fa57cc3133d1 39581 0 2021-05-21 08:50:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.165.193/32 cni.projectcalico.org/podIPs:10.100.165.193/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309b1f0 0xc00309b1f1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 08:50:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.297: INFO: Pod "webserver-deployment-795d758f88-k68qq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k68qq webserver-deployment-795d758f88- deployment-1088  a3ef07db-a709-4c25-b406-fa838f034ebb 39576 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309b3c0 0xc00309b3c1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.298: INFO: Pod "webserver-deployment-795d758f88-kdd8z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kdd8z webserver-deployment-795d758f88- deployment-1088  308599fb-4a9f-4bda-bf7a-c393806d573c 39577 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309b570 0xc00309b571}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.298: INFO: Pod "webserver-deployment-795d758f88-l7s7t" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-l7s7t webserver-deployment-795d758f88- deployment-1088  75ed516f-e952-42e2-9a55-5f2f3db7ed81 39467 0 2021-05-21 08:50:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.165.197/32 cni.projectcalico.org/podIPs:10.100.165.197/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309b710 0xc00309b711}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 08:50:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.299: INFO: Pod "webserver-deployment-795d758f88-lgbhj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lgbhj webserver-deployment-795d758f88- deployment-1088  86f6b2ff-e4dc-4811-82b0-b4794b437129 39603 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309b8d0 0xc00309b8d1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:51:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.299: INFO: Pod "webserver-deployment-795d758f88-mfm8k" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mfm8k webserver-deployment-795d758f88- deployment-1088  0d2b1add-4805-4524-875c-c0a496ea17b2 39591 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309ba70 0xc00309ba71}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:51:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.300: INFO: Pod "webserver-deployment-795d758f88-mrgqk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mrgqk webserver-deployment-795d758f88- deployment-1088  e50c932d-ea78-4ffe-806c-dd3d9145b563 39547 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309bc10 0xc00309bc11}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.300: INFO: Pod "webserver-deployment-795d758f88-p76fz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-p76fz webserver-deployment-795d758f88- deployment-1088  451ab733-5833-40f8-8fdb-26cddeb6a48e 39540 0 2021-05-21 08:50:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.165.195/32 cni.projectcalico.org/podIPs:10.100.165.195/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309bdc0 0xc00309bdc1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.301: INFO: Pod "webserver-deployment-795d758f88-wgv6c" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wgv6c webserver-deployment-795d758f88- deployment-1088  45bd754f-d3f7-4329-a7f7-c6e798d056a3 39464 0 2021-05-21 08:50:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.179.40/32 cni.projectcalico.org/podIPs:10.100.179.40/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc00309bfb0 0xc00309bfb1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 08:50:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.301: INFO: Pod "webserver-deployment-795d758f88-xlr2d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xlr2d webserver-deployment-795d758f88- deployment-1088  57300210-d2a3-49b6-ab48-199bae5edb33 39584 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 431aefd5-93c2-4489-a262-d2bb629f7977 0xc006fb2170 0xc006fb2171}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"431aefd5-93c2-4489-a262-d2bb629f7977\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.302: INFO: Pod "webserver-deployment-dd94f59b7-2t9cg" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2t9cg webserver-deployment-dd94f59b7- deployment-1088  39170c76-3cb7-4ca2-84c1-be50dfc21c45 39368 0 2021-05-21 08:50:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.165.252/32 cni.projectcalico.org/podIPs:10.100.165.252/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb2310 0xc006fb2311}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 08:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 08:50:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.165.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:10.100.165.252,StartTime:2021-05-21 08:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 08:50:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://14e20eeee523cc3b23988bc44b7b1e17bed22ee810fd4eb325a0cd061e4e08da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.165.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.304: INFO: Pod "webserver-deployment-dd94f59b7-84lj6" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-84lj6 webserver-deployment-dd94f59b7- deployment-1088  a9d160e1-b92f-4c7a-9643-e23fd0677e02 39352 0 2021-05-21 08:50:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.165.253/32 cni.projectcalico.org/podIPs:10.100.165.253/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb24d0 0xc006fb24d1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 08:50:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 08:50:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.165.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:10.100.165.253,StartTime:2021-05-21 08:50:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 08:50:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2fe4560d8a43721f7bf2fccad281aa64647f3be75da78d668f3b4448a9c31bd9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.165.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.305: INFO: Pod "webserver-deployment-dd94f59b7-8lf6n" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8lf6n webserver-deployment-dd94f59b7- deployment-1088  76549ffe-2968-4fde-bfae-d90cf253233f 39311 0 2021-05-21 08:50:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.165.249/32 cni.projectcalico.org/podIPs:10.100.165.249/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb2690 0xc006fb2691}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 08:50:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 08:50:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.165.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:10.100.165.249,StartTime:2021-05-21 08:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 08:50:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://5299f260344fd643c58ff9a332faf055a0e1b384efd857c7c807958203cf0b39,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.165.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.305: INFO: Pod "webserver-deployment-dd94f59b7-bdfvv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bdfvv webserver-deployment-dd94f59b7- deployment-1088  4f27f0c6-5a75-487d-9c30-b3a82d88e508 39596 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb2860 0xc006fb2861}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:51:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.306: INFO: Pod "webserver-deployment-dd94f59b7-cdqf4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cdqf4 webserver-deployment-dd94f59b7- deployment-1088  5506b2e1-6ea9-4af2-81f9-63b79acfa08e 39586 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb29e0 0xc006fb29e1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:51:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.306: INFO: Pod "webserver-deployment-dd94f59b7-d7tx5" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d7tx5 webserver-deployment-dd94f59b7- deployment-1088  96a51706-0090-4a4f-8fe8-9d8d186c77e4 39572 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb2b60 0xc006fb2b61}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.306: INFO: Pod "webserver-deployment-dd94f59b7-gbzwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gbzwx webserver-deployment-dd94f59b7- deployment-1088  7b92c54b-7c8d-48dd-8e09-4274b63764c3 39583 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb2ce0 0xc006fb2ce1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.307: INFO: Pod "webserver-deployment-dd94f59b7-j5hrc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-j5hrc webserver-deployment-dd94f59b7- deployment-1088  7fc59374-327a-47af-ab64-dca41604f752 39600 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.165.254/32 cni.projectcalico.org/podIPs:10.100.165.254/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb2e60 0xc006fb2e61}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 08:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.314: INFO: Pod "webserver-deployment-dd94f59b7-jk2qd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jk2qd webserver-deployment-dd94f59b7- deployment-1088  32755c86-d8af-4b63-ad45-3d0143755411 39564 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb3000 0xc006fb3001}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.314: INFO: Pod "webserver-deployment-dd94f59b7-ksvdr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ksvdr webserver-deployment-dd94f59b7- deployment-1088  8ae05f1f-3735-44c2-9b66-84d7956a04ce 39590 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb3180 0xc006fb3181}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:51:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.315: INFO: Pod "webserver-deployment-dd94f59b7-ljmws" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ljmws webserver-deployment-dd94f59b7- deployment-1088  c8490937-2a7e-4734-acb5-d78e722f3f86 39569 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb3300 0xc006fb3301}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.315: INFO: Pod "webserver-deployment-dd94f59b7-mq72x" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mq72x webserver-deployment-dd94f59b7- deployment-1088  b88897c5-a5c9-42e5-b943-661f1b813e7b 39377 0 2021-05-21 08:50:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.165.194/32 cni.projectcalico.org/podIPs:10.100.165.194/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb3480 0xc006fb3481}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 08:50:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 08:50:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.165.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:10.100.165.194,StartTime:2021-05-21 08:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 08:50:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://b11b0d4b93bae64beddb50a98451fe1be909eec8326a89b9008e8725a7ff754f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.165.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.316: INFO: Pod "webserver-deployment-dd94f59b7-nq2zj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nq2zj webserver-deployment-dd94f59b7- deployment-1088  dc3fd779-98ea-4cc2-9eb1-537b3a69fd46 39310 0 2021-05-21 08:50:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.179.37/32 cni.projectcalico.org/podIPs:10.100.179.37/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb3660 0xc006fb3661}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 08:50:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 08:50:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.179.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:10.100.179.37,StartTime:2021-05-21 08:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 08:50:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://fb21c42e6bc44a561973a8769568acde86e6ed4e0534c589f45facfe5c319cc1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.179.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.318: INFO: Pod "webserver-deployment-dd94f59b7-pjtts" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pjtts webserver-deployment-dd94f59b7- deployment-1088  228ff6d1-0775-406c-8616-7ebf65c70708 39587 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb3820 0xc006fb3821}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:51:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.319: INFO: Pod "webserver-deployment-dd94f59b7-pkj4n" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pkj4n webserver-deployment-dd94f59b7- deployment-1088  ed8aa0ff-8c55-4803-a0b7-a8cf792fe3d4 39613 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.165.198/32 cni.projectcalico.org/podIPs:10.100.165.198/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb39a0 0xc006fb39a1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 08:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.319: INFO: Pod "webserver-deployment-dd94f59b7-q2k5z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-q2k5z webserver-deployment-dd94f59b7- deployment-1088  002741b1-46c0-431b-902d-eb73510f9ebd 39332 0 2021-05-21 08:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.179.38/32 cni.projectcalico.org/podIPs:10.100.179.38/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb3b80 0xc006fb3b81}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 08:50:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 08:50:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.179.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:10.100.179.38,StartTime:2021-05-21 08:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 08:50:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a101834224a2ded886c5b350d94f246703d028291b68cb7e7962349da195c426,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.179.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.320: INFO: Pod "webserver-deployment-dd94f59b7-tfhjj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tfhjj webserver-deployment-dd94f59b7- deployment-1088  1b8c7d83-efbb-4969-871e-390bf045f794 39353 0 2021-05-21 08:50:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.179.39/32 cni.projectcalico.org/podIPs:10.100.179.39/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb3d60 0xc006fb3d61}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 08:50:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 08:50:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.179.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:10.100.179.39,StartTime:2021-05-21 08:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 08:50:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://3de48bde7202f67423da8c4626ced0ed37e127149be4b9db92a1050c0c494756,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.179.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.320: INFO: Pod "webserver-deployment-dd94f59b7-txnz4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-txnz4 webserver-deployment-dd94f59b7- deployment-1088  372cacc1-5ae0-4e91-a664-215b3c5307e2 39602 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.179.42/32 cni.projectcalico.org/podIPs:10.100.179.42/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc006fb3f40 0xc006fb3f41}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 08:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.321: INFO: Pod "webserver-deployment-dd94f59b7-v6wnt" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v6wnt webserver-deployment-dd94f59b7- deployment-1088  5b7720e4-9889-4d53-bd19-967582d7a171 39331 0 2021-05-21 08:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.165.248/32 cni.projectcalico.org/podIPs:10.100.165.248/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc0077ca0e0 0xc0077ca0e1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 08:50:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 08:50:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.165.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:10.100.165.248,StartTime:2021-05-21 08:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 08:50:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://24a63c884f576553abd5c821f9d2a44c0c05cb08ac10176c792118e808aa21f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.165.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 08:51:01.321: INFO: Pod "webserver-deployment-dd94f59b7-vhf4x" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vhf4x webserver-deployment-dd94f59b7- deployment-1088  eecab4a4-5a7d-4660-8eaf-13babf36b367 39561 0 2021-05-21 08:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 41a750db-90d1-4059-a77e-dc8d0e839f4d 0xc0077ca2a0 0xc0077ca2a1}] []  [{kube-controller-manager Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a750db-90d1-4059-a77e-dc8d0e839f4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 08:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74hs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74hs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74hs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:,StartTime:2021-05-21 08:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:51:01.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1088" for this suite.

• [SLOW TEST:18.702 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":137,"skipped":2701,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:51:01.741: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 21 08:51:02.673: INFO: Waiting up to 5m0s for pod "pod-0fd2f742-e7a0-4f40-ae93-48969b10df79" in namespace "emptydir-3289" to be "Succeeded or Failed"
May 21 08:51:03.187: INFO: Pod "pod-0fd2f742-e7a0-4f40-ae93-48969b10df79": Phase="Pending", Reason="", readiness=false. Elapsed: 514.729723ms
May 21 08:51:05.339: INFO: Pod "pod-0fd2f742-e7a0-4f40-ae93-48969b10df79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.666074267s
May 21 08:51:07.630: INFO: Pod "pod-0fd2f742-e7a0-4f40-ae93-48969b10df79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.957174242s
May 21 08:51:09.689: INFO: Pod "pod-0fd2f742-e7a0-4f40-ae93-48969b10df79": Phase="Pending", Reason="", readiness=false. Elapsed: 7.016559377s
May 21 08:51:11.743: INFO: Pod "pod-0fd2f742-e7a0-4f40-ae93-48969b10df79": Phase="Pending", Reason="", readiness=false. Elapsed: 9.06992264s
May 21 08:51:13.777: INFO: Pod "pod-0fd2f742-e7a0-4f40-ae93-48969b10df79": Phase="Running", Reason="", readiness=true. Elapsed: 11.104197706s
May 21 08:51:15.829: INFO: Pod "pod-0fd2f742-e7a0-4f40-ae93-48969b10df79": Phase="Running", Reason="", readiness=true. Elapsed: 13.156009891s
May 21 08:51:17.840: INFO: Pod "pod-0fd2f742-e7a0-4f40-ae93-48969b10df79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.166900549s
STEP: Saw pod success
May 21 08:51:17.840: INFO: Pod "pod-0fd2f742-e7a0-4f40-ae93-48969b10df79" satisfied condition "Succeeded or Failed"
May 21 08:51:17.854: INFO: Trying to get logs from node k8s-03 pod pod-0fd2f742-e7a0-4f40-ae93-48969b10df79 container test-container: <nil>
STEP: delete the pod
May 21 08:51:18.046: INFO: Waiting for pod pod-0fd2f742-e7a0-4f40-ae93-48969b10df79 to disappear
May 21 08:51:18.053: INFO: Pod pod-0fd2f742-e7a0-4f40-ae93-48969b10df79 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:51:18.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3289" for this suite.

• [SLOW TEST:16.337 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":138,"skipped":2707,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:51:18.078: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0521 08:51:58.773996      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 08:53:00.801: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 21 08:53:00.801: INFO: Deleting pod "simpletest.rc-9qprw" in namespace "gc-9550"
May 21 08:53:00.835: INFO: Deleting pod "simpletest.rc-gxkjs" in namespace "gc-9550"
May 21 08:53:00.948: INFO: Deleting pod "simpletest.rc-h8pd7" in namespace "gc-9550"
May 21 08:53:01.212: INFO: Deleting pod "simpletest.rc-kszzq" in namespace "gc-9550"
May 21 08:53:01.422: INFO: Deleting pod "simpletest.rc-nwfvt" in namespace "gc-9550"
May 21 08:53:01.569: INFO: Deleting pod "simpletest.rc-rlfq4" in namespace "gc-9550"
May 21 08:53:01.713: INFO: Deleting pod "simpletest.rc-tmb2k" in namespace "gc-9550"
May 21 08:53:01.860: INFO: Deleting pod "simpletest.rc-w9jqf" in namespace "gc-9550"
May 21 08:53:02.038: INFO: Deleting pod "simpletest.rc-wxxjg" in namespace "gc-9550"
May 21 08:53:02.338: INFO: Deleting pod "simpletest.rc-xvq5c" in namespace "gc-9550"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:53:02.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9550" for this suite.

• [SLOW TEST:104.634 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":139,"skipped":2713,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:53:02.712: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-acf36e85-ac36-42bc-b180-64f47bca370a
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-acf36e85-ac36-42bc-b180-64f47bca370a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:53:09.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2015" for this suite.

• [SLOW TEST:6.581 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":140,"skipped":2715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:53:09.294: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 21 08:53:14.011: INFO: Successfully updated pod "labelsupdateb5ffc1e9-ed46-4b30-93c7-d12a40aa24b9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:53:18.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7365" for this suite.

• [SLOW TEST:8.836 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":141,"skipped":2741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:53:18.131: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 21 08:53:19.803: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
May 21 08:53:21.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183999, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183999, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184000, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757183999, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5df4db4495\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 08:53:24.923: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:53:25.061: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:53:27.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1642" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.318 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":142,"skipped":2776,"failed":0}
SSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:53:27.450: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 08:53:28.525: INFO: starting watch
STEP: patching
STEP: updating
May 21 08:53:28.567: INFO: waiting for watch events with expected annotations
May 21 08:53:28.567: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:53:29.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5806" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":143,"skipped":2779,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:53:29.093: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
May 21 08:53:33.308: INFO: Pod pod-hostip-731cdca8-62ad-4fec-ba3b-dd6f2320c9b7 has hostIP: 8.16.0.95
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:53:33.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4475" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":144,"skipped":2784,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:53:33.322: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
May 21 08:53:33.466: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-7641 proxy --unix-socket=/tmp/kubectl-proxy-unix342948289/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:53:33.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7641" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":145,"skipped":2785,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:53:33.634: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:53:45.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1464" for this suite.

• [SLOW TEST:11.650 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":146,"skipped":2786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:53:45.285: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:53:45.615: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 08:53:50.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-8630 --namespace=crd-publish-openapi-8630 create -f -'
May 21 08:53:54.215: INFO: stderr: ""
May 21 08:53:54.215: INFO: stdout: "e2e-test-crd-publish-openapi-8652-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 21 08:53:54.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-8630 --namespace=crd-publish-openapi-8630 delete e2e-test-crd-publish-openapi-8652-crds test-cr'
May 21 08:53:54.443: INFO: stderr: ""
May 21 08:53:54.443: INFO: stdout: "e2e-test-crd-publish-openapi-8652-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 21 08:53:54.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-8630 --namespace=crd-publish-openapi-8630 apply -f -'
May 21 08:53:55.475: INFO: stderr: ""
May 21 08:53:55.475: INFO: stdout: "e2e-test-crd-publish-openapi-8652-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 21 08:53:55.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-8630 --namespace=crd-publish-openapi-8630 delete e2e-test-crd-publish-openapi-8652-crds test-cr'
May 21 08:53:55.652: INFO: stderr: ""
May 21 08:53:55.652: INFO: stdout: "e2e-test-crd-publish-openapi-8652-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 21 08:53:55.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-8630 explain e2e-test-crd-publish-openapi-8652-crds'
May 21 08:53:56.590: INFO: stderr: ""
May 21 08:53:56.590: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8652-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:54:02.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8630" for this suite.

• [SLOW TEST:16.860 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":147,"skipped":2830,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:54:02.145: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
May 21 08:54:08.768: INFO: 7 pods remaining
May 21 08:54:08.768: INFO: 0 pods has nil DeletionTimestamp
May 21 08:54:08.768: INFO: 
STEP: Gathering metrics
W0521 08:54:09.450996      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 08:55:11.493: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:55:11.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1019" for this suite.

• [SLOW TEST:69.362 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":148,"skipped":2840,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:55:11.509: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 21 08:55:11.702: INFO: Waiting up to 5m0s for pod "downward-api-9309e3fb-18c3-42c0-9432-66a93138c1ae" in namespace "downward-api-1402" to be "Succeeded or Failed"
May 21 08:55:11.708: INFO: Pod "downward-api-9309e3fb-18c3-42c0-9432-66a93138c1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200078ms
May 21 08:55:13.715: INFO: Pod "downward-api-9309e3fb-18c3-42c0-9432-66a93138c1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012606686s
May 21 08:55:15.723: INFO: Pod "downward-api-9309e3fb-18c3-42c0-9432-66a93138c1ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021525232s
STEP: Saw pod success
May 21 08:55:15.724: INFO: Pod "downward-api-9309e3fb-18c3-42c0-9432-66a93138c1ae" satisfied condition "Succeeded or Failed"
May 21 08:55:15.728: INFO: Trying to get logs from node k8s-03 pod downward-api-9309e3fb-18c3-42c0-9432-66a93138c1ae container dapi-container: <nil>
STEP: delete the pod
May 21 08:55:15.801: INFO: Waiting for pod downward-api-9309e3fb-18c3-42c0-9432-66a93138c1ae to disappear
May 21 08:55:15.825: INFO: Pod downward-api-9309e3fb-18c3-42c0-9432-66a93138c1ae no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:55:15.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1402" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":149,"skipped":2864,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:55:15.837: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:55:16.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4218" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":150,"skipped":2873,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:55:16.470: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:55:16.793: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 21 08:55:21.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 --namespace=crd-publish-openapi-2711 create -f -'
May 21 08:55:25.461: INFO: stderr: ""
May 21 08:55:25.462: INFO: stdout: "e2e-test-crd-publish-openapi-8163-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 21 08:55:25.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 --namespace=crd-publish-openapi-2711 delete e2e-test-crd-publish-openapi-8163-crds test-foo'
May 21 08:55:25.661: INFO: stderr: ""
May 21 08:55:25.661: INFO: stdout: "e2e-test-crd-publish-openapi-8163-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 21 08:55:25.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 --namespace=crd-publish-openapi-2711 apply -f -'
May 21 08:55:26.737: INFO: stderr: ""
May 21 08:55:26.737: INFO: stdout: "e2e-test-crd-publish-openapi-8163-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 21 08:55:26.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 --namespace=crd-publish-openapi-2711 delete e2e-test-crd-publish-openapi-8163-crds test-foo'
May 21 08:55:26.919: INFO: stderr: ""
May 21 08:55:26.919: INFO: stdout: "e2e-test-crd-publish-openapi-8163-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 21 08:55:26.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 --namespace=crd-publish-openapi-2711 create -f -'
May 21 08:55:27.710: INFO: rc: 1
May 21 08:55:27.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 --namespace=crd-publish-openapi-2711 apply -f -'
May 21 08:55:28.435: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 21 08:55:28.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 --namespace=crd-publish-openapi-2711 create -f -'
May 21 08:55:29.210: INFO: rc: 1
May 21 08:55:29.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 --namespace=crd-publish-openapi-2711 apply -f -'
May 21 08:55:29.906: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 21 08:55:29.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 explain e2e-test-crd-publish-openapi-8163-crds'
May 21 08:55:30.570: INFO: stderr: ""
May 21 08:55:30.570: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8163-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 21 08:55:30.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 explain e2e-test-crd-publish-openapi-8163-crds.metadata'
May 21 08:55:31.266: INFO: stderr: ""
May 21 08:55:31.266: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8163-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 21 08:55:31.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 explain e2e-test-crd-publish-openapi-8163-crds.spec'
May 21 08:55:31.980: INFO: stderr: ""
May 21 08:55:31.980: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8163-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 21 08:55:31.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 explain e2e-test-crd-publish-openapi-8163-crds.spec.bars'
May 21 08:55:32.625: INFO: stderr: ""
May 21 08:55:32.625: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8163-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 21 08:55:32.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-2711 explain e2e-test-crd-publish-openapi-8163-crds.spec.bars2'
May 21 08:55:33.329: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:55:38.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2711" for this suite.

• [SLOW TEST:21.874 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":151,"skipped":2893,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:55:38.344: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 21 08:55:46.543: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 08:55:46.551: INFO: Pod pod-with-prestop-http-hook still exists
May 21 08:55:48.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 08:55:48.556: INFO: Pod pod-with-prestop-http-hook still exists
May 21 08:55:50.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 08:55:50.561: INFO: Pod pod-with-prestop-http-hook still exists
May 21 08:55:52.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 08:55:52.561: INFO: Pod pod-with-prestop-http-hook still exists
May 21 08:55:54.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 08:55:54.558: INFO: Pod pod-with-prestop-http-hook still exists
May 21 08:55:56.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 08:55:56.560: INFO: Pod pod-with-prestop-http-hook still exists
May 21 08:55:58.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 08:55:58.561: INFO: Pod pod-with-prestop-http-hook still exists
May 21 08:56:00.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 08:56:00.560: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:56:00.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6695" for this suite.

• [SLOW TEST:22.245 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2901,"failed":0}
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:56:00.589: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 21 08:56:00.774: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:56:20.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5994" for this suite.

• [SLOW TEST:19.778 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2901,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:56:20.368: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 08:56:21.751: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 08:56:23.773: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184181, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184181, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184181, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184181, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 08:56:26.878: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:56:28.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-331" for this suite.
STEP: Destroying namespace "webhook-331-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.292 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":154,"skipped":2922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:56:28.660: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 21 08:56:28.834: INFO: namespace kubectl-3240
May 21 08:56:28.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3240 create -f -'
May 21 08:56:30.022: INFO: stderr: ""
May 21 08:56:30.022: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 08:56:31.043: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:56:31.043: INFO: Found 0 / 1
May 21 08:56:32.034: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:56:32.034: INFO: Found 0 / 1
May 21 08:56:33.030: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:56:33.030: INFO: Found 0 / 1
May 21 08:56:34.032: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:56:34.032: INFO: Found 1 / 1
May 21 08:56:34.032: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 21 08:56:34.036: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 08:56:34.036: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 08:56:34.036: INFO: wait on agnhost-primary startup in kubectl-3240 
May 21 08:56:34.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3240 logs agnhost-primary-fshld agnhost-primary'
May 21 08:56:34.264: INFO: stderr: ""
May 21 08:56:34.264: INFO: stdout: "Paused\n"
STEP: exposing RC
May 21 08:56:34.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3240 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 21 08:56:34.578: INFO: stderr: ""
May 21 08:56:34.578: INFO: stdout: "service/rm2 exposed\n"
May 21 08:56:34.647: INFO: Service rm2 in namespace kubectl-3240 found.
STEP: exposing service
May 21 08:56:36.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3240 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 21 08:56:36.941: INFO: stderr: ""
May 21 08:56:36.941: INFO: stdout: "service/rm3 exposed\n"
May 21 08:56:36.961: INFO: Service rm3 in namespace kubectl-3240 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:56:38.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3240" for this suite.

• [SLOW TEST:10.326 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":155,"skipped":2955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:56:38.987: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 21 08:56:39.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4749 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
May 21 08:56:39.283: INFO: stderr: ""
May 21 08:56:39.283: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
May 21 08:56:39.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4749 delete pods e2e-test-httpd-pod'
May 21 08:56:42.649: INFO: stderr: ""
May 21 08:56:42.649: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:56:42.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4749" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":156,"skipped":2983,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:56:42.684: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:56:43.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4447" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":157,"skipped":2984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:56:43.086: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-58906914-e440-41d5-bade-4398f4f0e28c
STEP: Creating a pod to test consume configMaps
May 21 08:56:43.218: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d9f0f1ef-7fe3-4e29-afb6-96a33861d840" in namespace "projected-1610" to be "Succeeded or Failed"
May 21 08:56:43.252: INFO: Pod "pod-projected-configmaps-d9f0f1ef-7fe3-4e29-afb6-96a33861d840": Phase="Pending", Reason="", readiness=false. Elapsed: 33.491731ms
May 21 08:56:45.256: INFO: Pod "pod-projected-configmaps-d9f0f1ef-7fe3-4e29-afb6-96a33861d840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038294636s
May 21 08:56:47.268: INFO: Pod "pod-projected-configmaps-d9f0f1ef-7fe3-4e29-afb6-96a33861d840": Phase="Running", Reason="", readiness=true. Elapsed: 4.049648255s
May 21 08:56:49.277: INFO: Pod "pod-projected-configmaps-d9f0f1ef-7fe3-4e29-afb6-96a33861d840": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058543551s
STEP: Saw pod success
May 21 08:56:49.277: INFO: Pod "pod-projected-configmaps-d9f0f1ef-7fe3-4e29-afb6-96a33861d840" satisfied condition "Succeeded or Failed"
May 21 08:56:49.280: INFO: Trying to get logs from node k8s-03 pod pod-projected-configmaps-d9f0f1ef-7fe3-4e29-afb6-96a33861d840 container agnhost-container: <nil>
STEP: delete the pod
May 21 08:56:49.324: INFO: Waiting for pod pod-projected-configmaps-d9f0f1ef-7fe3-4e29-afb6-96a33861d840 to disappear
May 21 08:56:49.361: INFO: Pod pod-projected-configmaps-d9f0f1ef-7fe3-4e29-afb6-96a33861d840 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:56:49.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1610" for this suite.

• [SLOW TEST:6.287 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":3011,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:56:49.374: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 08:56:49.811: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 21 08:56:54.825: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 08:56:54.826: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 21 08:56:56.995: INFO: Creating deployment "test-rollover-deployment"
May 21 08:56:57.124: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 21 08:56:59.191: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 21 08:56:59.199: INFO: Ensure that both replica sets have 1 created replica
May 21 08:56:59.207: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 21 08:56:59.220: INFO: Updating deployment test-rollover-deployment
May 21 08:56:59.220: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 21 08:57:01.244: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 21 08:57:01.253: INFO: Make sure deployment "test-rollover-deployment" is complete
May 21 08:57:01.261: INFO: all replica sets need to contain the pod-template-hash label
May 21 08:57:01.262: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184219, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7dc68c4bcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:57:03.272: INFO: all replica sets need to contain the pod-template-hash label
May 21 08:57:03.272: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184222, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7dc68c4bcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:57:05.274: INFO: all replica sets need to contain the pod-template-hash label
May 21 08:57:05.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184222, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7dc68c4bcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:57:07.278: INFO: all replica sets need to contain the pod-template-hash label
May 21 08:57:07.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184222, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7dc68c4bcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:57:09.273: INFO: all replica sets need to contain the pod-template-hash label
May 21 08:57:09.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184222, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7dc68c4bcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:57:11.271: INFO: all replica sets need to contain the pod-template-hash label
May 21 08:57:11.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184222, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184217, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7dc68c4bcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 08:57:13.273: INFO: 
May 21 08:57:13.274: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 08:57:13.285: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1439  3b2f08c6-a61e-4831-b93b-2dead092412d 42190 2 2021-05-21 08:56:56 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 08:56:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 08:57:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00643f478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-21 08:56:57 +0000 UTC,LastTransitionTime:2021-05-21 08:56:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7dc68c4bcf" has successfully progressed.,LastUpdateTime:2021-05-21 08:57:13 +0000 UTC,LastTransitionTime:2021-05-21 08:56:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 21 08:57:13.290: INFO: New ReplicaSet "test-rollover-deployment-7dc68c4bcf" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7dc68c4bcf  deployment-1439  a2a9da72-3306-4956-b16b-bef7a5f0fe97 42177 2 2021-05-21 08:56:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7dc68c4bcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3b2f08c6-a61e-4831-b93b-2dead092412d 0xc00643fdf7 0xc00643fdf8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 08:57:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2f08c6-a61e-4831-b93b-2dead092412d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7dc68c4bcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7dc68c4bcf] map[] [] []  []} {[] [] [{agnhost registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00643ff38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 08:57:13.290: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 21 08:57:13.290: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1439  92db4dc0-508f-4e13-8893-d374205d7013 42188 2 2021-05-21 08:56:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3b2f08c6-a61e-4831-b93b-2dead092412d 0xc00643f9a7 0xc00643f9a8}] []  [{e2e.test Update apps/v1 2021-05-21 08:56:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 08:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2f08c6-a61e-4831-b93b-2dead092412d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00643fac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 08:57:13.290: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-1439  bbef3375-2434-4493-b136-5ccb8962f94f 42137 2 2021-05-21 08:56:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3b2f08c6-a61e-4831-b93b-2dead092412d 0xc00643fbf7 0xc00643fbf8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 08:56:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b2f08c6-a61e-4831-b93b-2dead092412d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00643fd48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 08:57:13.295: INFO: Pod "test-rollover-deployment-7dc68c4bcf-sdw9c" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7dc68c4bcf-sdw9c test-rollover-deployment-7dc68c4bcf- deployment-1439  84f24e30-a998-43d1-b19d-97e944f38244 42155 0 2021-05-21 08:56:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7dc68c4bcf] map[cni.projectcalico.org/podIP:10.100.165.239/32 cni.projectcalico.org/podIPs:10.100.165.239/32] [{apps/v1 ReplicaSet test-rollover-deployment-7dc68c4bcf a2a9da72-3306-4956-b16b-bef7a5f0fe97 0xc0065565d7 0xc0065565d8}] []  [{kube-controller-manager Update v1 2021-05-21 08:56:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2a9da72-3306-4956-b16b-bef7a5f0fe97\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 08:57:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 08:57:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.165.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-j9c4z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-j9c4z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-j9c4z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:56:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:57:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:57:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 08:56:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:10.100.165.239,StartTime:2021-05-21 08:56:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 08:57:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21,ImageID:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost@sha256:e23f72f469927ead04b7bb2a5cedf8569906c38ced37a64767542e7d22831d45,ContainerID:containerd://06fd90c5a33d2615ac3d90ce3e1520b7eef8c17ad15642189bc4937ee4de9807,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.165.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:57:13.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1439" for this suite.

• [SLOW TEST:23.933 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":159,"skipped":3030,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:57:13.308: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-86866e95-7ef5-4c07-b26b-cfd2db52fdd6
STEP: Creating a pod to test consume configMaps
May 21 08:57:13.431: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-11fcb287-6cd4-4f0b-b6cd-6d3e90e39679" in namespace "projected-5263" to be "Succeeded or Failed"
May 21 08:57:13.508: INFO: Pod "pod-projected-configmaps-11fcb287-6cd4-4f0b-b6cd-6d3e90e39679": Phase="Pending", Reason="", readiness=false. Elapsed: 76.791965ms
May 21 08:57:15.516: INFO: Pod "pod-projected-configmaps-11fcb287-6cd4-4f0b-b6cd-6d3e90e39679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085452631s
May 21 08:57:17.523: INFO: Pod "pod-projected-configmaps-11fcb287-6cd4-4f0b-b6cd-6d3e90e39679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.091759268s
STEP: Saw pod success
May 21 08:57:17.523: INFO: Pod "pod-projected-configmaps-11fcb287-6cd4-4f0b-b6cd-6d3e90e39679" satisfied condition "Succeeded or Failed"
May 21 08:57:17.528: INFO: Trying to get logs from node k8s-03 pod pod-projected-configmaps-11fcb287-6cd4-4f0b-b6cd-6d3e90e39679 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 08:57:17.645: INFO: Waiting for pod pod-projected-configmaps-11fcb287-6cd4-4f0b-b6cd-6d3e90e39679 to disappear
May 21 08:57:17.652: INFO: Pod pod-projected-configmaps-11fcb287-6cd4-4f0b-b6cd-6d3e90e39679 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:57:17.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5263" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":3042,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:57:17.666: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-8741e00c-cf55-46f6-aa55-dc8ba531bba1
STEP: Creating secret with name s-test-opt-upd-82bb4e32-88bc-4f45-a024-03f98df6d981
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8741e00c-cf55-46f6-aa55-dc8ba531bba1
STEP: Updating secret s-test-opt-upd-82bb4e32-88bc-4f45-a024-03f98df6d981
STEP: Creating secret with name s-test-opt-create-aa73b295-d218-4d6e-aa31-d90ec815aff6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 08:58:56.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1519" for this suite.

• [SLOW TEST:99.213 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":3055,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 08:58:56.880: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 21 08:58:58.584: INFO: Pod name wrapped-volume-race-2f33d7d2-08b9-4ff9-945f-77914dc2da7b: Found 0 pods out of 5
May 21 08:59:03.599: INFO: Pod name wrapped-volume-race-2f33d7d2-08b9-4ff9-945f-77914dc2da7b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2f33d7d2-08b9-4ff9-945f-77914dc2da7b in namespace emptydir-wrapper-3744, will wait for the garbage collector to delete the pods
May 21 08:59:17.848: INFO: Deleting ReplicationController wrapped-volume-race-2f33d7d2-08b9-4ff9-945f-77914dc2da7b took: 107.81207ms
May 21 08:59:18.549: INFO: Terminating ReplicationController wrapped-volume-race-2f33d7d2-08b9-4ff9-945f-77914dc2da7b pods took: 701.258663ms
STEP: Creating RC which spawns configmap-volume pods
May 21 08:59:34.268: INFO: Pod name wrapped-volume-race-3b6105c8-4081-4091-acb3-2711925a0acc: Found 0 pods out of 5
May 21 08:59:39.281: INFO: Pod name wrapped-volume-race-3b6105c8-4081-4091-acb3-2711925a0acc: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3b6105c8-4081-4091-acb3-2711925a0acc in namespace emptydir-wrapper-3744, will wait for the garbage collector to delete the pods
May 21 08:59:53.509: INFO: Deleting ReplicationController wrapped-volume-race-3b6105c8-4081-4091-acb3-2711925a0acc took: 29.125934ms
May 21 08:59:56.310: INFO: Terminating ReplicationController wrapped-volume-race-3b6105c8-4081-4091-acb3-2711925a0acc pods took: 2.8004153s
STEP: Creating RC which spawns configmap-volume pods
May 21 09:00:11.077: INFO: Pod name wrapped-volume-race-e4652b82-d4a4-4bd0-8876-f6b070537a1d: Found 0 pods out of 5
May 21 09:00:16.091: INFO: Pod name wrapped-volume-race-e4652b82-d4a4-4bd0-8876-f6b070537a1d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e4652b82-d4a4-4bd0-8876-f6b070537a1d in namespace emptydir-wrapper-3744, will wait for the garbage collector to delete the pods
May 21 09:00:30.422: INFO: Deleting ReplicationController wrapped-volume-race-e4652b82-d4a4-4bd0-8876-f6b070537a1d took: 61.44206ms
May 21 09:00:31.223: INFO: Terminating ReplicationController wrapped-volume-race-e4652b82-d4a4-4bd0-8876-f6b070537a1d pods took: 800.272181ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:00:45.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3744" for this suite.

• [SLOW TEST:108.915 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":162,"skipped":3066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:00:45.796: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 09:00:46.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a91963b7-4a20-4b46-a14d-9198b06fe82c" in namespace "projected-1904" to be "Succeeded or Failed"
May 21 09:00:46.210: INFO: Pod "downwardapi-volume-a91963b7-4a20-4b46-a14d-9198b06fe82c": Phase="Pending", Reason="", readiness=false. Elapsed: 179.046792ms
May 21 09:00:48.313: INFO: Pod "downwardapi-volume-a91963b7-4a20-4b46-a14d-9198b06fe82c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282011164s
May 21 09:00:50.319: INFO: Pod "downwardapi-volume-a91963b7-4a20-4b46-a14d-9198b06fe82c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.288602708s
STEP: Saw pod success
May 21 09:00:50.319: INFO: Pod "downwardapi-volume-a91963b7-4a20-4b46-a14d-9198b06fe82c" satisfied condition "Succeeded or Failed"
May 21 09:00:50.323: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-a91963b7-4a20-4b46-a14d-9198b06fe82c container client-container: <nil>
STEP: delete the pod
May 21 09:00:50.398: INFO: Waiting for pod downwardapi-volume-a91963b7-4a20-4b46-a14d-9198b06fe82c to disappear
May 21 09:00:50.404: INFO: Pod downwardapi-volume-a91963b7-4a20-4b46-a14d-9198b06fe82c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:00:50.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1904" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":163,"skipped":3091,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:00:50.471: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
May 21 09:00:50.661: INFO: Waiting up to 5m0s for pod "client-containers-79274076-fba7-45a4-a2b9-758afc44cd96" in namespace "containers-477" to be "Succeeded or Failed"
May 21 09:00:50.707: INFO: Pod "client-containers-79274076-fba7-45a4-a2b9-758afc44cd96": Phase="Pending", Reason="", readiness=false. Elapsed: 45.532711ms
May 21 09:00:52.714: INFO: Pod "client-containers-79274076-fba7-45a4-a2b9-758afc44cd96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052925296s
May 21 09:00:54.763: INFO: Pod "client-containers-79274076-fba7-45a4-a2b9-758afc44cd96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.10149614s
STEP: Saw pod success
May 21 09:00:54.763: INFO: Pod "client-containers-79274076-fba7-45a4-a2b9-758afc44cd96" satisfied condition "Succeeded or Failed"
May 21 09:00:54.781: INFO: Trying to get logs from node k8s-03 pod client-containers-79274076-fba7-45a4-a2b9-758afc44cd96 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:00:54.907: INFO: Waiting for pod client-containers-79274076-fba7-45a4-a2b9-758afc44cd96 to disappear
May 21 09:00:54.914: INFO: Pod client-containers-79274076-fba7-45a4-a2b9-758afc44cd96 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:00:54.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-477" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":164,"skipped":3094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:00:54.985: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-a96c010a-93d7-4a2c-ad47-9deb172efe90
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:00:59.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1699" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":165,"skipped":3122,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:00:59.721: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 09:00:59.878: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec19c7be-67dd-4136-9755-9e5fdbb5574a" in namespace "downward-api-7763" to be "Succeeded or Failed"
May 21 09:00:59.887: INFO: Pod "downwardapi-volume-ec19c7be-67dd-4136-9755-9e5fdbb5574a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.7452ms
May 21 09:01:01.893: INFO: Pod "downwardapi-volume-ec19c7be-67dd-4136-9755-9e5fdbb5574a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014976746s
May 21 09:01:03.898: INFO: Pod "downwardapi-volume-ec19c7be-67dd-4136-9755-9e5fdbb5574a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019959334s
STEP: Saw pod success
May 21 09:01:03.898: INFO: Pod "downwardapi-volume-ec19c7be-67dd-4136-9755-9e5fdbb5574a" satisfied condition "Succeeded or Failed"
May 21 09:01:03.902: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-ec19c7be-67dd-4136-9755-9e5fdbb5574a container client-container: <nil>
STEP: delete the pod
May 21 09:01:03.999: INFO: Waiting for pod downwardapi-volume-ec19c7be-67dd-4136-9755-9e5fdbb5574a to disappear
May 21 09:01:04.006: INFO: Pod downwardapi-volume-ec19c7be-67dd-4136-9755-9e5fdbb5574a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:01:04.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7763" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":166,"skipped":3124,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:01:04.063: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:01:08.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7645" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":167,"skipped":3126,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:01:08.779: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8495, will wait for the garbage collector to delete the pods
May 21 09:01:13.248: INFO: Deleting Job.batch foo took: 65.005903ms
May 21 09:01:13.948: INFO: Terminating Job.batch foo pods took: 700.330059ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:01:50.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8495" for this suite.

• [SLOW TEST:41.491 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":168,"skipped":3134,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:01:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
May 21 09:01:50.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-7586 cluster-info'
May 21 09:01:50.535: INFO: stderr: ""
May 21 09:01:50.536: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:01:50.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7586" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":169,"skipped":3145,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:01:50.552: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:01:52.643: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:01:54.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184512, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184512, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184513, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184512, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:01:57.898: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:01:58.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7237" for this suite.
STEP: Destroying namespace "webhook-7237-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.897 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":170,"skipped":3225,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:01:58.450: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:01:58.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5070" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":171,"skipped":3234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:01:58.832: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 09:01:59.058: INFO: Waiting up to 5m0s for pod "downwardapi-volume-63277317-d61f-4e61-87e8-5d0ecff58591" in namespace "projected-5151" to be "Succeeded or Failed"
May 21 09:01:59.138: INFO: Pod "downwardapi-volume-63277317-d61f-4e61-87e8-5d0ecff58591": Phase="Pending", Reason="", readiness=false. Elapsed: 79.380939ms
May 21 09:02:01.209: INFO: Pod "downwardapi-volume-63277317-d61f-4e61-87e8-5d0ecff58591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.151173674s
May 21 09:02:03.238: INFO: Pod "downwardapi-volume-63277317-d61f-4e61-87e8-5d0ecff58591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180012096s
STEP: Saw pod success
May 21 09:02:03.238: INFO: Pod "downwardapi-volume-63277317-d61f-4e61-87e8-5d0ecff58591" satisfied condition "Succeeded or Failed"
May 21 09:02:03.244: INFO: Trying to get logs from node k8s-02 pod downwardapi-volume-63277317-d61f-4e61-87e8-5d0ecff58591 container client-container: <nil>
STEP: delete the pod
May 21 09:02:03.420: INFO: Waiting for pod downwardapi-volume-63277317-d61f-4e61-87e8-5d0ecff58591 to disappear
May 21 09:02:03.436: INFO: Pod downwardapi-volume-63277317-d61f-4e61-87e8-5d0ecff58591 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:02:03.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5151" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":3269,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:02:03.624: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 21 09:02:04.162: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:04.213: INFO: Number of nodes with available pods: 0
May 21 09:02:04.213: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:02:05.238: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:05.249: INFO: Number of nodes with available pods: 0
May 21 09:02:05.249: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:02:06.352: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:06.457: INFO: Number of nodes with available pods: 0
May 21 09:02:06.457: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:02:07.220: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:07.389: INFO: Number of nodes with available pods: 0
May 21 09:02:07.389: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:02:08.287: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:08.293: INFO: Number of nodes with available pods: 1
May 21 09:02:08.293: INFO: Node k8s-03 is running more than one daemon pod
May 21 09:02:09.229: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:09.235: INFO: Number of nodes with available pods: 2
May 21 09:02:09.235: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 21 09:02:09.390: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:09.447: INFO: Number of nodes with available pods: 1
May 21 09:02:09.448: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:02:10.455: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:10.468: INFO: Number of nodes with available pods: 1
May 21 09:02:10.468: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:02:11.459: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:11.464: INFO: Number of nodes with available pods: 1
May 21 09:02:11.464: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:02:12.464: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:12.469: INFO: Number of nodes with available pods: 1
May 21 09:02:12.469: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:02:13.460: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:02:13.467: INFO: Number of nodes with available pods: 2
May 21 09:02:13.467: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5220, will wait for the garbage collector to delete the pods
May 21 09:02:13.538: INFO: Deleting DaemonSet.extensions daemon-set took: 9.491372ms
May 21 09:02:13.638: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.27993ms
May 21 09:02:24.343: INFO: Number of nodes with available pods: 0
May 21 09:02:24.343: INFO: Number of running nodes: 0, number of available pods: 0
May 21 09:02:24.347: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44525"},"items":null}

May 21 09:02:24.351: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44525"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:02:24.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5220" for this suite.

• [SLOW TEST:20.758 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":173,"skipped":3287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:02:24.384: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
May 21 09:02:24.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4737 run logs-generator --image=registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 21 09:02:24.829: INFO: stderr: ""
May 21 09:02:24.829: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
May 21 09:02:24.829: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 21 09:02:24.829: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4737" to be "running and ready, or succeeded"
May 21 09:02:24.868: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 38.372168ms
May 21 09:02:26.917: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08734567s
May 21 09:02:28.923: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.093573459s
May 21 09:02:28.923: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 21 09:02:28.923: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 21 09:02:28.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4737 logs logs-generator logs-generator'
May 21 09:02:29.118: INFO: stderr: ""
May 21 09:02:29.118: INFO: stdout: "I0521 09:02:27.472629       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/47b7 418\nI0521 09:02:27.672776       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/h62 389\nI0521 09:02:27.872765       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/7l2 473\nI0521 09:02:28.072674       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/jwpk 434\nI0521 09:02:28.272888       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/kkb 435\nI0521 09:02:28.472815       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/qt5 521\nI0521 09:02:28.672783       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/rr9 256\nI0521 09:02:28.872877       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/sjb7 282\nI0521 09:02:29.072754       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/j42 556\n"
STEP: limiting log lines
May 21 09:02:29.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4737 logs logs-generator logs-generator --tail=1'
May 21 09:02:29.311: INFO: stderr: ""
May 21 09:02:29.311: INFO: stdout: "I0521 09:02:29.272872       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/nsns 455\n"
May 21 09:02:29.311: INFO: got output "I0521 09:02:29.272872       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/nsns 455\n"
STEP: limiting log bytes
May 21 09:02:29.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4737 logs logs-generator logs-generator --limit-bytes=1'
May 21 09:02:29.528: INFO: stderr: ""
May 21 09:02:29.528: INFO: stdout: "I"
May 21 09:02:29.528: INFO: got output "I"
STEP: exposing timestamps
May 21 09:02:29.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4737 logs logs-generator logs-generator --tail=1 --timestamps'
May 21 09:02:29.743: INFO: stderr: ""
May 21 09:02:29.743: INFO: stdout: "2021-05-21T17:02:29.673015974+08:00 I0521 09:02:29.672797       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/sxp 320\n"
May 21 09:02:29.743: INFO: got output "2021-05-21T17:02:29.673015974+08:00 I0521 09:02:29.672797       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/sxp 320\n"
STEP: restricting to a time range
May 21 09:02:32.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4737 logs logs-generator logs-generator --since=1s'
May 21 09:02:32.453: INFO: stderr: ""
May 21 09:02:32.453: INFO: stdout: "I0521 09:02:31.472766       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/t4s 338\nI0521 09:02:31.672796       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/lw4 538\nI0521 09:02:31.872764       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/tcvg 242\nI0521 09:02:32.072829       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/8227 537\nI0521 09:02:32.272843       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/kpkn 222\n"
May 21 09:02:32.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4737 logs logs-generator logs-generator --since=24h'
May 21 09:02:32.642: INFO: stderr: ""
May 21 09:02:32.642: INFO: stdout: "I0521 09:02:27.472629       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/47b7 418\nI0521 09:02:27.672776       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/h62 389\nI0521 09:02:27.872765       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/7l2 473\nI0521 09:02:28.072674       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/jwpk 434\nI0521 09:02:28.272888       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/kkb 435\nI0521 09:02:28.472815       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/qt5 521\nI0521 09:02:28.672783       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/rr9 256\nI0521 09:02:28.872877       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/sjb7 282\nI0521 09:02:29.072754       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/j42 556\nI0521 09:02:29.272872       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/nsns 455\nI0521 09:02:29.472853       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/jzw6 448\nI0521 09:02:29.672797       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/sxp 320\nI0521 09:02:29.872806       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/dtff 580\nI0521 09:02:30.072804       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/z4h 373\nI0521 09:02:30.272789       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/8xbz 247\nI0521 09:02:30.472801       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/thvg 306\nI0521 09:02:30.672799       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/f8fc 383\nI0521 09:02:30.872748       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/q9s 211\nI0521 09:02:31.072882       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/bw8 468\nI0521 09:02:31.272830       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/bdzj 240\nI0521 09:02:31.472766       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/t4s 338\nI0521 09:02:31.672796       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/lw4 538\nI0521 09:02:31.872764       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/tcvg 242\nI0521 09:02:32.072829       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/8227 537\nI0521 09:02:32.272843       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/kpkn 222\nI0521 09:02:32.472747       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/4pcs 267\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
May 21 09:02:32.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-4737 delete pod logs-generator'
May 21 09:02:40.218: INFO: stderr: ""
May 21 09:02:40.218: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:02:40.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4737" for this suite.

• [SLOW TEST:15.854 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":174,"skipped":3327,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:02:40.238: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:02:41.558: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:02:43.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184561, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184561, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184562, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184561, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 09:02:45.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184561, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184561, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184562, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184561, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:02:48.702: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:02:59.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7676" for this suite.
STEP: Destroying namespace "webhook-7676-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:18.938 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":175,"skipped":3328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:02:59.178: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 09:02:59.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32c11e09-a14b-4fe3-8921-64569323a17a" in namespace "downward-api-2716" to be "Succeeded or Failed"
May 21 09:02:59.335: INFO: Pod "downwardapi-volume-32c11e09-a14b-4fe3-8921-64569323a17a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.193766ms
May 21 09:03:01.342: INFO: Pod "downwardapi-volume-32c11e09-a14b-4fe3-8921-64569323a17a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014853365s
May 21 09:03:03.349: INFO: Pod "downwardapi-volume-32c11e09-a14b-4fe3-8921-64569323a17a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022338996s
STEP: Saw pod success
May 21 09:03:03.349: INFO: Pod "downwardapi-volume-32c11e09-a14b-4fe3-8921-64569323a17a" satisfied condition "Succeeded or Failed"
May 21 09:03:03.353: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-32c11e09-a14b-4fe3-8921-64569323a17a container client-container: <nil>
STEP: delete the pod
May 21 09:03:03.398: INFO: Waiting for pod downwardapi-volume-32c11e09-a14b-4fe3-8921-64569323a17a to disappear
May 21 09:03:03.407: INFO: Pod downwardapi-volume-32c11e09-a14b-4fe3-8921-64569323a17a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:03:03.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2716" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":3358,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:03:03.424: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:03:03.724: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 21 09:03:03.811: INFO: Pod name sample-pod: Found 0 pods out of 1
May 21 09:03:08.823: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 09:03:08.824: INFO: Creating deployment "test-rolling-update-deployment"
May 21 09:03:08.831: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 21 09:03:08.883: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 21 09:03:10.936: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 21 09:03:10.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184589, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184589, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184589, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184588, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-5ff5d796db\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 09:03:12.962: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 09:03:12.976: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2066  412df241-ae3d-4324-b364-8edb95b0b298 44869 1 2021-05-21 09:03:08 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-21 09:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 09:03:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003729a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-21 09:03:09 +0000 UTC,LastTransitionTime:2021-05-21 09:03:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-5ff5d796db" has successfully progressed.,LastUpdateTime:2021-05-21 09:03:12 +0000 UTC,LastTransitionTime:2021-05-21 09:03:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 21 09:03:12.980: INFO: New ReplicaSet "test-rolling-update-deployment-5ff5d796db" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-5ff5d796db  deployment-2066  c94c311e-e7ce-4d29-b1b9-b1cbfe29921f 44858 1 2021-05-21 09:03:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5ff5d796db] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 412df241-ae3d-4324-b364-8edb95b0b298 0xc003729ec7 0xc003729ec8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 09:03:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"412df241-ae3d-4324-b364-8edb95b0b298\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 5ff5d796db,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5ff5d796db] map[] [] []  []} {[] [] [{agnhost registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003729f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 09:03:12.980: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 21 09:03:12.980: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2066  a065041d-7a31-4d52-b1ed-98e30f35fbe5 44868 2 2021-05-21 09:03:03 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 412df241-ae3d-4324-b364-8edb95b0b298 0xc003729db7 0xc003729db8}] []  [{e2e.test Update apps/v1 2021-05-21 09:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 09:03:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"412df241-ae3d-4324-b364-8edb95b0b298\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003729e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 09:03:12.985: INFO: Pod "test-rolling-update-deployment-5ff5d796db-s9tjn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-5ff5d796db-s9tjn test-rolling-update-deployment-5ff5d796db- deployment-2066  fdd7e4cd-3934-466c-aa86-af1752414086 44857 0 2021-05-21 09:03:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5ff5d796db] map[cni.projectcalico.org/podIP:10.100.165.206/32 cni.projectcalico.org/podIPs:10.100.165.206/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-5ff5d796db c94c311e-e7ce-4d29-b1b9-b1cbfe29921f 0xc0049a4367 0xc0049a4368}] []  [{kube-controller-manager Update v1 2021-05-21 09:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c94c311e-e7ce-4d29-b1b9-b1cbfe29921f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 09:03:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 09:03:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.165.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d2vfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d2vfv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d2vfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:03:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:03:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:03:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:03:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:10.100.165.206,StartTime:2021-05-21 09:03:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 09:03:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21,ImageID:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost@sha256:e23f72f469927ead04b7bb2a5cedf8569906c38ced37a64767542e7d22831d45,ContainerID:containerd://4274b788ef809409656d98c169aef4f09d164ad6a9a8a493c4fda0f48c1a0b11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.165.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:03:12.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2066" for this suite.

• [SLOW TEST:9.575 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":177,"skipped":3361,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:03:13.000: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:03:14.955: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:03:16.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184594, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184594, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184595, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184594, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:03:20.061: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 21 09:03:24.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=webhook-3944 attach --namespace=webhook-3944 to-be-attached-pod -i -c=container1'
May 21 09:03:24.355: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:03:24.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3944" for this suite.
STEP: Destroying namespace "webhook-3944-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:11.544 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":178,"skipped":3372,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:03:24.544: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:03:26.145: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:03:28.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184606, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184606, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184606, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184606, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:03:31.404: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:03:31.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3519" for this suite.
STEP: Destroying namespace "webhook-3519-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.217 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":179,"skipped":3373,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:03:31.762: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:03:32.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-9833 create -f -'
May 21 09:03:33.107: INFO: stderr: ""
May 21 09:03:33.107: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 21 09:03:33.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-9833 create -f -'
May 21 09:03:34.255: INFO: stderr: ""
May 21 09:03:34.255: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 09:03:35.264: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 09:03:35.264: INFO: Found 0 / 1
May 21 09:03:36.310: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 09:03:36.310: INFO: Found 0 / 1
May 21 09:03:37.328: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 09:03:37.328: INFO: Found 0 / 1
May 21 09:03:38.273: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 09:03:38.273: INFO: Found 1 / 1
May 21 09:03:38.273: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 21 09:03:38.279: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 09:03:38.279: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 09:03:38.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-9833 describe pod agnhost-primary-2m9gm'
May 21 09:03:38.494: INFO: stderr: ""
May 21 09:03:38.494: INFO: stdout: "Name:         agnhost-primary-2m9gm\nNamespace:    kubectl-9833\nPriority:     0\nNode:         k8s-03/8.16.0.95\nStart Time:   Fri, 21 May 2021 09:03:33 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.100.165.198/32\n              cni.projectcalico.org/podIPs: 10.100.165.198/32\nStatus:       Running\nIP:           10.100.165.198\nIPs:\n  IP:           10.100.165.198\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://b3d12731f9e41d2f0c2dcb106f8e5a4aea358bc104d860fea6200c9e2042a823\n    Image:          registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21\n    Image ID:       registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost@sha256:e23f72f469927ead04b7bb2a5cedf8569906c38ced37a64767542e7d22831d45\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 21 May 2021 09:03:36 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-x2xzk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-x2xzk:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-x2xzk\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  5s    default-scheduler  Successfully assigned kubectl-9833/agnhost-primary-2m9gm to k8s-03\n  Normal  Pulled     3s    kubelet            Container image \"registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
May 21 09:03:38.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-9833 describe rc agnhost-primary'
May 21 09:03:38.730: INFO: stderr: ""
May 21 09:03:38.730: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9833\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-2m9gm\n"
May 21 09:03:38.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-9833 describe service agnhost-primary'
May 21 09:03:38.916: INFO: stderr: ""
May 21 09:03:38.916: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9833\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.96.148.143\nIPs:               10.96.148.143\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.165.198:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 21 09:03:38.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-9833 describe node k8s-01'
May 21 09:03:39.239: INFO: stderr: ""
May 21 09:03:39.239: INFO: stdout: "Name:               k8s-01\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-01\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 8.16.0.93/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.100.61.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 21 May 2021 05:41:07 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-01\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 21 May 2021 09:03:34 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 21 May 2021 05:43:08 +0000   Fri, 21 May 2021 05:43:08 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 21 May 2021 09:00:12 +0000   Fri, 21 May 2021 05:41:03 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 21 May 2021 09:00:12 +0000   Fri, 21 May 2021 05:41:03 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 21 May 2021 09:00:12 +0000   Fri, 21 May 2021 05:41:03 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 21 May 2021 09:00:12 +0000   Fri, 21 May 2021 05:42:43 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  8.16.0.93\n  Hostname:    k8s-01\nCapacity:\n  cpu:                4\n  ephemeral-storage:  13106Mi\n  hugepages-2Mi:      0\n  memory:             7955508Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  12368373330\n  hugepages-2Mi:      0\n  memory:             7853108Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 1b7f54bd16134311a2ce42e77e9f887e\n  System UUID:                c93c4d56-5a7e-d668-c507-39061b538636\n  Boot ID:                    e6a0989b-79ab-45b3-8d3c-2edc2b42645f\n  Kernel Version:             4.18.0-240.22.1.el8_3.x86_64\n  OS Image:                   CentOS Linux 8\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.3\n  Kubelet Version:            v1.20.7\n  Kube-Proxy Version:         v1.20.7\nPodCIDR:                      10.100.0.0/24\nPodCIDRs:                     10.100.0.0/24\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system               calico-kube-controllers-854b9dcf89-nff6d                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h21m\n  calico-system               calico-node-7rkml                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h21m\n  calico-system               calico-typha-7578696db5-qd85g                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h21m\n  kube-system                 coredns-669bd7db67-8xjmx                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     3h22m\n  kube-system                 coredns-669bd7db67-hdcfk                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     3h22m\n  kube-system                 etcd-k8s-01                                                100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         3h22m\n  kube-system                 kube-apiserver-k8s-01                                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         3h22m\n  kube-system                 kube-controller-manager-k8s-01                             200m (5%)     0 (0%)      0 (0%)           0 (0%)         3h22m\n  kube-system                 kube-proxy-jfgcv                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h22m\n  kube-system                 kube-scheduler-k8s-01                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         3h22m\n  lens-metrics                node-exporter-4z22f                                        10m (0%)      200m (5%)   24Mi (0%)        100Mi (1%)     34m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-5t5sj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  tigera-operator             tigera-operator-cf6b69777-t4bzr                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h22m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                860m (21%)  200m (5%)\n  memory             264Mi (3%)  440Mi (5%)\n  ephemeral-storage  100Mi (0%)  0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
May 21 09:03:39.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-9833 describe namespace kubectl-9833'
May 21 09:03:39.431: INFO: stderr: ""
May 21 09:03:39.431: INFO: stdout: "Name:         kubectl-9833\nLabels:       e2e-framework=kubectl\n              e2e-run=a0a8bd7c-5509-447c-ab5c-7ef292a7d5d2\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:03:39.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9833" for this suite.

• [SLOW TEST:7.685 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1090
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":180,"skipped":3375,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:03:39.448: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-1b148257-9dcd-4415-8ae6-55fb716a8c04
STEP: Creating a pod to test consume configMaps
May 21 09:03:39.657: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b8b36be-9878-4ac8-bff6-2110801c47a8" in namespace "projected-608" to be "Succeeded or Failed"
May 21 09:03:39.697: INFO: Pod "pod-projected-configmaps-7b8b36be-9878-4ac8-bff6-2110801c47a8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.481388ms
May 21 09:03:41.713: INFO: Pod "pod-projected-configmaps-7b8b36be-9878-4ac8-bff6-2110801c47a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056609311s
May 21 09:03:43.720: INFO: Pod "pod-projected-configmaps-7b8b36be-9878-4ac8-bff6-2110801c47a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062728079s
STEP: Saw pod success
May 21 09:03:43.720: INFO: Pod "pod-projected-configmaps-7b8b36be-9878-4ac8-bff6-2110801c47a8" satisfied condition "Succeeded or Failed"
May 21 09:03:43.724: INFO: Trying to get logs from node k8s-03 pod pod-projected-configmaps-7b8b36be-9878-4ac8-bff6-2110801c47a8 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:03:43.816: INFO: Waiting for pod pod-projected-configmaps-7b8b36be-9878-4ac8-bff6-2110801c47a8 to disappear
May 21 09:03:43.825: INFO: Pod pod-projected-configmaps-7b8b36be-9878-4ac8-bff6-2110801c47a8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:03:43.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-608" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3378,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:03:43.850: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:03:45.245: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:03:47.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184625, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184625, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184625, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184625, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:03:50.925: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:03:50.931: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7080-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:03:52.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9064" for this suite.
STEP: Destroying namespace "webhook-9064-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.455 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":182,"skipped":3379,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:03:52.307: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 21 09:03:52.514: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:04:24.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8139" for this suite.

• [SLOW TEST:32.205 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":183,"skipped":3446,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:04:24.511: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7162.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7162.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7162.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7162.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7162.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7162.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 09:04:30.855: INFO: DNS probes using dns-7162/dns-test-95868da0-ea24-476e-b33f-f0bb60660291 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:04:30.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7162" for this suite.

• [SLOW TEST:6.449 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":184,"skipped":3448,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:04:30.961: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:04:31.093: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-ec4180e1-446c-4c50-9514-5bd2214fbbd3" in namespace "security-context-test-6981" to be "Succeeded or Failed"
May 21 09:04:31.152: INFO: Pod "busybox-privileged-false-ec4180e1-446c-4c50-9514-5bd2214fbbd3": Phase="Pending", Reason="", readiness=false. Elapsed: 58.7893ms
May 21 09:04:33.159: INFO: Pod "busybox-privileged-false-ec4180e1-446c-4c50-9514-5bd2214fbbd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065957854s
May 21 09:04:35.198: INFO: Pod "busybox-privileged-false-ec4180e1-446c-4c50-9514-5bd2214fbbd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.10532364s
May 21 09:04:35.198: INFO: Pod "busybox-privileged-false-ec4180e1-446c-4c50-9514-5bd2214fbbd3" satisfied condition "Succeeded or Failed"
May 21 09:04:35.295: INFO: Got logs for pod "busybox-privileged-false-ec4180e1-446c-4c50-9514-5bd2214fbbd3": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:04:35.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6981" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":185,"skipped":3467,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:04:35.308: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:04:35.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5206" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":186,"skipped":3478,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:04:35.536: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
May 21 09:04:40.496: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2700 pod-service-account-8d6aca3a-96d8-4630-86b0-8a5723f41667 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 21 09:04:40.920: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2700 pod-service-account-8d6aca3a-96d8-4630-86b0-8a5723f41667 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 21 09:04:41.318: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2700 pod-service-account-8d6aca3a-96d8-4630-86b0-8a5723f41667 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:04:41.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2700" for this suite.

• [SLOW TEST:6.162 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":187,"skipped":3497,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:04:41.698: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:04:43.110: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:04:45.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184683, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184683, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184683, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184683, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:04:48.266: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:04:48.275: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7435-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:04:49.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6596" for this suite.
STEP: Destroying namespace "webhook-6596-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.514 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":188,"skipped":3514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:04:50.213: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
May 21 09:04:50.369: INFO: Waiting up to 5m0s for pod "pod-5b55fc99-77cd-4136-829b-92aaeb83ef68" in namespace "emptydir-4069" to be "Succeeded or Failed"
May 21 09:04:50.428: INFO: Pod "pod-5b55fc99-77cd-4136-829b-92aaeb83ef68": Phase="Pending", Reason="", readiness=false. Elapsed: 59.86136ms
May 21 09:04:52.439: INFO: Pod "pod-5b55fc99-77cd-4136-829b-92aaeb83ef68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070394841s
May 21 09:04:54.447: INFO: Pod "pod-5b55fc99-77cd-4136-829b-92aaeb83ef68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078133876s
STEP: Saw pod success
May 21 09:04:54.447: INFO: Pod "pod-5b55fc99-77cd-4136-829b-92aaeb83ef68" satisfied condition "Succeeded or Failed"
May 21 09:04:54.451: INFO: Trying to get logs from node k8s-03 pod pod-5b55fc99-77cd-4136-829b-92aaeb83ef68 container test-container: <nil>
STEP: delete the pod
May 21 09:04:54.507: INFO: Waiting for pod pod-5b55fc99-77cd-4136-829b-92aaeb83ef68 to disappear
May 21 09:04:54.514: INFO: Pod pod-5b55fc99-77cd-4136-829b-92aaeb83ef68 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:04:54.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4069" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3541,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:04:54.527: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 21 09:04:54.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 create -f -'
May 21 09:04:55.733: INFO: stderr: ""
May 21 09:04:55.733: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 09:04:55.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:04:55.904: INFO: stderr: ""
May 21 09:04:55.904: INFO: stdout: "update-demo-nautilus-bpfh2 update-demo-nautilus-m2tvv "
May 21 09:04:55.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods update-demo-nautilus-bpfh2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:04:56.075: INFO: stderr: ""
May 21 09:04:56.075: INFO: stdout: ""
May 21 09:04:56.075: INFO: update-demo-nautilus-bpfh2 is created but not running
May 21 09:05:01.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:05:01.257: INFO: stderr: ""
May 21 09:05:01.257: INFO: stdout: "update-demo-nautilus-bpfh2 update-demo-nautilus-m2tvv "
May 21 09:05:01.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods update-demo-nautilus-bpfh2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:05:01.424: INFO: stderr: ""
May 21 09:05:01.424: INFO: stdout: ""
May 21 09:05:01.424: INFO: update-demo-nautilus-bpfh2 is created but not running
May 21 09:05:06.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:05:06.611: INFO: stderr: ""
May 21 09:05:06.611: INFO: stdout: "update-demo-nautilus-bpfh2 update-demo-nautilus-m2tvv "
May 21 09:05:06.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods update-demo-nautilus-bpfh2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:05:06.773: INFO: stderr: ""
May 21 09:05:06.773: INFO: stdout: "true"
May 21 09:05:06.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods update-demo-nautilus-bpfh2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 09:05:06.984: INFO: stderr: ""
May 21 09:05:06.984: INFO: stdout: "registry.cn-hangzhou.aliyuncs.com/ks8gcr/nautilus:1.0"
May 21 09:05:06.984: INFO: validating pod update-demo-nautilus-bpfh2
May 21 09:05:06.990: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 09:05:06.990: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 09:05:06.990: INFO: update-demo-nautilus-bpfh2 is verified up and running
May 21 09:05:06.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods update-demo-nautilus-m2tvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:05:07.166: INFO: stderr: ""
May 21 09:05:07.167: INFO: stdout: "true"
May 21 09:05:07.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods update-demo-nautilus-m2tvv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 09:05:07.341: INFO: stderr: ""
May 21 09:05:07.341: INFO: stdout: "registry.cn-hangzhou.aliyuncs.com/ks8gcr/nautilus:1.0"
May 21 09:05:07.341: INFO: validating pod update-demo-nautilus-m2tvv
May 21 09:05:07.349: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 09:05:07.349: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 09:05:07.349: INFO: update-demo-nautilus-m2tvv is verified up and running
STEP: using delete to clean up resources
May 21 09:05:07.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 delete --grace-period=0 --force -f -'
May 21 09:05:07.540: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 09:05:07.540: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 21 09:05:07.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get rc,svc -l name=update-demo --no-headers'
May 21 09:05:07.719: INFO: stderr: "No resources found in kubectl-5327 namespace.\n"
May 21 09:05:07.719: INFO: stdout: ""
May 21 09:05:07.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 09:05:07.897: INFO: stderr: ""
May 21 09:05:07.897: INFO: stdout: "update-demo-nautilus-bpfh2\nupdate-demo-nautilus-m2tvv\n"
May 21 09:05:08.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get rc,svc -l name=update-demo --no-headers'
May 21 09:05:08.576: INFO: stderr: "No resources found in kubectl-5327 namespace.\n"
May 21 09:05:08.576: INFO: stdout: ""
May 21 09:05:08.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-5327 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 09:05:08.753: INFO: stderr: ""
May 21 09:05:08.753: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:05:08.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5327" for this suite.

• [SLOW TEST:14.243 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":190,"skipped":3547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:05:08.771: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:05:13.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2307" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3573,"failed":0}
SSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:05:13.025: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:05:13.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-979" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":192,"skipped":3579,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:05:13.375: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-793b6fb5-fafb-4d8a-8ced-7e59d6ae4f82
STEP: Creating a pod to test consume configMaps
May 21 09:05:13.562: INFO: Waiting up to 5m0s for pod "pod-configmaps-efc562f6-47c5-4ca2-a3ee-ab6f60f59667" in namespace "configmap-1669" to be "Succeeded or Failed"
May 21 09:05:13.569: INFO: Pod "pod-configmaps-efc562f6-47c5-4ca2-a3ee-ab6f60f59667": Phase="Pending", Reason="", readiness=false. Elapsed: 6.911549ms
May 21 09:05:15.642: INFO: Pod "pod-configmaps-efc562f6-47c5-4ca2-a3ee-ab6f60f59667": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079185056s
May 21 09:05:17.657: INFO: Pod "pod-configmaps-efc562f6-47c5-4ca2-a3ee-ab6f60f59667": Phase="Running", Reason="", readiness=true. Elapsed: 4.094812672s
May 21 09:05:19.670: INFO: Pod "pod-configmaps-efc562f6-47c5-4ca2-a3ee-ab6f60f59667": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.107700314s
STEP: Saw pod success
May 21 09:05:19.670: INFO: Pod "pod-configmaps-efc562f6-47c5-4ca2-a3ee-ab6f60f59667" satisfied condition "Succeeded or Failed"
May 21 09:05:19.675: INFO: Trying to get logs from node k8s-03 pod pod-configmaps-efc562f6-47c5-4ca2-a3ee-ab6f60f59667 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:05:19.732: INFO: Waiting for pod pod-configmaps-efc562f6-47c5-4ca2-a3ee-ab6f60f59667 to disappear
May 21 09:05:19.740: INFO: Pod pod-configmaps-efc562f6-47c5-4ca2-a3ee-ab6f60f59667 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:05:19.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1669" for this suite.

• [SLOW TEST:6.376 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":193,"skipped":3599,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:05:19.751: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-453f1e36-8477-4fa9-b4a1-baffb0411e3a in namespace container-probe-5504
May 21 09:05:24.351: INFO: Started pod busybox-453f1e36-8477-4fa9-b4a1-baffb0411e3a in namespace container-probe-5504
STEP: checking the pod's current state and verifying that restartCount is present
May 21 09:05:24.355: INFO: Initial restart count of pod busybox-453f1e36-8477-4fa9-b4a1-baffb0411e3a is 0
May 21 09:06:16.762: INFO: Restart count of pod container-probe-5504/busybox-453f1e36-8477-4fa9-b4a1-baffb0411e3a is now 1 (52.406982638s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:06:16.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5504" for this suite.

• [SLOW TEST:57.104 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":194,"skipped":3616,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:06:16.857: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:06:18.432: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:06:20.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184778, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184778, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184778, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757184778, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:06:23.555: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:06:24.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2438" for this suite.
STEP: Destroying namespace "webhook-2438-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.149 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":195,"skipped":3637,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:06:25.007: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-3377832d-b09a-4ed2-a1ac-27110ccb6377
STEP: Creating a pod to test consume configMaps
May 21 09:06:25.184: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-060d74a9-dc0b-44c5-a884-75d07fb2b179" in namespace "projected-5115" to be "Succeeded or Failed"
May 21 09:06:25.266: INFO: Pod "pod-projected-configmaps-060d74a9-dc0b-44c5-a884-75d07fb2b179": Phase="Pending", Reason="", readiness=false. Elapsed: 81.635228ms
May 21 09:06:27.274: INFO: Pod "pod-projected-configmaps-060d74a9-dc0b-44c5-a884-75d07fb2b179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08980676s
May 21 09:06:29.281: INFO: Pod "pod-projected-configmaps-060d74a9-dc0b-44c5-a884-75d07fb2b179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.096728142s
STEP: Saw pod success
May 21 09:06:29.281: INFO: Pod "pod-projected-configmaps-060d74a9-dc0b-44c5-a884-75d07fb2b179" satisfied condition "Succeeded or Failed"
May 21 09:06:29.285: INFO: Trying to get logs from node k8s-03 pod pod-projected-configmaps-060d74a9-dc0b-44c5-a884-75d07fb2b179 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:06:29.380: INFO: Waiting for pod pod-projected-configmaps-060d74a9-dc0b-44c5-a884-75d07fb2b179 to disappear
May 21 09:06:29.406: INFO: Pod pod-projected-configmaps-060d74a9-dc0b-44c5-a884-75d07fb2b179 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:06:29.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5115" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3639,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:06:29.422: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 21 09:06:29.593: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8276  5c396b86-343d-4ed6-be79-055dc574f33f 46255 0 2021-05-21 09:06:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 09:06:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 09:06:29.594: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8276  5c396b86-343d-4ed6-be79-055dc574f33f 46256 0 2021-05-21 09:06:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 09:06:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 09:06:29.594: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8276  5c396b86-343d-4ed6-be79-055dc574f33f 46257 0 2021-05-21 09:06:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 09:06:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 21 09:06:39.663: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8276  5c396b86-343d-4ed6-be79-055dc574f33f 46312 0 2021-05-21 09:06:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 09:06:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 09:06:39.663: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8276  5c396b86-343d-4ed6-be79-055dc574f33f 46313 0 2021-05-21 09:06:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 09:06:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 09:06:39.663: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8276  5c396b86-343d-4ed6-be79-055dc574f33f 46314 0 2021-05-21 09:06:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 09:06:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:06:39.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8276" for this suite.

• [SLOW TEST:10.256 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":197,"skipped":3654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:06:39.679: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 21 09:06:39.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-1467 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 21 09:06:42.141: INFO: stderr: ""
May 21 09:06:42.141: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 21 09:06:42.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-1467 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
May 21 09:06:42.972: INFO: stderr: ""
May 21 09:06:42.972: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
May 21 09:06:42.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-1467 delete pods e2e-test-httpd-pod'
May 21 09:06:44.638: INFO: stderr: ""
May 21 09:06:44.638: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:06:44.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1467" for this suite.

• [SLOW TEST:5.020 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":198,"skipped":3688,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:06:44.700: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-3e122778-b850-4565-ac97-604d14ec22a5
STEP: Creating a pod to test consume secrets
May 21 09:06:45.143: INFO: Waiting up to 5m0s for pod "pod-secrets-e537bff1-a804-47f1-a58e-719e97cf5c62" in namespace "secrets-8058" to be "Succeeded or Failed"
May 21 09:06:45.264: INFO: Pod "pod-secrets-e537bff1-a804-47f1-a58e-719e97cf5c62": Phase="Pending", Reason="", readiness=false. Elapsed: 121.115403ms
May 21 09:06:47.275: INFO: Pod "pod-secrets-e537bff1-a804-47f1-a58e-719e97cf5c62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13244512s
May 21 09:06:49.281: INFO: Pod "pod-secrets-e537bff1-a804-47f1-a58e-719e97cf5c62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.137921599s
STEP: Saw pod success
May 21 09:06:49.281: INFO: Pod "pod-secrets-e537bff1-a804-47f1-a58e-719e97cf5c62" satisfied condition "Succeeded or Failed"
May 21 09:06:49.284: INFO: Trying to get logs from node k8s-03 pod pod-secrets-e537bff1-a804-47f1-a58e-719e97cf5c62 container secret-volume-test: <nil>
STEP: delete the pod
May 21 09:06:49.457: INFO: Waiting for pod pod-secrets-e537bff1-a804-47f1-a58e-719e97cf5c62 to disappear
May 21 09:06:49.479: INFO: Pod pod-secrets-e537bff1-a804-47f1-a58e-719e97cf5c62 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:06:49.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8058" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":199,"skipped":3693,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:06:49.526: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-kjxs
STEP: Creating a pod to test atomic-volume-subpath
May 21 09:06:49.801: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kjxs" in namespace "subpath-2933" to be "Succeeded or Failed"
May 21 09:06:49.831: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Pending", Reason="", readiness=false. Elapsed: 29.340395ms
May 21 09:06:51.837: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036000111s
May 21 09:06:53.844: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Running", Reason="", readiness=true. Elapsed: 4.042732043s
May 21 09:06:55.852: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Running", Reason="", readiness=true. Elapsed: 6.050430287s
May 21 09:06:57.859: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Running", Reason="", readiness=true. Elapsed: 8.057399785s
May 21 09:06:59.866: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Running", Reason="", readiness=true. Elapsed: 10.064611007s
May 21 09:07:01.873: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Running", Reason="", readiness=true. Elapsed: 12.07193718s
May 21 09:07:03.881: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Running", Reason="", readiness=true. Elapsed: 14.07946796s
May 21 09:07:05.887: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Running", Reason="", readiness=true. Elapsed: 16.086059207s
May 21 09:07:07.924: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Running", Reason="", readiness=true. Elapsed: 18.122393961s
May 21 09:07:09.932: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Running", Reason="", readiness=true. Elapsed: 20.130455513s
May 21 09:07:11.938: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Running", Reason="", readiness=true. Elapsed: 22.137205494s
May 21 09:07:13.944: INFO: Pod "pod-subpath-test-configmap-kjxs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.142407046s
STEP: Saw pod success
May 21 09:07:13.944: INFO: Pod "pod-subpath-test-configmap-kjxs" satisfied condition "Succeeded or Failed"
May 21 09:07:13.948: INFO: Trying to get logs from node k8s-03 pod pod-subpath-test-configmap-kjxs container test-container-subpath-configmap-kjxs: <nil>
STEP: delete the pod
May 21 09:07:14.026: INFO: Waiting for pod pod-subpath-test-configmap-kjxs to disappear
May 21 09:07:14.038: INFO: Pod pod-subpath-test-configmap-kjxs no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kjxs
May 21 09:07:14.038: INFO: Deleting pod "pod-subpath-test-configmap-kjxs" in namespace "subpath-2933"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:07:14.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2933" for this suite.

• [SLOW TEST:24.561 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":200,"skipped":3698,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:07:14.087: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 21 09:07:14.203: INFO: Waiting up to 5m0s for pod "pod-24348edb-233b-4f21-9b98-bd4a2248718c" in namespace "emptydir-8690" to be "Succeeded or Failed"
May 21 09:07:14.233: INFO: Pod "pod-24348edb-233b-4f21-9b98-bd4a2248718c": Phase="Pending", Reason="", readiness=false. Elapsed: 29.754501ms
May 21 09:07:16.256: INFO: Pod "pod-24348edb-233b-4f21-9b98-bd4a2248718c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05292967s
May 21 09:07:18.263: INFO: Pod "pod-24348edb-233b-4f21-9b98-bd4a2248718c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059574245s
STEP: Saw pod success
May 21 09:07:18.263: INFO: Pod "pod-24348edb-233b-4f21-9b98-bd4a2248718c" satisfied condition "Succeeded or Failed"
May 21 09:07:18.267: INFO: Trying to get logs from node k8s-03 pod pod-24348edb-233b-4f21-9b98-bd4a2248718c container test-container: <nil>
STEP: delete the pod
May 21 09:07:18.350: INFO: Waiting for pod pod-24348edb-233b-4f21-9b98-bd4a2248718c to disappear
May 21 09:07:18.360: INFO: Pod pod-24348edb-233b-4f21-9b98-bd4a2248718c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:07:18.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8690" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":201,"skipped":3700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:07:18.382: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:07:18.522: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 09:07:23.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-9212 --namespace=crd-publish-openapi-9212 create -f -'
May 21 09:07:27.419: INFO: stderr: ""
May 21 09:07:27.419: INFO: stdout: "e2e-test-crd-publish-openapi-639-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 21 09:07:27.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-9212 --namespace=crd-publish-openapi-9212 delete e2e-test-crd-publish-openapi-639-crds test-cr'
May 21 09:07:27.745: INFO: stderr: ""
May 21 09:07:27.745: INFO: stdout: "e2e-test-crd-publish-openapi-639-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 21 09:07:27.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-9212 --namespace=crd-publish-openapi-9212 apply -f -'
May 21 09:07:28.649: INFO: stderr: ""
May 21 09:07:28.649: INFO: stdout: "e2e-test-crd-publish-openapi-639-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 21 09:07:28.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-9212 --namespace=crd-publish-openapi-9212 delete e2e-test-crd-publish-openapi-639-crds test-cr'
May 21 09:07:28.878: INFO: stderr: ""
May 21 09:07:28.878: INFO: stdout: "e2e-test-crd-publish-openapi-639-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 21 09:07:28.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=crd-publish-openapi-9212 explain e2e-test-crd-publish-openapi-639-crds'
May 21 09:07:29.626: INFO: stderr: ""
May 21 09:07:29.626: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-639-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:07:35.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9212" for this suite.

• [SLOW TEST:17.096 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":202,"skipped":3760,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:07:35.479: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1318.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1318.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1318.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1318.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1318.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1318.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1318.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1318.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1318.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1318.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 133.91.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.91.133_udp@PTR;check="$$(dig +tcp +noall +answer +search 133.91.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.91.133_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1318.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1318.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1318.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1318.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1318.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1318.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1318.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1318.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1318.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1318.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1318.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 133.91.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.91.133_udp@PTR;check="$$(dig +tcp +noall +answer +search 133.91.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.91.133_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 09:07:42.095: INFO: Unable to read wheezy_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:42.100: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:42.104: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:42.108: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:42.182: INFO: Unable to read jessie_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:42.187: INFO: Unable to read jessie_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:42.192: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:42.197: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:42.225: INFO: Lookups using dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6 failed for: [wheezy_udp@dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_udp@dns-test-service.dns-1318.svc.cluster.local jessie_tcp@dns-test-service.dns-1318.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local]

May 21 09:07:47.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:47.241: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:47.248: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:47.255: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:47.301: INFO: Unable to read jessie_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:47.307: INFO: Unable to read jessie_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:47.332: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:47.416: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:47.445: INFO: Lookups using dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6 failed for: [wheezy_udp@dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_udp@dns-test-service.dns-1318.svc.cluster.local jessie_tcp@dns-test-service.dns-1318.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local]

May 21 09:07:52.231: INFO: Unable to read wheezy_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:52.237: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:52.242: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:52.249: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:52.302: INFO: Unable to read jessie_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:52.306: INFO: Unable to read jessie_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:52.342: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:52.352: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:52.382: INFO: Lookups using dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6 failed for: [wheezy_udp@dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_udp@dns-test-service.dns-1318.svc.cluster.local jessie_tcp@dns-test-service.dns-1318.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local]

May 21 09:07:57.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:57.237: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:57.242: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:57.248: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:57.285: INFO: Unable to read jessie_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:57.290: INFO: Unable to read jessie_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:57.296: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:57.301: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:07:57.343: INFO: Lookups using dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6 failed for: [wheezy_udp@dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_udp@dns-test-service.dns-1318.svc.cluster.local jessie_tcp@dns-test-service.dns-1318.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local]

May 21 09:08:02.233: INFO: Unable to read wheezy_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:02.238: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:02.242: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:02.247: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:02.281: INFO: Unable to read jessie_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:02.285: INFO: Unable to read jessie_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:02.290: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:02.295: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:02.323: INFO: Lookups using dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6 failed for: [wheezy_udp@dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_udp@dns-test-service.dns-1318.svc.cluster.local jessie_tcp@dns-test-service.dns-1318.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local]

May 21 09:08:07.233: INFO: Unable to read wheezy_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:07.240: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:07.246: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:07.252: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:07.286: INFO: Unable to read jessie_udp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:07.290: INFO: Unable to read jessie_tcp@dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:07.295: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:07.300: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local from pod dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6: the server could not find the requested resource (get pods dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6)
May 21 09:08:07.331: INFO: Lookups using dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6 failed for: [wheezy_udp@dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@dns-test-service.dns-1318.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_udp@dns-test-service.dns-1318.svc.cluster.local jessie_tcp@dns-test-service.dns-1318.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1318.svc.cluster.local]

May 21 09:08:12.326: INFO: DNS probes using dns-1318/dns-test-b4d96518-3f56-4ce5-95fc-78533f1bfce6 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:08:12.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1318" for this suite.

• [SLOW TEST:37.475 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":203,"skipped":3764,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:08:12.954: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
May 21 09:08:13.096: INFO: Waiting up to 5m0s for pod "var-expansion-cf490281-fcdb-45f2-971f-ed553c7ec438" in namespace "var-expansion-8957" to be "Succeeded or Failed"
May 21 09:08:13.151: INFO: Pod "var-expansion-cf490281-fcdb-45f2-971f-ed553c7ec438": Phase="Pending", Reason="", readiness=false. Elapsed: 54.106745ms
May 21 09:08:15.157: INFO: Pod "var-expansion-cf490281-fcdb-45f2-971f-ed553c7ec438": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061089505s
May 21 09:08:17.164: INFO: Pod "var-expansion-cf490281-fcdb-45f2-971f-ed553c7ec438": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067973259s
STEP: Saw pod success
May 21 09:08:17.164: INFO: Pod "var-expansion-cf490281-fcdb-45f2-971f-ed553c7ec438" satisfied condition "Succeeded or Failed"
May 21 09:08:17.191: INFO: Trying to get logs from node k8s-03 pod var-expansion-cf490281-fcdb-45f2-971f-ed553c7ec438 container dapi-container: <nil>
STEP: delete the pod
May 21 09:08:17.253: INFO: Waiting for pod var-expansion-cf490281-fcdb-45f2-971f-ed553c7ec438 to disappear
May 21 09:08:17.263: INFO: Pod var-expansion-cf490281-fcdb-45f2-971f-ed553c7ec438 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:08:17.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8957" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":204,"skipped":3768,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:08:17.282: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:08:28.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5674" for this suite.

• [SLOW TEST:11.372 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":205,"skipped":3793,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:08:28.655: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-25bvg in namespace proxy-9768
I0521 09:08:28.826823      25 runners.go:190] Created replication controller with name: proxy-service-25bvg, namespace: proxy-9768, replica count: 1
I0521 09:08:29.877380      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:08:30.877849      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:08:31.878242      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:08:32.878687      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 09:08:33.879049      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 09:08:34.879386      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 09:08:35.879787      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 09:08:36.880157      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 09:08:37.880486      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 09:08:38.889801      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 09:08:39.890015      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 09:08:40.890361      25 runners.go:190] proxy-service-25bvg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 09:08:40.900: INFO: setup took 12.127018641s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 21 09:08:40.921: INFO: (0) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 20.176638ms)
May 21 09:08:40.921: INFO: (0) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 20.290712ms)
May 21 09:08:40.923: INFO: (0) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 22.617611ms)
May 21 09:08:40.924: INFO: (0) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 23.54278ms)
May 21 09:08:40.927: INFO: (0) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 26.144089ms)
May 21 09:08:40.928: INFO: (0) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 27.430952ms)
May 21 09:08:40.928: INFO: (0) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 27.971724ms)
May 21 09:08:40.929: INFO: (0) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 29.116039ms)
May 21 09:08:40.929: INFO: (0) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 28.866183ms)
May 21 09:08:40.929: INFO: (0) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 29.059248ms)
May 21 09:08:40.930: INFO: (0) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 29.20066ms)
May 21 09:08:40.930: INFO: (0) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 30.580107ms)
May 21 09:08:40.931: INFO: (0) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 29.897669ms)
May 21 09:08:40.931: INFO: (0) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 31.351194ms)
May 21 09:08:40.931: INFO: (0) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 31.187805ms)
May 21 09:08:40.931: INFO: (0) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 30.943572ms)
May 21 09:08:40.942: INFO: (1) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 10.441666ms)
May 21 09:08:40.943: INFO: (1) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 11.667889ms)
May 21 09:08:40.945: INFO: (1) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 13.198878ms)
May 21 09:08:40.945: INFO: (1) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 12.82351ms)
May 21 09:08:40.945: INFO: (1) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 12.948061ms)
May 21 09:08:40.945: INFO: (1) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 13.368534ms)
May 21 09:08:40.945: INFO: (1) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 13.128651ms)
May 21 09:08:40.945: INFO: (1) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 13.239318ms)
May 21 09:08:40.945: INFO: (1) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 13.026093ms)
May 21 09:08:40.945: INFO: (1) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 13.282109ms)
May 21 09:08:40.947: INFO: (1) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 15.341854ms)
May 21 09:08:40.948: INFO: (1) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 16.252586ms)
May 21 09:08:40.949: INFO: (1) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 16.948224ms)
May 21 09:08:40.949: INFO: (1) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 16.853743ms)
May 21 09:08:40.949: INFO: (1) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 17.271776ms)
May 21 09:08:40.949: INFO: (1) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 17.0946ms)
May 21 09:08:40.996: INFO: (2) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 46.2701ms)
May 21 09:08:40.996: INFO: (2) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 46.645498ms)
May 21 09:08:40.996: INFO: (2) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 46.61629ms)
May 21 09:08:40.996: INFO: (2) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 46.469913ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 49.609192ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 49.526154ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 49.951446ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 49.903599ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 49.953049ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 50.206002ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 50.113722ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 50.01598ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 50.041674ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 50.036501ms)
May 21 09:08:40.999: INFO: (2) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 50.003523ms)
May 21 09:08:41.000: INFO: (2) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 50.564603ms)
May 21 09:08:41.006: INFO: (3) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 5.780846ms)
May 21 09:08:41.011: INFO: (3) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 10.928559ms)
May 21 09:08:41.011: INFO: (3) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 11.393048ms)
May 21 09:08:41.012: INFO: (3) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 11.129349ms)
May 21 09:08:41.012: INFO: (3) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 11.426529ms)
May 21 09:08:41.012: INFO: (3) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 11.51508ms)
May 21 09:08:41.012: INFO: (3) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 11.933362ms)
May 21 09:08:41.013: INFO: (3) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 12.406834ms)
May 21 09:08:41.013: INFO: (3) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 12.704338ms)
May 21 09:08:41.014: INFO: (3) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 13.288465ms)
May 21 09:08:41.014: INFO: (3) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 13.437804ms)
May 21 09:08:41.020: INFO: (3) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 19.616834ms)
May 21 09:08:41.021: INFO: (3) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 20.451952ms)
May 21 09:08:41.021: INFO: (3) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 20.515085ms)
May 21 09:08:41.021: INFO: (3) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 20.836846ms)
May 21 09:08:41.021: INFO: (3) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 20.484025ms)
May 21 09:08:41.042: INFO: (4) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 20.668101ms)
May 21 09:08:41.043: INFO: (4) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 21.107604ms)
May 21 09:08:41.043: INFO: (4) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 21.099897ms)
May 21 09:08:41.045: INFO: (4) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 23.735533ms)
May 21 09:08:41.046: INFO: (4) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 23.991239ms)
May 21 09:08:41.046: INFO: (4) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 24.003886ms)
May 21 09:08:41.046: INFO: (4) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 24.115347ms)
May 21 09:08:41.046: INFO: (4) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 23.957339ms)
May 21 09:08:41.046: INFO: (4) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 24.424417ms)
May 21 09:08:41.047: INFO: (4) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 25.096526ms)
May 21 09:08:41.047: INFO: (4) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 25.302655ms)
May 21 09:08:41.047: INFO: (4) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 25.646203ms)
May 21 09:08:41.048: INFO: (4) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 25.992247ms)
May 21 09:08:41.048: INFO: (4) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 26.364152ms)
May 21 09:08:41.049: INFO: (4) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 27.381088ms)
May 21 09:08:41.049: INFO: (4) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 27.139548ms)
May 21 09:08:41.056: INFO: (5) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 7.371238ms)
May 21 09:08:41.056: INFO: (5) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 7.635729ms)
May 21 09:08:41.057: INFO: (5) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 7.937383ms)
May 21 09:08:41.062: INFO: (5) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 12.974122ms)
May 21 09:08:41.062: INFO: (5) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 13.389281ms)
May 21 09:08:41.063: INFO: (5) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 13.974784ms)
May 21 09:08:41.063: INFO: (5) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 14.013328ms)
May 21 09:08:41.063: INFO: (5) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 13.868332ms)
May 21 09:08:41.063: INFO: (5) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 13.859326ms)
May 21 09:08:41.063: INFO: (5) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 13.842159ms)
May 21 09:08:41.066: INFO: (5) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 17.503558ms)
May 21 09:08:41.066: INFO: (5) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 17.490451ms)
May 21 09:08:41.066: INFO: (5) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 17.347556ms)
May 21 09:08:41.067: INFO: (5) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 18.000217ms)
May 21 09:08:41.067: INFO: (5) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 18.061975ms)
May 21 09:08:41.067: INFO: (5) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 18.237204ms)
May 21 09:08:41.081: INFO: (6) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 13.534582ms)
May 21 09:08:41.082: INFO: (6) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 13.751595ms)
May 21 09:08:41.082: INFO: (6) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 14.082142ms)
May 21 09:08:41.083: INFO: (6) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 15.219646ms)
May 21 09:08:41.084: INFO: (6) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 16.06119ms)
May 21 09:08:41.086: INFO: (6) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 18.093036ms)
May 21 09:08:41.087: INFO: (6) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 19.27201ms)
May 21 09:08:41.087: INFO: (6) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 19.393885ms)
May 21 09:08:41.087: INFO: (6) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 19.093814ms)
May 21 09:08:41.087: INFO: (6) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 19.321397ms)
May 21 09:08:41.087: INFO: (6) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 19.192213ms)
May 21 09:08:41.087: INFO: (6) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 19.573134ms)
May 21 09:08:41.087: INFO: (6) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 19.942999ms)
May 21 09:08:41.088: INFO: (6) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 20.513243ms)
May 21 09:08:41.089: INFO: (6) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 20.973098ms)
May 21 09:08:41.089: INFO: (6) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 21.210611ms)
May 21 09:08:41.094: INFO: (7) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 5.527626ms)
May 21 09:08:41.096: INFO: (7) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 6.259758ms)
May 21 09:08:41.100: INFO: (7) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 9.588403ms)
May 21 09:08:41.102: INFO: (7) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 12.409865ms)
May 21 09:08:41.102: INFO: (7) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 11.242336ms)
May 21 09:08:41.102: INFO: (7) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 11.685429ms)
May 21 09:08:41.102: INFO: (7) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 11.211473ms)
May 21 09:08:41.102: INFO: (7) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 11.561474ms)
May 21 09:08:41.102: INFO: (7) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 11.194207ms)
May 21 09:08:41.102: INFO: (7) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 11.093695ms)
May 21 09:08:41.102: INFO: (7) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 11.526224ms)
May 21 09:08:41.106: INFO: (7) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 15.175775ms)
May 21 09:08:41.106: INFO: (7) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 15.036304ms)
May 21 09:08:41.106: INFO: (7) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 15.231223ms)
May 21 09:08:41.106: INFO: (7) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 16.213432ms)
May 21 09:08:41.106: INFO: (7) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 16.161588ms)
May 21 09:08:41.118: INFO: (8) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 11.707653ms)
May 21 09:08:41.118: INFO: (8) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 11.566241ms)
May 21 09:08:41.118: INFO: (8) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 11.322092ms)
May 21 09:08:41.118: INFO: (8) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 11.826681ms)
May 21 09:08:41.118: INFO: (8) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 11.943836ms)
May 21 09:08:41.119: INFO: (8) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 12.181162ms)
May 21 09:08:41.120: INFO: (8) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 13.275682ms)
May 21 09:08:41.124: INFO: (8) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 17.166177ms)
May 21 09:08:41.124: INFO: (8) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 17.040789ms)
May 21 09:08:41.124: INFO: (8) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 17.169048ms)
May 21 09:08:41.124: INFO: (8) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 17.481151ms)
May 21 09:08:41.125: INFO: (8) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 18.282285ms)
May 21 09:08:41.125: INFO: (8) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 18.371329ms)
May 21 09:08:41.125: INFO: (8) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 18.73332ms)
May 21 09:08:41.127: INFO: (8) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 20.052117ms)
May 21 09:08:41.127: INFO: (8) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 20.378524ms)
May 21 09:08:41.142: INFO: (9) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 13.539738ms)
May 21 09:08:41.143: INFO: (9) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 15.136952ms)
May 21 09:08:41.143: INFO: (9) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 15.167192ms)
May 21 09:08:41.143: INFO: (9) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 15.364071ms)
May 21 09:08:41.143: INFO: (9) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 15.006697ms)
May 21 09:08:41.143: INFO: (9) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 15.801174ms)
May 21 09:08:41.144: INFO: (9) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 15.840748ms)
May 21 09:08:41.144: INFO: (9) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 16.182732ms)
May 21 09:08:41.144: INFO: (9) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 16.359208ms)
May 21 09:08:41.144: INFO: (9) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 15.987539ms)
May 21 09:08:41.144: INFO: (9) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 17.204941ms)
May 21 09:08:41.144: INFO: (9) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 16.407094ms)
May 21 09:08:41.144: INFO: (9) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 16.477006ms)
May 21 09:08:41.145: INFO: (9) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 17.608509ms)
May 21 09:08:41.146: INFO: (9) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 17.861009ms)
May 21 09:08:41.146: INFO: (9) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 18.658925ms)
May 21 09:08:41.155: INFO: (10) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 8.584753ms)
May 21 09:08:41.156: INFO: (10) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 8.855777ms)
May 21 09:08:41.156: INFO: (10) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 8.53414ms)
May 21 09:08:41.156: INFO: (10) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 8.686552ms)
May 21 09:08:41.162: INFO: (10) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 15.873358ms)
May 21 09:08:41.166: INFO: (10) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 18.272184ms)
May 21 09:08:41.166: INFO: (10) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 18.414699ms)
May 21 09:08:41.166: INFO: (10) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 18.440227ms)
May 21 09:08:41.166: INFO: (10) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 18.372196ms)
May 21 09:08:41.166: INFO: (10) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 18.309101ms)
May 21 09:08:41.167: INFO: (10) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 19.579484ms)
May 21 09:08:41.168: INFO: (10) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 20.341024ms)
May 21 09:08:41.168: INFO: (10) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 20.592879ms)
May 21 09:08:41.168: INFO: (10) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 20.994172ms)
May 21 09:08:41.168: INFO: (10) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 21.084957ms)
May 21 09:08:41.168: INFO: (10) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 21.039762ms)
May 21 09:08:41.177: INFO: (11) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 8.654601ms)
May 21 09:08:41.178: INFO: (11) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 9.080356ms)
May 21 09:08:41.186: INFO: (11) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 17.282505ms)
May 21 09:08:41.186: INFO: (11) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 17.35306ms)
May 21 09:08:41.186: INFO: (11) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 17.420574ms)
May 21 09:08:41.187: INFO: (11) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 17.326829ms)
May 21 09:08:41.187: INFO: (11) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 17.93287ms)
May 21 09:08:41.187: INFO: (11) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 17.703218ms)
May 21 09:08:41.188: INFO: (11) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 19.228703ms)
May 21 09:08:41.189: INFO: (11) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 19.679669ms)
May 21 09:08:41.189: INFO: (11) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 19.650195ms)
May 21 09:08:41.189: INFO: (11) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 19.983855ms)
May 21 09:08:41.189: INFO: (11) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 19.638745ms)
May 21 09:08:41.189: INFO: (11) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 20.230133ms)
May 21 09:08:41.189: INFO: (11) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 20.317334ms)
May 21 09:08:41.190: INFO: (11) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 20.801063ms)
May 21 09:08:41.195: INFO: (12) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 5.555829ms)
May 21 09:08:41.205: INFO: (12) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 14.857813ms)
May 21 09:08:41.206: INFO: (12) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 15.700076ms)
May 21 09:08:41.206: INFO: (12) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 15.785228ms)
May 21 09:08:41.206: INFO: (12) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 15.720749ms)
May 21 09:08:41.206: INFO: (12) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 15.751206ms)
May 21 09:08:41.206: INFO: (12) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 15.872838ms)
May 21 09:08:41.207: INFO: (12) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 16.474976ms)
May 21 09:08:41.207: INFO: (12) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 16.652158ms)
May 21 09:08:41.207: INFO: (12) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 16.79968ms)
May 21 09:08:41.207: INFO: (12) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 16.800404ms)
May 21 09:08:41.210: INFO: (12) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 19.652536ms)
May 21 09:08:41.210: INFO: (12) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 20.376442ms)
May 21 09:08:41.210: INFO: (12) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 20.26627ms)
May 21 09:08:41.211: INFO: (12) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 20.460718ms)
May 21 09:08:41.211: INFO: (12) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 21.183044ms)
May 21 09:08:41.223: INFO: (13) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 11.702688ms)
May 21 09:08:41.224: INFO: (13) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 12.3081ms)
May 21 09:08:41.224: INFO: (13) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 12.438911ms)
May 21 09:08:41.224: INFO: (13) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 12.761322ms)
May 21 09:08:41.225: INFO: (13) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 13.110996ms)
May 21 09:08:41.225: INFO: (13) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 13.291192ms)
May 21 09:08:41.225: INFO: (13) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 13.046862ms)
May 21 09:08:41.225: INFO: (13) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 13.200228ms)
May 21 09:08:41.225: INFO: (13) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 13.027895ms)
May 21 09:08:41.225: INFO: (13) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 13.209701ms)
May 21 09:08:41.228: INFO: (13) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 16.267086ms)
May 21 09:08:41.229: INFO: (13) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 17.200797ms)
May 21 09:08:41.229: INFO: (13) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 17.123066ms)
May 21 09:08:41.230: INFO: (13) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 17.990337ms)
May 21 09:08:41.230: INFO: (13) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 18.662548ms)
May 21 09:08:41.230: INFO: (13) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 19.03687ms)
May 21 09:08:41.238: INFO: (14) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 7.136452ms)
May 21 09:08:41.238: INFO: (14) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 6.787574ms)
May 21 09:08:41.238: INFO: (14) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 7.24567ms)
May 21 09:08:41.244: INFO: (14) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 12.211226ms)
May 21 09:08:41.243: INFO: (14) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 12.555884ms)
May 21 09:08:41.244: INFO: (14) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 13.365759ms)
May 21 09:08:41.244: INFO: (14) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 12.906254ms)
May 21 09:08:41.245: INFO: (14) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 13.139327ms)
May 21 09:08:41.246: INFO: (14) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 14.194007ms)
May 21 09:08:41.246: INFO: (14) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 14.068982ms)
May 21 09:08:41.249: INFO: (14) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 17.429621ms)
May 21 09:08:41.250: INFO: (14) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 18.159919ms)
May 21 09:08:41.250: INFO: (14) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 18.999954ms)
May 21 09:08:41.250: INFO: (14) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 19.652282ms)
May 21 09:08:41.251: INFO: (14) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 18.871381ms)
May 21 09:08:41.251: INFO: (14) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 19.949858ms)
May 21 09:08:41.261: INFO: (15) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 10.213538ms)
May 21 09:08:41.262: INFO: (15) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 10.346889ms)
May 21 09:08:41.262: INFO: (15) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 11.4715ms)
May 21 09:08:41.264: INFO: (15) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 12.10611ms)
May 21 09:08:41.266: INFO: (15) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 13.540059ms)
May 21 09:08:41.266: INFO: (15) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 14.322562ms)
May 21 09:08:41.266: INFO: (15) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 14.896389ms)
May 21 09:08:41.266: INFO: (15) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 13.813836ms)
May 21 09:08:41.266: INFO: (15) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 13.95968ms)
May 21 09:08:41.266: INFO: (15) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 13.534738ms)
May 21 09:08:41.266: INFO: (15) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 14.355289ms)
May 21 09:08:41.266: INFO: (15) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 14.282251ms)
May 21 09:08:41.277: INFO: (15) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 24.687514ms)
May 21 09:08:41.277: INFO: (15) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 25.272058ms)
May 21 09:08:41.278: INFO: (15) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 27.094305ms)
May 21 09:08:41.365: INFO: (15) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 113.845749ms)
May 21 09:08:41.377: INFO: (16) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 11.370082ms)
May 21 09:08:41.379: INFO: (16) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 14.343859ms)
May 21 09:08:41.380: INFO: (16) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 14.056031ms)
May 21 09:08:41.381: INFO: (16) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 15.692939ms)
May 21 09:08:41.381: INFO: (16) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 15.813164ms)
May 21 09:08:41.382: INFO: (16) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 16.707627ms)
May 21 09:08:41.382: INFO: (16) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 16.744385ms)
May 21 09:08:41.382: INFO: (16) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 16.767586ms)
May 21 09:08:41.382: INFO: (16) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 17.163621ms)
May 21 09:08:41.382: INFO: (16) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 16.516136ms)
May 21 09:08:41.382: INFO: (16) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 16.566169ms)
May 21 09:08:41.383: INFO: (16) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 17.334856ms)
May 21 09:08:41.383: INFO: (16) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 17.378459ms)
May 21 09:08:41.383: INFO: (16) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 17.595889ms)
May 21 09:08:41.383: INFO: (16) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 17.529369ms)
May 21 09:08:41.383: INFO: (16) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 18.173193ms)
May 21 09:08:41.397: INFO: (17) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 13.451017ms)
May 21 09:08:41.397: INFO: (17) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 13.799755ms)
May 21 09:08:41.397: INFO: (17) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 13.614527ms)
May 21 09:08:41.398: INFO: (17) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 14.13347ms)
May 21 09:08:41.400: INFO: (17) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 16.179422ms)
May 21 09:08:41.400: INFO: (17) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 16.36403ms)
May 21 09:08:41.400: INFO: (17) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 16.364834ms)
May 21 09:08:41.400: INFO: (17) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 16.611774ms)
May 21 09:08:41.401: INFO: (17) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 17.325646ms)
May 21 09:08:41.401: INFO: (17) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 17.712328ms)
May 21 09:08:41.403: INFO: (17) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 19.647368ms)
May 21 09:08:41.403: INFO: (17) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 19.358941ms)
May 21 09:08:41.403: INFO: (17) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 20.120024ms)
May 21 09:08:41.404: INFO: (17) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 20.224106ms)
May 21 09:08:41.404: INFO: (17) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 20.277446ms)
May 21 09:08:41.404: INFO: (17) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 20.147131ms)
May 21 09:08:41.417: INFO: (18) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 10.945557ms)
May 21 09:08:41.417: INFO: (18) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 11.776787ms)
May 21 09:08:41.418: INFO: (18) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 12.045414ms)
May 21 09:08:41.418: INFO: (18) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 12.232412ms)
May 21 09:08:41.418: INFO: (18) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 13.016249ms)
May 21 09:08:41.418: INFO: (18) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 14.39729ms)
May 21 09:08:41.419: INFO: (18) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 13.417587ms)
May 21 09:08:41.420: INFO: (18) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 15.411315ms)
May 21 09:08:41.420: INFO: (18) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 15.626064ms)
May 21 09:08:41.421: INFO: (18) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 15.991932ms)
May 21 09:08:41.423: INFO: (18) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 16.925194ms)
May 21 09:08:41.423: INFO: (18) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 17.769201ms)
May 21 09:08:41.424: INFO: (18) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 18.026798ms)
May 21 09:08:41.424: INFO: (18) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 18.285725ms)
May 21 09:08:41.424: INFO: (18) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 19.943579ms)
May 21 09:08:41.424: INFO: (18) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 19.133412ms)
May 21 09:08:41.436: INFO: (19) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 11.306478ms)
May 21 09:08:41.437: INFO: (19) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 12.351738ms)
May 21 09:08:41.437: INFO: (19) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">test<... (200; 11.940089ms)
May 21 09:08:41.437: INFO: (19) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:460/proxy/: tls baz (200; 12.165022ms)
May 21 09:08:41.437: INFO: (19) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:162/proxy/: bar (200; 11.743694ms)
May 21 09:08:41.437: INFO: (19) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5/proxy/rewriteme">test</a> (200; 11.586222ms)
May 21 09:08:41.437: INFO: (19) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:462/proxy/: tls qux (200; 12.032007ms)
May 21 09:08:41.437: INFO: (19) /api/v1/namespaces/proxy-9768/pods/proxy-service-25bvg-tvgd5:160/proxy/: foo (200; 12.413975ms)
May 21 09:08:41.437: INFO: (19) /api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/http:proxy-service-25bvg-tvgd5:1080/proxy/rewriteme">... (200; 12.511062ms)
May 21 09:08:41.437: INFO: (19) /api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/: <a href="/api/v1/namespaces/proxy-9768/pods/https:proxy-service-25bvg-tvgd5:443/proxy/tlsrewritem... (200; 12.591474ms)
May 21 09:08:41.442: INFO: (19) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname1/proxy/: foo (200; 16.804446ms)
May 21 09:08:41.443: INFO: (19) /api/v1/namespaces/proxy-9768/services/proxy-service-25bvg:portname2/proxy/: bar (200; 17.613263ms)
May 21 09:08:41.443: INFO: (19) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname2/proxy/: bar (200; 17.838035ms)
May 21 09:08:41.443: INFO: (19) /api/v1/namespaces/proxy-9768/services/http:proxy-service-25bvg:portname1/proxy/: foo (200; 18.162709ms)
May 21 09:08:41.443: INFO: (19) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname1/proxy/: tls baz (200; 18.445743ms)
May 21 09:08:41.443: INFO: (19) /api/v1/namespaces/proxy-9768/services/https:proxy-service-25bvg:tlsportname2/proxy/: tls qux (200; 18.445473ms)
STEP: deleting ReplicationController proxy-service-25bvg in namespace proxy-9768, will wait for the garbage collector to delete the pods
May 21 09:08:41.509: INFO: Deleting ReplicationController proxy-service-25bvg took: 9.513975ms
May 21 09:08:42.110: INFO: Terminating ReplicationController proxy-service-25bvg pods took: 600.279978ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:08:50.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9768" for this suite.

• [SLOW TEST:21.619 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":206,"skipped":3812,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:08:50.274: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-38787b66-d3ad-45ec-99cc-c08741cd447e in namespace container-probe-8451
May 21 09:08:54.566: INFO: Started pod busybox-38787b66-d3ad-45ec-99cc-c08741cd447e in namespace container-probe-8451
STEP: checking the pod's current state and verifying that restartCount is present
May 21 09:08:54.570: INFO: Initial restart count of pod busybox-38787b66-d3ad-45ec-99cc-c08741cd447e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:12:56.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8451" for this suite.

• [SLOW TEST:246.012 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3817,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:12:56.287: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
May 21 09:12:56.565: INFO: Major version: 1
STEP: Confirm minor version
May 21 09:12:56.565: INFO: cleanMinorVersion: 20
May 21 09:12:56.565: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:12:56.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6301" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":208,"skipped":3831,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:12:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 21 09:12:56.716: INFO: Waiting up to 5m0s for pod "downward-api-08204495-1b5f-4148-ac12-e3ffb6825cf9" in namespace "downward-api-3079" to be "Succeeded or Failed"
May 21 09:12:56.722: INFO: Pod "downward-api-08204495-1b5f-4148-ac12-e3ffb6825cf9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.317839ms
May 21 09:12:58.729: INFO: Pod "downward-api-08204495-1b5f-4148-ac12-e3ffb6825cf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01278001s
May 21 09:13:00.817: INFO: Pod "downward-api-08204495-1b5f-4148-ac12-e3ffb6825cf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.100616029s
STEP: Saw pod success
May 21 09:13:00.817: INFO: Pod "downward-api-08204495-1b5f-4148-ac12-e3ffb6825cf9" satisfied condition "Succeeded or Failed"
May 21 09:13:00.834: INFO: Trying to get logs from node k8s-03 pod downward-api-08204495-1b5f-4148-ac12-e3ffb6825cf9 container dapi-container: <nil>
STEP: delete the pod
May 21 09:13:01.034: INFO: Waiting for pod downward-api-08204495-1b5f-4148-ac12-e3ffb6825cf9 to disappear
May 21 09:13:01.103: INFO: Pod downward-api-08204495-1b5f-4148-ac12-e3ffb6825cf9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:13:01.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3079" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":209,"skipped":3834,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:13:01.130: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:13:01.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2026" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":210,"skipped":3848,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:13:01.409: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-1b4f531d-698c-4942-b763-db9df8712012
STEP: Creating a pod to test consume secrets
May 21 09:13:01.823: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8456dfc5-c1a7-4a10-aa99-bd41bf40da65" in namespace "projected-2289" to be "Succeeded or Failed"
May 21 09:13:01.871: INFO: Pod "pod-projected-secrets-8456dfc5-c1a7-4a10-aa99-bd41bf40da65": Phase="Pending", Reason="", readiness=false. Elapsed: 48.281613ms
May 21 09:13:04.000: INFO: Pod "pod-projected-secrets-8456dfc5-c1a7-4a10-aa99-bd41bf40da65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.177266893s
May 21 09:13:06.033: INFO: Pod "pod-projected-secrets-8456dfc5-c1a7-4a10-aa99-bd41bf40da65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.210201113s
STEP: Saw pod success
May 21 09:13:06.033: INFO: Pod "pod-projected-secrets-8456dfc5-c1a7-4a10-aa99-bd41bf40da65" satisfied condition "Succeeded or Failed"
May 21 09:13:06.037: INFO: Trying to get logs from node k8s-03 pod pod-projected-secrets-8456dfc5-c1a7-4a10-aa99-bd41bf40da65 container secret-volume-test: <nil>
STEP: delete the pod
May 21 09:13:06.099: INFO: Waiting for pod pod-projected-secrets-8456dfc5-c1a7-4a10-aa99-bd41bf40da65 to disappear
May 21 09:13:06.200: INFO: Pod pod-projected-secrets-8456dfc5-c1a7-4a10-aa99-bd41bf40da65 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:13:06.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2289" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":211,"skipped":3852,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:13:06.291: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-26da7f61-e02d-4545-b31b-6b0aafba7160
STEP: Creating a pod to test consume configMaps
May 21 09:13:06.607: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c0d54cf-a3f1-49d1-84e7-5798b3eea0cc" in namespace "projected-8302" to be "Succeeded or Failed"
May 21 09:13:06.657: INFO: Pod "pod-projected-configmaps-9c0d54cf-a3f1-49d1-84e7-5798b3eea0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 49.673316ms
May 21 09:13:08.723: INFO: Pod "pod-projected-configmaps-9c0d54cf-a3f1-49d1-84e7-5798b3eea0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115603985s
May 21 09:13:10.731: INFO: Pod "pod-projected-configmaps-9c0d54cf-a3f1-49d1-84e7-5798b3eea0cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.123598722s
STEP: Saw pod success
May 21 09:13:10.731: INFO: Pod "pod-projected-configmaps-9c0d54cf-a3f1-49d1-84e7-5798b3eea0cc" satisfied condition "Succeeded or Failed"
May 21 09:13:10.735: INFO: Trying to get logs from node k8s-03 pod pod-projected-configmaps-9c0d54cf-a3f1-49d1-84e7-5798b3eea0cc container agnhost-container: <nil>
STEP: delete the pod
May 21 09:13:10.785: INFO: Waiting for pod pod-projected-configmaps-9c0d54cf-a3f1-49d1-84e7-5798b3eea0cc to disappear
May 21 09:13:10.900: INFO: Pod pod-projected-configmaps-9c0d54cf-a3f1-49d1-84e7-5798b3eea0cc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:13:10.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8302" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3859,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:13:10.977: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 09:13:11.431: INFO: Waiting up to 5m0s for pod "downwardapi-volume-067cd320-21a3-450c-9c29-3aedb297adc3" in namespace "projected-1204" to be "Succeeded or Failed"
May 21 09:13:11.502: INFO: Pod "downwardapi-volume-067cd320-21a3-450c-9c29-3aedb297adc3": Phase="Pending", Reason="", readiness=false. Elapsed: 71.196561ms
May 21 09:13:13.507: INFO: Pod "downwardapi-volume-067cd320-21a3-450c-9c29-3aedb297adc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076054617s
May 21 09:13:15.516: INFO: Pod "downwardapi-volume-067cd320-21a3-450c-9c29-3aedb297adc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085067261s
STEP: Saw pod success
May 21 09:13:15.516: INFO: Pod "downwardapi-volume-067cd320-21a3-450c-9c29-3aedb297adc3" satisfied condition "Succeeded or Failed"
May 21 09:13:15.520: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-067cd320-21a3-450c-9c29-3aedb297adc3 container client-container: <nil>
STEP: delete the pod
May 21 09:13:15.645: INFO: Waiting for pod downwardapi-volume-067cd320-21a3-450c-9c29-3aedb297adc3 to disappear
May 21 09:13:15.668: INFO: Pod downwardapi-volume-067cd320-21a3-450c-9c29-3aedb297adc3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:13:15.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1204" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":213,"skipped":3878,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:13:15.682: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 21 09:13:15.977: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 09:13:16.035: INFO: Waiting for terminating namespaces to be deleted...
May 21 09:13:16.039: INFO: 
Logging pods the apiserver thinks is on node k8s-02 before test
May 21 09:13:16.075: INFO: calico-node-gpcrh from calico-system started at 2021-05-21 05:44:44 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.075: INFO: 	Container calico-node ready: true, restart count 0
May 21 09:13:16.075: INFO: calico-typha-7578696db5-f4hhm from calico-system started at 2021-05-21 05:45:42 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.075: INFO: 	Container calico-typha ready: true, restart count 0
May 21 09:13:16.075: INFO: kube-proxy-kl6jm from kube-system started at 2021-05-21 05:44:44 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.075: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 09:13:16.075: INFO: node-exporter-vff7w from lens-metrics started at 2021-05-21 08:29:05 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.075: INFO: 	Container node-exporter ready: true, restart count 0
May 21 09:13:16.075: INFO: prometheus-0 from lens-metrics started at 2021-05-21 08:29:05 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.075: INFO: 	Container prometheus ready: true, restart count 0
May 21 09:13:16.075: INFO: sonobuoy-e2e-job-4762d948ca704da5 from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 09:13:16.075: INFO: 	Container e2e ready: true, restart count 0
May 21 09:13:16.075: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 09:13:16.075: INFO: sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-ww6zf from sonobuoy started at 2021-05-21 09:12:34 +0000 UTC (2 container statuses recorded)
May 21 09:13:16.075: INFO: 	Container sonobuoy-worker ready: false, restart count 2
May 21 09:13:16.075: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 09:13:16.075: INFO: 
Logging pods the apiserver thinks is on node k8s-03 before test
May 21 09:13:16.094: INFO: calico-node-s987r from calico-system started at 2021-05-21 05:45:50 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.094: INFO: 	Container calico-node ready: true, restart count 0
May 21 09:13:16.094: INFO: calico-typha-7578696db5-qh5gh from calico-system started at 2021-05-21 05:47:49 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.094: INFO: 	Container calico-typha ready: true, restart count 0
May 21 09:13:16.094: INFO: kube-proxy-7n7r8 from kube-system started at 2021-05-21 05:45:50 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.094: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 09:13:16.094: INFO: kube-state-metrics-565db9f7c-zn72r from lens-metrics started at 2021-05-21 08:29:04 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.094: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 21 09:13:16.094: INFO: node-exporter-ldzgf from lens-metrics started at 2021-05-21 08:29:05 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.094: INFO: 	Container node-exporter ready: true, restart count 0
May 21 09:13:16.094: INFO: sonobuoy from sonobuoy started at 2021-05-21 08:00:13 +0000 UTC (1 container statuses recorded)
May 21 09:13:16.094: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 09:13:16.094: INFO: sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-wkknk from sonobuoy started at 2021-05-21 09:12:40 +0000 UTC (2 container statuses recorded)
May 21 09:13:16.094: INFO: 	Container sonobuoy-worker ready: false, restart count 2
May 21 09:13:16.094: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a3e33d1c-df67-4d9f-af73-aeb697dce0e7 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 8.16.0.95 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 8.16.0.95 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 21 09:13:36.770: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 8.16.0.95 http://127.0.0.1:54321/hostname] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:36.770: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 8.16.0.95, port: 54321
May 21 09:13:37.071: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://8.16.0.95:54321/hostname] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:37.072: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 8.16.0.95, port: 54321 UDP
May 21 09:13:37.241: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 8.16.0.95 54321] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:37.241: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 21 09:13:42.391: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 8.16.0.95 http://127.0.0.1:54321/hostname] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:42.391: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 8.16.0.95, port: 54321
May 21 09:13:42.552: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://8.16.0.95:54321/hostname] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:42.552: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 8.16.0.95, port: 54321 UDP
May 21 09:13:42.707: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 8.16.0.95 54321] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:42.707: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 21 09:13:47.861: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 8.16.0.95 http://127.0.0.1:54321/hostname] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:47.861: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 8.16.0.95, port: 54321
May 21 09:13:48.057: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://8.16.0.95:54321/hostname] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:48.057: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 8.16.0.95, port: 54321 UDP
May 21 09:13:48.234: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 8.16.0.95 54321] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:48.234: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 21 09:13:53.401: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 8.16.0.95 http://127.0.0.1:54321/hostname] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:53.401: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 8.16.0.95, port: 54321
May 21 09:13:53.666: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://8.16.0.95:54321/hostname] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:53.666: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 8.16.0.95, port: 54321 UDP
May 21 09:13:53.825: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 8.16.0.95 54321] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:53.825: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 21 09:13:58.980: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 8.16.0.95 http://127.0.0.1:54321/hostname] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:58.980: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 8.16.0.95, port: 54321
May 21 09:13:59.136: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://8.16.0.95:54321/hostname] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:59.136: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: checking connectivity from pod e2e-host-exec to serverIP: 8.16.0.95, port: 54321 UDP
May 21 09:13:59.284: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 8.16.0.95 54321] Namespace:sched-pred-685 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:13:59.284: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: removing the label kubernetes.io/e2e-a3e33d1c-df67-4d9f-af73-aeb697dce0e7 off the node k8s-03
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a3e33d1c-df67-4d9f-af73-aeb697dce0e7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:14:04.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-685" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:48.855 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":214,"skipped":3889,"failed":0}
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:14:04.538: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:14:09.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2021" for this suite.

• [SLOW TEST:5.346 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":215,"skipped":3889,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:14:09.884: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
May 21 09:14:10.059: INFO: Waiting up to 5m0s for pod "var-expansion-0576b478-3ce5-4541-af5f-66cabec027bd" in namespace "var-expansion-7181" to be "Succeeded or Failed"
May 21 09:14:10.066: INFO: Pod "var-expansion-0576b478-3ce5-4541-af5f-66cabec027bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.995067ms
May 21 09:14:12.100: INFO: Pod "var-expansion-0576b478-3ce5-4541-af5f-66cabec027bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040699406s
May 21 09:14:14.106: INFO: Pod "var-expansion-0576b478-3ce5-4541-af5f-66cabec027bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04640609s
May 21 09:14:16.152: INFO: Pod "var-expansion-0576b478-3ce5-4541-af5f-66cabec027bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092619997s
STEP: Saw pod success
May 21 09:14:16.152: INFO: Pod "var-expansion-0576b478-3ce5-4541-af5f-66cabec027bd" satisfied condition "Succeeded or Failed"
May 21 09:14:16.165: INFO: Trying to get logs from node k8s-03 pod var-expansion-0576b478-3ce5-4541-af5f-66cabec027bd container dapi-container: <nil>
STEP: delete the pod
May 21 09:14:16.306: INFO: Waiting for pod var-expansion-0576b478-3ce5-4541-af5f-66cabec027bd to disappear
May 21 09:14:16.330: INFO: Pod var-expansion-0576b478-3ce5-4541-af5f-66cabec027bd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:14:16.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7181" for this suite.

• [SLOW TEST:6.566 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":216,"skipped":3901,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:14:16.451: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-a6c375b3-4f2e-4ac7-bf1c-5b1e8a6c0958
STEP: Creating a pod to test consume secrets
May 21 09:14:16.851: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b586999-02ec-4903-b645-a102af7aee0c" in namespace "projected-9489" to be "Succeeded or Failed"
May 21 09:14:16.995: INFO: Pod "pod-projected-secrets-1b586999-02ec-4903-b645-a102af7aee0c": Phase="Pending", Reason="", readiness=false. Elapsed: 143.261386ms
May 21 09:14:19.006: INFO: Pod "pod-projected-secrets-1b586999-02ec-4903-b645-a102af7aee0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.154670466s
May 21 09:14:21.027: INFO: Pod "pod-projected-secrets-1b586999-02ec-4903-b645-a102af7aee0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.175422018s
STEP: Saw pod success
May 21 09:14:21.027: INFO: Pod "pod-projected-secrets-1b586999-02ec-4903-b645-a102af7aee0c" satisfied condition "Succeeded or Failed"
May 21 09:14:21.031: INFO: Trying to get logs from node k8s-03 pod pod-projected-secrets-1b586999-02ec-4903-b645-a102af7aee0c container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 09:14:21.084: INFO: Waiting for pod pod-projected-secrets-1b586999-02ec-4903-b645-a102af7aee0c to disappear
May 21 09:14:21.091: INFO: Pod pod-projected-secrets-1b586999-02ec-4903-b645-a102af7aee0c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:14:21.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9489" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3908,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:14:21.172: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:14:21.505: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 21 09:14:21.620: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:21.818: INFO: Number of nodes with available pods: 0
May 21 09:14:21.818: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:14:22.849: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:22.854: INFO: Number of nodes with available pods: 0
May 21 09:14:22.854: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:14:23.906: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:23.909: INFO: Number of nodes with available pods: 0
May 21 09:14:23.909: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:14:24.828: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:24.833: INFO: Number of nodes with available pods: 1
May 21 09:14:24.833: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:14:25.826: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:25.831: INFO: Number of nodes with available pods: 2
May 21 09:14:25.831: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 21 09:14:25.922: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:25.922: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:25.958: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:26.968: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:26.968: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:26.974: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:27.964: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:27.964: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:27.970: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:28.964: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:28.964: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:28.964: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:28.969: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:29.967: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:29.967: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:29.967: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:29.973: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:30.968: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:30.968: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:30.968: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:30.974: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:31.964: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:31.964: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:31.964: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:31.970: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:32.966: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:32.966: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:32.966: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:32.972: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:33.968: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:33.968: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:33.968: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:33.974: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:34.973: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:34.973: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:34.973: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:34.979: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:35.966: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:35.966: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:35.966: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:35.972: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:36.967: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:36.967: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:36.967: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:36.973: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:37.974: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:37.974: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:37.974: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:37.980: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:38.970: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:38.970: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:38.970: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:38.975: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:39.973: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:39.973: INFO: Wrong image for pod: daemon-set-v2djt. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:39.973: INFO: Pod daemon-set-v2djt is not available
May 21 09:14:39.979: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:40.976: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:40.976: INFO: Pod daemon-set-wqw5d is not available
May 21 09:14:40.982: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:42.013: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:42.013: INFO: Pod daemon-set-wqw5d is not available
May 21 09:14:42.077: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:42.969: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:42.969: INFO: Pod daemon-set-wqw5d is not available
May 21 09:14:42.975: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:43.972: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:43.978: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:44.964: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:44.970: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:45.966: INFO: Wrong image for pod: daemon-set-dctvk. Expected: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 21 09:14:45.966: INFO: Pod daemon-set-dctvk is not available
May 21 09:14:45.974: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:46.967: INFO: Pod daemon-set-r997r is not available
May 21 09:14:46.976: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 21 09:14:46.982: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:46.987: INFO: Number of nodes with available pods: 1
May 21 09:14:46.987: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:14:47.996: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:48.001: INFO: Number of nodes with available pods: 1
May 21 09:14:48.001: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:14:48.993: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:48.996: INFO: Number of nodes with available pods: 1
May 21 09:14:48.996: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:14:49.996: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:50.001: INFO: Number of nodes with available pods: 1
May 21 09:14:50.001: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:14:51.009: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:14:51.015: INFO: Number of nodes with available pods: 2
May 21 09:14:51.015: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2955, will wait for the garbage collector to delete the pods
May 21 09:14:51.101: INFO: Deleting DaemonSet.extensions daemon-set took: 9.671917ms
May 21 09:14:51.801: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.392755ms
May 21 09:15:04.106: INFO: Number of nodes with available pods: 0
May 21 09:15:04.106: INFO: Number of running nodes: 0, number of available pods: 0
May 21 09:15:04.110: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48382"},"items":null}

May 21 09:15:04.115: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48382"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:15:04.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2955" for this suite.

• [SLOW TEST:42.970 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":218,"skipped":3940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:15:04.143: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-1731
STEP: creating service affinity-nodeport in namespace services-1731
STEP: creating replication controller affinity-nodeport in namespace services-1731
I0521 09:15:04.381554      25 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-1731, replica count: 3
I0521 09:15:07.432377      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:15:10.432772      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 09:15:10.491: INFO: Creating new exec pod
May 21 09:15:15.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-1731 exec execpod-affinitykrcqs -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
May 21 09:15:15.879: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 21 09:15:15.879: INFO: stdout: ""
May 21 09:15:15.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-1731 exec execpod-affinitykrcqs -- /bin/sh -x -c nc -zv -t -w 2 10.96.196.31 80'
May 21 09:15:16.236: INFO: stderr: "+ nc -zv -t -w 2 10.96.196.31 80\nConnection to 10.96.196.31 80 port [tcp/http] succeeded!\n"
May 21 09:15:16.237: INFO: stdout: ""
May 21 09:15:16.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-1731 exec execpod-affinitykrcqs -- /bin/sh -x -c nc -zv -t -w 2 8.16.0.94 30041'
May 21 09:15:16.580: INFO: stderr: "+ nc -zv -t -w 2 8.16.0.94 30041\nConnection to 8.16.0.94 30041 port [tcp/30041] succeeded!\n"
May 21 09:15:16.580: INFO: stdout: ""
May 21 09:15:16.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-1731 exec execpod-affinitykrcqs -- /bin/sh -x -c nc -zv -t -w 2 8.16.0.95 30041'
May 21 09:15:16.924: INFO: stderr: "+ nc -zv -t -w 2 8.16.0.95 30041\nConnection to 8.16.0.95 30041 port [tcp/30041] succeeded!\n"
May 21 09:15:16.924: INFO: stdout: ""
May 21 09:15:16.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-1731 exec execpod-affinitykrcqs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://8.16.0.94:30041/ ; done'
May 21 09:15:17.425: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:30041/\n"
May 21 09:15:17.426: INFO: stdout: "\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw\naffinity-nodeport-wjhzw"
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Received response from host: affinity-nodeport-wjhzw
May 21 09:15:17.426: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1731, will wait for the garbage collector to delete the pods
May 21 09:15:17.651: INFO: Deleting ReplicationController affinity-nodeport took: 75.898459ms
May 21 09:15:18.552: INFO: Terminating ReplicationController affinity-nodeport pods took: 901.116729ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:15:30.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1731" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:26.636 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":219,"skipped":3966,"failed":0}
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:15:30.779: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 09:15:30.892: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36e4bcf3-1ff3-4e6a-9f55-26b8abc7c8da" in namespace "downward-api-555" to be "Succeeded or Failed"
May 21 09:15:30.931: INFO: Pod "downwardapi-volume-36e4bcf3-1ff3-4e6a-9f55-26b8abc7c8da": Phase="Pending", Reason="", readiness=false. Elapsed: 39.066821ms
May 21 09:15:32.938: INFO: Pod "downwardapi-volume-36e4bcf3-1ff3-4e6a-9f55-26b8abc7c8da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04664302s
May 21 09:15:34.944: INFO: Pod "downwardapi-volume-36e4bcf3-1ff3-4e6a-9f55-26b8abc7c8da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052359245s
STEP: Saw pod success
May 21 09:15:34.944: INFO: Pod "downwardapi-volume-36e4bcf3-1ff3-4e6a-9f55-26b8abc7c8da" satisfied condition "Succeeded or Failed"
May 21 09:15:34.948: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-36e4bcf3-1ff3-4e6a-9f55-26b8abc7c8da container client-container: <nil>
STEP: delete the pod
May 21 09:15:35.160: INFO: Waiting for pod downwardapi-volume-36e4bcf3-1ff3-4e6a-9f55-26b8abc7c8da to disappear
May 21 09:15:35.222: INFO: Pod downwardapi-volume-36e4bcf3-1ff3-4e6a-9f55-26b8abc7c8da no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:15:35.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-555" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":220,"skipped":3966,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:15:35.243: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4816
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-4816
May 21 09:15:35.535: INFO: Found 0 stateful pods, waiting for 1
May 21 09:15:45.545: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 09:15:45.581: INFO: Deleting all statefulset in ns statefulset-4816
May 21 09:15:45.720: INFO: Scaling statefulset ss to 0
May 21 09:16:15.894: INFO: Waiting for statefulset status.replicas updated to 0
May 21 09:16:15.898: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:16:15.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4816" for this suite.

• [SLOW TEST:40.715 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":221,"skipped":3975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:16:15.959: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 21 09:16:20.213: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1581 PodName:var-expansion-d54da7b2-38ca-4fbd-9672-e70dbcb0312c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:16:20.213: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: test for file in mounted path
May 21 09:16:20.385: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1581 PodName:var-expansion-d54da7b2-38ca-4fbd-9672-e70dbcb0312c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:16:20.385: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: updating the annotation value
May 21 09:16:21.140: INFO: Successfully updated pod "var-expansion-d54da7b2-38ca-4fbd-9672-e70dbcb0312c"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 21 09:16:21.193: INFO: Deleting pod "var-expansion-d54da7b2-38ca-4fbd-9672-e70dbcb0312c" in namespace "var-expansion-1581"
May 21 09:16:21.213: INFO: Wait up to 5m0s for pod "var-expansion-d54da7b2-38ca-4fbd-9672-e70dbcb0312c" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:16:55.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1581" for this suite.

• [SLOW TEST:39.319 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":222,"skipped":4001,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:16:55.279: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
May 21 09:16:55.453: INFO: Waiting up to 5m0s for pod "pod-2e7a19cc-bda4-419a-a7bd-e5446b587268" in namespace "emptydir-9567" to be "Succeeded or Failed"
May 21 09:16:55.527: INFO: Pod "pod-2e7a19cc-bda4-419a-a7bd-e5446b587268": Phase="Pending", Reason="", readiness=false. Elapsed: 73.310603ms
May 21 09:16:57.543: INFO: Pod "pod-2e7a19cc-bda4-419a-a7bd-e5446b587268": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089528265s
May 21 09:16:59.556: INFO: Pod "pod-2e7a19cc-bda4-419a-a7bd-e5446b587268": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.102613005s
STEP: Saw pod success
May 21 09:16:59.556: INFO: Pod "pod-2e7a19cc-bda4-419a-a7bd-e5446b587268" satisfied condition "Succeeded or Failed"
May 21 09:16:59.560: INFO: Trying to get logs from node k8s-03 pod pod-2e7a19cc-bda4-419a-a7bd-e5446b587268 container test-container: <nil>
STEP: delete the pod
May 21 09:16:59.659: INFO: Waiting for pod pod-2e7a19cc-bda4-419a-a7bd-e5446b587268 to disappear
May 21 09:16:59.666: INFO: Pod pod-2e7a19cc-bda4-419a-a7bd-e5446b587268 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:16:59.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9567" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":223,"skipped":4020,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:16:59.678: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:17:01.144: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:17:03.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185421, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185421, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185421, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185421, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:17:06.349: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:17:06.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5906" for this suite.
STEP: Destroying namespace "webhook-5906-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.125 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":224,"skipped":4027,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:17:06.804: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 21 09:17:06.979: INFO: PodSpec: initContainers in spec.initContainers
May 21 09:18:03.722: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-569a7495-9885-4af6-a6e9-9e9bb82b0689", GenerateName:"", Namespace:"init-container-7470", SelfLink:"", UID:"0b1c026b-96af-401b-8ce2-ca13b836472e", ResourceVersion:"49257", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63757185426, loc:(*time.Location)(0x7977f00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"979022863"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.100.165.251/32", "cni.projectcalico.org/podIPs":"10.100.165.251/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc000d08060), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d08080)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc000d080a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d080c0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc000d080e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d08100)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-4g9ld", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003534240), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4g9ld", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4g9ld", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.cn-hangzhou.aliyuncs.com/ks8gcr/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4g9ld", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00408a0a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-03", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0026be000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00408a130)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00408a150)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00408a158), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00408a15c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0043f4030), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185426, loc:(*time.Location)(0x7977f00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185426, loc:(*time.Location)(0x7977f00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185426, loc:(*time.Location)(0x7977f00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185426, loc:(*time.Location)(0x7977f00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"8.16.0.95", PodIP:"10.100.165.251", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.165.251"}}, StartTime:(*v1.Time)(0xc000d08120), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0026be1c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0026be230)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://197674b2375acc672a5374cfcdff13d5ddce2ed20032bb0b3c9401da92ec7fee", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000d08160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000d08140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.cn-hangzhou.aliyuncs.com/ks8gcr/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00408a1df)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:18:03.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7470" for this suite.

• [SLOW TEST:56.937 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":225,"skipped":4044,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:18:03.741: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:18:20.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9128" for this suite.

• [SLOW TEST:16.623 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":226,"skipped":4058,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:18:20.365: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 21 09:18:25.055: INFO: Successfully updated pod "labelsupdate1c6ab8a2-665a-4cdb-8fbe-8bb17a65566a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:18:29.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6818" for this suite.

• [SLOW TEST:8.787 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":227,"skipped":4067,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:18:29.152: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-ff2197d6-4e39-4ed4-a60e-392d628c45f9
STEP: Creating a pod to test consume secrets
May 21 09:18:29.302: INFO: Waiting up to 5m0s for pod "pod-secrets-90b7d4d2-fece-4b84-b729-694bab1da47f" in namespace "secrets-7943" to be "Succeeded or Failed"
May 21 09:18:29.316: INFO: Pod "pod-secrets-90b7d4d2-fece-4b84-b729-694bab1da47f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.513089ms
May 21 09:18:31.323: INFO: Pod "pod-secrets-90b7d4d2-fece-4b84-b729-694bab1da47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020789177s
May 21 09:18:33.331: INFO: Pod "pod-secrets-90b7d4d2-fece-4b84-b729-694bab1da47f": Phase="Running", Reason="", readiness=true. Elapsed: 4.028669519s
May 21 09:18:35.361: INFO: Pod "pod-secrets-90b7d4d2-fece-4b84-b729-694bab1da47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058545814s
STEP: Saw pod success
May 21 09:18:35.361: INFO: Pod "pod-secrets-90b7d4d2-fece-4b84-b729-694bab1da47f" satisfied condition "Succeeded or Failed"
May 21 09:18:35.365: INFO: Trying to get logs from node k8s-03 pod pod-secrets-90b7d4d2-fece-4b84-b729-694bab1da47f container secret-volume-test: <nil>
STEP: delete the pod
May 21 09:18:35.426: INFO: Waiting for pod pod-secrets-90b7d4d2-fece-4b84-b729-694bab1da47f to disappear
May 21 09:18:35.435: INFO: Pod pod-secrets-90b7d4d2-fece-4b84-b729-694bab1da47f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:18:35.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7943" for this suite.

• [SLOW TEST:6.295 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":4103,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:18:35.448: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 21 09:18:39.866: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5730 PodName:pod-sharedvolume-24e7e113-1d65-4890-8af3-19aa3a13a481 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:18:39.866: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:18:40.027: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:18:40.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5730" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":229,"skipped":4110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:18:40.043: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 21 09:18:40.398: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:18:52.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8940" for this suite.

• [SLOW TEST:12.745 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":230,"skipped":4135,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:18:52.789: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0521 09:18:56.490344      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 09:19:58.519: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:19:58.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7303" for this suite.

• [SLOW TEST:65.746 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":231,"skipped":4150,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:19:58.536: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
May 21 09:19:58.663: INFO: Waiting up to 5m0s for pod "client-containers-fde18c5c-a85d-4abb-85f7-4326fe23f8e0" in namespace "containers-8936" to be "Succeeded or Failed"
May 21 09:19:58.698: INFO: Pod "client-containers-fde18c5c-a85d-4abb-85f7-4326fe23f8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.854199ms
May 21 09:20:00.707: INFO: Pod "client-containers-fde18c5c-a85d-4abb-85f7-4326fe23f8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043227347s
May 21 09:20:02.714: INFO: Pod "client-containers-fde18c5c-a85d-4abb-85f7-4326fe23f8e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05013352s
STEP: Saw pod success
May 21 09:20:02.714: INFO: Pod "client-containers-fde18c5c-a85d-4abb-85f7-4326fe23f8e0" satisfied condition "Succeeded or Failed"
May 21 09:20:02.718: INFO: Trying to get logs from node k8s-03 pod client-containers-fde18c5c-a85d-4abb-85f7-4326fe23f8e0 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:20:02.778: INFO: Waiting for pod client-containers-fde18c5c-a85d-4abb-85f7-4326fe23f8e0 to disappear
May 21 09:20:02.786: INFO: Pod client-containers-fde18c5c-a85d-4abb-85f7-4326fe23f8e0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:20:02.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8936" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4160,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:20:02.801: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 09:20:03.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16d74d82-4e0b-4e5c-9ae7-7ddc60d1404f" in namespace "downward-api-2170" to be "Succeeded or Failed"
May 21 09:20:03.032: INFO: Pod "downwardapi-volume-16d74d82-4e0b-4e5c-9ae7-7ddc60d1404f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.808886ms
May 21 09:20:05.038: INFO: Pod "downwardapi-volume-16d74d82-4e0b-4e5c-9ae7-7ddc60d1404f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011793747s
May 21 09:20:07.114: INFO: Pod "downwardapi-volume-16d74d82-4e0b-4e5c-9ae7-7ddc60d1404f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087795767s
STEP: Saw pod success
May 21 09:20:07.114: INFO: Pod "downwardapi-volume-16d74d82-4e0b-4e5c-9ae7-7ddc60d1404f" satisfied condition "Succeeded or Failed"
May 21 09:20:07.120: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-16d74d82-4e0b-4e5c-9ae7-7ddc60d1404f container client-container: <nil>
STEP: delete the pod
May 21 09:20:07.251: INFO: Waiting for pod downwardapi-volume-16d74d82-4e0b-4e5c-9ae7-7ddc60d1404f to disappear
May 21 09:20:07.271: INFO: Pod downwardapi-volume-16d74d82-4e0b-4e5c-9ae7-7ddc60d1404f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:20:07.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2170" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":233,"skipped":4162,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:20:07.285: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:20:11.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7936" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":234,"skipped":4179,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:20:11.620: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:20:18.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1168" for this suite.

• [SLOW TEST:7.368 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":235,"skipped":4183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:20:19.026: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:20:20.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:20:22.416: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185620, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185620, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185620, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757185620, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:20:25.468: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:20:37.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6367" for this suite.
STEP: Destroying namespace "webhook-6367-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:19.040 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":236,"skipped":4213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:20:38.068: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 21 09:20:38.237: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 09:21:38.304: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 21 09:21:38.548: INFO: Created pod: pod0-sched-preemption-low-priority
May 21 09:21:38.682: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:22:08.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7610" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:91.030 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":237,"skipped":4273,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:22:09.098: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9203
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-9203
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9203
May 21 09:22:09.495: INFO: Found 0 stateful pods, waiting for 1
May 21 09:22:19.500: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 21 09:22:19.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 09:22:22.132: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 09:22:22.132: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 09:22:22.132: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 09:22:22.139: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 21 09:22:32.152: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 09:22:32.152: INFO: Waiting for statefulset status.replicas updated to 0
May 21 09:22:32.206: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:22:32.206: INFO: ss-0  k8s-03  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:22:32.206: INFO: 
May 21 09:22:32.206: INFO: StatefulSet ss has not reached scale 3, at 1
May 21 09:22:33.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994824996s
May 21 09:22:34.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.933639442s
May 21 09:22:35.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.884955009s
May 21 09:22:36.339: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.873339319s
May 21 09:22:37.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.86229035s
May 21 09:22:38.360: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.854108516s
May 21 09:22:39.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.840798982s
May 21 09:22:40.382: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.83285916s
May 21 09:22:41.392: INFO: Verifying statefulset ss doesn't scale past 3 for another 819.527425ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9203
May 21 09:22:42.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:22:42.739: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 09:22:42.739: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 09:22:42.739: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 09:22:42.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:22:43.080: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 21 09:22:43.080: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 09:22:43.080: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 09:22:43.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:22:43.645: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 21 09:22:43.645: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 09:22:43.645: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 09:22:43.654: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 09:22:43.654: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 09:22:43.654: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 21 09:22:43.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 09:22:43.992: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 09:22:43.992: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 09:22:43.992: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 09:22:43.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 09:22:44.412: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 09:22:44.412: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 09:22:44.412: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 09:22:44.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 09:22:44.872: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 09:22:44.872: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 09:22:44.872: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 09:22:44.872: INFO: Waiting for statefulset status.replicas updated to 0
May 21 09:22:44.878: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 21 09:22:54.893: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 09:22:54.893: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 21 09:22:54.893: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 21 09:22:55.000: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:22:55.000: INFO: ss-0  k8s-03  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:22:55.000: INFO: ss-1  k8s-03  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:22:55.000: INFO: ss-2  k8s-03  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:22:55.000: INFO: 
May 21 09:22:55.000: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 09:22:56.034: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:22:56.034: INFO: ss-0  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:22:56.034: INFO: ss-1  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:22:56.034: INFO: ss-2  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:22:56.034: INFO: 
May 21 09:22:56.034: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 09:22:57.040: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:22:57.040: INFO: ss-0  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:22:57.041: INFO: ss-1  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:22:57.041: INFO: ss-2  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:22:57.041: INFO: 
May 21 09:22:57.041: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 09:22:58.049: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:22:58.049: INFO: ss-0  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:22:58.049: INFO: ss-1  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:22:58.049: INFO: ss-2  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:22:58.049: INFO: 
May 21 09:22:58.049: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 09:22:59.056: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:22:59.056: INFO: ss-0  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:22:59.056: INFO: ss-1  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:22:59.056: INFO: ss-2  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:22:59.056: INFO: 
May 21 09:22:59.056: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 09:23:00.065: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:23:00.065: INFO: ss-0  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:23:00.065: INFO: ss-1  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:23:00.065: INFO: ss-2  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:23:00.065: INFO: 
May 21 09:23:00.065: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 09:23:01.078: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:23:01.078: INFO: ss-0  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:23:01.078: INFO: ss-1  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:23:01.079: INFO: 
May 21 09:23:01.079: INFO: StatefulSet ss has not reached scale 0, at 2
May 21 09:23:02.088: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:23:02.088: INFO: ss-0  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:23:02.088: INFO: ss-1  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:23:02.088: INFO: 
May 21 09:23:02.088: INFO: StatefulSet ss has not reached scale 0, at 2
May 21 09:23:03.100: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:23:03.100: INFO: ss-0  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:23:03.100: INFO: ss-1  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:23:03.100: INFO: 
May 21 09:23:03.100: INFO: StatefulSet ss has not reached scale 0, at 2
May 21 09:23:04.109: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May 21 09:23:04.109: INFO: ss-0  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:09 +0000 UTC  }]
May 21 09:23:04.109: INFO: ss-1  k8s-03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 09:22:32 +0000 UTC  }]
May 21 09:23:04.109: INFO: 
May 21 09:23:04.109: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9203
May 21 09:23:05.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:23:05.363: INFO: rc: 1
May 21 09:23:05.363: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 21 09:23:15.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:23:15.548: INFO: rc: 1
May 21 09:23:15.548: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:23:25.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:23:25.731: INFO: rc: 1
May 21 09:23:25.731: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:23:35.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:23:35.907: INFO: rc: 1
May 21 09:23:35.908: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:23:45.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:23:46.158: INFO: rc: 1
May 21 09:23:46.158: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:23:56.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:23:56.352: INFO: rc: 1
May 21 09:23:56.353: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:24:06.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:24:06.531: INFO: rc: 1
May 21 09:24:06.531: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:24:16.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:24:16.825: INFO: rc: 1
May 21 09:24:16.825: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:24:26.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:24:26.999: INFO: rc: 1
May 21 09:24:27.000: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:24:37.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:24:37.182: INFO: rc: 1
May 21 09:24:37.182: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:24:47.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:24:47.398: INFO: rc: 1
May 21 09:24:47.398: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:24:57.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:24:57.597: INFO: rc: 1
May 21 09:24:57.597: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:25:07.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:25:07.781: INFO: rc: 1
May 21 09:25:07.781: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:25:17.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:25:17.958: INFO: rc: 1
May 21 09:25:17.958: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:25:27.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:25:28.140: INFO: rc: 1
May 21 09:25:28.140: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:25:38.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:25:38.317: INFO: rc: 1
May 21 09:25:38.317: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:25:48.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:25:48.562: INFO: rc: 1
May 21 09:25:48.562: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:25:58.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:25:58.751: INFO: rc: 1
May 21 09:25:58.751: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:26:08.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:26:08.942: INFO: rc: 1
May 21 09:26:08.942: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:26:18.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:26:19.146: INFO: rc: 1
May 21 09:26:19.147: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:26:29.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:26:29.321: INFO: rc: 1
May 21 09:26:29.321: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:26:39.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:26:39.565: INFO: rc: 1
May 21 09:26:39.565: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:26:49.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:26:49.661: INFO: rc: 1
May 21 09:26:49.661: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:26:59.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:26:59.847: INFO: rc: 1
May 21 09:26:59.847: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:27:09.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:27:10.052: INFO: rc: 1
May 21 09:27:10.052: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:27:20.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:27:20.241: INFO: rc: 1
May 21 09:27:20.241: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:27:30.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:27:30.426: INFO: rc: 1
May 21 09:27:30.427: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:27:40.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:27:40.742: INFO: rc: 1
May 21 09:27:40.742: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:27:50.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:27:50.927: INFO: rc: 1
May 21 09:27:50.928: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:28:00.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:28:01.124: INFO: rc: 1
May 21 09:28:01.124: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 21 09:28:11.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-9203 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:28:11.368: INFO: rc: 1
May 21 09:28:11.368: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
May 21 09:28:11.368: INFO: Scaling statefulset ss to 0
May 21 09:28:11.385: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 09:28:11.395: INFO: Deleting all statefulset in ns statefulset-9203
May 21 09:28:11.399: INFO: Scaling statefulset ss to 0
May 21 09:28:11.412: INFO: Waiting for statefulset status.replicas updated to 0
May 21 09:28:11.416: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:28:11.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9203" for this suite.

• [SLOW TEST:362.361 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":238,"skipped":4274,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:28:11.460: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-d69ccdc3-00ec-4795-a1b4-1324dcd423f4
STEP: Creating a pod to test consume configMaps
May 21 09:28:11.720: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dbc2bc0b-f5b2-4cfc-9d1e-11a7e7ad1a78" in namespace "projected-6403" to be "Succeeded or Failed"
May 21 09:28:11.729: INFO: Pod "pod-projected-configmaps-dbc2bc0b-f5b2-4cfc-9d1e-11a7e7ad1a78": Phase="Pending", Reason="", readiness=false. Elapsed: 8.666927ms
May 21 09:28:13.742: INFO: Pod "pod-projected-configmaps-dbc2bc0b-f5b2-4cfc-9d1e-11a7e7ad1a78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022191554s
May 21 09:28:15.832: INFO: Pod "pod-projected-configmaps-dbc2bc0b-f5b2-4cfc-9d1e-11a7e7ad1a78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.112417304s
STEP: Saw pod success
May 21 09:28:15.832: INFO: Pod "pod-projected-configmaps-dbc2bc0b-f5b2-4cfc-9d1e-11a7e7ad1a78" satisfied condition "Succeeded or Failed"
May 21 09:28:15.836: INFO: Trying to get logs from node k8s-03 pod pod-projected-configmaps-dbc2bc0b-f5b2-4cfc-9d1e-11a7e7ad1a78 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:28:15.978: INFO: Waiting for pod pod-projected-configmaps-dbc2bc0b-f5b2-4cfc-9d1e-11a7e7ad1a78 to disappear
May 21 09:28:15.986: INFO: Pod pod-projected-configmaps-dbc2bc0b-f5b2-4cfc-9d1e-11a7e7ad1a78 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:28:15.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6403" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":239,"skipped":4284,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:28:16.004: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-e94728dd-0091-45ab-a3f0-0134b3a7cd37
STEP: Creating a pod to test consume configMaps
May 21 09:28:16.199: INFO: Waiting up to 5m0s for pod "pod-configmaps-d63a2ae0-5ec9-4bb7-a671-0bf26be116e9" in namespace "configmap-6722" to be "Succeeded or Failed"
May 21 09:28:16.233: INFO: Pod "pod-configmaps-d63a2ae0-5ec9-4bb7-a671-0bf26be116e9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.163245ms
May 21 09:28:18.242: INFO: Pod "pod-configmaps-d63a2ae0-5ec9-4bb7-a671-0bf26be116e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043390141s
May 21 09:28:20.250: INFO: Pod "pod-configmaps-d63a2ae0-5ec9-4bb7-a671-0bf26be116e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051082222s
STEP: Saw pod success
May 21 09:28:20.250: INFO: Pod "pod-configmaps-d63a2ae0-5ec9-4bb7-a671-0bf26be116e9" satisfied condition "Succeeded or Failed"
May 21 09:28:20.254: INFO: Trying to get logs from node k8s-03 pod pod-configmaps-d63a2ae0-5ec9-4bb7-a671-0bf26be116e9 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:28:20.344: INFO: Waiting for pod pod-configmaps-d63a2ae0-5ec9-4bb7-a671-0bf26be116e9 to disappear
May 21 09:28:20.351: INFO: Pod pod-configmaps-d63a2ae0-5ec9-4bb7-a671-0bf26be116e9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:28:20.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6722" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":240,"skipped":4294,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:28:20.364: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-a2d912c7-e2b1-46cb-8d26-036786c2bdcf
STEP: Creating a pod to test consume configMaps
May 21 09:28:20.510: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-77bd03b3-166a-4de5-9584-80e3d5dab863" in namespace "projected-2690" to be "Succeeded or Failed"
May 21 09:28:20.542: INFO: Pod "pod-projected-configmaps-77bd03b3-166a-4de5-9584-80e3d5dab863": Phase="Pending", Reason="", readiness=false. Elapsed: 32.262042ms
May 21 09:28:22.551: INFO: Pod "pod-projected-configmaps-77bd03b3-166a-4de5-9584-80e3d5dab863": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04120587s
May 21 09:28:24.562: INFO: Pod "pod-projected-configmaps-77bd03b3-166a-4de5-9584-80e3d5dab863": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052205875s
STEP: Saw pod success
May 21 09:28:24.562: INFO: Pod "pod-projected-configmaps-77bd03b3-166a-4de5-9584-80e3d5dab863" satisfied condition "Succeeded or Failed"
May 21 09:28:24.566: INFO: Trying to get logs from node k8s-03 pod pod-projected-configmaps-77bd03b3-166a-4de5-9584-80e3d5dab863 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:28:24.671: INFO: Waiting for pod pod-projected-configmaps-77bd03b3-166a-4de5-9584-80e3d5dab863 to disappear
May 21 09:28:24.678: INFO: Pod pod-projected-configmaps-77bd03b3-166a-4de5-9584-80e3d5dab863 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:28:24.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2690" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":241,"skipped":4294,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:28:24.692: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7878
STEP: creating service affinity-nodeport-transition in namespace services-7878
STEP: creating replication controller affinity-nodeport-transition in namespace services-7878
I0521 09:28:25.018313      25 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-7878, replica count: 3
I0521 09:28:28.069040      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:28:31.069418      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 09:28:31.092: INFO: Creating new exec pod
May 21 09:28:36.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-7878 exec execpod-affinity9pkvf -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 21 09:28:36.508: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 21 09:28:36.508: INFO: stdout: ""
May 21 09:28:36.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-7878 exec execpod-affinity9pkvf -- /bin/sh -x -c nc -zv -t -w 2 10.96.197.9 80'
May 21 09:28:36.849: INFO: stderr: "+ nc -zv -t -w 2 10.96.197.9 80\nConnection to 10.96.197.9 80 port [tcp/http] succeeded!\n"
May 21 09:28:36.849: INFO: stdout: ""
May 21 09:28:36.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-7878 exec execpod-affinity9pkvf -- /bin/sh -x -c nc -zv -t -w 2 8.16.0.94 32759'
May 21 09:28:37.192: INFO: stderr: "+ nc -zv -t -w 2 8.16.0.94 32759\nConnection to 8.16.0.94 32759 port [tcp/32759] succeeded!\n"
May 21 09:28:37.192: INFO: stdout: ""
May 21 09:28:37.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-7878 exec execpod-affinity9pkvf -- /bin/sh -x -c nc -zv -t -w 2 8.16.0.95 32759'
May 21 09:28:37.531: INFO: stderr: "+ nc -zv -t -w 2 8.16.0.95 32759\nConnection to 8.16.0.95 32759 port [tcp/32759] succeeded!\n"
May 21 09:28:37.531: INFO: stdout: ""
May 21 09:28:37.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-7878 exec execpod-affinity9pkvf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://8.16.0.94:32759/ ; done'
May 21 09:28:38.073: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n"
May 21 09:28:38.073: INFO: stdout: "\naffinity-nodeport-transition-gszlm\naffinity-nodeport-transition-m9jtk\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-gszlm\naffinity-nodeport-transition-m9jtk\naffinity-nodeport-transition-m9jtk\naffinity-nodeport-transition-gszlm\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-gszlm\naffinity-nodeport-transition-gszlm\naffinity-nodeport-transition-m9jtk\naffinity-nodeport-transition-gszlm\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-gszlm\naffinity-nodeport-transition-m9jtk\naffinity-nodeport-transition-7k8dw"
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-gszlm
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-m9jtk
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-gszlm
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-m9jtk
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-m9jtk
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-gszlm
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-gszlm
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-gszlm
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-m9jtk
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-gszlm
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-gszlm
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-m9jtk
May 21 09:28:38.073: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-7878 exec execpod-affinity9pkvf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://8.16.0.94:32759/ ; done'
May 21 09:28:38.593: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:32759/\n"
May 21 09:28:38.593: INFO: stdout: "\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw\naffinity-nodeport-transition-7k8dw"
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Received response from host: affinity-nodeport-transition-7k8dw
May 21 09:28:38.593: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7878, will wait for the garbage collector to delete the pods
May 21 09:28:38.721: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.352707ms
May 21 09:28:39.422: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 700.348152ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:28:50.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7878" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:25.897 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":242,"skipped":4295,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:28:50.589: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-3b0132dc-2d31-4085-8616-eb2c1162e340
STEP: Creating a pod to test consume configMaps
May 21 09:28:50.732: INFO: Waiting up to 5m0s for pod "pod-configmaps-a6ad86ba-5f8a-4782-b67a-c9bac5c06ff1" in namespace "configmap-4823" to be "Succeeded or Failed"
May 21 09:28:50.738: INFO: Pod "pod-configmaps-a6ad86ba-5f8a-4782-b67a-c9bac5c06ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.151321ms
May 21 09:28:52.755: INFO: Pod "pod-configmaps-a6ad86ba-5f8a-4782-b67a-c9bac5c06ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02243784s
May 21 09:28:54.763: INFO: Pod "pod-configmaps-a6ad86ba-5f8a-4782-b67a-c9bac5c06ff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031248933s
STEP: Saw pod success
May 21 09:28:54.764: INFO: Pod "pod-configmaps-a6ad86ba-5f8a-4782-b67a-c9bac5c06ff1" satisfied condition "Succeeded or Failed"
May 21 09:28:54.768: INFO: Trying to get logs from node k8s-03 pod pod-configmaps-a6ad86ba-5f8a-4782-b67a-c9bac5c06ff1 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:28:54.845: INFO: Waiting for pod pod-configmaps-a6ad86ba-5f8a-4782-b67a-c9bac5c06ff1 to disappear
May 21 09:28:54.856: INFO: Pod pod-configmaps-a6ad86ba-5f8a-4782-b67a-c9bac5c06ff1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:28:54.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4823" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:28:54.877: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 09:28:55.054: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc56835a-5f8c-47ff-b072-73b113e9597e" in namespace "projected-2455" to be "Succeeded or Failed"
May 21 09:28:55.059: INFO: Pod "downwardapi-volume-fc56835a-5f8c-47ff-b072-73b113e9597e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.628917ms
May 21 09:28:57.092: INFO: Pod "downwardapi-volume-fc56835a-5f8c-47ff-b072-73b113e9597e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037444535s
May 21 09:28:59.096: INFO: Pod "downwardapi-volume-fc56835a-5f8c-47ff-b072-73b113e9597e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041803975s
STEP: Saw pod success
May 21 09:28:59.096: INFO: Pod "downwardapi-volume-fc56835a-5f8c-47ff-b072-73b113e9597e" satisfied condition "Succeeded or Failed"
May 21 09:28:59.098: INFO: Trying to get logs from node k8s-03 pod downwardapi-volume-fc56835a-5f8c-47ff-b072-73b113e9597e container client-container: <nil>
STEP: delete the pod
May 21 09:28:59.233: INFO: Waiting for pod downwardapi-volume-fc56835a-5f8c-47ff-b072-73b113e9597e to disappear
May 21 09:28:59.263: INFO: Pod downwardapi-volume-fc56835a-5f8c-47ff-b072-73b113e9597e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:28:59.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2455" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":244,"skipped":4324,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:28:59.289: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 21 09:28:59.501: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 09:29:59.577: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:29:59.588: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 21 09:30:03.886: INFO: found a healthy node: k8s-03
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:30:24.616: INFO: pods created so far: [1 1 1]
May 21 09:30:24.616: INFO: length of pods created so far: 3
May 21 09:30:34.637: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:30:41.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3292" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:30:41.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3086" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:102.563 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":245,"skipped":4333,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:30:41.852: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:30:42.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6105" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":246,"skipped":4339,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:30:42.158: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-9946
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 09:30:42.307: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 09:30:42.412: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 09:30:44.424: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 09:30:46.433: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 09:30:48.479: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 09:30:50.422: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 09:30:52.420: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 09:30:54.419: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 09:30:56.422: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 09:30:58.421: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 09:30:58.429: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 09:31:00.439: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 09:31:02.438: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 09:31:04.440: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 09:31:06.439: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 21 09:31:10.597: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 21 09:31:10.597: INFO: Going to poll 10.100.179.14 on port 8080 at least 0 times, with a maximum of 34 tries before failing
May 21 09:31:10.601: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.179.14:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9946 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:31:10.601: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:31:10.775: INFO: Found all 1 expected endpoints: [netserver-0]
May 21 09:31:10.775: INFO: Going to poll 10.100.165.225 on port 8080 at least 0 times, with a maximum of 34 tries before failing
May 21 09:31:10.781: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.165.225:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9946 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:31:10.781: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:31:10.986: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:31:10.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9946" for this suite.

• [SLOW TEST:28.848 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":247,"skipped":4350,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:31:11.007: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:31:15.322: INFO: Deleting pod "var-expansion-f9b9d5d4-3df8-4b22-baef-341d45450797" in namespace "var-expansion-8567"
May 21 09:31:15.331: INFO: Wait up to 5m0s for pod "var-expansion-f9b9d5d4-3df8-4b22-baef-341d45450797" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:31:17.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8567" for this suite.

• [SLOW TEST:6.387 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":248,"skipped":4364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:31:17.395: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0521 09:31:19.389328      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 09:32:21.481: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:32:21.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-348" for this suite.

• [SLOW TEST:64.102 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":249,"skipped":4409,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:32:21.497: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-834
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-834
STEP: Deleting pre-stop pod
May 21 09:32:34.728: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:32:34.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-834" for this suite.

• [SLOW TEST:13.359 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":250,"skipped":4430,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:32:34.857: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6612
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6612
I0521 09:32:35.327300      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6612, replica count: 2
I0521 09:32:38.377939      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 09:32:41.378: INFO: Creating new exec pod
I0521 09:32:41.378253      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 09:32:46.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-6612 exec execpodtm4dq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 21 09:32:49.227: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 21 09:32:49.227: INFO: stdout: ""
May 21 09:32:49.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-6612 exec execpodtm4dq -- /bin/sh -x -c nc -zv -t -w 2 10.96.68.75 80'
May 21 09:32:49.534: INFO: stderr: "+ nc -zv -t -w 2 10.96.68.75 80\nConnection to 10.96.68.75 80 port [tcp/http] succeeded!\n"
May 21 09:32:49.534: INFO: stdout: ""
May 21 09:32:49.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-6612 exec execpodtm4dq -- /bin/sh -x -c nc -zv -t -w 2 8.16.0.94 30885'
May 21 09:32:49.836: INFO: stderr: "+ nc -zv -t -w 2 8.16.0.94 30885\nConnection to 8.16.0.94 30885 port [tcp/30885] succeeded!\n"
May 21 09:32:49.836: INFO: stdout: ""
May 21 09:32:49.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-6612 exec execpodtm4dq -- /bin/sh -x -c nc -zv -t -w 2 8.16.0.95 30885'
May 21 09:32:50.156: INFO: stderr: "+ nc -zv -t -w 2 8.16.0.95 30885\nConnection to 8.16.0.95 30885 port [tcp/30885] succeeded!\n"
May 21 09:32:50.156: INFO: stdout: ""
May 21 09:32:50.156: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:32:50.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6612" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.519 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":251,"skipped":4448,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:32:50.376: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 09:32:54.673: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:32:54.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1537" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":252,"skipped":4457,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:32:54.816: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5929
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 21 09:32:55.037: INFO: Found 0 stateful pods, waiting for 3
May 21 09:33:05.047: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 09:33:05.047: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 09:33:05.047: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 21 09:33:15.047: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 09:33:15.047: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 09:33:15.047: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 21 09:33:15.084: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 21 09:33:25.181: INFO: Updating stateful set ss2
May 21 09:33:25.241: INFO: Waiting for Pod statefulset-5929/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May 21 09:33:35.697: INFO: Found 2 stateful pods, waiting for 3
May 21 09:33:45.710: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 09:33:45.710: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 09:33:45.710: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 21 09:33:45.745: INFO: Updating stateful set ss2
May 21 09:33:45.818: INFO: Waiting for Pod statefulset-5929/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 21 09:33:55.860: INFO: Updating stateful set ss2
May 21 09:33:55.910: INFO: Waiting for StatefulSet statefulset-5929/ss2 to complete update
May 21 09:33:55.910: INFO: Waiting for Pod statefulset-5929/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 09:34:05.925: INFO: Deleting all statefulset in ns statefulset-5929
May 21 09:34:05.929: INFO: Scaling statefulset ss2 to 0
May 21 09:34:35.975: INFO: Waiting for statefulset status.replicas updated to 0
May 21 09:34:35.979: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:34:36.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5929" for this suite.

• [SLOW TEST:101.210 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":253,"skipped":4457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:34:36.026: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 21 09:34:44.444: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 09:34:44.455: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 09:34:46.455: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 09:34:46.467: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 09:34:48.456: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 09:34:48.464: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 09:34:50.456: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 09:34:50.466: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:34:50.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4885" for this suite.

• [SLOW TEST:14.455 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":254,"skipped":4481,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:34:50.482: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:34:50.647: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 21 09:34:55.655: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 09:34:55.656: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 09:34:55.822: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1844  8781fa8f-625a-4660-8dce-5876c98ea599 53568 1 2021-05-21 09:34:55 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-05-21 09:34:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ce5378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 21 09:34:56.115: INFO: New ReplicaSet "test-cleanup-deployment-597f4cf8fb" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-597f4cf8fb  deployment-1844  a60da42c-95b5-45e6-ae90-d34b94031585 53570 1 2021-05-21 09:34:55 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:597f4cf8fb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8781fa8f-625a-4660-8dce-5876c98ea599 0xc003ce58d7 0xc003ce58d8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 09:34:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8781fa8f-625a-4660-8dce-5876c98ea599\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 597f4cf8fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:597f4cf8fb] map[] [] []  []} {[] [] [{agnhost registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ce5968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 09:34:56.115: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 21 09:34:56.116: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1844  f9d0e1df-4f34-4f3b-8425-54151ba9a48a 53569 1 2021-05-21 09:34:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8781fa8f-625a-4660-8dce-5876c98ea599 0xc003ce57c7 0xc003ce57c8}] []  [{e2e.test Update apps/v1 2021-05-21 09:34:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 09:34:55 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"8781fa8f-625a-4660-8dce-5876c98ea599\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ce5868 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 09:34:56.347: INFO: Pod "test-cleanup-controller-4tcmd" is available:
&Pod{ObjectMeta:{test-cleanup-controller-4tcmd test-cleanup-controller- deployment-1844  0b7c3d25-763a-4aec-9870-dea864f3672b 53558 0 2021-05-21 09:34:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.100.165.245/32 cni.projectcalico.org/podIPs:10.100.165.245/32] [{apps/v1 ReplicaSet test-cleanup-controller f9d0e1df-4f34-4f3b-8425-54151ba9a48a 0xc0041a66d7 0xc0041a66d8}] []  [{kube-controller-manager Update v1 2021-05-21 09:34:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9d0e1df-4f34-4f3b-8425-54151ba9a48a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 09:34:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 09:34:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.165.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqm29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqm29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqm29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:34:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:34:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:34:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:34:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:10.100.165.245,StartTime:2021-05-21 09:34:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 09:34:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://87bc69623f31d537bda8746a7020757a80a24244b3a8baefec29e4f7e44857d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.165.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 09:34:56.347: INFO: Pod "test-cleanup-deployment-597f4cf8fb-mqbl6" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-597f4cf8fb-mqbl6 test-cleanup-deployment-597f4cf8fb- deployment-1844  176463d8-9919-4999-a79a-df23a82d244d 53575 0 2021-05-21 09:34:55 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:597f4cf8fb] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-597f4cf8fb a60da42c-95b5-45e6-ae90-d34b94031585 0xc0041a6897 0xc0041a6898}] []  [{kube-controller-manager Update v1 2021-05-21 09:34:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a60da42c-95b5-45e6-ae90-d34b94031585\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqm29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqm29,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqm29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:34:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:34:56.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1844" for this suite.

• [SLOW TEST:6.043 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":255,"skipped":4499,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:34:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:34:57.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6356" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":256,"skipped":4522,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:34:57.261: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 21 09:35:04.075: INFO: Successfully updated pod "adopt-release-9tz5t"
STEP: Checking that the Job readopts the Pod
May 21 09:35:04.075: INFO: Waiting up to 15m0s for pod "adopt-release-9tz5t" in namespace "job-507" to be "adopted"
May 21 09:35:04.181: INFO: Pod "adopt-release-9tz5t": Phase="Running", Reason="", readiness=true. Elapsed: 105.274883ms
May 21 09:35:06.189: INFO: Pod "adopt-release-9tz5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.113677345s
May 21 09:35:06.189: INFO: Pod "adopt-release-9tz5t" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 21 09:35:06.713: INFO: Successfully updated pod "adopt-release-9tz5t"
STEP: Checking that the Job releases the Pod
May 21 09:35:06.713: INFO: Waiting up to 15m0s for pod "adopt-release-9tz5t" in namespace "job-507" to be "released"
May 21 09:35:06.778: INFO: Pod "adopt-release-9tz5t": Phase="Running", Reason="", readiness=true. Elapsed: 65.313482ms
May 21 09:35:08.791: INFO: Pod "adopt-release-9tz5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.077674095s
May 21 09:35:08.791: INFO: Pod "adopt-release-9tz5t" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:35:08.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-507" for this suite.

• [SLOW TEST:11.543 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":257,"skipped":4527,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:35:08.805: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3cd6f39a-6442-4c35-a90f-a74abd1b4aea
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-3cd6f39a-6442-4c35-a90f-a74abd1b4aea
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:36:26.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3045" for this suite.

• [SLOW TEST:77.455 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":258,"skipped":4533,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:36:26.260: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 21 09:36:26.426: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 09:36:26.678: INFO: Waiting for terminating namespaces to be deleted...
May 21 09:36:26.682: INFO: 
Logging pods the apiserver thinks is on node k8s-02 before test
May 21 09:36:26.694: INFO: calico-node-gpcrh from calico-system started at 2021-05-21 05:44:44 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.694: INFO: 	Container calico-node ready: true, restart count 0
May 21 09:36:26.694: INFO: calico-typha-7578696db5-f4hhm from calico-system started at 2021-05-21 05:45:42 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.694: INFO: 	Container calico-typha ready: true, restart count 0
May 21 09:36:26.694: INFO: kube-proxy-kl6jm from kube-system started at 2021-05-21 05:44:44 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.694: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 09:36:26.694: INFO: node-exporter-vff7w from lens-metrics started at 2021-05-21 08:29:05 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.694: INFO: 	Container node-exporter ready: true, restart count 0
May 21 09:36:26.694: INFO: prometheus-0 from lens-metrics started at 2021-05-21 08:29:05 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.694: INFO: 	Container prometheus ready: true, restart count 0
May 21 09:36:26.694: INFO: sonobuoy-e2e-job-4762d948ca704da5 from sonobuoy started at 2021-05-21 08:00:17 +0000 UTC (2 container statuses recorded)
May 21 09:36:26.694: INFO: 	Container e2e ready: true, restart count 0
May 21 09:36:26.694: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 09:36:26.694: INFO: sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-ww6zf from sonobuoy started at 2021-05-21 09:12:34 +0000 UTC (2 container statuses recorded)
May 21 09:36:26.694: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 21 09:36:26.695: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 09:36:26.695: INFO: 
Logging pods the apiserver thinks is on node k8s-03 before test
May 21 09:36:26.706: INFO: calico-node-s987r from calico-system started at 2021-05-21 05:45:50 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.706: INFO: 	Container calico-node ready: true, restart count 0
May 21 09:36:26.706: INFO: calico-typha-7578696db5-qh5gh from calico-system started at 2021-05-21 05:47:49 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.706: INFO: 	Container calico-typha ready: true, restart count 0
May 21 09:36:26.706: INFO: kube-proxy-7n7r8 from kube-system started at 2021-05-21 05:45:50 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.706: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 09:36:26.706: INFO: kube-state-metrics-565db9f7c-zn72r from lens-metrics started at 2021-05-21 08:29:04 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.706: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 21 09:36:26.706: INFO: node-exporter-ldzgf from lens-metrics started at 2021-05-21 08:29:05 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.706: INFO: 	Container node-exporter ready: true, restart count 0
May 21 09:36:26.706: INFO: pod-projected-configmaps-2ca1e2e4-f368-4b22-b2ed-fc7c01539e5a from projected-3045 started at 2021-05-21 09:35:09 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.706: INFO: 	Container agnhost-container ready: true, restart count 0
May 21 09:36:26.706: INFO: sonobuoy from sonobuoy started at 2021-05-21 08:00:13 +0000 UTC (1 container statuses recorded)
May 21 09:36:26.706: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 09:36:26.706: INFO: sonobuoy-systemd-logs-daemon-set-5e31936608a6473c-wkknk from sonobuoy started at 2021-05-21 09:12:40 +0000 UTC (2 container statuses recorded)
May 21 09:36:26.706: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 21 09:36:26.706: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7fc7b193-4c11-4138-bb3d-40d946daabb7 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-7fc7b193-4c11-4138-bb3d-40d946daabb7 off the node k8s-03
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7fc7b193-4c11-4138-bb3d-40d946daabb7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:36:35.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-826" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:8.895 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":259,"skipped":4536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:36:35.156: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 21 09:36:39.952: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c9e1e11f-3f71-447f-8db6-5c35d38e344c"
May 21 09:36:39.952: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c9e1e11f-3f71-447f-8db6-5c35d38e344c" in namespace "pods-7401" to be "terminated due to deadline exceeded"
May 21 09:36:40.074: INFO: Pod "pod-update-activedeadlineseconds-c9e1e11f-3f71-447f-8db6-5c35d38e344c": Phase="Running", Reason="", readiness=true. Elapsed: 121.359153ms
May 21 09:36:42.084: INFO: Pod "pod-update-activedeadlineseconds-c9e1e11f-3f71-447f-8db6-5c35d38e344c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.132023374s
May 21 09:36:42.084: INFO: Pod "pod-update-activedeadlineseconds-c9e1e11f-3f71-447f-8db6-5c35d38e344c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:36:42.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7401" for this suite.

• [SLOW TEST:6.942 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4579,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:36:42.099: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 21 09:36:42.235: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 09:37:42.302: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:37:42.307: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:37:42.538: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 21 09:37:42.544: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:37:42.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6597" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:37:42.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3602" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.621 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":261,"skipped":4625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:37:42.721: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
May 21 09:37:42.882: INFO: Waiting up to 5m0s for pod "client-containers-b9d6ad78-ecbd-48c3-965c-c0ddd73008b6" in namespace "containers-4244" to be "Succeeded or Failed"
May 21 09:37:42.891: INFO: Pod "client-containers-b9d6ad78-ecbd-48c3-965c-c0ddd73008b6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.216588ms
May 21 09:37:44.902: INFO: Pod "client-containers-b9d6ad78-ecbd-48c3-965c-c0ddd73008b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019578141s
May 21 09:37:46.912: INFO: Pod "client-containers-b9d6ad78-ecbd-48c3-965c-c0ddd73008b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029197655s
STEP: Saw pod success
May 21 09:37:46.912: INFO: Pod "client-containers-b9d6ad78-ecbd-48c3-965c-c0ddd73008b6" satisfied condition "Succeeded or Failed"
May 21 09:37:46.915: INFO: Trying to get logs from node k8s-03 pod client-containers-b9d6ad78-ecbd-48c3-965c-c0ddd73008b6 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:37:46.998: INFO: Waiting for pod client-containers-b9d6ad78-ecbd-48c3-965c-c0ddd73008b6 to disappear
May 21 09:37:47.009: INFO: Pod client-containers-b9d6ad78-ecbd-48c3-965c-c0ddd73008b6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:37:47.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4244" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":262,"skipped":4653,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:37:47.022: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 21 09:37:47.327: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:47.433: INFO: Number of nodes with available pods: 0
May 21 09:37:47.433: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:48.471: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:48.505: INFO: Number of nodes with available pods: 0
May 21 09:37:48.505: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:49.438: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:49.441: INFO: Number of nodes with available pods: 0
May 21 09:37:49.441: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:50.445: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:50.450: INFO: Number of nodes with available pods: 0
May 21 09:37:50.450: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:51.459: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:51.464: INFO: Number of nodes with available pods: 1
May 21 09:37:51.464: INFO: Node k8s-03 is running more than one daemon pod
May 21 09:37:52.451: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:52.470: INFO: Number of nodes with available pods: 2
May 21 09:37:52.470: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 21 09:37:52.525: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:52.530: INFO: Number of nodes with available pods: 1
May 21 09:37:52.530: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:53.540: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:53.546: INFO: Number of nodes with available pods: 1
May 21 09:37:53.546: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:54.537: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:54.542: INFO: Number of nodes with available pods: 1
May 21 09:37:54.542: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:55.556: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:55.569: INFO: Number of nodes with available pods: 1
May 21 09:37:55.569: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:56.545: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:56.550: INFO: Number of nodes with available pods: 1
May 21 09:37:56.550: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:57.541: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:57.546: INFO: Number of nodes with available pods: 1
May 21 09:37:57.546: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:58.541: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:58.546: INFO: Number of nodes with available pods: 1
May 21 09:37:58.546: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:37:59.539: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:37:59.545: INFO: Number of nodes with available pods: 1
May 21 09:37:59.545: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:38:00.556: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:38:00.562: INFO: Number of nodes with available pods: 1
May 21 09:38:00.562: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:38:01.540: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:38:01.545: INFO: Number of nodes with available pods: 1
May 21 09:38:01.545: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:38:02.548: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:38:02.555: INFO: Number of nodes with available pods: 1
May 21 09:38:02.555: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:38:03.540: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:38:03.545: INFO: Number of nodes with available pods: 1
May 21 09:38:03.545: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:38:04.538: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:38:04.542: INFO: Number of nodes with available pods: 1
May 21 09:38:04.543: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:38:05.540: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:38:05.545: INFO: Number of nodes with available pods: 1
May 21 09:38:05.545: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:38:06.543: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:38:06.548: INFO: Number of nodes with available pods: 1
May 21 09:38:06.548: INFO: Node k8s-02 is running more than one daemon pod
May 21 09:38:07.577: INFO: DaemonSet pods can't tolerate node k8s-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:38:07.653: INFO: Number of nodes with available pods: 2
May 21 09:38:07.653: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9010, will wait for the garbage collector to delete the pods
May 21 09:38:07.790: INFO: Deleting DaemonSet.extensions daemon-set took: 76.966931ms
May 21 09:38:08.391: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.366858ms
May 21 09:38:14.099: INFO: Number of nodes with available pods: 0
May 21 09:38:14.099: INFO: Number of running nodes: 0, number of available pods: 0
May 21 09:38:14.103: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54417"},"items":null}

May 21 09:38:14.106: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54417"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:38:14.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9010" for this suite.

• [SLOW TEST:27.112 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":263,"skipped":4656,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:38:14.136: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:38:14.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7771" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":264,"skipped":4673,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:38:14.784: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 21 09:38:15.015: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5391  7c43a26d-5e9e-4ab4-8fe0-e6f6f3d92c3d 54435 0 2021-05-21 09:38:15 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-05-21 09:38:15 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tbh7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tbh7h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tbh7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 09:38:15.021: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 21 09:38:17.031: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 21 09:38:19.038: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 21 09:38:19.038: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5391 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:38:19.038: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Verifying customized DNS server is configured on pod...
May 21 09:38:19.226: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5391 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:38:19.226: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:38:19.450: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:38:19.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5391" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":265,"skipped":4689,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:38:19.548: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-7e83f86e-b1ed-4d1f-bfba-f3f8760965ee in namespace container-probe-3725
May 21 09:38:23.849: INFO: Started pod liveness-7e83f86e-b1ed-4d1f-bfba-f3f8760965ee in namespace container-probe-3725
STEP: checking the pod's current state and verifying that restartCount is present
May 21 09:38:23.852: INFO: Initial restart count of pod liveness-7e83f86e-b1ed-4d1f-bfba-f3f8760965ee is 0
May 21 09:38:40.199: INFO: Restart count of pod container-probe-3725/liveness-7e83f86e-b1ed-4d1f-bfba-f3f8760965ee is now 1 (16.346156467s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:38:40.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3725" for this suite.

• [SLOW TEST:20.826 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":266,"skipped":4701,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:38:40.374: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-6f4fcda7-18ee-47b8-a759-1d8740a7afd6
STEP: Creating a pod to test consume secrets
May 21 09:38:40.558: INFO: Waiting up to 5m0s for pod "pod-secrets-58515917-8c20-4ff1-9d73-b460209ac48c" in namespace "secrets-1848" to be "Succeeded or Failed"
May 21 09:38:40.574: INFO: Pod "pod-secrets-58515917-8c20-4ff1-9d73-b460209ac48c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.719509ms
May 21 09:38:42.586: INFO: Pod "pod-secrets-58515917-8c20-4ff1-9d73-b460209ac48c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027835791s
May 21 09:38:44.707: INFO: Pod "pod-secrets-58515917-8c20-4ff1-9d73-b460209ac48c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.148763738s
STEP: Saw pod success
May 21 09:38:44.707: INFO: Pod "pod-secrets-58515917-8c20-4ff1-9d73-b460209ac48c" satisfied condition "Succeeded or Failed"
May 21 09:38:44.715: INFO: Trying to get logs from node k8s-03 pod pod-secrets-58515917-8c20-4ff1-9d73-b460209ac48c container secret-volume-test: <nil>
STEP: delete the pod
May 21 09:38:44.790: INFO: Waiting for pod pod-secrets-58515917-8c20-4ff1-9d73-b460209ac48c to disappear
May 21 09:38:44.845: INFO: Pod pod-secrets-58515917-8c20-4ff1-9d73-b460209ac48c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:38:44.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1848" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4707,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:38:44.860: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 21 09:38:45.331: INFO: Pod name pod-release: Found 0 pods out of 1
May 21 09:38:50.371: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:38:50.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-103" for this suite.

• [SLOW TEST:5.701 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":268,"skipped":4718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:38:50.562: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:39:04.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4590" for this suite.

• [SLOW TEST:14.186 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":269,"skipped":4771,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:39:04.749: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
May 21 09:39:04.892: INFO: Waiting up to 5m0s for pod "var-expansion-69cf5a9f-a4e2-468c-8e3d-e861efde1003" in namespace "var-expansion-4862" to be "Succeeded or Failed"
May 21 09:39:04.901: INFO: Pod "var-expansion-69cf5a9f-a4e2-468c-8e3d-e861efde1003": Phase="Pending", Reason="", readiness=false. Elapsed: 8.557329ms
May 21 09:39:06.912: INFO: Pod "var-expansion-69cf5a9f-a4e2-468c-8e3d-e861efde1003": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019810544s
May 21 09:39:08.925: INFO: Pod "var-expansion-69cf5a9f-a4e2-468c-8e3d-e861efde1003": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033046263s
STEP: Saw pod success
May 21 09:39:08.926: INFO: Pod "var-expansion-69cf5a9f-a4e2-468c-8e3d-e861efde1003" satisfied condition "Succeeded or Failed"
May 21 09:39:08.929: INFO: Trying to get logs from node k8s-03 pod var-expansion-69cf5a9f-a4e2-468c-8e3d-e861efde1003 container dapi-container: <nil>
STEP: delete the pod
May 21 09:39:08.983: INFO: Waiting for pod var-expansion-69cf5a9f-a4e2-468c-8e3d-e861efde1003 to disappear
May 21 09:39:08.986: INFO: Pod var-expansion-69cf5a9f-a4e2-468c-8e3d-e861efde1003 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:39:08.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4862" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":270,"skipped":4792,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:39:09.001: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2563
May 21 09:39:13.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 21 09:39:13.717: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 21 09:39:13.718: INFO: stdout: "iptables"
May 21 09:39:13.718: INFO: proxyMode: iptables
May 21 09:39:13.837: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 09:39:13.850: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2563
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2563
I0521 09:39:13.915722      25 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2563, replica count: 3
I0521 09:39:16.966405      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:39:19.966670      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 09:39:19.977: INFO: Creating new exec pod
May 21 09:39:25.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
May 21 09:39:25.376: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 21 09:39:25.376: INFO: stdout: ""
May 21 09:39:25.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c nc -zv -t -w 2 10.96.218.171 80'
May 21 09:39:25.725: INFO: stderr: "+ nc -zv -t -w 2 10.96.218.171 80\nConnection to 10.96.218.171 80 port [tcp/http] succeeded!\n"
May 21 09:39:25.725: INFO: stdout: ""
May 21 09:39:25.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c nc -zv -t -w 2 8.16.0.94 31119'
May 21 09:39:26.090: INFO: stderr: "+ nc -zv -t -w 2 8.16.0.94 31119\nConnection to 8.16.0.94 31119 port [tcp/31119] succeeded!\n"
May 21 09:39:26.090: INFO: stdout: ""
May 21 09:39:26.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c nc -zv -t -w 2 8.16.0.95 31119'
May 21 09:39:26.485: INFO: stderr: "+ nc -zv -t -w 2 8.16.0.95 31119\nConnection to 8.16.0.95 31119 port [tcp/31119] succeeded!\n"
May 21 09:39:26.486: INFO: stdout: ""
May 21 09:39:26.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://8.16.0.94:31119/ ; done'
May 21 09:39:27.067: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n+ echo\n+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n"
May 21 09:39:27.068: INFO: stdout: "\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf\naffinity-nodeport-timeout-b8pnf"
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Received response from host: affinity-nodeport-timeout-b8pnf
May 21 09:39:27.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://8.16.0.94:31119/'
May 21 09:39:27.451: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n"
May 21 09:39:27.451: INFO: stdout: "affinity-nodeport-timeout-b8pnf"
May 21 09:39:47.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://8.16.0.94:31119/'
May 21 09:39:47.794: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n"
May 21 09:39:47.794: INFO: stdout: "affinity-nodeport-timeout-b8pnf"
May 21 09:40:07.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://8.16.0.94:31119/'
May 21 09:40:08.149: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n"
May 21 09:40:08.149: INFO: stdout: "affinity-nodeport-timeout-b8pnf"
May 21 09:40:28.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://8.16.0.94:31119/'
May 21 09:40:28.501: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n"
May 21 09:40:28.501: INFO: stdout: "affinity-nodeport-timeout-b8pnf"
May 21 09:40:48.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://8.16.0.94:31119/'
May 21 09:40:48.878: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n"
May 21 09:40:48.878: INFO: stdout: "affinity-nodeport-timeout-b8pnf"
May 21 09:41:08.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=services-2563 exec execpod-affinity4n2qf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://8.16.0.94:31119/'
May 21 09:41:09.236: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://8.16.0.94:31119/\n"
May 21 09:41:09.236: INFO: stdout: "affinity-nodeport-timeout-v5l6h"
May 21 09:41:09.236: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2563, will wait for the garbage collector to delete the pods
May 21 09:41:09.469: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 41.847204ms
May 21 09:41:10.170: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 700.413722ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:41:20.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2563" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:131.431 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":271,"skipped":4837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:41:20.433: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 21 09:41:32.700: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:41:32.700: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:41:32.847: INFO: Exec stderr: ""
May 21 09:41:32.847: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:41:32.847: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:41:32.990: INFO: Exec stderr: ""
May 21 09:41:32.990: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:41:32.990: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:41:33.156: INFO: Exec stderr: ""
May 21 09:41:33.156: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:41:33.156: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:41:33.305: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 21 09:41:33.306: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:41:33.306: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:41:33.461: INFO: Exec stderr: ""
May 21 09:41:33.461: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:41:33.461: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:41:33.611: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 21 09:41:33.611: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:41:33.611: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:41:33.770: INFO: Exec stderr: ""
May 21 09:41:33.770: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:41:33.770: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:41:33.920: INFO: Exec stderr: ""
May 21 09:41:33.920: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:41:33.920: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:41:34.064: INFO: Exec stderr: ""
May 21 09:41:34.064: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 09:41:34.064: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
May 21 09:41:34.215: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:41:34.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3655" for this suite.

• [SLOW TEST:13.844 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4878,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:41:34.277: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-bc5127f9-b828-430c-bb51-873182a2b3b3
STEP: Creating a pod to test consume secrets
May 21 09:41:34.452: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4e109f64-be5d-46a9-8e30-578aa52cf1df" in namespace "projected-8181" to be "Succeeded or Failed"
May 21 09:41:34.459: INFO: Pod "pod-projected-secrets-4e109f64-be5d-46a9-8e30-578aa52cf1df": Phase="Pending", Reason="", readiness=false. Elapsed: 7.011784ms
May 21 09:41:36.473: INFO: Pod "pod-projected-secrets-4e109f64-be5d-46a9-8e30-578aa52cf1df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020760957s
May 21 09:41:38.488: INFO: Pod "pod-projected-secrets-4e109f64-be5d-46a9-8e30-578aa52cf1df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035961634s
STEP: Saw pod success
May 21 09:41:38.488: INFO: Pod "pod-projected-secrets-4e109f64-be5d-46a9-8e30-578aa52cf1df" satisfied condition "Succeeded or Failed"
May 21 09:41:38.493: INFO: Trying to get logs from node k8s-02 pod pod-projected-secrets-4e109f64-be5d-46a9-8e30-578aa52cf1df container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 09:41:38.577: INFO: Waiting for pod pod-projected-secrets-4e109f64-be5d-46a9-8e30-578aa52cf1df to disappear
May 21 09:41:38.589: INFO: Pod pod-projected-secrets-4e109f64-be5d-46a9-8e30-578aa52cf1df no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:41:38.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8181" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4879,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:41:38.607: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 21 09:41:38.763: INFO: Waiting up to 5m0s for pod "pod-1a937c50-aa12-4878-8ac5-76482d2ab016" in namespace "emptydir-7586" to be "Succeeded or Failed"
May 21 09:41:38.807: INFO: Pod "pod-1a937c50-aa12-4878-8ac5-76482d2ab016": Phase="Pending", Reason="", readiness=false. Elapsed: 43.40661ms
May 21 09:41:40.820: INFO: Pod "pod-1a937c50-aa12-4878-8ac5-76482d2ab016": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056467291s
May 21 09:41:42.832: INFO: Pod "pod-1a937c50-aa12-4878-8ac5-76482d2ab016": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068150021s
STEP: Saw pod success
May 21 09:41:42.832: INFO: Pod "pod-1a937c50-aa12-4878-8ac5-76482d2ab016" satisfied condition "Succeeded or Failed"
May 21 09:41:42.836: INFO: Trying to get logs from node k8s-02 pod pod-1a937c50-aa12-4878-8ac5-76482d2ab016 container test-container: <nil>
STEP: delete the pod
May 21 09:41:42.933: INFO: Waiting for pod pod-1a937c50-aa12-4878-8ac5-76482d2ab016 to disappear
May 21 09:41:42.993: INFO: Pod pod-1a937c50-aa12-4878-8ac5-76482d2ab016 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:41:42.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7586" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4881,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:41:43.101: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 21 09:41:43.427: INFO: Waiting up to 5m0s for pod "downward-api-53440850-8b0d-41c7-8d96-71f24ef30988" in namespace "downward-api-2" to be "Succeeded or Failed"
May 21 09:41:43.590: INFO: Pod "downward-api-53440850-8b0d-41c7-8d96-71f24ef30988": Phase="Pending", Reason="", readiness=false. Elapsed: 162.301232ms
May 21 09:41:45.723: INFO: Pod "downward-api-53440850-8b0d-41c7-8d96-71f24ef30988": Phase="Pending", Reason="", readiness=false. Elapsed: 2.295589798s
May 21 09:41:47.731: INFO: Pod "downward-api-53440850-8b0d-41c7-8d96-71f24ef30988": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.30367895s
STEP: Saw pod success
May 21 09:41:47.731: INFO: Pod "downward-api-53440850-8b0d-41c7-8d96-71f24ef30988" satisfied condition "Succeeded or Failed"
May 21 09:41:47.736: INFO: Trying to get logs from node k8s-03 pod downward-api-53440850-8b0d-41c7-8d96-71f24ef30988 container dapi-container: <nil>
STEP: delete the pod
May 21 09:41:47.802: INFO: Waiting for pod downward-api-53440850-8b0d-41c7-8d96-71f24ef30988 to disappear
May 21 09:41:47.810: INFO: Pod downward-api-53440850-8b0d-41c7-8d96-71f24ef30988 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:41:47.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4891,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:41:47.824: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:41:48.877: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 09:41:50.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757186908, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757186908, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757186909, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757186908, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-57bb7b8b8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:41:53.990: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:41:54.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7537" for this suite.
STEP: Destroying namespace "webhook-7537-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.599 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":276,"skipped":4895,"failed":0}
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:41:54.423: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-761ea1ab-cf1d-40dd-a54c-b42b3ce6942a
STEP: Creating secret with name s-test-opt-upd-af541415-3312-4832-8c70-a2567573e916
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-761ea1ab-cf1d-40dd-a54c-b42b3ce6942a
STEP: Updating secret s-test-opt-upd-af541415-3312-4832-8c70-a2567573e916
STEP: Creating secret with name s-test-opt-create-f828468a-c36d-477e-a8ac-d78e3da2cf72
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:43:09.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2772" for this suite.

• [SLOW TEST:75.210 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":277,"skipped":4895,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:43:09.634: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 21 09:43:09.871: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 09:43:09.871: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 09:43:09.955: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 09:43:09.955: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 09:43:10.082: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 09:43:10.082: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 09:43:10.232: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 09:43:10.232: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 09:43:13.200: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 21 09:43:13.200: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 21 09:43:13.449: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 21 09:43:13.467: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 21 09:43:13.469: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0
May 21 09:43:13.469: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0
May 21 09:43:13.469: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0
May 21 09:43:13.469: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0
May 21 09:43:13.470: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0
May 21 09:43:13.470: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0
May 21 09:43:13.470: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0
May 21 09:43:13.470: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 0
May 21 09:43:13.470: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1
May 21 09:43:13.470: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1
May 21 09:43:13.470: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2
May 21 09:43:13.470: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2
May 21 09:43:13.470: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2
May 21 09:43:13.470: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2
May 21 09:43:13.592: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2
May 21 09:43:13.592: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2
May 21 09:43:13.825: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2
May 21 09:43:13.825: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2
May 21 09:43:13.894: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2
May 21 09:43:13.894: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 2
May 21 09:43:14.016: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1
STEP: listing Deployments
May 21 09:43:14.075: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 21 09:43:14.096: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 21 09:43:14.166: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 09:43:14.275: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 09:43:14.562: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 09:43:14.851: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 09:43:15.049: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 09:43:15.184: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 21 09:43:18.840: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1
May 21 09:43:18.840: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1
May 21 09:43:18.840: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1
May 21 09:43:18.840: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1
May 21 09:43:18.841: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1
May 21 09:43:18.841: INFO: observed Deployment test-deployment in namespace deployment-4191 with ReadyReplicas 1
STEP: deleting the Deployment
May 21 09:43:19.046: INFO: observed event type MODIFIED
May 21 09:43:19.046: INFO: observed event type MODIFIED
May 21 09:43:19.047: INFO: observed event type MODIFIED
May 21 09:43:19.047: INFO: observed event type MODIFIED
May 21 09:43:19.047: INFO: observed event type MODIFIED
May 21 09:43:19.047: INFO: observed event type MODIFIED
May 21 09:43:19.047: INFO: observed event type MODIFIED
May 21 09:43:19.047: INFO: observed event type MODIFIED
May 21 09:43:19.048: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 09:43:19.091: INFO: Log out all the ReplicaSets if there is no deployment created
May 21 09:43:19.190: INFO: ReplicaSet "test-deployment-5c88f87db4":
&ReplicaSet{ObjectMeta:{test-deployment-5c88f87db4  deployment-4191  afb2d0b1-d9f6-43ac-aa82-d4ed5437aecf 56001 4 2021-05-21 09:43:13 +0000 UTC <nil> <nil> map[pod-template-hash:5c88f87db4 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 912469a1-ab76-4a46-a56a-23b8b10372f9 0xc00523d8a7 0xc00523d8a8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 09:43:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"912469a1-ab76-4a46-a56a-23b8b10372f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5c88f87db4,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5c88f87db4 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment registry.cn-hangzhou.aliyuncs.com/ks8gcr/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00523d928 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 21 09:43:19.241: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-4191  cfb4aeca-82aa-4f58-a2f5-58c54f5356f1 55995 3 2021-05-21 09:43:14 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 912469a1-ab76-4a46-a56a-23b8b10372f9 0xc00523d987 0xc00523d988}] []  [{kube-controller-manager Update apps/v1 2021-05-21 09:43:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"912469a1-ab76-4a46-a56a-23b8b10372f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00523d9f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

May 21 09:43:19.246: INFO: pod: "test-deployment-768947d6f5-5lqvv":
&Pod{ObjectMeta:{test-deployment-768947d6f5-5lqvv test-deployment-768947d6f5- deployment-4191  0964ed2f-6b2a-429a-942b-1b8ff463c312 55980 0 2021-05-21 09:43:14 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[cni.projectcalico.org/podIP:10.100.179.24/32 cni.projectcalico.org/podIPs:10.100.179.24/32] [{apps/v1 ReplicaSet test-deployment-768947d6f5 cfb4aeca-82aa-4f58-a2f5-58c54f5356f1 0xc00523df27 0xc00523df28}] []  [{kube-controller-manager Update v1 2021-05-21 09:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfb4aeca-82aa-4f58-a2f5-58c54f5356f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 09:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 09:43:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.179.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tv6nx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tv6nx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tv6nx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:10.100.179.24,StartTime:2021-05-21 09:43:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 09:43:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://87e656a1975e037c3e2755981291f377acb9f80d5ce27e8dda0285b422bca473,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.179.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 21 09:43:19.247: INFO: pod: "test-deployment-768947d6f5-qd27j":
&Pod{ObjectMeta:{test-deployment-768947d6f5-qd27j test-deployment-768947d6f5- deployment-4191  367d893c-ee63-477d-b388-3834c34f7a00 56000 0 2021-05-21 09:43:18 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 cfb4aeca-82aa-4f58-a2f5-58c54f5356f1 0xc00412a0e7 0xc00412a0e8}] []  [{kube-controller-manager Update v1 2021-05-21 09:43:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfb4aeca-82aa-4f58-a2f5-58c54f5356f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 09:43:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tv6nx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tv6nx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tv6nx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.95,PodIP:,StartTime:2021-05-21 09:43:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 21 09:43:19.247: INFO: ReplicaSet "test-deployment-77c46c966d":
&ReplicaSet{ObjectMeta:{test-deployment-77c46c966d  deployment-4191  cded31b8-18ce-46c5-8d39-c771ac7afd47 55899 2 2021-05-21 09:43:09 +0000 UTC <nil> <nil> map[pod-template-hash:77c46c966d test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 912469a1-ab76-4a46-a56a-23b8b10372f9 0xc00523da57 0xc00523da58}] []  [{kube-controller-manager Update apps/v1 2021-05-21 09:43:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"912469a1-ab76-4a46-a56a-23b8b10372f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 77c46c966d,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:77c46c966d test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00523dac0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

May 21 09:43:19.251: INFO: pod: "test-deployment-77c46c966d-9ll9n":
&Pod{ObjectMeta:{test-deployment-77c46c966d-9ll9n test-deployment-77c46c966d- deployment-4191  3a75d4be-4cce-4717-a77e-c372f2b521b7 55867 0 2021-05-21 09:43:09 +0000 UTC <nil> <nil> map[pod-template-hash:77c46c966d test-deployment-static:true] map[cni.projectcalico.org/podIP:10.100.179.23/32 cni.projectcalico.org/podIPs:10.100.179.23/32] [{apps/v1 ReplicaSet test-deployment-77c46c966d cded31b8-18ce-46c5-8d39-c771ac7afd47 0xc00412b2d7 0xc00412b2d8}] []  [{kube-controller-manager Update v1 2021-05-21 09:43:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cded31b8-18ce-46c5-8d39-c771ac7afd47\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 09:43:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 09:43:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.179.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tv6nx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tv6nx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tv6nx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 09:43:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:8.16.0.94,PodIP:10.100.179.23,StartTime:2021-05-21 09:43:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 09:43:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21,ImageID:registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost@sha256:e23f72f469927ead04b7bb2a5cedf8569906c38ced37a64767542e7d22831d45,ContainerID:containerd://64f5f0c866747094c4d0e6894472fa8cc5070725914a979cec426f454202b4ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.179.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:43:19.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4191" for this suite.

• [SLOW TEST:9.628 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":278,"skipped":4905,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:43:19.262: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 21 09:43:19.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 create -f -'
May 21 09:43:23.560: INFO: stderr: ""
May 21 09:43:23.560: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 09:43:23.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:43:23.864: INFO: stderr: ""
May 21 09:43:23.864: INFO: stdout: "update-demo-nautilus-8mv5v update-demo-nautilus-fvmx8 "
May 21 09:43:23.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-8mv5v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:43:24.047: INFO: stderr: ""
May 21 09:43:24.047: INFO: stdout: ""
May 21 09:43:24.047: INFO: update-demo-nautilus-8mv5v is created but not running
May 21 09:43:29.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:43:29.233: INFO: stderr: ""
May 21 09:43:29.233: INFO: stdout: "update-demo-nautilus-8mv5v update-demo-nautilus-fvmx8 "
May 21 09:43:29.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-8mv5v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:43:29.414: INFO: stderr: ""
May 21 09:43:29.414: INFO: stdout: "true"
May 21 09:43:29.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-8mv5v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 09:43:29.601: INFO: stderr: ""
May 21 09:43:29.601: INFO: stdout: "registry.cn-hangzhou.aliyuncs.com/ks8gcr/nautilus:1.0"
May 21 09:43:29.601: INFO: validating pod update-demo-nautilus-8mv5v
May 21 09:43:29.608: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 09:43:29.608: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 09:43:29.608: INFO: update-demo-nautilus-8mv5v is verified up and running
May 21 09:43:29.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-fvmx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:43:29.788: INFO: stderr: ""
May 21 09:43:29.788: INFO: stdout: "true"
May 21 09:43:29.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-fvmx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 09:43:29.980: INFO: stderr: ""
May 21 09:43:29.980: INFO: stdout: "registry.cn-hangzhou.aliyuncs.com/ks8gcr/nautilus:1.0"
May 21 09:43:29.980: INFO: validating pod update-demo-nautilus-fvmx8
May 21 09:43:29.994: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 09:43:29.994: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 09:43:29.994: INFO: update-demo-nautilus-fvmx8 is verified up and running
STEP: scaling down the replication controller
May 21 09:43:30.000: INFO: scanned /root for discovery docs: <nil>
May 21 09:43:30.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 21 09:43:31.471: INFO: stderr: ""
May 21 09:43:31.471: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 09:43:31.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:43:31.656: INFO: stderr: ""
May 21 09:43:31.656: INFO: stdout: "update-demo-nautilus-8mv5v update-demo-nautilus-fvmx8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 21 09:43:36.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:43:36.834: INFO: stderr: ""
May 21 09:43:36.834: INFO: stdout: "update-demo-nautilus-8mv5v update-demo-nautilus-fvmx8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 21 09:43:41.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:43:42.020: INFO: stderr: ""
May 21 09:43:42.020: INFO: stdout: "update-demo-nautilus-fvmx8 "
May 21 09:43:42.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-fvmx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:43:42.203: INFO: stderr: ""
May 21 09:43:42.203: INFO: stdout: "true"
May 21 09:43:42.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-fvmx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 09:43:42.379: INFO: stderr: ""
May 21 09:43:42.380: INFO: stdout: "registry.cn-hangzhou.aliyuncs.com/ks8gcr/nautilus:1.0"
May 21 09:43:42.380: INFO: validating pod update-demo-nautilus-fvmx8
May 21 09:43:42.385: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 09:43:42.386: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 09:43:42.386: INFO: update-demo-nautilus-fvmx8 is verified up and running
STEP: scaling up the replication controller
May 21 09:43:42.390: INFO: scanned /root for discovery docs: <nil>
May 21 09:43:42.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 21 09:43:43.680: INFO: stderr: ""
May 21 09:43:43.680: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 09:43:43.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:43:43.878: INFO: stderr: ""
May 21 09:43:43.878: INFO: stdout: "update-demo-nautilus-fvmx8 update-demo-nautilus-xnxcj "
May 21 09:43:43.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-fvmx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:43:44.051: INFO: stderr: ""
May 21 09:43:44.051: INFO: stdout: "true"
May 21 09:43:44.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-fvmx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 09:43:44.245: INFO: stderr: ""
May 21 09:43:44.245: INFO: stdout: "registry.cn-hangzhou.aliyuncs.com/ks8gcr/nautilus:1.0"
May 21 09:43:44.245: INFO: validating pod update-demo-nautilus-fvmx8
May 21 09:43:44.251: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 09:43:44.251: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 09:43:44.251: INFO: update-demo-nautilus-fvmx8 is verified up and running
May 21 09:43:44.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-xnxcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:43:44.415: INFO: stderr: ""
May 21 09:43:44.415: INFO: stdout: ""
May 21 09:43:44.415: INFO: update-demo-nautilus-xnxcj is created but not running
May 21 09:43:49.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:43:49.576: INFO: stderr: ""
May 21 09:43:49.576: INFO: stdout: "update-demo-nautilus-fvmx8 update-demo-nautilus-xnxcj "
May 21 09:43:49.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-fvmx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:43:49.947: INFO: stderr: ""
May 21 09:43:49.947: INFO: stdout: "true"
May 21 09:43:49.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-fvmx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 09:43:50.125: INFO: stderr: ""
May 21 09:43:50.125: INFO: stdout: "registry.cn-hangzhou.aliyuncs.com/ks8gcr/nautilus:1.0"
May 21 09:43:50.125: INFO: validating pod update-demo-nautilus-fvmx8
May 21 09:43:50.130: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 09:43:50.130: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 09:43:50.130: INFO: update-demo-nautilus-fvmx8 is verified up and running
May 21 09:43:50.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-xnxcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:43:50.311: INFO: stderr: ""
May 21 09:43:50.312: INFO: stdout: "true"
May 21 09:43:50.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods update-demo-nautilus-xnxcj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 09:43:50.483: INFO: stderr: ""
May 21 09:43:50.483: INFO: stdout: "registry.cn-hangzhou.aliyuncs.com/ks8gcr/nautilus:1.0"
May 21 09:43:50.483: INFO: validating pod update-demo-nautilus-xnxcj
May 21 09:43:50.489: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 09:43:50.490: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 09:43:50.490: INFO: update-demo-nautilus-xnxcj is verified up and running
STEP: using delete to clean up resources
May 21 09:43:50.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 delete --grace-period=0 --force -f -'
May 21 09:43:50.672: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 09:43:50.672: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 21 09:43:50.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get rc,svc -l name=update-demo --no-headers'
May 21 09:43:50.852: INFO: stderr: "No resources found in kubectl-3570 namespace.\n"
May 21 09:43:50.852: INFO: stdout: ""
May 21 09:43:50.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 09:43:51.030: INFO: stderr: ""
May 21 09:43:51.031: INFO: stdout: "update-demo-nautilus-fvmx8\nupdate-demo-nautilus-xnxcj\n"
May 21 09:43:51.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get rc,svc -l name=update-demo --no-headers'
May 21 09:43:51.708: INFO: stderr: "No resources found in kubectl-3570 namespace.\n"
May 21 09:43:51.708: INFO: stdout: ""
May 21 09:43:51.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3570 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 09:43:51.896: INFO: stderr: ""
May 21 09:43:51.896: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:43:51.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3570" for this suite.

• [SLOW TEST:32.664 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":279,"skipped":4929,"failed":0}
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:43:51.927: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-6027/secret-test-c5ed079c-5f7c-445a-83ed-f56197d7df7e
STEP: Creating a pod to test consume secrets
May 21 09:43:52.163: INFO: Waiting up to 5m0s for pod "pod-configmaps-11c24552-073e-47c0-af7c-d927bf1da2cc" in namespace "secrets-6027" to be "Succeeded or Failed"
May 21 09:43:52.199: INFO: Pod "pod-configmaps-11c24552-073e-47c0-af7c-d927bf1da2cc": Phase="Pending", Reason="", readiness=false. Elapsed: 35.716418ms
May 21 09:43:54.205: INFO: Pod "pod-configmaps-11c24552-073e-47c0-af7c-d927bf1da2cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041364356s
May 21 09:43:56.212: INFO: Pod "pod-configmaps-11c24552-073e-47c0-af7c-d927bf1da2cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048702362s
STEP: Saw pod success
May 21 09:43:56.212: INFO: Pod "pod-configmaps-11c24552-073e-47c0-af7c-d927bf1da2cc" satisfied condition "Succeeded or Failed"
May 21 09:43:56.216: INFO: Trying to get logs from node k8s-03 pod pod-configmaps-11c24552-073e-47c0-af7c-d927bf1da2cc container env-test: <nil>
STEP: delete the pod
May 21 09:43:56.277: INFO: Waiting for pod pod-configmaps-11c24552-073e-47c0-af7c-d927bf1da2cc to disappear
May 21 09:43:56.340: INFO: Pod pod-configmaps-11c24552-073e-47c0-af7c-d927bf1da2cc no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:43:56.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6027" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":280,"skipped":4929,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:43:56.469: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-c8e90842-a93d-4cc2-aac1-c342a5cd7587
STEP: Creating a pod to test consume secrets
May 21 09:43:56.748: INFO: Waiting up to 5m0s for pod "pod-secrets-0005e111-128f-4641-a6e8-340769956da5" in namespace "secrets-6074" to be "Succeeded or Failed"
May 21 09:43:56.901: INFO: Pod "pod-secrets-0005e111-128f-4641-a6e8-340769956da5": Phase="Pending", Reason="", readiness=false. Elapsed: 153.22745ms
May 21 09:43:58.913: INFO: Pod "pod-secrets-0005e111-128f-4641-a6e8-340769956da5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165867786s
May 21 09:44:00.983: INFO: Pod "pod-secrets-0005e111-128f-4641-a6e8-340769956da5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.235199891s
STEP: Saw pod success
May 21 09:44:00.983: INFO: Pod "pod-secrets-0005e111-128f-4641-a6e8-340769956da5" satisfied condition "Succeeded or Failed"
May 21 09:44:00.988: INFO: Trying to get logs from node k8s-03 pod pod-secrets-0005e111-128f-4641-a6e8-340769956da5 container secret-env-test: <nil>
STEP: delete the pod
May 21 09:44:01.135: INFO: Waiting for pod pod-secrets-0005e111-128f-4641-a6e8-340769956da5 to disappear
May 21 09:44:01.144: INFO: Pod pod-secrets-0005e111-128f-4641-a6e8-340769956da5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:44:01.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6074" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4934,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:44:01.157: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:44:29.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9513" for this suite.

• [SLOW TEST:28.411 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":282,"skipped":4941,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:44:29.568: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 21 09:44:37.882: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 09:44:37.953: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 09:44:39.954: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 09:44:39.964: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 09:44:41.954: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 09:44:41.966: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 09:44:43.954: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 09:44:43.963: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 09:44:45.953: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 09:44:45.963: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 09:44:47.954: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 09:44:47.964: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 09:44:49.954: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 09:44:49.963: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 09:44:51.954: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 09:44:51.962: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:44:51.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8382" for this suite.

• [SLOW TEST:22.415 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4941,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:44:51.984: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 21 09:44:52.340: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:44:52.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2556" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":284,"skipped":4954,"failed":0}
S
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:44:52.400: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 21 09:44:52.595: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 09:45:52.654: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:45:52.659: INFO: Starting informer...
STEP: Starting pod...
May 21 09:45:52.879: INFO: Pod is running on k8s-03. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 21 09:45:53.012: INFO: Pod wasn't evicted. Proceeding
May 21 09:45:53.012: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 21 09:47:08.223: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:08.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7941" for this suite.

• [SLOW TEST:135.843 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":285,"skipped":4955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:08.244: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:13.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2186" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":286,"skipped":5020,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:13.170: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-1729
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1729 to expose endpoints map[]
May 21 09:47:13.608: INFO: successfully validated that service endpoint-test2 in namespace services-1729 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1729
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1729 to expose endpoints map[pod1:[80]]
May 21 09:47:17.909: INFO: successfully validated that service endpoint-test2 in namespace services-1729 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-1729
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1729 to expose endpoints map[pod1:[80] pod2:[80]]
May 21 09:47:21.152: INFO: successfully validated that service endpoint-test2 in namespace services-1729 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-1729
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1729 to expose endpoints map[pod2:[80]]
May 21 09:47:21.287: INFO: successfully validated that service endpoint-test2 in namespace services-1729 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-1729
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1729 to expose endpoints map[]
May 21 09:47:21.603: INFO: successfully validated that service endpoint-test2 in namespace services-1729 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:21.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1729" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:8.731 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":287,"skipped":5021,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:21.901: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 21 09:47:22.093: INFO: Waiting up to 5m0s for pod "pod-4e0813f0-566d-45ac-ae3d-812cb1c91013" in namespace "emptydir-7208" to be "Succeeded or Failed"
May 21 09:47:22.099: INFO: Pod "pod-4e0813f0-566d-45ac-ae3d-812cb1c91013": Phase="Pending", Reason="", readiness=false. Elapsed: 5.553093ms
May 21 09:47:24.198: INFO: Pod "pod-4e0813f0-566d-45ac-ae3d-812cb1c91013": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104976075s
May 21 09:47:26.203: INFO: Pod "pod-4e0813f0-566d-45ac-ae3d-812cb1c91013": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.10940192s
STEP: Saw pod success
May 21 09:47:26.203: INFO: Pod "pod-4e0813f0-566d-45ac-ae3d-812cb1c91013" satisfied condition "Succeeded or Failed"
May 21 09:47:26.207: INFO: Trying to get logs from node k8s-03 pod pod-4e0813f0-566d-45ac-ae3d-812cb1c91013 container test-container: <nil>
STEP: delete the pod
May 21 09:47:26.295: INFO: Waiting for pod pod-4e0813f0-566d-45ac-ae3d-812cb1c91013 to disappear
May 21 09:47:26.305: INFO: Pod pod-4e0813f0-566d-45ac-ae3d-812cb1c91013 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:26.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7208" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":288,"skipped":5025,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:26.394: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:26.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4562" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":289,"skipped":5036,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:26.749: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
May 21 09:47:26.868: INFO: Waiting up to 5m0s for pod "test-pod-a87f8b75-fa21-46a9-ac38-b10bd3ce8b31" in namespace "svcaccounts-7064" to be "Succeeded or Failed"
May 21 09:47:26.908: INFO: Pod "test-pod-a87f8b75-fa21-46a9-ac38-b10bd3ce8b31": Phase="Pending", Reason="", readiness=false. Elapsed: 40.64583ms
May 21 09:47:28.921: INFO: Pod "test-pod-a87f8b75-fa21-46a9-ac38-b10bd3ce8b31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053207998s
May 21 09:47:30.929: INFO: Pod "test-pod-a87f8b75-fa21-46a9-ac38-b10bd3ce8b31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061218841s
STEP: Saw pod success
May 21 09:47:30.929: INFO: Pod "test-pod-a87f8b75-fa21-46a9-ac38-b10bd3ce8b31" satisfied condition "Succeeded or Failed"
May 21 09:47:30.933: INFO: Trying to get logs from node k8s-03 pod test-pod-a87f8b75-fa21-46a9-ac38-b10bd3ce8b31 container agnhost-container: <nil>
STEP: delete the pod
May 21 09:47:30.986: INFO: Waiting for pod test-pod-a87f8b75-fa21-46a9-ac38-b10bd3ce8b31 to disappear
May 21 09:47:31.028: INFO: Pod test-pod-a87f8b75-fa21-46a9-ac38-b10bd3ce8b31 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:31.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7064" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":290,"skipped":5048,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:31.075: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
May 21 09:47:31.184: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 21 09:47:31.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 create -f -'
May 21 09:47:32.292: INFO: stderr: ""
May 21 09:47:32.292: INFO: stdout: "service/agnhost-replica created\n"
May 21 09:47:32.292: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 21 09:47:32.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 create -f -'
May 21 09:47:33.165: INFO: stderr: ""
May 21 09:47:33.165: INFO: stdout: "service/agnhost-primary created\n"
May 21 09:47:33.165: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 21 09:47:33.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 create -f -'
May 21 09:47:33.977: INFO: stderr: ""
May 21 09:47:33.977: INFO: stdout: "service/frontend created\n"
May 21 09:47:33.977: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 21 09:47:33.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 create -f -'
May 21 09:47:34.830: INFO: stderr: ""
May 21 09:47:34.830: INFO: stdout: "deployment.apps/frontend created\n"
May 21 09:47:34.830: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 21 09:47:34.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 create -f -'
May 21 09:47:35.571: INFO: stderr: ""
May 21 09:47:35.571: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 21 09:47:35.571: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.cn-hangzhou.aliyuncs.com/ks8gcr/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 21 09:47:35.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 create -f -'
May 21 09:47:36.575: INFO: stderr: ""
May 21 09:47:36.575: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 21 09:47:36.575: INFO: Waiting for all frontend pods to be Running.
May 21 09:47:41.625: INFO: Waiting for frontend to serve content.
May 21 09:47:42.695: INFO: Trying to add a new entry to the guestbook.
May 21 09:47:42.734: INFO: Verifying that added entry can be retrieved.
May 21 09:47:42.746: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
May 21 09:47:47.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 delete --grace-period=0 --force -f -'
May 21 09:47:48.154: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 09:47:48.154: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 21 09:47:48.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 delete --grace-period=0 --force -f -'
May 21 09:47:48.659: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 09:47:48.659: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 21 09:47:48.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 delete --grace-period=0 --force -f -'
May 21 09:47:49.094: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 09:47:49.094: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 21 09:47:49.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 delete --grace-period=0 --force -f -'
May 21 09:47:49.273: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 09:47:49.273: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 21 09:47:49.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 delete --grace-period=0 --force -f -'
May 21 09:47:49.493: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 09:47:49.493: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 21 09:47:49.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=kubectl-3749 delete --grace-period=0 --force -f -'
May 21 09:47:49.702: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 09:47:49.702: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:49.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3749" for this suite.

• [SLOW TEST:18.645 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":291,"skipped":5063,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:49.720: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 09:47:50.297: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f686d05-eeba-459b-b964-e4c92c1ac0e2" in namespace "downward-api-9435" to be "Succeeded or Failed"
May 21 09:47:50.446: INFO: Pod "downwardapi-volume-1f686d05-eeba-459b-b964-e4c92c1ac0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 149.666492ms
May 21 09:47:52.543: INFO: Pod "downwardapi-volume-1f686d05-eeba-459b-b964-e4c92c1ac0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246175588s
May 21 09:47:54.549: INFO: Pod "downwardapi-volume-1f686d05-eeba-459b-b964-e4c92c1ac0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.252380209s
May 21 09:47:56.554: INFO: Pod "downwardapi-volume-1f686d05-eeba-459b-b964-e4c92c1ac0e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.257922445s
STEP: Saw pod success
May 21 09:47:56.555: INFO: Pod "downwardapi-volume-1f686d05-eeba-459b-b964-e4c92c1ac0e2" satisfied condition "Succeeded or Failed"
May 21 09:47:56.559: INFO: Trying to get logs from node k8s-02 pod downwardapi-volume-1f686d05-eeba-459b-b964-e4c92c1ac0e2 container client-container: <nil>
STEP: delete the pod
May 21 09:47:56.628: INFO: Waiting for pod downwardapi-volume-1f686d05-eeba-459b-b964-e4c92c1ac0e2 to disappear
May 21 09:47:56.633: INFO: Pod downwardapi-volume-1f686d05-eeba-459b-b964-e4c92c1ac0e2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:56.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9435" for this suite.

• [SLOW TEST:6.925 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":5072,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:56.646: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-9d36ee69-20ab-41b1-8477-d3a396163187 in namespace container-probe-5295
May 21 09:48:00.870: INFO: Started pod liveness-9d36ee69-20ab-41b1-8477-d3a396163187 in namespace container-probe-5295
STEP: checking the pod's current state and verifying that restartCount is present
May 21 09:48:00.875: INFO: Initial restart count of pod liveness-9d36ee69-20ab-41b1-8477-d3a396163187 is 0
May 21 09:48:16.965: INFO: Restart count of pod container-probe-5295/liveness-9d36ee69-20ab-41b1-8477-d3a396163187 is now 1 (16.090318794s elapsed)
May 21 09:48:37.099: INFO: Restart count of pod container-probe-5295/liveness-9d36ee69-20ab-41b1-8477-d3a396163187 is now 2 (36.224443373s elapsed)
May 21 09:48:57.227: INFO: Restart count of pod container-probe-5295/liveness-9d36ee69-20ab-41b1-8477-d3a396163187 is now 3 (56.352835829s elapsed)
May 21 09:49:17.449: INFO: Restart count of pod container-probe-5295/liveness-9d36ee69-20ab-41b1-8477-d3a396163187 is now 4 (1m16.574772282s elapsed)
May 21 09:50:28.462: INFO: Restart count of pod container-probe-5295/liveness-9d36ee69-20ab-41b1-8477-d3a396163187 is now 5 (2m27.587313493s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:50:28.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5295" for this suite.

• [SLOW TEST:151.948 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":5081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:50:28.596: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-z65v
STEP: Creating a pod to test atomic-volume-subpath
May 21 09:50:28.845: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-z65v" in namespace "subpath-2473" to be "Succeeded or Failed"
May 21 09:50:28.880: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Pending", Reason="", readiness=false. Elapsed: 35.547665ms
May 21 09:50:30.971: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.125887638s
May 21 09:50:32.983: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Running", Reason="", readiness=true. Elapsed: 4.138638047s
May 21 09:50:34.993: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Running", Reason="", readiness=true. Elapsed: 6.148307365s
May 21 09:50:37.013: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Running", Reason="", readiness=true. Elapsed: 8.168682254s
May 21 09:50:39.023: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Running", Reason="", readiness=true. Elapsed: 10.178752683s
May 21 09:50:41.042: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Running", Reason="", readiness=true. Elapsed: 12.197430216s
May 21 09:50:43.056: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Running", Reason="", readiness=true. Elapsed: 14.210855884s
May 21 09:50:45.069: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Running", Reason="", readiness=true. Elapsed: 16.223993449s
May 21 09:50:47.076: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Running", Reason="", readiness=true. Elapsed: 18.231017791s
May 21 09:50:49.086: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Running", Reason="", readiness=true. Elapsed: 20.240862065s
May 21 09:50:51.098: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Running", Reason="", readiness=true. Elapsed: 22.253350104s
May 21 09:50:53.107: INFO: Pod "pod-subpath-test-secret-z65v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.262540273s
STEP: Saw pod success
May 21 09:50:53.107: INFO: Pod "pod-subpath-test-secret-z65v" satisfied condition "Succeeded or Failed"
May 21 09:50:53.112: INFO: Trying to get logs from node k8s-03 pod pod-subpath-test-secret-z65v container test-container-subpath-secret-z65v: <nil>
STEP: delete the pod
May 21 09:50:53.212: INFO: Waiting for pod pod-subpath-test-secret-z65v to disappear
May 21 09:50:53.219: INFO: Pod pod-subpath-test-secret-z65v no longer exists
STEP: Deleting pod pod-subpath-test-secret-z65v
May 21 09:50:53.219: INFO: Deleting pod "pod-subpath-test-secret-z65v" in namespace "subpath-2473"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:50:53.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2473" for this suite.

• [SLOW TEST:24.645 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":294,"skipped":5109,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:50:53.241: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:50:53.419: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:50:54.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1124" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":295,"skipped":5110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:50:54.846: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-cb265f6b-b362-4c20-b425-6e40f88ed3f4
STEP: Creating configMap with name cm-test-opt-upd-23b11497-35ab-4f6c-8a30-3f0c5422550b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-cb265f6b-b362-4c20-b425-6e40f88ed3f4
STEP: Updating configmap cm-test-opt-upd-23b11497-35ab-4f6c-8a30-3f0c5422550b
STEP: Creating configMap with name cm-test-opt-create-72be8a5c-25ec-4af6-950f-7259c9a0c304
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:24.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-66" for this suite.

• [SLOW TEST:89.631 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":296,"skipped":5157,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:24.477: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:40.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5603" for this suite.
STEP: Destroying namespace "nsdeletetest-2613" for this suite.
May 21 09:52:40.037: INFO: Namespace nsdeletetest-2613 was already deleted
STEP: Destroying namespace "nsdeletetest-2388" for this suite.

• [SLOW TEST:15.566 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":297,"skipped":5174,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:40.044: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 09:52:40.259: INFO: starting watch
STEP: patching
STEP: updating
May 21 09:52:40.300: INFO: waiting for watch events with expected annotations
May 21 09:52:40.300: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:40.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3958" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":298,"skipped":5176,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:40.399: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-876a15b9-5a40-4be5-acef-9dd8eb339517
STEP: Creating a pod to test consume configMaps
May 21 09:52:40.639: INFO: Waiting up to 5m0s for pod "pod-configmaps-1b6671b4-2cc9-4173-b214-2a3ac7d7428c" in namespace "configmap-9229" to be "Succeeded or Failed"
May 21 09:52:40.644: INFO: Pod "pod-configmaps-1b6671b4-2cc9-4173-b214-2a3ac7d7428c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.274606ms
May 21 09:52:42.655: INFO: Pod "pod-configmaps-1b6671b4-2cc9-4173-b214-2a3ac7d7428c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015288568s
May 21 09:52:44.666: INFO: Pod "pod-configmaps-1b6671b4-2cc9-4173-b214-2a3ac7d7428c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026857822s
STEP: Saw pod success
May 21 09:52:44.666: INFO: Pod "pod-configmaps-1b6671b4-2cc9-4173-b214-2a3ac7d7428c" satisfied condition "Succeeded or Failed"
May 21 09:52:44.671: INFO: Trying to get logs from node k8s-03 pod pod-configmaps-1b6671b4-2cc9-4173-b214-2a3ac7d7428c container agnhost-container: <nil>
STEP: delete the pod
May 21 09:52:44.947: INFO: Waiting for pod pod-configmaps-1b6671b4-2cc9-4173-b214-2a3ac7d7428c to disappear
May 21 09:52:44.952: INFO: Pod pod-configmaps-1b6671b4-2cc9-4173-b214-2a3ac7d7428c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:44.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9229" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":299,"skipped":5176,"failed":0}
SSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:44.968: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
May 21 09:52:45.096: INFO: created test-podtemplate-1
May 21 09:52:45.118: INFO: created test-podtemplate-2
May 21 09:52:45.123: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 21 09:52:45.128: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 21 09:52:45.225: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:45.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5738" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":300,"skipped":5182,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:45.247: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-b3c770dd-d6c8-4105-96bd-e4c8e8db29c4
STEP: Creating a pod to test consume secrets
May 21 09:52:45.538: INFO: Waiting up to 5m0s for pod "pod-secrets-db3e6d7d-28cf-41a0-b221-4b3a33d8d779" in namespace "secrets-5302" to be "Succeeded or Failed"
May 21 09:52:45.583: INFO: Pod "pod-secrets-db3e6d7d-28cf-41a0-b221-4b3a33d8d779": Phase="Pending", Reason="", readiness=false. Elapsed: 44.960529ms
May 21 09:52:47.593: INFO: Pod "pod-secrets-db3e6d7d-28cf-41a0-b221-4b3a33d8d779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055081946s
May 21 09:52:49.601: INFO: Pod "pod-secrets-db3e6d7d-28cf-41a0-b221-4b3a33d8d779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062558289s
STEP: Saw pod success
May 21 09:52:49.601: INFO: Pod "pod-secrets-db3e6d7d-28cf-41a0-b221-4b3a33d8d779" satisfied condition "Succeeded or Failed"
May 21 09:52:49.604: INFO: Trying to get logs from node k8s-03 pod pod-secrets-db3e6d7d-28cf-41a0-b221-4b3a33d8d779 container secret-volume-test: <nil>
STEP: delete the pod
May 21 09:52:49.662: INFO: Waiting for pod pod-secrets-db3e6d7d-28cf-41a0-b221-4b3a33d8d779 to disappear
May 21 09:52:49.674: INFO: Pod pod-secrets-db3e6d7d-28cf-41a0-b221-4b3a33d8d779 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:49.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5302" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":301,"skipped":5190,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:49.688: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 21 09:52:49.869: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:55.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9266" for this suite.

• [SLOW TEST:5.660 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":302,"skipped":5201,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:55.348: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4952
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4952
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4952
May 21 09:52:55.682: INFO: Found 0 stateful pods, waiting for 1
May 21 09:53:05.692: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 21 09:53:05.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-4952 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 09:53:06.199: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 09:53:06.199: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 09:53:06.199: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 09:53:06.207: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 21 09:53:16.218: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 09:53:16.218: INFO: Waiting for statefulset status.replicas updated to 0
May 21 09:53:16.255: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999363s
May 21 09:53:17.272: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.981254757s
May 21 09:53:18.300: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.963833587s
May 21 09:53:19.305: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.936342033s
May 21 09:53:20.317: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.931339006s
May 21 09:53:21.327: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.919541292s
May 21 09:53:22.341: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.908837526s
May 21 09:53:23.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.895528648s
May 21 09:53:24.358: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.885336446s
May 21 09:53:25.365: INFO: Verifying statefulset ss doesn't scale past 1 for another 877.956453ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4952
May 21 09:53:26.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-4952 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:53:29.202: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 09:53:29.202: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 09:53:29.202: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 09:53:29.250: INFO: Found 1 stateful pods, waiting for 3
May 21 09:53:39.258: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 09:53:39.258: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 09:53:39.258: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 21 09:53:39.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-4952 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 09:53:39.606: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 09:53:39.606: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 09:53:39.606: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 09:53:39.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-4952 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 09:53:40.191: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 09:53:40.191: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 09:53:40.192: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 09:53:40.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-4952 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 09:53:40.557: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 09:53:40.557: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 09:53:40.557: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 09:53:40.557: INFO: Waiting for statefulset status.replicas updated to 0
May 21 09:53:40.563: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 21 09:53:50.579: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 09:53:50.579: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 21 09:53:50.579: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 21 09:53:50.654: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999934s
May 21 09:53:51.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.939954257s
May 21 09:53:52.679: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.927815537s
May 21 09:53:53.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.914481135s
May 21 09:53:54.707: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.897268949s
May 21 09:53:55.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.886809619s
May 21 09:53:56.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.876600061s
May 21 09:53:57.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.852673581s
May 21 09:53:58.759: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.84207758s
May 21 09:53:59.770: INFO: Verifying statefulset ss doesn't scale past 3 for another 834.593201ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4952
May 21 09:54:00.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-4952 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:54:01.152: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 09:54:01.153: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 09:54:01.153: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 09:54:01.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-4952 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:54:01.487: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 09:54:01.487: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 09:54:01.487: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 09:54:01.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940571618 --namespace=statefulset-4952 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 09:54:01.837: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 09:54:01.837: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 09:54:01.837: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 09:54:01.837: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 09:54:21.914: INFO: Deleting all statefulset in ns statefulset-4952
May 21 09:54:21.918: INFO: Scaling statefulset ss to 0
May 21 09:54:21.936: INFO: Waiting for statefulset status.replicas updated to 0
May 21 09:54:21.940: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:22.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4952" for this suite.

• [SLOW TEST:86.714 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":303,"skipped":5208,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:22.062: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 21 09:54:22.202: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 21 09:54:22.241: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 21 09:54:22.241: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 21 09:54:22.273: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 21 09:54:22.273: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 21 09:54:22.374: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 21 09:54:22.374: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 21 09:54:29.627: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:29.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6562" for this suite.

• [SLOW TEST:7.766 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":304,"skipped":5226,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:29.828: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 21 09:54:30.125: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2109  c95dd1b8-5e9f-4bd0-aab1-8bd175283989 59037 0 2021-05-21 09:54:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-21 09:54:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 09:54:30.127: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2109  c95dd1b8-5e9f-4bd0-aab1-8bd175283989 59038 0 2021-05-21 09:54:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-21 09:54:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:30.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2109" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":305,"skipped":5230,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:30.148: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:55:30.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7994" for this suite.

• [SLOW TEST:60.209 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5239,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:55:30.357: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 21 09:55:30.722: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 09:56:30.798: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 21 09:56:30.925: INFO: Created pod: pod0-sched-preemption-low-priority
May 21 09:56:30.981: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:56:57.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6258" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:86.806 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":307,"skipped":5242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:56:57.165: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 09:56:57.300: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Creating first CR 
May 21 09:56:58.081: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T09:56:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T09:56:58Z]] name:name1 resourceVersion:59486 uid:29749af7-bb9f-4886-bcd8-0c67cc6b4832] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 21 09:57:08.093: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T09:57:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T09:57:08Z]] name:name2 resourceVersion:59543 uid:a51fb603-ed05-4cfe-9d05-e3d67a0343b9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 21 09:57:18.108: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T09:56:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T09:57:18Z]] name:name1 resourceVersion:59570 uid:29749af7-bb9f-4886-bcd8-0c67cc6b4832] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 21 09:57:28.123: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T09:57:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T09:57:28Z]] name:name2 resourceVersion:59590 uid:a51fb603-ed05-4cfe-9d05-e3d67a0343b9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 21 09:57:38.144: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T09:56:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T09:57:18Z]] name:name1 resourceVersion:59610 uid:29749af7-bb9f-4886-bcd8-0c67cc6b4832] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 21 09:57:48.160: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T09:57:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T09:57:28Z]] name:name2 resourceVersion:59632 uid:a51fb603-ed05-4cfe-9d05-e3d67a0343b9] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:57:58.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6034" for this suite.

• [SLOW TEST:61.536 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":308,"skipped":5325,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:57:58.701: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-hww4
STEP: Creating a pod to test atomic-volume-subpath
May 21 09:57:58.843: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hww4" in namespace "subpath-4778" to be "Succeeded or Failed"
May 21 09:57:58.849: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217218ms
May 21 09:58:00.860: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016561111s
May 21 09:58:02.873: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Running", Reason="", readiness=true. Elapsed: 4.029402179s
May 21 09:58:04.886: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Running", Reason="", readiness=true. Elapsed: 6.042932689s
May 21 09:58:06.902: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Running", Reason="", readiness=true. Elapsed: 8.058049527s
May 21 09:58:08.915: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Running", Reason="", readiness=true. Elapsed: 10.071090138s
May 21 09:58:10.928: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Running", Reason="", readiness=true. Elapsed: 12.084504565s
May 21 09:58:12.943: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Running", Reason="", readiness=true. Elapsed: 14.099877378s
May 21 09:58:14.959: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Running", Reason="", readiness=true. Elapsed: 16.115514377s
May 21 09:58:16.974: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Running", Reason="", readiness=true. Elapsed: 18.130582439s
May 21 09:58:18.982: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Running", Reason="", readiness=true. Elapsed: 20.138184844s
May 21 09:58:20.995: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Running", Reason="", readiness=true. Elapsed: 22.15142326s
May 21 09:58:23.006: INFO: Pod "pod-subpath-test-projected-hww4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.163000741s
STEP: Saw pod success
May 21 09:58:23.007: INFO: Pod "pod-subpath-test-projected-hww4" satisfied condition "Succeeded or Failed"
May 21 09:58:23.011: INFO: Trying to get logs from node k8s-03 pod pod-subpath-test-projected-hww4 container test-container-subpath-projected-hww4: <nil>
STEP: delete the pod
May 21 09:58:23.098: INFO: Waiting for pod pod-subpath-test-projected-hww4 to disappear
May 21 09:58:23.106: INFO: Pod pod-subpath-test-projected-hww4 no longer exists
STEP: Deleting pod pod-subpath-test-projected-hww4
May 21 09:58:23.106: INFO: Deleting pod "pod-subpath-test-projected-hww4" in namespace "subpath-4778"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:58:23.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4778" for this suite.

• [SLOW TEST:24.453 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":309,"skipped":5332,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:58:23.154: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 09:58:26.455: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:58:26.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1448" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":310,"skipped":5347,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:58:26.609: INFO: >>> kubeConfig: /tmp/kubeconfig-940571618
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:58:33.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5299" for this suite.
STEP: Destroying namespace "nsdeletetest-3423" for this suite.
May 21 09:58:33.346: INFO: Namespace nsdeletetest-3423 was already deleted
STEP: Destroying namespace "nsdeletetest-8376" for this suite.

• [SLOW TEST:6.746 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":311,"skipped":5350,"failed":0}
SSSSSSSMay 21 09:58:33.355: INFO: Running AfterSuite actions on all nodes
May 21 09:58:33.355: INFO: Running AfterSuite actions on node 1
May 21 09:58:33.355: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5357,"failed":0}

Ran 311 of 5668 Specs in 7050.591 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5357 Skipped
PASS

Ginkgo ran 1 suite in 1h57m33.101461259s
Test Suite Passed
