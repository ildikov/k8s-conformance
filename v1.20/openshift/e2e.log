May 11 17:36:26.704: INFO: The --provider flag is not set. Continuing as if --provider=skeleton had been used.
I0511 17:36:26.704601   44967 test_context.go:459] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0511 17:36:26.704725   44967 e2e.go:129] Starting e2e run "c7caf306-8b0a-4c51-9ab9-87d922491c1c" on Ginkgo node 1
{"msg":"Test Suite starting","total":20,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1620754586 - Will randomize all specs
Will run 20 of 5670 specs

May 11 17:36:26.724: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
May 11 17:36:27.111: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 11 17:36:27.318: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 11 17:36:27.318: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
May 11 17:36:27.318: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 11 17:36:27.386: INFO: e2e test version: v1.20.0-1066+75370d3fb99594
May 11 17:36:27.448: INFO: kube-apiserver version: v1.20.0-1066+75370d3fb99594-dirty
May 11 17:36:27.516: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-pred
May 11 17:36:27.780: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 11 17:36:27.907: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
May 11 17:36:28.045: INFO: Waiting for terminating namespaces to be deleted...
May 11 17:36:28.113: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-128-72.us-west-1.compute.internal before test
May 11 17:36:28.196: INFO: aws-ebs-csi-driver-node-d96jg from openshift-cluster-csi-drivers started at 2021-05-11 17:10:09 +0000 UTC (3 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:36:28.196: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:36:28.196: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:36:28.196: INFO: tuned-j6rbx from openshift-cluster-node-tuning-operator started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container tuned ready: true, restart count 0
May 11 17:36:28.196: INFO: dns-default-hxb5r from openshift-dns started at 2021-05-11 17:10:09 +0000 UTC (3 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container dns ready: true, restart count 0
May 11 17:36:28.196: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:36:28.196: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.196: INFO: node-ca-7m6gk from openshift-image-registry started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:36:28.196: INFO: ingress-canary-pwbw2 from openshift-ingress-canary started at 2021-05-11 17:11:18 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:36:28.196: INFO: machine-config-daemon-s2xtz from openshift-machine-config-operator started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:36:28.196: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:36:28.196: INFO: node-exporter-m2lc8 from openshift-monitoring started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.196: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:36:28.196: INFO: multus-wgjrz from openshift-multus started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:36:28.196: INFO: network-metrics-daemon-rn9fw from openshift-multus started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.196: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:36:28.196: INFO: network-check-target-9pvtg from openshift-network-diagnostics started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:36:28.196: INFO: sdn-24ltc from openshift-sdn started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.196: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.196: INFO: 	Container sdn ready: true, restart count 0
May 11 17:36:28.196: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-143-31.us-west-1.compute.internal before test
May 11 17:36:28.284: INFO: aws-ebs-csi-driver-node-7kf75 from openshift-cluster-csi-drivers started at 2021-05-11 17:08:22 +0000 UTC (3 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:36:28.284: INFO: tuned-6k8lw from openshift-cluster-node-tuning-operator started at 2021-05-11 17:08:22 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container tuned ready: true, restart count 0
May 11 17:36:28.284: INFO: dns-default-bxtmf from openshift-dns started at 2021-05-11 17:08:22 +0000 UTC (3 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container dns ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: image-registry-cc56678c5-2khxp from openshift-image-registry started at 2021-05-11 17:09:35 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container registry ready: true, restart count 0
May 11 17:36:28.284: INFO: image-registry-cc56678c5-kjc4z from openshift-image-registry started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container registry ready: true, restart count 0
May 11 17:36:28.284: INFO: node-ca-klps8 from openshift-image-registry started at 2021-05-11 17:08:23 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:36:28.284: INFO: ingress-canary-x6pbb from openshift-ingress-canary started at 2021-05-11 17:09:22 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:36:28.284: INFO: router-default-6c7bbf6754-ghh4f from openshift-ingress started at 2021-05-11 17:09:24 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container router ready: true, restart count 0
May 11 17:36:28.284: INFO: migrator-6d46dbf87b-vm9sc from openshift-kube-storage-version-migrator started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container migrator ready: true, restart count 0
May 11 17:36:28.284: INFO: machine-config-daemon-nrm9d from openshift-machine-config-operator started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: certified-operators-n29b4 from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:36:28.284: INFO: community-operators-kbpfk from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:36:28.284: INFO: redhat-marketplace-2c4w8 from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:36:28.284: INFO: redhat-operators-54x5r from openshift-marketplace started at 2021-05-11 17:09:28 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:36:28.284: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: kube-state-metrics-5fc7c95654-ft26b from openshift-monitoring started at 2021-05-11 17:09:24 +0000 UTC (3 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 11 17:36:28.284: INFO: node-exporter-ptf8w from openshift-monitoring started at 2021-05-11 17:08:23 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:36:28.284: INFO: openshift-state-metrics-5f4899f56-9kjp8 from openshift-monitoring started at 2021-05-11 17:09:23 +0000 UTC (3 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 11 17:36:28.284: INFO: prometheus-adapter-765cd8f8f6-6cwkh from openshift-monitoring started at 2021-05-11 17:09:22 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 11 17:36:28.284: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-05-11 17:10:35 +0000 UTC (7 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container prometheus ready: true, restart count 1
May 11 17:36:28.284: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 11 17:36:28.284: INFO: telemeter-client-bb5bd7f5b-mklzz from openshift-monitoring started at 2021-05-11 17:09:23 +0000 UTC (3 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container reload ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container telemeter-client ready: true, restart count 0
May 11 17:36:28.284: INFO: thanos-querier-8687487f9c-nrqbq from openshift-monitoring started at 2021-05-11 17:10:28 +0000 UTC (5 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container thanos-query ready: true, restart count 0
May 11 17:36:28.284: INFO: multus-n7bwv from openshift-multus started at 2021-05-11 17:08:22 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:36:28.284: INFO: network-metrics-daemon-ff8d9 from openshift-multus started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:36:28.284: INFO: network-check-source-5f86c94bf8-js6td from openshift-network-diagnostics started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container check-endpoints ready: true, restart count 0
May 11 17:36:28.284: INFO: network-check-target-8ksz6 from openshift-network-diagnostics started at 2021-05-11 17:08:23 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:36:28.284: INFO: sdn-8tgg6 from openshift-sdn started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.284: INFO: 	Container sdn ready: true, restart count 0
May 11 17:36:28.284: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-198-156.us-west-1.compute.internal before test
May 11 17:36:28.389: INFO: aws-ebs-csi-driver-node-h7x2b from openshift-cluster-csi-drivers started at 2021-05-11 17:08:49 +0000 UTC (3 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:36:28.389: INFO: tuned-b72ln from openshift-cluster-node-tuning-operator started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container tuned ready: true, restart count 0
May 11 17:36:28.389: INFO: dns-default-jwnl2 from openshift-dns started at 2021-05-11 17:08:49 +0000 UTC (3 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container dns ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: node-ca-xft27 from openshift-image-registry started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:36:28.389: INFO: ingress-canary-kd7ff from openshift-ingress-canary started at 2021-05-11 17:09:59 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:36:28.389: INFO: router-default-6c7bbf6754-2lbrv from openshift-ingress started at 2021-05-11 17:10:08 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container router ready: true, restart count 0
May 11 17:36:28.389: INFO: machine-config-daemon-zczzn from openshift-machine-config-operator started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: grafana-c59c57695-x2l2j from openshift-monitoring started at 2021-05-11 17:10:27 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container grafana ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container grafana-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: node-exporter-h5v6q from openshift-monitoring started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:36:28.389: INFO: prometheus-adapter-765cd8f8f6-n87s5 from openshift-monitoring started at 2021-05-11 17:10:34 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 11 17:36:28.389: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-05-11 17:10:35 +0000 UTC (7 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container prometheus ready: true, restart count 1
May 11 17:36:28.389: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 11 17:36:28.389: INFO: thanos-querier-8687487f9c-956tb from openshift-monitoring started at 2021-05-11 17:10:28 +0000 UTC (5 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container thanos-query ready: true, restart count 0
May 11 17:36:28.389: INFO: multus-bx695 from openshift-multus started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:36:28.389: INFO: network-metrics-daemon-2hds6 from openshift-multus started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:36:28.389: INFO: network-check-target-h9b4b from openshift-network-diagnostics started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:36:28.389: INFO: sdn-qnv76 from openshift-sdn started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:36:28.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:36:28.389: INFO: 	Container sdn ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-9097.167e13b5d87943c0], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.167e13b618f0ca3a], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity, 3 node(s) were unschedulable.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.167e13b6199de59f], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity, 3 node(s) were unschedulable.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:36:29.816: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-9097" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":20,"completed":1,"skipped":216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:36:30.728: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 11 17:36:30.860: INFO: Number of nodes with available pods: 0
May 11 17:36:30.860: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 11 17:36:31.188: INFO: Number of nodes with available pods: 0
May 11 17:36:31.188: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:32.253: INFO: Number of nodes with available pods: 0
May 11 17:36:32.253: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:33.256: INFO: Number of nodes with available pods: 0
May 11 17:36:33.256: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:34.252: INFO: Number of nodes with available pods: 0
May 11 17:36:34.253: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:35.253: INFO: Number of nodes with available pods: 0
May 11 17:36:35.253: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:36.253: INFO: Number of nodes with available pods: 0
May 11 17:36:36.253: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:37.256: INFO: Number of nodes with available pods: 0
May 11 17:36:37.256: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:38.253: INFO: Number of nodes with available pods: 0
May 11 17:36:38.253: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:39.253: INFO: Number of nodes with available pods: 0
May 11 17:36:39.253: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:40.253: INFO: Number of nodes with available pods: 0
May 11 17:36:40.253: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:41.256: INFO: Number of nodes with available pods: 0
May 11 17:36:41.256: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:42.301: INFO: Number of nodes with available pods: 0
May 11 17:36:42.301: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:43.253: INFO: Number of nodes with available pods: 1
May 11 17:36:43.253: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 11 17:36:43.640: INFO: Number of nodes with available pods: 0
May 11 17:36:43.640: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 11 17:36:43.779: INFO: Number of nodes with available pods: 0
May 11 17:36:43.779: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:44.844: INFO: Number of nodes with available pods: 0
May 11 17:36:44.844: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:45.847: INFO: Number of nodes with available pods: 0
May 11 17:36:45.847: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:46.843: INFO: Number of nodes with available pods: 0
May 11 17:36:46.843: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:47.843: INFO: Number of nodes with available pods: 0
May 11 17:36:47.843: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:48.844: INFO: Number of nodes with available pods: 0
May 11 17:36:48.844: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:49.846: INFO: Number of nodes with available pods: 0
May 11 17:36:49.847: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:50.843: INFO: Number of nodes with available pods: 0
May 11 17:36:50.843: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:51.843: INFO: Number of nodes with available pods: 0
May 11 17:36:51.843: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:52.847: INFO: Number of nodes with available pods: 0
May 11 17:36:52.847: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:53.844: INFO: Number of nodes with available pods: 0
May 11 17:36:53.844: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:36:54.846: INFO: Number of nodes with available pods: 1
May 11 17:36:54.846: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1758, will wait for the garbage collector to delete the pods
May 11 17:36:55.211: INFO: Deleting DaemonSet.extensions daemon-set took: 68.632758ms
May 11 17:36:55.311: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.172378ms
May 11 17:36:58.576: INFO: Number of nodes with available pods: 0
May 11 17:36:58.576: INFO: Number of running nodes: 0, number of available pods: 0
May 11 17:36:58.644: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1758/daemonsets","resourceVersion":"32682"},"items":null}

May 11 17:36:58.712: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1758/pods","resourceVersion":"32682"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:36:59.165: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-1758" for this suite.

• [SLOW TEST:29.352 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":20,"completed":2,"skipped":475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 11 17:36:59.894: INFO: Waiting up to 1m0s for all nodes to be ready
May 11 17:38:00.728: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 11 17:38:00.954: INFO: Created pod: pod0-sched-preemption-low-priority
May 11 17:38:01.104: INFO: Created pod: pod1-sched-preemption-medium-priority
May 11 17:38:01.249: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:38:28.121: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-preemption-5744" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:89.431 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":20,"completed":3,"skipped":903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 11 17:38:29.135: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
May 11 17:38:29.271: INFO: Waiting for terminating namespaces to be deleted...
May 11 17:38:29.347: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-128-72.us-west-1.compute.internal before test
May 11 17:38:29.429: INFO: aws-ebs-csi-driver-node-d96jg from openshift-cluster-csi-drivers started at 2021-05-11 17:10:09 +0000 UTC (3 container statuses recorded)
May 11 17:38:29.429: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:38:29.429: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:38:29.429: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:38:29.429: INFO: tuned-j6rbx from openshift-cluster-node-tuning-operator started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.429: INFO: 	Container tuned ready: true, restart count 0
May 11 17:38:29.429: INFO: dns-default-hxb5r from openshift-dns started at 2021-05-11 17:10:09 +0000 UTC (3 container statuses recorded)
May 11 17:38:29.429: INFO: 	Container dns ready: true, restart count 0
May 11 17:38:29.429: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:38:29.429: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.429: INFO: node-ca-7m6gk from openshift-image-registry started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.429: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:38:29.429: INFO: ingress-canary-pwbw2 from openshift-ingress-canary started at 2021-05-11 17:11:18 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.429: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:38:29.429: INFO: machine-config-daemon-s2xtz from openshift-machine-config-operator started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.429: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:38:29.429: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:38:29.429: INFO: certified-operators-mwnm6 from openshift-marketplace started at 2021-05-11 17:38:28 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.429: INFO: 	Container registry-server ready: false, restart count 0
May 11 17:38:29.429: INFO: node-exporter-m2lc8 from openshift-monitoring started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.429: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.429: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:38:29.429: INFO: multus-wgjrz from openshift-multus started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.430: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:38:29.430: INFO: network-metrics-daemon-rn9fw from openshift-multus started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.430: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.430: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:38:29.430: INFO: network-check-target-9pvtg from openshift-network-diagnostics started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.430: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:38:29.430: INFO: sdn-24ltc from openshift-sdn started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.430: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.430: INFO: 	Container sdn ready: true, restart count 0
May 11 17:38:29.430: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-143-31.us-west-1.compute.internal before test
May 11 17:38:29.515: INFO: aws-ebs-csi-driver-node-7kf75 from openshift-cluster-csi-drivers started at 2021-05-11 17:08:22 +0000 UTC (3 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:38:29.515: INFO: tuned-6k8lw from openshift-cluster-node-tuning-operator started at 2021-05-11 17:08:22 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container tuned ready: true, restart count 0
May 11 17:38:29.515: INFO: dns-default-bxtmf from openshift-dns started at 2021-05-11 17:08:22 +0000 UTC (3 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container dns ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: image-registry-cc56678c5-2khxp from openshift-image-registry started at 2021-05-11 17:09:35 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container registry ready: true, restart count 0
May 11 17:38:29.515: INFO: image-registry-cc56678c5-kjc4z from openshift-image-registry started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container registry ready: true, restart count 0
May 11 17:38:29.515: INFO: node-ca-klps8 from openshift-image-registry started at 2021-05-11 17:08:23 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:38:29.515: INFO: ingress-canary-x6pbb from openshift-ingress-canary started at 2021-05-11 17:09:22 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:38:29.515: INFO: router-default-6c7bbf6754-ghh4f from openshift-ingress started at 2021-05-11 17:09:24 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container router ready: true, restart count 0
May 11 17:38:29.515: INFO: migrator-6d46dbf87b-vm9sc from openshift-kube-storage-version-migrator started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container migrator ready: true, restart count 0
May 11 17:38:29.515: INFO: machine-config-daemon-nrm9d from openshift-machine-config-operator started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: certified-operators-n29b4 from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:38:29.515: INFO: community-operators-kbpfk from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:38:29.515: INFO: redhat-marketplace-2c4w8 from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:38:29.515: INFO: redhat-operators-54x5r from openshift-marketplace started at 2021-05-11 17:09:28 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:38:29.515: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: kube-state-metrics-5fc7c95654-ft26b from openshift-monitoring started at 2021-05-11 17:09:24 +0000 UTC (3 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 11 17:38:29.515: INFO: node-exporter-ptf8w from openshift-monitoring started at 2021-05-11 17:08:23 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:38:29.515: INFO: openshift-state-metrics-5f4899f56-9kjp8 from openshift-monitoring started at 2021-05-11 17:09:23 +0000 UTC (3 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 11 17:38:29.515: INFO: prometheus-adapter-765cd8f8f6-6cwkh from openshift-monitoring started at 2021-05-11 17:09:22 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 11 17:38:29.515: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-05-11 17:10:35 +0000 UTC (7 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container prometheus ready: true, restart count 1
May 11 17:38:29.515: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 11 17:38:29.515: INFO: telemeter-client-bb5bd7f5b-mklzz from openshift-monitoring started at 2021-05-11 17:09:23 +0000 UTC (3 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container reload ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container telemeter-client ready: true, restart count 0
May 11 17:38:29.515: INFO: thanos-querier-8687487f9c-nrqbq from openshift-monitoring started at 2021-05-11 17:10:28 +0000 UTC (5 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container thanos-query ready: true, restart count 0
May 11 17:38:29.515: INFO: multus-n7bwv from openshift-multus started at 2021-05-11 17:08:22 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:38:29.515: INFO: network-metrics-daemon-ff8d9 from openshift-multus started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:38:29.515: INFO: network-check-source-5f86c94bf8-js6td from openshift-network-diagnostics started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container check-endpoints ready: true, restart count 0
May 11 17:38:29.515: INFO: network-check-target-8ksz6 from openshift-network-diagnostics started at 2021-05-11 17:08:23 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:38:29.515: INFO: sdn-8tgg6 from openshift-sdn started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.515: INFO: 	Container sdn ready: true, restart count 0
May 11 17:38:29.515: INFO: pod1-sched-preemption-medium-priority from sched-preemption-5744 started at 2021-05-11 17:38:07 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.515: INFO: 	Container pod1-sched-preemption-medium-priority ready: true, restart count 0
May 11 17:38:29.515: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-198-156.us-west-1.compute.internal before test
May 11 17:38:29.600: INFO: aws-ebs-csi-driver-node-h7x2b from openshift-cluster-csi-drivers started at 2021-05-11 17:08:49 +0000 UTC (3 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:38:29.600: INFO: tuned-b72ln from openshift-cluster-node-tuning-operator started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container tuned ready: true, restart count 0
May 11 17:38:29.600: INFO: dns-default-jwnl2 from openshift-dns started at 2021-05-11 17:08:49 +0000 UTC (3 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container dns ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: node-ca-xft27 from openshift-image-registry started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:38:29.600: INFO: ingress-canary-kd7ff from openshift-ingress-canary started at 2021-05-11 17:09:59 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:38:29.600: INFO: router-default-6c7bbf6754-2lbrv from openshift-ingress started at 2021-05-11 17:10:08 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container router ready: true, restart count 0
May 11 17:38:29.600: INFO: machine-config-daemon-zczzn from openshift-machine-config-operator started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: grafana-c59c57695-x2l2j from openshift-monitoring started at 2021-05-11 17:10:27 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container grafana ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container grafana-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: node-exporter-h5v6q from openshift-monitoring started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:38:29.600: INFO: prometheus-adapter-765cd8f8f6-n87s5 from openshift-monitoring started at 2021-05-11 17:10:34 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 11 17:38:29.600: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-05-11 17:10:35 +0000 UTC (7 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container prometheus ready: true, restart count 1
May 11 17:38:29.600: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 11 17:38:29.600: INFO: thanos-querier-8687487f9c-956tb from openshift-monitoring started at 2021-05-11 17:10:28 +0000 UTC (5 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container thanos-query ready: true, restart count 0
May 11 17:38:29.600: INFO: multus-bx695 from openshift-multus started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:38:29.600: INFO: network-metrics-daemon-2hds6 from openshift-multus started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:38:29.600: INFO: network-check-target-h9b4b from openshift-network-diagnostics started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:38:29.600: INFO: sdn-qnv76 from openshift-sdn started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:38:29.600: INFO: 	Container sdn ready: true, restart count 0
May 11 17:38:29.600: INFO: pod2-sched-preemption-medium-priority from sched-preemption-5744 started at 2021-05-11 17:38:04 +0000 UTC (1 container statuses recorded)
May 11 17:38:29.600: INFO: 	Container pod2-sched-preemption-medium-priority ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3011eb42-ddc2-4840-81f9-cd707344dbc6 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.0.128.72 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.0.128.72 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 11 17:38:55.071: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.128.72 http://127.0.0.1:54321/hostname] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.128.72, port: 54321
May 11 17:38:55.580: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.128.72:54321/hostname] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.128.72, port: 54321 UDP
May 11 17:38:56.046: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.128.72 54321] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 11 17:39:01.519: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.128.72 http://127.0.0.1:54321/hostname] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.128.72, port: 54321
May 11 17:39:02.005: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.128.72:54321/hostname] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.128.72, port: 54321 UDP
May 11 17:39:02.494: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.128.72 54321] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 11 17:39:07.967: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.128.72 http://127.0.0.1:54321/hostname] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.128.72, port: 54321
May 11 17:39:08.449: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.128.72:54321/hostname] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.128.72, port: 54321 UDP
May 11 17:39:08.920: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.128.72 54321] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 11 17:39:14.387: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.128.72 http://127.0.0.1:54321/hostname] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.128.72, port: 54321
May 11 17:39:14.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.128.72:54321/hostname] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.128.72, port: 54321 UDP
May 11 17:39:15.328: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.128.72 54321] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 11 17:39:20.796: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.128.72 http://127.0.0.1:54321/hostname] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.128.72, port: 54321
May 11 17:39:21.274: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.128.72:54321/hostname] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.128.72, port: 54321 UDP
May 11 17:39:21.738: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.128.72 54321] Namespace:sched-pred-1991 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: removing the label kubernetes.io/e2e-3011eb42-ddc2-4840-81f9-cd707344dbc6 off the node ip-10-0-128-72.us-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3011eb42-ddc2-4840-81f9-cd707344dbc6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:39:27.416: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-1991" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:58.814 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":20,"completed":4,"skipped":1777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:39:35.742: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-2454" for this suite.
STEP: Destroying namespace "nsdeletetest-4845" for this suite.
May 11 17:39:36.073: INFO: Namespace nsdeletetest-4845 was already deleted
STEP: Destroying namespace "nsdeletetest-7134" for this suite.

• [SLOW TEST:8.520 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":20,"completed":5,"skipped":2121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 11 17:39:36.474: INFO: Waiting up to 1m0s for all nodes to be ready
May 11 17:40:37.275: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:40:37.344: INFO: Starting informer...
STEP: Starting pod...
May 11 17:40:37.502: INFO: Pod is running on ip-10-0-128-72.us-west-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 11 17:40:37.704: INFO: Pod wasn't evicted. Proceeding
May 11 17:40:37.704: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 11 17:41:52.906: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:41:52.906: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9799" for this suite.

• [SLOW TEST:137.025 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":20,"completed":6,"skipped":2524,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:41:53.831: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-189" for this suite.
STEP: Destroying namespace "nspatchtest-1ed71ab2-1a72-438e-af81-831f43829093-2313" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":20,"completed":7,"skipped":2542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 11 17:41:54.368: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
May 11 17:41:54.501: INFO: Waiting for terminating namespaces to be deleted...
May 11 17:41:54.569: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-128-72.us-west-1.compute.internal before test
May 11 17:41:54.651: INFO: aws-ebs-csi-driver-node-d96jg from openshift-cluster-csi-drivers started at 2021-05-11 17:10:09 +0000 UTC (3 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:41:54.651: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:41:54.651: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:41:54.651: INFO: tuned-j6rbx from openshift-cluster-node-tuning-operator started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container tuned ready: true, restart count 0
May 11 17:41:54.651: INFO: dns-default-hxb5r from openshift-dns started at 2021-05-11 17:10:09 +0000 UTC (3 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container dns ready: true, restart count 0
May 11 17:41:54.651: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:41:54.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.651: INFO: node-ca-7m6gk from openshift-image-registry started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:41:54.651: INFO: ingress-canary-9sn7q from openshift-ingress-canary started at 2021-05-11 17:40:48 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:41:54.651: INFO: machine-config-daemon-s2xtz from openshift-machine-config-operator started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:41:54.651: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:41:54.651: INFO: node-exporter-m2lc8 from openshift-monitoring started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.651: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:41:54.651: INFO: multus-wgjrz from openshift-multus started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:41:54.651: INFO: network-metrics-daemon-rn9fw from openshift-multus started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.651: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:41:54.651: INFO: network-check-target-9pvtg from openshift-network-diagnostics started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:41:54.651: INFO: sdn-24ltc from openshift-sdn started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.651: INFO: 	Container sdn ready: true, restart count 0
May 11 17:41:54.651: INFO: taint-eviction-4 from taint-single-pod-9799 started at 2021-05-11 17:40:37 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.651: INFO: 	Container pause ready: true, restart count 0
May 11 17:41:54.651: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-143-31.us-west-1.compute.internal before test
May 11 17:41:54.738: INFO: aws-ebs-csi-driver-node-7kf75 from openshift-cluster-csi-drivers started at 2021-05-11 17:08:22 +0000 UTC (3 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:41:54.738: INFO: tuned-6k8lw from openshift-cluster-node-tuning-operator started at 2021-05-11 17:08:22 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container tuned ready: true, restart count 0
May 11 17:41:54.738: INFO: dns-default-bxtmf from openshift-dns started at 2021-05-11 17:08:22 +0000 UTC (3 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container dns ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: image-registry-cc56678c5-2khxp from openshift-image-registry started at 2021-05-11 17:09:35 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container registry ready: true, restart count 0
May 11 17:41:54.738: INFO: image-registry-cc56678c5-kjc4z from openshift-image-registry started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container registry ready: true, restart count 0
May 11 17:41:54.738: INFO: node-ca-klps8 from openshift-image-registry started at 2021-05-11 17:08:23 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:41:54.738: INFO: ingress-canary-x6pbb from openshift-ingress-canary started at 2021-05-11 17:09:22 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:41:54.738: INFO: router-default-6c7bbf6754-ghh4f from openshift-ingress started at 2021-05-11 17:09:24 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container router ready: true, restart count 0
May 11 17:41:54.738: INFO: migrator-6d46dbf87b-vm9sc from openshift-kube-storage-version-migrator started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container migrator ready: true, restart count 0
May 11 17:41:54.738: INFO: machine-config-daemon-nrm9d from openshift-machine-config-operator started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: certified-operators-n29b4 from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:41:54.738: INFO: community-operators-kbpfk from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:41:54.738: INFO: redhat-marketplace-2c4w8 from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:41:54.738: INFO: redhat-operators-54x5r from openshift-marketplace started at 2021-05-11 17:09:28 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:41:54.738: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: kube-state-metrics-5fc7c95654-ft26b from openshift-monitoring started at 2021-05-11 17:09:24 +0000 UTC (3 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 11 17:41:54.738: INFO: node-exporter-ptf8w from openshift-monitoring started at 2021-05-11 17:08:23 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:41:54.738: INFO: openshift-state-metrics-5f4899f56-9kjp8 from openshift-monitoring started at 2021-05-11 17:09:23 +0000 UTC (3 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 11 17:41:54.738: INFO: prometheus-adapter-765cd8f8f6-6cwkh from openshift-monitoring started at 2021-05-11 17:09:22 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 11 17:41:54.738: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-05-11 17:10:35 +0000 UTC (7 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container prometheus ready: true, restart count 1
May 11 17:41:54.738: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 11 17:41:54.738: INFO: telemeter-client-bb5bd7f5b-mklzz from openshift-monitoring started at 2021-05-11 17:09:23 +0000 UTC (3 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container reload ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container telemeter-client ready: true, restart count 0
May 11 17:41:54.738: INFO: thanos-querier-8687487f9c-nrqbq from openshift-monitoring started at 2021-05-11 17:10:28 +0000 UTC (5 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container thanos-query ready: true, restart count 0
May 11 17:41:54.738: INFO: multus-n7bwv from openshift-multus started at 2021-05-11 17:08:22 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:41:54.738: INFO: network-metrics-daemon-ff8d9 from openshift-multus started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:41:54.738: INFO: network-check-source-5f86c94bf8-js6td from openshift-network-diagnostics started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container check-endpoints ready: true, restart count 0
May 11 17:41:54.738: INFO: network-check-target-8ksz6 from openshift-network-diagnostics started at 2021-05-11 17:08:23 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:41:54.738: INFO: sdn-8tgg6 from openshift-sdn started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.738: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.738: INFO: 	Container sdn ready: true, restart count 0
May 11 17:41:54.738: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-198-156.us-west-1.compute.internal before test
May 11 17:41:54.839: INFO: aws-ebs-csi-driver-node-h7x2b from openshift-cluster-csi-drivers started at 2021-05-11 17:08:49 +0000 UTC (3 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:41:54.839: INFO: tuned-b72ln from openshift-cluster-node-tuning-operator started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container tuned ready: true, restart count 0
May 11 17:41:54.839: INFO: dns-default-jwnl2 from openshift-dns started at 2021-05-11 17:08:49 +0000 UTC (3 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container dns ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: node-ca-xft27 from openshift-image-registry started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:41:54.839: INFO: ingress-canary-kd7ff from openshift-ingress-canary started at 2021-05-11 17:09:59 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:41:54.839: INFO: router-default-6c7bbf6754-2lbrv from openshift-ingress started at 2021-05-11 17:10:08 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container router ready: true, restart count 0
May 11 17:41:54.839: INFO: machine-config-daemon-zczzn from openshift-machine-config-operator started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: grafana-c59c57695-x2l2j from openshift-monitoring started at 2021-05-11 17:10:27 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container grafana ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container grafana-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: node-exporter-h5v6q from openshift-monitoring started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:41:54.839: INFO: prometheus-adapter-765cd8f8f6-n87s5 from openshift-monitoring started at 2021-05-11 17:10:34 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 11 17:41:54.839: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-05-11 17:10:35 +0000 UTC (7 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container prometheus ready: true, restart count 1
May 11 17:41:54.839: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 11 17:41:54.839: INFO: thanos-querier-8687487f9c-956tb from openshift-monitoring started at 2021-05-11 17:10:28 +0000 UTC (5 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container thanos-query ready: true, restart count 0
May 11 17:41:54.839: INFO: multus-bx695 from openshift-multus started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:41:54.839: INFO: network-metrics-daemon-2hds6 from openshift-multus started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:41:54.839: INFO: network-check-target-h9b4b from openshift-network-diagnostics started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:41:54.839: INFO: sdn-qnv76 from openshift-sdn started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:41:54.839: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:41:54.839: INFO: 	Container sdn ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node ip-10-0-128-72.us-west-1.compute.internal
STEP: verifying the node has the label node ip-10-0-143-31.us-west-1.compute.internal
STEP: verifying the node has the label node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod aws-ebs-csi-driver-node-7kf75 requesting resource cpu=30m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod aws-ebs-csi-driver-node-d96jg requesting resource cpu=30m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod aws-ebs-csi-driver-node-h7x2b requesting resource cpu=30m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod tuned-6k8lw requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod tuned-b72ln requesting resource cpu=10m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod tuned-j6rbx requesting resource cpu=10m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod dns-default-bxtmf requesting resource cpu=65m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod dns-default-hxb5r requesting resource cpu=65m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod dns-default-jwnl2 requesting resource cpu=65m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod image-registry-cc56678c5-2khxp requesting resource cpu=100m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod image-registry-cc56678c5-kjc4z requesting resource cpu=100m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod node-ca-7m6gk requesting resource cpu=10m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod node-ca-klps8 requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod node-ca-xft27 requesting resource cpu=10m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod ingress-canary-9sn7q requesting resource cpu=10m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod ingress-canary-kd7ff requesting resource cpu=10m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod ingress-canary-x6pbb requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod router-default-6c7bbf6754-2lbrv requesting resource cpu=100m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod router-default-6c7bbf6754-ghh4f requesting resource cpu=100m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod migrator-6d46dbf87b-vm9sc requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod machine-config-daemon-nrm9d requesting resource cpu=40m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod machine-config-daemon-s2xtz requesting resource cpu=40m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod machine-config-daemon-zczzn requesting resource cpu=40m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod certified-operators-n29b4 requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod community-operators-kbpfk requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod redhat-marketplace-2c4w8 requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod redhat-operators-54x5r requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod alertmanager-main-0 requesting resource cpu=8m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod alertmanager-main-1 requesting resource cpu=8m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod alertmanager-main-2 requesting resource cpu=8m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod grafana-c59c57695-x2l2j requesting resource cpu=5m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod kube-state-metrics-5fc7c95654-ft26b requesting resource cpu=4m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod node-exporter-h5v6q requesting resource cpu=9m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod node-exporter-m2lc8 requesting resource cpu=9m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod node-exporter-ptf8w requesting resource cpu=9m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod openshift-state-metrics-5f4899f56-9kjp8 requesting resource cpu=3m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod prometheus-adapter-765cd8f8f6-6cwkh requesting resource cpu=1m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod prometheus-adapter-765cd8f8f6-n87s5 requesting resource cpu=1m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod telemeter-client-bb5bd7f5b-mklzz requesting resource cpu=3m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod thanos-querier-8687487f9c-956tb requesting resource cpu=9m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod thanos-querier-8687487f9c-nrqbq requesting resource cpu=9m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod multus-bx695 requesting resource cpu=10m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod multus-n7bwv requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod multus-wgjrz requesting resource cpu=10m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod network-metrics-daemon-2hds6 requesting resource cpu=20m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod network-metrics-daemon-ff8d9 requesting resource cpu=20m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod network-metrics-daemon-rn9fw requesting resource cpu=20m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod network-check-source-5f86c94bf8-js6td requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod network-check-target-8ksz6 requesting resource cpu=10m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod network-check-target-9pvtg requesting resource cpu=10m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod network-check-target-h9b4b requesting resource cpu=10m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod sdn-24ltc requesting resource cpu=110m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod sdn-8tgg6 requesting resource cpu=110m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod sdn-qnv76 requesting resource cpu=110m on Node ip-10-0-198-156.us-west-1.compute.internal
May 11 17:41:55.521: INFO: Pod taint-eviction-4 requesting resource cpu=0m on Node ip-10-0-128-72.us-west-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
May 11 17:41:55.521: INFO: Creating a pod which consumes cpu=2223m on Node ip-10-0-128-72.us-west-1.compute.internal
May 11 17:41:55.621: INFO: Creating a pod which consumes cpu=1898m on Node ip-10-0-143-31.us-west-1.compute.internal
May 11 17:41:55.702: INFO: Creating a pod which consumes cpu=2078m on Node ip-10-0-198-156.us-west-1.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b680bef-fd3e-4528-9848-6a672ab10c7e.167e14023444283f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-135/filler-pod-4b680bef-fd3e-4528-9848-6a672ab10c7e to ip-10-0-128-72.us-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b680bef-fd3e-4528-9848-6a672ab10c7e.167e14029362c13d], Reason = [AddedInterface], Message = [Add eth0 [10.129.2.24/23]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b680bef-fd3e-4528-9848-6a672ab10c7e.167e1402aa8fbf82], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b680bef-fd3e-4528-9848-6a672ab10c7e.167e1402b36f388b], Reason = [Created], Message = [Created container filler-pod-4b680bef-fd3e-4528-9848-6a672ab10c7e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b680bef-fd3e-4528-9848-6a672ab10c7e.167e1402b4ec35e2], Reason = [Started], Message = [Started container filler-pod-4b680bef-fd3e-4528-9848-6a672ab10c7e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54e5e3b0-490a-4c6e-a531-8cf7263cda40.167e14023936bead], Reason = [Scheduled], Message = [Successfully assigned sched-pred-135/filler-pod-54e5e3b0-490a-4c6e-a531-8cf7263cda40 to ip-10-0-143-31.us-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54e5e3b0-490a-4c6e-a531-8cf7263cda40.167e1402ae4b963d], Reason = [AddedInterface], Message = [Add eth0 [10.131.0.27/23]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54e5e3b0-490a-4c6e-a531-8cf7263cda40.167e1402c62ef083], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54e5e3b0-490a-4c6e-a531-8cf7263cda40.167e1402cf821481], Reason = [Created], Message = [Created container filler-pod-54e5e3b0-490a-4c6e-a531-8cf7263cda40]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54e5e3b0-490a-4c6e-a531-8cf7263cda40.167e1402d14ef8a1], Reason = [Started], Message = [Started container filler-pod-54e5e3b0-490a-4c6e-a531-8cf7263cda40]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-588c2927-7830-482d-aad0-8ffa35071eb6.167e14023db63c92], Reason = [Scheduled], Message = [Successfully assigned sched-pred-135/filler-pod-588c2927-7830-482d-aad0-8ffa35071eb6 to ip-10-0-198-156.us-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-588c2927-7830-482d-aad0-8ffa35071eb6.167e1402bf0d5421], Reason = [AddedInterface], Message = [Add eth0 [10.128.2.14/23]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-588c2927-7830-482d-aad0-8ffa35071eb6.167e1402d7da7d70], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-588c2927-7830-482d-aad0-8ffa35071eb6.167e1402e2063233], Reason = [Created], Message = [Created container filler-pod-588c2927-7830-482d-aad0-8ffa35071eb6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-588c2927-7830-482d-aad0-8ffa35071eb6.167e1402e3ffa473], Reason = [Started], Message = [Started container filler-pod-588c2927-7830-482d-aad0-8ffa35071eb6]
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-135.167e1401db2a2a18], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.167e140347d0716e], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) were unschedulable.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.167e1403487b9e65], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) were unschedulable.]
STEP: removing the label node off the node ip-10-0-128-72.us-west-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-143-31.us-west-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-198-156.us-west-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:42:01.916: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-135" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:8.074 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":20,"completed":8,"skipped":2711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 11 17:42:02.444: INFO: Waiting up to 1m0s for all nodes to be ready
May 11 17:43:03.215: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:43:03.281: INFO: Starting informer...
STEP: Starting pods...
May 11 17:43:03.495: INFO: Pod1 is running on ip-10-0-128-72.us-west-1.compute.internal. Tainting Node
May 11 17:43:07.829: INFO: Pod2 is running on ip-10-0-128-72.us-west-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 11 17:43:15.184: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 11 17:43:38.350: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:43:38.553: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8258" for this suite.

• [SLOW TEST:96.644 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":20,"completed":9,"skipped":2910,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 11 17:43:39.086: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
May 11 17:43:39.221: INFO: Waiting for terminating namespaces to be deleted...
May 11 17:43:39.294: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-128-72.us-west-1.compute.internal before test
May 11 17:43:39.380: INFO: aws-ebs-csi-driver-node-d96jg from openshift-cluster-csi-drivers started at 2021-05-11 17:10:09 +0000 UTC (3 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:43:39.380: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:43:39.380: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:43:39.380: INFO: tuned-j6rbx from openshift-cluster-node-tuning-operator started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container tuned ready: true, restart count 0
May 11 17:43:39.380: INFO: dns-default-hxb5r from openshift-dns started at 2021-05-11 17:10:09 +0000 UTC (3 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container dns ready: true, restart count 0
May 11 17:43:39.380: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:43:39.380: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.380: INFO: node-ca-7m6gk from openshift-image-registry started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:43:39.380: INFO: ingress-canary-vspp4 from openshift-ingress-canary started at 2021-05-11 17:43:38 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container hello-openshift-canary ready: false, restart count 0
May 11 17:43:39.380: INFO: machine-config-daemon-s2xtz from openshift-machine-config-operator started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:43:39.380: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:43:39.380: INFO: node-exporter-m2lc8 from openshift-monitoring started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.380: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:43:39.380: INFO: multus-wgjrz from openshift-multus started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:43:39.380: INFO: network-metrics-daemon-rn9fw from openshift-multus started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.380: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:43:39.380: INFO: network-check-target-9pvtg from openshift-network-diagnostics started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:43:39.380: INFO: sdn-24ltc from openshift-sdn started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.380: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.380: INFO: 	Container sdn ready: true, restart count 0
May 11 17:43:39.380: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-143-31.us-west-1.compute.internal before test
May 11 17:43:39.502: INFO: aws-ebs-csi-driver-node-7kf75 from openshift-cluster-csi-drivers started at 2021-05-11 17:08:22 +0000 UTC (3 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:43:39.502: INFO: tuned-6k8lw from openshift-cluster-node-tuning-operator started at 2021-05-11 17:08:22 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container tuned ready: true, restart count 0
May 11 17:43:39.502: INFO: dns-default-bxtmf from openshift-dns started at 2021-05-11 17:08:22 +0000 UTC (3 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container dns ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: image-registry-cc56678c5-2khxp from openshift-image-registry started at 2021-05-11 17:09:35 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container registry ready: true, restart count 0
May 11 17:43:39.502: INFO: image-registry-cc56678c5-kjc4z from openshift-image-registry started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container registry ready: true, restart count 0
May 11 17:43:39.502: INFO: node-ca-klps8 from openshift-image-registry started at 2021-05-11 17:08:23 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:43:39.502: INFO: ingress-canary-x6pbb from openshift-ingress-canary started at 2021-05-11 17:09:22 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:43:39.502: INFO: router-default-6c7bbf6754-ghh4f from openshift-ingress started at 2021-05-11 17:09:24 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container router ready: true, restart count 0
May 11 17:43:39.502: INFO: migrator-6d46dbf87b-vm9sc from openshift-kube-storage-version-migrator started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container migrator ready: true, restart count 0
May 11 17:43:39.502: INFO: machine-config-daemon-nrm9d from openshift-machine-config-operator started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: certified-operators-n29b4 from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:43:39.502: INFO: community-operators-kbpfk from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:43:39.502: INFO: redhat-marketplace-2c4w8 from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:43:39.502: INFO: redhat-operators-54x5r from openshift-marketplace started at 2021-05-11 17:09:28 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:43:39.502: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: kube-state-metrics-5fc7c95654-ft26b from openshift-monitoring started at 2021-05-11 17:09:24 +0000 UTC (3 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 11 17:43:39.502: INFO: node-exporter-ptf8w from openshift-monitoring started at 2021-05-11 17:08:23 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:43:39.502: INFO: openshift-state-metrics-5f4899f56-9kjp8 from openshift-monitoring started at 2021-05-11 17:09:23 +0000 UTC (3 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 11 17:43:39.502: INFO: prometheus-adapter-765cd8f8f6-6cwkh from openshift-monitoring started at 2021-05-11 17:09:22 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 11 17:43:39.502: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-05-11 17:10:35 +0000 UTC (7 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container prometheus ready: true, restart count 1
May 11 17:43:39.502: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 11 17:43:39.502: INFO: telemeter-client-bb5bd7f5b-mklzz from openshift-monitoring started at 2021-05-11 17:09:23 +0000 UTC (3 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container reload ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container telemeter-client ready: true, restart count 0
May 11 17:43:39.502: INFO: thanos-querier-8687487f9c-nrqbq from openshift-monitoring started at 2021-05-11 17:10:28 +0000 UTC (5 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container thanos-query ready: true, restart count 0
May 11 17:43:39.502: INFO: multus-n7bwv from openshift-multus started at 2021-05-11 17:08:22 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:43:39.502: INFO: network-metrics-daemon-ff8d9 from openshift-multus started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:43:39.502: INFO: network-check-source-5f86c94bf8-js6td from openshift-network-diagnostics started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container check-endpoints ready: true, restart count 0
May 11 17:43:39.502: INFO: network-check-target-8ksz6 from openshift-network-diagnostics started at 2021-05-11 17:08:23 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:43:39.502: INFO: sdn-8tgg6 from openshift-sdn started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.502: INFO: 	Container sdn ready: true, restart count 0
May 11 17:43:39.502: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-198-156.us-west-1.compute.internal before test
May 11 17:43:39.590: INFO: aws-ebs-csi-driver-node-h7x2b from openshift-cluster-csi-drivers started at 2021-05-11 17:08:49 +0000 UTC (3 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:43:39.590: INFO: tuned-b72ln from openshift-cluster-node-tuning-operator started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container tuned ready: true, restart count 0
May 11 17:43:39.590: INFO: dns-default-jwnl2 from openshift-dns started at 2021-05-11 17:08:49 +0000 UTC (3 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container dns ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: node-ca-xft27 from openshift-image-registry started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:43:39.590: INFO: ingress-canary-kd7ff from openshift-ingress-canary started at 2021-05-11 17:09:59 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:43:39.590: INFO: router-default-6c7bbf6754-2lbrv from openshift-ingress started at 2021-05-11 17:10:08 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container router ready: true, restart count 0
May 11 17:43:39.590: INFO: machine-config-daemon-zczzn from openshift-machine-config-operator started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: grafana-c59c57695-x2l2j from openshift-monitoring started at 2021-05-11 17:10:27 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container grafana ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container grafana-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: node-exporter-h5v6q from openshift-monitoring started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:43:39.590: INFO: prometheus-adapter-765cd8f8f6-n87s5 from openshift-monitoring started at 2021-05-11 17:10:34 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 11 17:43:39.590: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-05-11 17:10:35 +0000 UTC (7 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container prometheus ready: true, restart count 1
May 11 17:43:39.590: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 11 17:43:39.590: INFO: thanos-querier-8687487f9c-956tb from openshift-monitoring started at 2021-05-11 17:10:28 +0000 UTC (5 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container thanos-query ready: true, restart count 0
May 11 17:43:39.590: INFO: multus-bx695 from openshift-multus started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:43:39.590: INFO: network-metrics-daemon-2hds6 from openshift-multus started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:43:39.590: INFO: network-check-target-h9b4b from openshift-network-diagnostics started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:43:39.590: INFO: sdn-qnv76 from openshift-sdn started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:43:39.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:43:39.590: INFO: 	Container sdn ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-58b56885-f264-4104-b755-d763cd0249aa 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.128.72 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-58b56885-f264-4104-b755-d763cd0249aa off the node ip-10-0-128-72.us-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-58b56885-f264-4104-b755-d763cd0249aa
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:48:48.857: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-3888" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:310.296 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":20,"completed":10,"skipped":3047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 11 17:48:52.947: INFO: Pod name wrapped-volume-race-196a9a96-1858-4fc3-a984-418338477687: Found 3 pods out of 5
May 11 17:48:58.182: INFO: Pod name wrapped-volume-race-196a9a96-1858-4fc3-a984-418338477687: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-196a9a96-1858-4fc3-a984-418338477687 in namespace emptydir-wrapper-7316, will wait for the garbage collector to delete the pods
May 11 17:48:58.747: INFO: Deleting ReplicationController wrapped-volume-race-196a9a96-1858-4fc3-a984-418338477687 took: 71.552727ms
May 11 17:48:58.847: INFO: Terminating ReplicationController wrapped-volume-race-196a9a96-1858-4fc3-a984-418338477687 pods took: 100.192181ms
STEP: Creating RC which spawns configmap-volume pods
May 11 17:49:09.057: INFO: Pod name wrapped-volume-race-87817947-4c3d-4558-9e03-2191ee5bd639: Found 3 pods out of 5
May 11 17:49:14.248: INFO: Pod name wrapped-volume-race-87817947-4c3d-4558-9e03-2191ee5bd639: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-87817947-4c3d-4558-9e03-2191ee5bd639 in namespace emptydir-wrapper-7316, will wait for the garbage collector to delete the pods
May 11 17:49:14.818: INFO: Deleting ReplicationController wrapped-volume-race-87817947-4c3d-4558-9e03-2191ee5bd639 took: 73.401108ms
May 11 17:49:14.918: INFO: Terminating ReplicationController wrapped-volume-race-87817947-4c3d-4558-9e03-2191ee5bd639 pods took: 100.161113ms
STEP: Creating RC which spawns configmap-volume pods
May 11 17:49:22.627: INFO: Pod name wrapped-volume-race-56f07879-cd06-4c80-acd9-d8f26436f3c2: Found 1 pods out of 5
May 11 17:49:27.819: INFO: Pod name wrapped-volume-race-56f07879-cd06-4c80-acd9-d8f26436f3c2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-56f07879-cd06-4c80-acd9-d8f26436f3c2 in namespace emptydir-wrapper-7316, will wait for the garbage collector to delete the pods
May 11 17:49:28.385: INFO: Deleting ReplicationController wrapped-volume-race-56f07879-cd06-4c80-acd9-d8f26436f3c2 took: 68.154996ms
May 11 17:49:28.485: INFO: Terminating ReplicationController wrapped-volume-race-56f07879-cd06-4c80-acd9-d8f26436f3c2 pods took: 100.175419ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:49:42.302: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7316" for this suite.

• [SLOW TEST:53.437 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":20,"completed":11,"skipped":3249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:49:43.284: INFO: Create a RollingUpdate DaemonSet
May 11 17:49:43.351: INFO: Check that daemon pods launch on every node of the cluster
May 11 17:49:43.419: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:43.419: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:43.419: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:43.487: INFO: Number of nodes with available pods: 0
May 11 17:49:43.487: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:44.682: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:44.682: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:44.682: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:44.746: INFO: Number of nodes with available pods: 0
May 11 17:49:44.746: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:45.676: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:45.676: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:45.677: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:45.741: INFO: Number of nodes with available pods: 0
May 11 17:49:45.741: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:46.677: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:46.677: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:46.677: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:46.742: INFO: Number of nodes with available pods: 1
May 11 17:49:46.743: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:47.677: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:47.677: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:47.677: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:47.746: INFO: Number of nodes with available pods: 1
May 11 17:49:47.746: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:48.682: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:48.682: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:48.682: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:48.746: INFO: Number of nodes with available pods: 1
May 11 17:49:48.746: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:49.677: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:49.677: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:49.677: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:49.741: INFO: Number of nodes with available pods: 1
May 11 17:49:49.741: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:50.677: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:50.677: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:50.677: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:50.742: INFO: Number of nodes with available pods: 1
May 11 17:49:50.742: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:51.677: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:51.677: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:51.677: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:51.745: INFO: Number of nodes with available pods: 1
May 11 17:49:51.745: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:52.681: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:52.681: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:52.682: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:52.746: INFO: Number of nodes with available pods: 1
May 11 17:49:52.746: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:53.677: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:53.677: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:53.677: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:53.742: INFO: Number of nodes with available pods: 2
May 11 17:49:53.742: INFO: Node ip-10-0-198-156.us-west-1.compute.internal is running more than one daemon pod
May 11 17:49:54.677: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:54.677: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:54.677: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:54.742: INFO: Number of nodes with available pods: 3
May 11 17:49:54.742: INFO: Number of running nodes: 3, number of available pods: 3
May 11 17:49:54.742: INFO: Update the DaemonSet to trigger a rollout
May 11 17:49:54.913: INFO: Updating DaemonSet daemon-set
May 11 17:49:57.237: INFO: Roll back the DaemonSet before rollout is complete
May 11 17:49:57.373: INFO: Updating DaemonSet daemon-set
May 11 17:49:57.373: INFO: Make sure DaemonSet rollback is complete
May 11 17:49:57.441: INFO: Wrong image for pod: daemon-set-jpr95. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 11 17:49:57.441: INFO: Pod daemon-set-jpr95 is not available
May 11 17:49:57.573: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:57.573: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:57.573: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:58.638: INFO: Wrong image for pod: daemon-set-jpr95. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 11 17:49:58.638: INFO: Pod daemon-set-jpr95 is not available
May 11 17:49:58.826: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:58.826: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:58.826: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:59.638: INFO: Pod daemon-set-79dnl is not available
May 11 17:49:59.806: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:49:59.806: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:49:59.806: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2208, will wait for the garbage collector to delete the pods
May 11 17:50:00.174: INFO: Deleting DaemonSet.extensions daemon-set took: 72.993327ms
May 11 17:50:00.274: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.168826ms
May 11 17:50:08.739: INFO: Number of nodes with available pods: 0
May 11 17:50:08.739: INFO: Number of running nodes: 0, number of available pods: 0
May 11 17:50:08.803: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2208/daemonsets","resourceVersion":"38268"},"items":null}

May 11 17:50:08.871: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2208/pods","resourceVersion":"38268"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:50:09.196: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-2208" for this suite.

• [SLOW TEST:26.901 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":20,"completed":12,"skipped":3285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 11 17:50:09.727: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
May 11 17:50:09.860: INFO: Waiting for terminating namespaces to be deleted...
May 11 17:50:09.933: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-128-72.us-west-1.compute.internal before test
May 11 17:50:10.017: INFO: aws-ebs-csi-driver-node-d96jg from openshift-cluster-csi-drivers started at 2021-05-11 17:10:09 +0000 UTC (3 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:50:10.018: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:50:10.018: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:50:10.018: INFO: tuned-j6rbx from openshift-cluster-node-tuning-operator started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container tuned ready: true, restart count 0
May 11 17:50:10.018: INFO: dns-default-hxb5r from openshift-dns started at 2021-05-11 17:10:09 +0000 UTC (3 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container dns ready: true, restart count 0
May 11 17:50:10.018: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:50:10.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.018: INFO: node-ca-7m6gk from openshift-image-registry started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:50:10.018: INFO: ingress-canary-vspp4 from openshift-ingress-canary started at 2021-05-11 17:43:38 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:50:10.018: INFO: machine-config-daemon-s2xtz from openshift-machine-config-operator started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:50:10.018: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:50:10.018: INFO: node-exporter-m2lc8 from openshift-monitoring started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.018: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:50:10.018: INFO: multus-wgjrz from openshift-multus started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:50:10.018: INFO: network-metrics-daemon-rn9fw from openshift-multus started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.018: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:50:10.018: INFO: network-check-target-9pvtg from openshift-network-diagnostics started at 2021-05-11 17:10:09 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:50:10.018: INFO: sdn-24ltc from openshift-sdn started at 2021-05-11 17:10:09 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.018: INFO: 	Container sdn ready: true, restart count 0
May 11 17:50:10.018: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-143-31.us-west-1.compute.internal before test
May 11 17:50:10.103: INFO: aws-ebs-csi-driver-node-7kf75 from openshift-cluster-csi-drivers started at 2021-05-11 17:08:22 +0000 UTC (3 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:50:10.103: INFO: tuned-6k8lw from openshift-cluster-node-tuning-operator started at 2021-05-11 17:08:22 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container tuned ready: true, restart count 0
May 11 17:50:10.103: INFO: dns-default-bxtmf from openshift-dns started at 2021-05-11 17:08:22 +0000 UTC (3 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container dns ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: image-registry-cc56678c5-2khxp from openshift-image-registry started at 2021-05-11 17:09:35 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container registry ready: true, restart count 0
May 11 17:50:10.103: INFO: image-registry-cc56678c5-kjc4z from openshift-image-registry started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container registry ready: true, restart count 0
May 11 17:50:10.103: INFO: node-ca-klps8 from openshift-image-registry started at 2021-05-11 17:08:23 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:50:10.103: INFO: ingress-canary-x6pbb from openshift-ingress-canary started at 2021-05-11 17:09:22 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:50:10.103: INFO: router-default-6c7bbf6754-ghh4f from openshift-ingress started at 2021-05-11 17:09:24 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container router ready: true, restart count 0
May 11 17:50:10.103: INFO: migrator-6d46dbf87b-vm9sc from openshift-kube-storage-version-migrator started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container migrator ready: true, restart count 0
May 11 17:50:10.103: INFO: machine-config-daemon-nrm9d from openshift-machine-config-operator started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: certified-operators-n29b4 from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:50:10.103: INFO: community-operators-kbpfk from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:50:10.103: INFO: redhat-marketplace-2c4w8 from openshift-marketplace started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:50:10.103: INFO: redhat-operators-54x5r from openshift-marketplace started at 2021-05-11 17:09:28 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container registry-server ready: true, restart count 0
May 11 17:50:10.103: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: kube-state-metrics-5fc7c95654-ft26b from openshift-monitoring started at 2021-05-11 17:09:24 +0000 UTC (3 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 11 17:50:10.103: INFO: node-exporter-ptf8w from openshift-monitoring started at 2021-05-11 17:08:23 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:50:10.103: INFO: openshift-state-metrics-5f4899f56-9kjp8 from openshift-monitoring started at 2021-05-11 17:09:23 +0000 UTC (3 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 11 17:50:10.103: INFO: prometheus-adapter-765cd8f8f6-6cwkh from openshift-monitoring started at 2021-05-11 17:09:22 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 11 17:50:10.103: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-05-11 17:10:35 +0000 UTC (7 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container prometheus ready: true, restart count 1
May 11 17:50:10.103: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 11 17:50:10.103: INFO: telemeter-client-bb5bd7f5b-mklzz from openshift-monitoring started at 2021-05-11 17:09:23 +0000 UTC (3 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container reload ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container telemeter-client ready: true, restart count 0
May 11 17:50:10.103: INFO: thanos-querier-8687487f9c-nrqbq from openshift-monitoring started at 2021-05-11 17:10:28 +0000 UTC (5 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container thanos-query ready: true, restart count 0
May 11 17:50:10.103: INFO: multus-n7bwv from openshift-multus started at 2021-05-11 17:08:22 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:50:10.103: INFO: network-metrics-daemon-ff8d9 from openshift-multus started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:50:10.103: INFO: network-check-source-5f86c94bf8-js6td from openshift-network-diagnostics started at 2021-05-11 17:09:25 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container check-endpoints ready: true, restart count 0
May 11 17:50:10.103: INFO: network-check-target-8ksz6 from openshift-network-diagnostics started at 2021-05-11 17:08:23 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:50:10.103: INFO: sdn-8tgg6 from openshift-sdn started at 2021-05-11 17:08:22 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.103: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.103: INFO: 	Container sdn ready: true, restart count 0
May 11 17:50:10.103: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-198-156.us-west-1.compute.internal before test
May 11 17:50:10.187: INFO: aws-ebs-csi-driver-node-h7x2b from openshift-cluster-csi-drivers started at 2021-05-11 17:08:49 +0000 UTC (3 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container csi-driver ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 11 17:50:10.187: INFO: tuned-b72ln from openshift-cluster-node-tuning-operator started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container tuned ready: true, restart count 0
May 11 17:50:10.187: INFO: dns-default-jwnl2 from openshift-dns started at 2021-05-11 17:08:49 +0000 UTC (3 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container dns ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: node-ca-xft27 from openshift-image-registry started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container node-ca ready: true, restart count 0
May 11 17:50:10.187: INFO: ingress-canary-kd7ff from openshift-ingress-canary started at 2021-05-11 17:09:59 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container hello-openshift-canary ready: true, restart count 0
May 11 17:50:10.187: INFO: router-default-6c7bbf6754-2lbrv from openshift-ingress started at 2021-05-11 17:10:08 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container router ready: true, restart count 0
May 11 17:50:10.187: INFO: machine-config-daemon-zczzn from openshift-machine-config-operator started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-05-11 17:10:26 +0000 UTC (5 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container alertmanager ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: grafana-c59c57695-x2l2j from openshift-monitoring started at 2021-05-11 17:10:27 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container grafana ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container grafana-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: node-exporter-h5v6q from openshift-monitoring started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container node-exporter ready: true, restart count 0
May 11 17:50:10.187: INFO: prometheus-adapter-765cd8f8f6-n87s5 from openshift-monitoring started at 2021-05-11 17:10:34 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 11 17:50:10.187: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-05-11 17:10:35 +0000 UTC (7 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container config-reloader ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container prometheus ready: true, restart count 1
May 11 17:50:10.187: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 11 17:50:10.187: INFO: thanos-querier-8687487f9c-956tb from openshift-monitoring started at 2021-05-11 17:10:28 +0000 UTC (5 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container oauth-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 11 17:50:10.187: INFO: 	Container thanos-query ready: true, restart count 0
May 11 17:50:10.187: INFO: multus-bx695 from openshift-multus started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.187: INFO: 	Container kube-multus ready: true, restart count 0
May 11 17:50:10.188: INFO: network-metrics-daemon-2hds6 from openshift-multus started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.188: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.188: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 11 17:50:10.188: INFO: network-check-target-h9b4b from openshift-network-diagnostics started at 2021-05-11 17:08:49 +0000 UTC (1 container statuses recorded)
May 11 17:50:10.188: INFO: 	Container network-check-target-container ready: true, restart count 0
May 11 17:50:10.188: INFO: sdn-qnv76 from openshift-sdn started at 2021-05-11 17:08:49 +0000 UTC (2 container statuses recorded)
May 11 17:50:10.188: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 11 17:50:10.188: INFO: 	Container sdn ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6ddf6979-cdc3-4705-8c47-dccb635fed4f 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-6ddf6979-cdc3-4705-8c47-dccb635fed4f off the node ip-10-0-128-72.us-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6ddf6979-cdc3-4705-8c47-dccb635fed4f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:50:19.283: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-1132" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:10.082 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":20,"completed":13,"skipped":4212,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 11 17:50:20.006: INFO: Waiting up to 1m0s for all nodes to be ready
May 11 17:51:20.839: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:51:21.433: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 11 17:51:21.499: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:51:21.830: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5920" for this suite.
[AfterEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:51:22.044: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-preemption-1275" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:63.121 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":20,"completed":14,"skipped":4226,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 11 17:51:23.128: INFO: Waiting up to 1m0s for all nodes to be ready
May 11 17:52:23.931: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 11 17:52:28.733: INFO: found a healthy node: ip-10-0-128-72.us-west-1.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:52:43.735: INFO: pods created so far: [1 1 1]
May 11 17:52:43.735: INFO: length of pods created so far: 3
May 11 17:52:59.882: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:53:06.882: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6402" for this suite.
[AfterEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:53:07.495: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-preemption-2486" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:105.440 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":20,"completed":15,"skipped":4280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 11 17:53:08.903: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:08.903: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:08.903: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:09.004: INFO: Number of nodes with available pods: 0
May 11 17:53:09.004: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:53:10.195: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:10.195: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:10.195: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:10.259: INFO: Number of nodes with available pods: 0
May 11 17:53:10.259: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:53:11.194: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:11.194: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:11.194: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:11.259: INFO: Number of nodes with available pods: 0
May 11 17:53:11.259: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:53:12.199: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:12.199: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:12.199: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:12.264: INFO: Number of nodes with available pods: 2
May 11 17:53:12.264: INFO: Node ip-10-0-198-156.us-west-1.compute.internal is running more than one daemon pod
May 11 17:53:13.195: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:13.195: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:13.195: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:13.259: INFO: Number of nodes with available pods: 3
May 11 17:53:13.259: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 11 17:53:13.529: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:13.529: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:13.529: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:13.594: INFO: Number of nodes with available pods: 2
May 11 17:53:13.594: INFO: Node ip-10-0-198-156.us-west-1.compute.internal is running more than one daemon pod
May 11 17:53:14.783: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:14.783: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:14.783: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:14.848: INFO: Number of nodes with available pods: 2
May 11 17:53:14.848: INFO: Node ip-10-0-198-156.us-west-1.compute.internal is running more than one daemon pod
May 11 17:53:15.789: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:15.789: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:15.789: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:15.853: INFO: Number of nodes with available pods: 2
May 11 17:53:15.853: INFO: Node ip-10-0-198-156.us-west-1.compute.internal is running more than one daemon pod
May 11 17:53:16.784: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:16.784: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:16.784: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:16.849: INFO: Number of nodes with available pods: 2
May 11 17:53:16.849: INFO: Node ip-10-0-198-156.us-west-1.compute.internal is running more than one daemon pod
May 11 17:53:17.784: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:17.784: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:17.784: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:17.852: INFO: Number of nodes with available pods: 2
May 11 17:53:17.852: INFO: Node ip-10-0-198-156.us-west-1.compute.internal is running more than one daemon pod
May 11 17:53:18.798: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:18.798: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:18.798: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:18.863: INFO: Number of nodes with available pods: 2
May 11 17:53:18.863: INFO: Node ip-10-0-198-156.us-west-1.compute.internal is running more than one daemon pod
May 11 17:53:19.789: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:53:19.790: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:19.790: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:53:19.858: INFO: Number of nodes with available pods: 3
May 11 17:53:19.858: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1531, will wait for the garbage collector to delete the pods
May 11 17:53:20.162: INFO: Deleting DaemonSet.extensions daemon-set took: 72.985503ms
May 11 17:53:20.263: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.5793ms
May 11 17:53:28.727: INFO: Number of nodes with available pods: 0
May 11 17:53:28.727: INFO: Number of running nodes: 0, number of available pods: 0
May 11 17:53:28.791: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1531/daemonsets","resourceVersion":"39909"},"items":null}

May 11 17:53:28.856: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1531/pods","resourceVersion":"39909"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:53:29.207: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-1531" for this suite.

• [SLOW TEST:21.355 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":20,"completed":16,"skipped":4665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 11 17:53:29.936: INFO: Waiting up to 1m0s for all nodes to be ready
May 11 17:54:30.773: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 11 17:54:31.005: INFO: Created pod: pod0-sched-preemption-low-priority
May 11 17:54:31.151: INFO: Created pod: pod1-sched-preemption-medium-priority
May 11 17:54:31.302: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:54:52.102: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-preemption-4708" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:83.379 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":20,"completed":17,"skipped":5050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:54:53.507: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 11 17:54:53.647: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:54:53.647: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:53.647: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:53.715: INFO: Number of nodes with available pods: 0
May 11 17:54:53.715: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:54:54.910: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:54:54.910: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:54.910: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:54.975: INFO: Number of nodes with available pods: 0
May 11 17:54:54.975: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:54:55.905: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:54:55.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:55.905: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:55.970: INFO: Number of nodes with available pods: 0
May 11 17:54:55.970: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:54:56.905: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:54:56.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:56.905: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:56.970: INFO: Number of nodes with available pods: 1
May 11 17:54:56.970: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:54:57.905: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:54:57.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:57.905: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:57.970: INFO: Number of nodes with available pods: 3
May 11 17:54:57.970: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 11 17:54:58.444: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:54:58.444: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:54:58.444: INFO: Wrong image for pod: daemon-set-hplv8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:54:58.511: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:54:58.511: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:58.511: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:59.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:54:59.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:54:59.576: INFO: Wrong image for pod: daemon-set-hplv8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:54:59.704: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:54:59.704: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:54:59.705: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:00.580: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:00.580: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:00.580: INFO: Wrong image for pod: daemon-set-hplv8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:00.580: INFO: Pod daemon-set-hplv8 is not available
May 11 17:55:00.748: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:00.748: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:00.748: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:01.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:01.576: INFO: Pod daemon-set-5t76f is not available
May 11 17:55:01.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:01.774: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:01.774: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:01.774: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:02.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:02.576: INFO: Pod daemon-set-5t76f is not available
May 11 17:55:02.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:02.765: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:02.766: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:02.766: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:03.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:03.576: INFO: Pod daemon-set-5t76f is not available
May 11 17:55:03.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:03.766: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:03.766: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:03.766: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:04.579: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:04.579: INFO: Pod daemon-set-5t76f is not available
May 11 17:55:04.579: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:04.769: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:04.769: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:04.769: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:05.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:05.576: INFO: Pod daemon-set-5t76f is not available
May 11 17:55:05.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:05.772: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:05.772: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:05.772: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:06.575: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:06.575: INFO: Pod daemon-set-5t76f is not available
May 11 17:55:06.575: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:06.765: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:06.765: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:06.765: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:07.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:07.576: INFO: Pod daemon-set-5t76f is not available
May 11 17:55:07.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:07.765: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:07.765: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:07.765: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:08.579: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:08.579: INFO: Pod daemon-set-5t76f is not available
May 11 17:55:08.579: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:08.768: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:08.768: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:08.768: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:09.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:09.576: INFO: Pod daemon-set-5t76f is not available
May 11 17:55:09.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:09.749: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:09.749: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:09.749: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:10.579: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:10.579: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:10.747: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:10.747: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:10.747: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:11.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:11.576: INFO: Pod daemon-set-2xjlg is not available
May 11 17:55:11.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:11.744: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:11.744: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:11.744: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:12.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:12.576: INFO: Pod daemon-set-2xjlg is not available
May 11 17:55:12.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:12.745: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:12.745: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:12.745: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:13.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:13.576: INFO: Pod daemon-set-2xjlg is not available
May 11 17:55:13.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:13.710: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:13.710: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:13.710: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:14.580: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:14.580: INFO: Pod daemon-set-2xjlg is not available
May 11 17:55:14.580: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:14.748: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:14.748: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:14.748: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:15.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:15.576: INFO: Pod daemon-set-2xjlg is not available
May 11 17:55:15.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:15.704: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:15.704: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:15.704: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:16.576: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:16.576: INFO: Pod daemon-set-2xjlg is not available
May 11 17:55:16.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:16.745: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:16.745: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:16.745: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:17.637: INFO: Wrong image for pod: daemon-set-2xjlg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:17.637: INFO: Pod daemon-set-2xjlg is not available
May 11 17:55:17.637: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:17.811: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:17.811: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:17.811: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:18.579: INFO: Pod daemon-set-7xhhr is not available
May 11 17:55:18.579: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:18.747: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:18.747: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:18.747: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:19.576: INFO: Pod daemon-set-7xhhr is not available
May 11 17:55:19.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:19.765: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:19.765: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:19.765: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:20.576: INFO: Pod daemon-set-7xhhr is not available
May 11 17:55:20.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:20.745: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:20.745: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:20.745: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:21.576: INFO: Pod daemon-set-7xhhr is not available
May 11 17:55:21.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:21.771: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:21.771: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:21.771: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:22.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:22.576: INFO: Pod daemon-set-ccjhr is not available
May 11 17:55:22.744: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:22.744: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:22.744: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:23.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:23.576: INFO: Pod daemon-set-ccjhr is not available
May 11 17:55:23.745: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:23.745: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:23.745: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:24.579: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:24.579: INFO: Pod daemon-set-ccjhr is not available
May 11 17:55:24.748: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:24.748: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:24.748: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:25.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:25.576: INFO: Pod daemon-set-ccjhr is not available
May 11 17:55:25.748: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:25.748: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:25.748: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:26.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:26.576: INFO: Pod daemon-set-ccjhr is not available
May 11 17:55:26.744: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:26.744: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:26.744: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:27.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:27.576: INFO: Pod daemon-set-ccjhr is not available
May 11 17:55:27.744: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:27.744: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:27.744: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:28.580: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:28.580: INFO: Pod daemon-set-ccjhr is not available
May 11 17:55:28.752: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:28.752: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:28.752: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:29.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:29.576: INFO: Pod daemon-set-ccjhr is not available
May 11 17:55:29.745: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:29.745: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:29.745: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:30.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:30.576: INFO: Pod daemon-set-ccjhr is not available
May 11 17:55:30.744: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:30.744: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:30.744: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:31.576: INFO: Wrong image for pod: daemon-set-ccjhr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 11 17:55:31.576: INFO: Pod daemon-set-ccjhr is not available
May 11 17:55:31.744: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:31.744: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:31.744: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:32.580: INFO: Pod daemon-set-d5bmw is not available
May 11 17:55:32.753: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:32.753: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:32.753: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 11 17:55:32.821: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:32.821: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:32.822: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:32.886: INFO: Number of nodes with available pods: 2
May 11 17:55:32.886: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:55:34.077: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:34.077: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:34.077: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:34.141: INFO: Number of nodes with available pods: 2
May 11 17:55:34.141: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:55:35.077: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:35.077: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:35.077: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:35.143: INFO: Number of nodes with available pods: 2
May 11 17:55:35.143: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:55:36.081: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:36.081: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:36.081: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:36.149: INFO: Number of nodes with available pods: 2
May 11 17:55:36.149: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:55:37.076: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:37.076: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:37.077: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:37.141: INFO: Number of nodes with available pods: 2
May 11 17:55:37.141: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:55:38.077: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:38.077: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:38.077: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:38.142: INFO: Number of nodes with available pods: 2
May 11 17:55:38.142: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:55:39.076: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:39.076: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:39.076: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:39.141: INFO: Number of nodes with available pods: 2
May 11 17:55:39.141: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:55:40.081: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:40.081: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:40.081: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:40.166: INFO: Number of nodes with available pods: 2
May 11 17:55:40.166: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:55:41.075: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:55:41.075: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:41.075: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:55:41.140: INFO: Number of nodes with available pods: 3
May 11 17:55:41.140: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2012, will wait for the garbage collector to delete the pods
May 11 17:55:41.700: INFO: Deleting DaemonSet.extensions daemon-set took: 69.677523ms
May 11 17:55:41.800: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.172968ms
May 11 17:55:48.764: INFO: Number of nodes with available pods: 0
May 11 17:55:48.764: INFO: Number of running nodes: 0, number of available pods: 0
May 11 17:55:48.831: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2012/daemonsets","resourceVersion":"40998"},"items":null}

May 11 17:55:48.899: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2012/pods","resourceVersion":"40999"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:55:49.224: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-2012" for this suite.

• [SLOW TEST:56.635 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":20,"completed":18,"skipped":5160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:56:23.704: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-4814" for this suite.
STEP: Destroying namespace "nsdeletetest-9046" for this suite.
May 11 17:56:24.025: INFO: Namespace nsdeletetest-9046 was already deleted
STEP: Destroying namespace "nsdeletetest-6518" for this suite.

• [SLOW TEST:34.671 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":20,"completed":19,"skipped":5499,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 11 17:56:24.957: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:56:24.957: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:24.957: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:25.021: INFO: Number of nodes with available pods: 0
May 11 17:56:25.021: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:56:26.213: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:56:26.213: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:26.213: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:26.277: INFO: Number of nodes with available pods: 0
May 11 17:56:26.278: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:56:27.211: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:56:27.211: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:27.211: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:27.280: INFO: Number of nodes with available pods: 0
May 11 17:56:27.280: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:56:28.211: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:56:28.211: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:28.211: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:28.277: INFO: Number of nodes with available pods: 1
May 11 17:56:28.277: INFO: Node ip-10-0-143-31.us-west-1.compute.internal is running more than one daemon pod
May 11 17:56:29.216: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:56:29.216: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:29.216: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:29.281: INFO: Number of nodes with available pods: 3
May 11 17:56:29.281: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 11 17:56:29.550: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:56:29.550: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:29.550: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:29.615: INFO: Number of nodes with available pods: 2
May 11 17:56:29.615: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:56:30.805: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:56:30.805: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:30.805: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:30.870: INFO: Number of nodes with available pods: 2
May 11 17:56:30.870: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:56:31.805: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:56:31.805: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:31.805: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:31.870: INFO: Number of nodes with available pods: 2
May 11 17:56:31.870: INFO: Node ip-10-0-128-72.us-west-1.compute.internal is running more than one daemon pod
May 11 17:56:32.809: INFO: DaemonSet pods can't tolerate node ip-10-0-136-157.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:41 +0000 UTC}], skip checking this node
May 11 17:56:32.809: INFO: DaemonSet pods can't tolerate node ip-10-0-159-13.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:32.809: INFO: DaemonSet pods can't tolerate node ip-10-0-252-104.us-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-05-11 17:33:42 +0000 UTC}], skip checking this node
May 11 17:56:32.877: INFO: Number of nodes with available pods: 3
May 11 17:56:32.877: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1328, will wait for the garbage collector to delete the pods
May 11 17:56:33.247: INFO: Deleting DaemonSet.extensions daemon-set took: 74.952401ms
May 11 17:56:33.347: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.158143ms
May 11 17:56:42.212: INFO: Number of nodes with available pods: 0
May 11 17:56:42.212: INFO: Number of running nodes: 0, number of available pods: 0
May 11 17:56:42.277: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1328/daemonsets","resourceVersion":"41622"},"items":null}

May 11 17:56:42.342: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1328/pods","resourceVersion":"41622"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:56:42.667: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-1328" for this suite.

• [SLOW TEST:18.772 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":20,"completed":20,"skipped":5596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSMay 11 17:56:42.868: INFO: Running AfterSuite actions on all nodes
May 11 17:56:42.868: INFO: Running AfterSuite actions on node 1
May 11 17:56:42.868: INFO: Dumping logs locally to: /logs/artifacts
May 11 17:56:42.868: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory
{"msg":"Test Suite completed","total":20,"completed":20,"skipped":5650,"failed":0}

Ran 20 of 5670 Specs in 1216.148 seconds
SUCCESS! -- 20 Passed | 0 Failed | 0 Pending | 5650 Skipped
PASS

Ginkgo ran 1 suite in 20m16.281335295s
Test Suite Passed
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1620755802 - Will randomize all specs
Will run 5670 specs

Running in parallel across 4 nodes

May 11 17:56:43.038: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
May 11 17:56:43.440: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 11 17:56:43.653: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 11 17:56:43.653: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
May 11 17:56:43.653: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 11 17:56:43.722: INFO: e2e test version: v1.20.0-1066+75370d3fb99594
May 11 17:56:43.786: INFO: kube-apiserver version: v1.20.0-1066+75370d3fb99594-dirty
May 11 17:56:43.853: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
May 11 17:56:43.999: INFO: Cluster IP family: ipv4


May 11 17:56:44.011: INFO: Cluster IP family: ipv4

SSSSSS
------------------------------
May 11 17:56:44.016: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
May 11 17:56:44.311: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-b3042139-5fa7-492a-a5c5-fb6bfec892c7
STEP: Creating a pod to test consume secrets
May 11 17:56:44.696: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-500daad9-bfff-45cd-808c-6e2499935b83" in namespace "projected-7134" to be "Succeeded or Failed"
May 11 17:56:44.765: INFO: Pod "pod-projected-secrets-500daad9-bfff-45cd-808c-6e2499935b83": Phase="Pending", Reason="", readiness=false. Elapsed: 68.324033ms
May 11 17:56:46.835: INFO: Pod "pod-projected-secrets-500daad9-bfff-45cd-808c-6e2499935b83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138568666s
May 11 17:56:48.903: INFO: Pod "pod-projected-secrets-500daad9-bfff-45cd-808c-6e2499935b83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.20695631s
STEP: Saw pod success
May 11 17:56:48.903: INFO: Pod "pod-projected-secrets-500daad9-bfff-45cd-808c-6e2499935b83" satisfied condition "Succeeded or Failed"
May 11 17:56:48.972: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-projected-secrets-500daad9-bfff-45cd-808c-6e2499935b83 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 11 17:56:49.137: INFO: Waiting for pod pod-projected-secrets-500daad9-bfff-45cd-808c-6e2499935b83 to disappear
May 11 17:56:49.204: INFO: Pod pod-projected-secrets-500daad9-bfff-45cd-808c-6e2499935b83 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:56:49.205: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7134" for this suite.


• [SLOW TEST:5.461 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":1,"skipped":0,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
May 11 17:56:44.141: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 11 17:56:44.269: INFO: namespace kubectl-6160
May 11 17:56:44.269: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-6160 create -f -'
May 11 17:56:45.551: INFO: stderr: ""
May 11 17:56:45.551: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 11 17:56:46.617: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 17:56:46.617: INFO: Found 0 / 1
May 11 17:56:47.624: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 17:56:47.624: INFO: Found 0 / 1
May 11 17:56:48.616: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 17:56:48.616: INFO: Found 0 / 1
May 11 17:56:49.618: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 17:56:49.618: INFO: Found 1 / 1
May 11 17:56:49.618: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 11 17:56:49.687: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 17:56:49.687: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 11 17:56:49.687: INFO: wait on agnhost-primary startup in kubectl-6160 
May 11 17:56:49.687: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-6160 logs agnhost-primary-2jxdt agnhost-primary'
May 11 17:56:50.018: INFO: stderr: ""
May 11 17:56:50.018: INFO: stdout: "Paused\n"
STEP: exposing RC
May 11 17:56:50.018: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-6160 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 11 17:56:50.356: INFO: stderr: ""
May 11 17:56:50.356: INFO: stdout: "service/rm2 exposed\n"
May 11 17:56:50.421: INFO: Service rm2 in namespace kubectl-6160 found.
STEP: exposing service
May 11 17:56:52.556: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-6160 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 11 17:56:52.899: INFO: stderr: ""
May 11 17:56:52.899: INFO: stdout: "service/rm3 exposed\n"
May 11 17:56:52.964: INFO: Service rm3 in namespace kubectl-6160 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:56:55.100: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6160" for this suite.


• [SLOW TEST:11.496 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":-1,"completed":1,"skipped":23,"failed":0}

SSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 11 17:56:58.846: INFO: Successfully updated pod "pod-update-activedeadlineseconds-0dca011e-ef9b-4746-875f-fded40863ded"
May 11 17:56:58.846: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-0dca011e-ef9b-4746-875f-fded40863ded" in namespace "pods-697" to be "terminated due to deadline exceeded"
May 11 17:56:58.914: INFO: Pod "pod-update-activedeadlineseconds-0dca011e-ef9b-4746-875f-fded40863ded": Phase="Running", Reason="", readiness=true. Elapsed: 68.100458ms
May 11 17:57:00.983: INFO: Pod "pod-update-activedeadlineseconds-0dca011e-ef9b-4746-875f-fded40863ded": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.137210915s
May 11 17:57:00.984: INFO: Pod "pod-update-activedeadlineseconds-0dca011e-ef9b-4746-875f-fded40863ded" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:00.984: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-697" for this suite.


• [SLOW TEST:11.750 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":-1,"completed":2,"skipped":45,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-probe
May 11 17:56:44.346: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-7bb46984-1585-4a3e-9b42-892cb6765a44 in namespace container-probe-8630
May 11 17:56:48.759: INFO: Started pod liveness-7bb46984-1585-4a3e-9b42-892cb6765a44 in namespace container-probe-8630
STEP: checking the pod's current state and verifying that restartCount is present
May 11 17:56:48.828: INFO: Initial restart count of pod liveness-7bb46984-1585-4a3e-9b42-892cb6765a44 is 0
May 11 17:57:05.427: INFO: Restart count of pod container-probe-8630/liveness-7bb46984-1585-4a3e-9b42-892cb6765a44 is now 1 (16.599542795s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:05.508: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-8630" for this suite.


• [SLOW TEST:21.725 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":-1,"completed":1,"skipped":60,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0511 17:57:06.102514   45012 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W0511 17:57:06.102566   45012 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0511 17:57:06.102572   45012 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0511 17:57:06.102577   45012 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 11 17:57:06.102: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:06.102: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-4373" for this suite.


• [SLOW TEST:10.994 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:57:06.133: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-1534 version'
May 11 17:57:06.390: INFO: stderr: ""
May 11 17:57:06.390: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20+\", GitVersion:\"v1.20.0-1066+75370d3fb99594\", GitCommit:\"75370d3fb99594d6f0263f3de0bd08237381b77d\", GitTreeState:\"clean\", BuildDate:\"2021-05-11T17:34:50Z\", GoVersion:\"go1.15.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20+\", GitVersion:\"v1.20.0-1066+75370d3fb99594-dirty\", GitCommit:\"75370d3fb99594d6f0263f3de0bd08237381b77d\", GitTreeState:\"dirty\", BuildDate:\"2021-05-11T15:42:23Z\", GoVersion:\"go1.15.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:06.390: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1534" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":-1,"completed":2,"skipped":121,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-34f6129e-ddc5-4d7d-9fe2-c73000846d98
STEP: Creating a pod to test consume secrets
May 11 17:57:02.050: INFO: Waiting up to 5m0s for pod "pod-secrets-285e8df5-2f2d-441f-8743-5e044e3253ec" in namespace "secrets-1137" to be "Succeeded or Failed"
May 11 17:57:02.120: INFO: Pod "pod-secrets-285e8df5-2f2d-441f-8743-5e044e3253ec": Phase="Pending", Reason="", readiness=false. Elapsed: 69.899956ms
May 11 17:57:04.189: INFO: Pod "pod-secrets-285e8df5-2f2d-441f-8743-5e044e3253ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138451324s
May 11 17:57:06.257: INFO: Pod "pod-secrets-285e8df5-2f2d-441f-8743-5e044e3253ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206721011s
May 11 17:57:08.326: INFO: Pod "pod-secrets-285e8df5-2f2d-441f-8743-5e044e3253ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.275245055s
STEP: Saw pod success
May 11 17:57:08.326: INFO: Pod "pod-secrets-285e8df5-2f2d-441f-8743-5e044e3253ec" satisfied condition "Succeeded or Failed"
May 11 17:57:08.394: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-secrets-285e8df5-2f2d-441f-8743-5e044e3253ec container secret-volume-test: <nil>
STEP: delete the pod
May 11 17:57:08.555: INFO: Waiting for pod pod-secrets-285e8df5-2f2d-441f-8743-5e044e3253ec to disappear
May 11 17:57:08.623: INFO: Pod pod-secrets-285e8df5-2f2d-441f-8743-5e044e3253ec no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:08.623: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-1137" for this suite.
STEP: Destroying namespace "secret-namespace-3792" for this suite.


• [SLOW TEST:7.698 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":-1,"completed":3,"skipped":64,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 11 17:57:08.063: INFO: starting watch
STEP: patching
STEP: updating
May 11 17:57:08.263: INFO: waiting for watch events with expected annotations
May 11 17:57:08.263: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:09.022: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "certificates-1268" for this suite.

•
------------------------------
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":-1,"completed":3,"skipped":148,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 11 17:57:09.718: INFO: Waiting up to 5m0s for pod "pod-29706d02-bcb6-43b2-8937-9f6ac5b7b069" in namespace "emptydir-2454" to be "Succeeded or Failed"
May 11 17:57:09.784: INFO: Pod "pod-29706d02-bcb6-43b2-8937-9f6ac5b7b069": Phase="Pending", Reason="", readiness=false. Elapsed: 65.105383ms
May 11 17:57:11.849: INFO: Pod "pod-29706d02-bcb6-43b2-8937-9f6ac5b7b069": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130248584s
May 11 17:57:13.914: INFO: Pod "pod-29706d02-bcb6-43b2-8937-9f6ac5b7b069": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.195578753s
STEP: Saw pod success
May 11 17:57:13.914: INFO: Pod "pod-29706d02-bcb6-43b2-8937-9f6ac5b7b069" satisfied condition "Succeeded or Failed"
May 11 17:57:13.979: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-29706d02-bcb6-43b2-8937-9f6ac5b7b069 container test-container: <nil>
STEP: delete the pod
May 11 17:57:14.130: INFO: Waiting for pod pod-29706d02-bcb6-43b2-8937-9f6ac5b7b069 to disappear
May 11 17:57:14.199: INFO: Pod pod-29706d02-bcb6-43b2-8937-9f6ac5b7b069 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:14.199: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-2454" for this suite.


• [SLOW TEST:5.164 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":4,"skipped":163,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 11 17:57:09.405: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4365 /api/v1/namespaces/dns-4365/pods/test-dns-nameservers 5fefa230-680e-457b-97ed-ddf60f7b7489 42477 0 2021-05-11 17:57:09 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-05-11 17:57:09 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kps4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kps4q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kps4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c32,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-d5658,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 17:57:09.479: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 11 17:57:11.549: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 11 17:57:13.548: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 11 17:57:13.548: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4365 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: Verifying customized DNS server is configured on pod...
May 11 17:57:14.035: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4365 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 17:57:14.512: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:14.600: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-4365" for this suite.


• [SLOW TEST:5.906 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":-1,"completed":4,"skipped":70,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:15.828: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-1534" for this suite.

•
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":-1,"completed":5,"skipped":97,"failed":0}

S
------------------------------
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:16.451: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "tables-45" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":-1,"completed":6,"skipped":98,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 11 17:57:20.372: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:20.522: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-6157" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":-1,"completed":7,"skipped":112,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:25.505: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-7644" for this suite.


• [SLOW TEST:5.032 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox Pod with hostAliases
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":8,"skipped":118,"failed":0}

SS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8558
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8558
I0511 17:57:26.472821   45011 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8558, replica count: 2
I0511 17:57:29.573090   45011 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 17:57:29.573: INFO: Creating new exec pod
May 11 17:57:34.992: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-8558 exec execpod64rbc -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 11 17:57:35.749: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 11 17:57:35.749: INFO: stdout: ""
May 11 17:57:35.749: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-8558 exec execpod64rbc -- /bin/sh -x -c nc -zv -t -w 2 172.30.107.240 80'
May 11 17:57:36.479: INFO: stderr: "+ nc -zv -t -w 2 172.30.107.240 80\nConnection to 172.30.107.240 80 port [tcp/http] succeeded!\n"
May 11 17:57:36.479: INFO: stdout: ""
May 11 17:57:36.479: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-8558 exec execpod64rbc -- /bin/sh -x -c nc -zv -t -w 2 10.0.128.72 31022'
May 11 17:57:37.209: INFO: stderr: "+ nc -zv -t -w 2 10.0.128.72 31022\nConnection to 10.0.128.72 31022 port [tcp/31022] succeeded!\n"
May 11 17:57:37.209: INFO: stdout: ""
May 11 17:57:37.209: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-8558 exec execpod64rbc -- /bin/sh -x -c nc -zv -t -w 2 10.0.143.31 31022'
May 11 17:57:37.959: INFO: stderr: "+ nc -zv -t -w 2 10.0.143.31 31022\nConnection to 10.0.143.31 31022 port [tcp/31022] succeeded!\n"
May 11 17:57:37.959: INFO: stdout: ""
May 11 17:57:37.959: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:38.068: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-8558" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:12.505 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":-1,"completed":9,"skipped":120,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:39.593: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-5951" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

•
------------------------------
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":-1,"completed":10,"skipped":138,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 11 17:57:40.162: INFO: Waiting up to 5m0s for pod "pod-7600faee-5bc4-4015-adcf-4835ff18b30d" in namespace "emptydir-9209" to be "Succeeded or Failed"
May 11 17:57:40.233: INFO: Pod "pod-7600faee-5bc4-4015-adcf-4835ff18b30d": Phase="Pending", Reason="", readiness=false. Elapsed: 70.636145ms
May 11 17:57:42.301: INFO: Pod "pod-7600faee-5bc4-4015-adcf-4835ff18b30d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139158349s
May 11 17:57:44.370: INFO: Pod "pod-7600faee-5bc4-4015-adcf-4835ff18b30d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.207304014s
STEP: Saw pod success
May 11 17:57:44.370: INFO: Pod "pod-7600faee-5bc4-4015-adcf-4835ff18b30d" satisfied condition "Succeeded or Failed"
May 11 17:57:44.437: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-7600faee-5bc4-4015-adcf-4835ff18b30d container test-container: <nil>
STEP: delete the pod
May 11 17:57:44.591: INFO: Waiting for pod pod-7600faee-5bc4-4015-adcf-4835ff18b30d to disappear
May 11 17:57:44.660: INFO: Pod pod-7600faee-5bc4-4015-adcf-4835ff18b30d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:44.660: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-9209" for this suite.


• [SLOW TEST:5.188 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":11,"skipped":140,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:57:45.360: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-f929a452-65aa-4ade-aae0-03a6b8eab599
STEP: Creating secret with name s-test-opt-upd-a6c59002-4688-4670-ab33-9e30d4021176
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f929a452-65aa-4ade-aae0-03a6b8eab599
STEP: Updating secret s-test-opt-upd-a6c59002-4688-4670-ab33-9e30d4021176
STEP: Creating secret with name s-test-opt-create-b6934d31-a816-445d-ad8f-5b3b4c6d9d90
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:57:54.614: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-3216" for this suite.


• [SLOW TEST:9.941 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":168,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-publish-openapi
May 11 17:56:44.267: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 11 17:56:55.903: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:58:13.569: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8552" for this suite.


• [SLOW TEST:89.838 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":-1,"completed":1,"skipped":0,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 11 17:58:14.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2777 /api/v1/namespaces/watch-2777/configmaps/e2e-watch-test-watch-closed 473f81ce-a313-4124-bb29-7538dc4b8596 43976 0 2021-05-11 17:58:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-11 17:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 17:58:14.469: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2777 /api/v1/namespaces/watch-2777/configmaps/e2e-watch-test-watch-closed 473f81ce-a313-4124-bb29-7538dc4b8596 43977 0 2021-05-11 17:58:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-11 17:58:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 11 17:58:14.742: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2777 /api/v1/namespaces/watch-2777/configmaps/e2e-watch-test-watch-closed 473f81ce-a313-4124-bb29-7538dc4b8596 43978 0 2021-05-11 17:58:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-11 17:58:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 17:58:14.742: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2777 /api/v1/namespaces/watch-2777/configmaps/e2e-watch-test-watch-closed 473f81ce-a313-4124-bb29-7538dc4b8596 43979 0 2021-05-11 17:58:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-11 17:58:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:58:14.742: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-2777" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":-1,"completed":2,"skipped":11,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-ab8e21b0-7215-47f3-b336-1a23699382ae
STEP: Creating a pod to test consume secrets
May 11 17:58:15.411: INFO: Waiting up to 5m0s for pod "pod-secrets-02e52f58-64b4-45cf-935d-414ae1a282e7" in namespace "secrets-163" to be "Succeeded or Failed"
May 11 17:58:15.481: INFO: Pod "pod-secrets-02e52f58-64b4-45cf-935d-414ae1a282e7": Phase="Pending", Reason="", readiness=false. Elapsed: 69.890963ms
May 11 17:58:17.548: INFO: Pod "pod-secrets-02e52f58-64b4-45cf-935d-414ae1a282e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136842929s
May 11 17:58:19.615: INFO: Pod "pod-secrets-02e52f58-64b4-45cf-935d-414ae1a282e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.203550336s
May 11 17:58:21.682: INFO: Pod "pod-secrets-02e52f58-64b4-45cf-935d-414ae1a282e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.270808405s
May 11 17:58:23.753: INFO: Pod "pod-secrets-02e52f58-64b4-45cf-935d-414ae1a282e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.341355595s
STEP: Saw pod success
May 11 17:58:23.753: INFO: Pod "pod-secrets-02e52f58-64b4-45cf-935d-414ae1a282e7" satisfied condition "Succeeded or Failed"
May 11 17:58:23.819: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-secrets-02e52f58-64b4-45cf-935d-414ae1a282e7 container secret-env-test: <nil>
STEP: delete the pod
May 11 17:58:23.980: INFO: Waiting for pod pod-secrets-02e52f58-64b4-45cf-935d-414ae1a282e7 to disappear
May 11 17:58:24.047: INFO: Pod pod-secrets-02e52f58-64b4-45cf-935d-414ae1a282e7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:58:24.047: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-163" for this suite.


• [SLOW TEST:9.392 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":-1,"completed":3,"skipped":69,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 17:58:24.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16f1b91d-5218-4b8d-b44c-3e387b503fba" in namespace "downward-api-6731" to be "Succeeded or Failed"
May 11 17:58:24.807: INFO: Pod "downwardapi-volume-16f1b91d-5218-4b8d-b44c-3e387b503fba": Phase="Pending", Reason="", readiness=false. Elapsed: 66.423735ms
May 11 17:58:26.874: INFO: Pod "downwardapi-volume-16f1b91d-5218-4b8d-b44c-3e387b503fba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133740152s
May 11 17:58:28.941: INFO: Pod "downwardapi-volume-16f1b91d-5218-4b8d-b44c-3e387b503fba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.200579414s
STEP: Saw pod success
May 11 17:58:28.941: INFO: Pod "downwardapi-volume-16f1b91d-5218-4b8d-b44c-3e387b503fba" satisfied condition "Succeeded or Failed"
May 11 17:58:29.008: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downwardapi-volume-16f1b91d-5218-4b8d-b44c-3e387b503fba container client-container: <nil>
STEP: delete the pod
May 11 17:58:29.195: INFO: Waiting for pod downwardapi-volume-16f1b91d-5218-4b8d-b44c-3e387b503fba to disappear
May 11 17:58:29.262: INFO: Pod downwardapi-volume-16f1b91d-5218-4b8d-b44c-3e387b503fba no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:58:29.262: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6731" for this suite.


• [SLOW TEST:5.214 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":4,"skipped":75,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 11 17:57:25.860: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:58:39.681: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7610" for this suite.


• [SLOW TEST:85.471 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":-1,"completed":5,"skipped":181,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-7976/configmap-test-4ddd930e-e96b-438c-af2a-1b2402ae1ec2
STEP: Creating a pod to test consume configMaps
May 11 17:58:40.435: INFO: Waiting up to 5m0s for pod "pod-configmaps-550d81eb-260f-4f69-b49d-cacd42dd999f" in namespace "configmap-7976" to be "Succeeded or Failed"
May 11 17:58:40.501: INFO: Pod "pod-configmaps-550d81eb-260f-4f69-b49d-cacd42dd999f": Phase="Pending", Reason="", readiness=false. Elapsed: 65.955546ms
May 11 17:58:42.567: INFO: Pod "pod-configmaps-550d81eb-260f-4f69-b49d-cacd42dd999f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13257186s
May 11 17:58:44.633: INFO: Pod "pod-configmaps-550d81eb-260f-4f69-b49d-cacd42dd999f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.19822167s
STEP: Saw pod success
May 11 17:58:44.633: INFO: Pod "pod-configmaps-550d81eb-260f-4f69-b49d-cacd42dd999f" satisfied condition "Succeeded or Failed"
May 11 17:58:44.699: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-configmaps-550d81eb-260f-4f69-b49d-cacd42dd999f container env-test: <nil>
STEP: delete the pod
May 11 17:58:44.849: INFO: Waiting for pod pod-configmaps-550d81eb-260f-4f69-b49d-cacd42dd999f to disappear
May 11 17:58:44.914: INFO: Pod pod-configmaps-550d81eb-260f-4f69-b49d-cacd42dd999f no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:58:44.914: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-7976" for this suite.


• [SLOW TEST:5.230 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":-1,"completed":6,"skipped":200,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 11 17:58:45.613: INFO: Waiting up to 5m0s for pod "downward-api-a6158ad1-f01f-4bf3-b84e-64b5103901dd" in namespace "downward-api-7829" to be "Succeeded or Failed"
May 11 17:58:45.679: INFO: Pod "downward-api-a6158ad1-f01f-4bf3-b84e-64b5103901dd": Phase="Pending", Reason="", readiness=false. Elapsed: 65.238346ms
May 11 17:58:47.744: INFO: Pod "downward-api-a6158ad1-f01f-4bf3-b84e-64b5103901dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130939646s
May 11 17:58:49.810: INFO: Pod "downward-api-a6158ad1-f01f-4bf3-b84e-64b5103901dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.196578345s
STEP: Saw pod success
May 11 17:58:49.810: INFO: Pod "downward-api-a6158ad1-f01f-4bf3-b84e-64b5103901dd" satisfied condition "Succeeded or Failed"
May 11 17:58:49.879: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downward-api-a6158ad1-f01f-4bf3-b84e-64b5103901dd container dapi-container: <nil>
STEP: delete the pod
May 11 17:58:50.029: INFO: Waiting for pod downward-api-a6158ad1-f01f-4bf3-b84e-64b5103901dd to disappear
May 11 17:58:50.094: INFO: Pod downward-api-a6158ad1-f01f-4bf3-b84e-64b5103901dd no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:58:50.094: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-7829" for this suite.


• [SLOW TEST:5.157 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":-1,"completed":7,"skipped":214,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-780a674a-eccb-4dce-8a16-5b40aefd56f8
STEP: Creating a pod to test consume secrets
May 11 17:58:50.848: INFO: Waiting up to 5m0s for pod "pod-secrets-ac169584-d2dd-4a95-9d91-6258aea3af7d" in namespace "secrets-8606" to be "Succeeded or Failed"
May 11 17:58:50.913: INFO: Pod "pod-secrets-ac169584-d2dd-4a95-9d91-6258aea3af7d": Phase="Pending", Reason="", readiness=false. Elapsed: 65.114283ms
May 11 17:58:52.982: INFO: Pod "pod-secrets-ac169584-d2dd-4a95-9d91-6258aea3af7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133980675s
May 11 17:58:55.047: INFO: Pod "pod-secrets-ac169584-d2dd-4a95-9d91-6258aea3af7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.199649161s
STEP: Saw pod success
May 11 17:58:55.047: INFO: Pod "pod-secrets-ac169584-d2dd-4a95-9d91-6258aea3af7d" satisfied condition "Succeeded or Failed"
May 11 17:58:55.113: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-secrets-ac169584-d2dd-4a95-9d91-6258aea3af7d container secret-volume-test: <nil>
STEP: delete the pod
May 11 17:58:55.271: INFO: Waiting for pod pod-secrets-ac169584-d2dd-4a95-9d91-6258aea3af7d to disappear
May 11 17:58:55.340: INFO: Pod pod-secrets-ac169584-d2dd-4a95-9d91-6258aea3af7d no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:58:55.340: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-8606" for this suite.


• [SLOW TEST:5.246 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":8,"skipped":219,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-280
May 11 17:57:57.529: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-280 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 11 17:57:58.266: INFO: rc: 7
May 11 17:57:58.347: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 11 17:57:58.415: INFO: Pod kube-proxy-mode-detector no longer exists
May 11 17:57:58.415: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-280 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-280
STEP: creating replication controller affinity-clusterip-timeout in namespace services-280
I0511 17:57:58.573774   45011 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-280, replica count: 3
I0511 17:58:01.674073   45011 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 17:58:04.674256   45011 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 17:58:04.824: INFO: Creating new exec pod
May 11 17:58:10.107: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-280 exec execpod-affinity74wb9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
May 11 17:58:10.847: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 11 17:58:10.847: INFO: stdout: ""
May 11 17:58:10.847: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-280 exec execpod-affinity74wb9 -- /bin/sh -x -c nc -zv -t -w 2 172.30.180.161 80'
May 11 17:58:11.593: INFO: stderr: "+ nc -zv -t -w 2 172.30.180.161 80\nConnection to 172.30.180.161 80 port [tcp/http] succeeded!\n"
May 11 17:58:11.593: INFO: stdout: ""
May 11 17:58:11.593: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-280 exec execpod-affinity74wb9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.180.161:80/ ; done'
May 11 17:58:12.388: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n"
May 11 17:58:12.388: INFO: stdout: "\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz\naffinity-clusterip-timeout-d9wlz"
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Received response from host: affinity-clusterip-timeout-d9wlz
May 11 17:58:12.388: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-280 exec execpod-affinity74wb9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.180.161:80/'
May 11 17:58:13.130: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n"
May 11 17:58:13.130: INFO: stdout: "affinity-clusterip-timeout-d9wlz"
May 11 17:58:33.130: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-280 exec execpod-affinity74wb9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.180.161:80/'
May 11 17:58:33.876: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n"
May 11 17:58:33.876: INFO: stdout: "affinity-clusterip-timeout-d9wlz"
May 11 17:58:53.877: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-280 exec execpod-affinity74wb9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.180.161:80/'
May 11 17:58:54.644: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.180.161:80/\n"
May 11 17:58:54.644: INFO: stdout: "affinity-clusterip-timeout-kx2qh"
May 11 17:58:54.644: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-280, will wait for the garbage collector to delete the pods
May 11 17:58:54.971: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 77.236072ms
May 11 17:58:55.071: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.202982ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:59:08.467: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-280" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:73.838 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":-1,"completed":13,"skipped":199,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3129
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3129
I0511 17:58:56.213788   45014 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3129, replica count: 2
I0511 17:58:59.314102   45014 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 17:59:02.314288   45014 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 17:59:02.314: INFO: Creating new exec pod
May 11 17:59:07.598: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3129 exec execpodpnbnz -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 11 17:59:08.344: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 11 17:59:08.344: INFO: stdout: ""
May 11 17:59:08.344: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3129 exec execpodpnbnz -- /bin/sh -x -c nc -zv -t -w 2 172.30.24.240 80'
May 11 17:59:09.066: INFO: stderr: "+ nc -zv -t -w 2 172.30.24.240 80\nConnection to 172.30.24.240 80 port [tcp/http] succeeded!\n"
May 11 17:59:09.066: INFO: stdout: ""
May 11 17:59:09.066: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:59:09.155: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-3129" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:13.814 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:59:09.409: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d6852707-ea81-4309-ac1f-20a8f2ed1071", Controller:(*bool)(0xc001c338f2), BlockOwnerDeletion:(*bool)(0xc001c338f3)}}
May 11 17:59:09.483: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c3d12a9a-b507-467c-8c78-d22f6789d24b", Controller:(*bool)(0xc001bc2732), BlockOwnerDeletion:(*bool)(0xc001bc2733)}}
May 11 17:59:09.567: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"11a2599d-141e-4536-b1f5-10fffb7926e0", Controller:(*bool)(0xc001c33b32), BlockOwnerDeletion:(*bool)(0xc001c33b33)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:59:14.710: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-3925" for this suite.


• [SLOW TEST:6.230 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":-1,"completed":14,"skipped":214,"failed":0}

SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 11 17:59:15.334: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 create -f -'
May 11 17:59:16.167: INFO: stderr: ""
May 11 17:59:16.167: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 11 17:59:16.167: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 11 17:59:16.428: INFO: stderr: ""
May 11 17:59:16.428: INFO: stdout: "update-demo-nautilus-9v8td update-demo-nautilus-g9wcd "
May 11 17:59:16.428: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 get pods update-demo-nautilus-9v8td -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 17:59:16.687: INFO: stderr: ""
May 11 17:59:16.687: INFO: stdout: ""
May 11 17:59:16.687: INFO: update-demo-nautilus-9v8td is created but not running
May 11 17:59:21.688: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 11 17:59:22.009: INFO: stderr: ""
May 11 17:59:22.009: INFO: stdout: "update-demo-nautilus-9v8td update-demo-nautilus-g9wcd "
May 11 17:59:22.009: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 get pods update-demo-nautilus-9v8td -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 17:59:22.257: INFO: stderr: ""
May 11 17:59:22.257: INFO: stdout: "true"
May 11 17:59:22.257: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 get pods update-demo-nautilus-9v8td -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 11 17:59:22.512: INFO: stderr: ""
May 11 17:59:22.513: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 17:59:22.513: INFO: validating pod update-demo-nautilus-9v8td
May 11 17:59:22.583: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 17:59:22.583: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 17:59:22.583: INFO: update-demo-nautilus-9v8td is verified up and running
May 11 17:59:22.583: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 get pods update-demo-nautilus-g9wcd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 17:59:22.841: INFO: stderr: ""
May 11 17:59:22.841: INFO: stdout: "true"
May 11 17:59:22.841: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 get pods update-demo-nautilus-g9wcd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 11 17:59:23.100: INFO: stderr: ""
May 11 17:59:23.100: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 17:59:23.100: INFO: validating pod update-demo-nautilus-g9wcd
May 11 17:59:23.176: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 17:59:23.176: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 17:59:23.176: INFO: update-demo-nautilus-g9wcd is verified up and running
STEP: using delete to clean up resources
May 11 17:59:23.176: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 delete --grace-period=0 --force -f -'
May 11 17:59:23.507: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 17:59:23.507: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 11 17:59:23.507: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 get rc,svc -l name=update-demo --no-headers'
May 11 17:59:23.831: INFO: stderr: "No resources found in kubectl-2398 namespace.\n"
May 11 17:59:23.831: INFO: stdout: ""
May 11 17:59:23.831: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2398 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 11 17:59:24.153: INFO: stderr: ""
May 11 17:59:24.153: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:59:24.153: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2398" for this suite.


• [SLOW TEST:9.432 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":-1,"completed":15,"skipped":234,"failed":0}

SSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":-1,"completed":9,"skipped":224,"failed":0}
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-p2fs
STEP: Creating a pod to test atomic-volume-subpath
May 11 17:59:09.973: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p2fs" in namespace "subpath-329" to be "Succeeded or Failed"
May 11 17:59:10.039: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Pending", Reason="", readiness=false. Elapsed: 65.792881ms
May 11 17:59:12.105: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131676362s
May 11 17:59:14.170: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Running", Reason="", readiness=true. Elapsed: 4.197479352s
May 11 17:59:16.237: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Running", Reason="", readiness=true. Elapsed: 6.264163572s
May 11 17:59:18.303: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Running", Reason="", readiness=true. Elapsed: 8.330050506s
May 11 17:59:20.369: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Running", Reason="", readiness=true. Elapsed: 10.396409715s
May 11 17:59:22.435: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Running", Reason="", readiness=true. Elapsed: 12.462164619s
May 11 17:59:24.500: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Running", Reason="", readiness=true. Elapsed: 14.527490918s
May 11 17:59:26.573: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Running", Reason="", readiness=true. Elapsed: 16.599835131s
May 11 17:59:28.639: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Running", Reason="", readiness=true. Elapsed: 18.665871036s
May 11 17:59:30.705: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Running", Reason="", readiness=true. Elapsed: 20.732267324s
May 11 17:59:32.771: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Running", Reason="", readiness=true. Elapsed: 22.797997942s
May 11 17:59:34.840: INFO: Pod "pod-subpath-test-projected-p2fs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.867386795s
STEP: Saw pod success
May 11 17:59:34.840: INFO: Pod "pod-subpath-test-projected-p2fs" satisfied condition "Succeeded or Failed"
May 11 17:59:34.906: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-subpath-test-projected-p2fs container test-container-subpath-projected-p2fs: <nil>
STEP: delete the pod
May 11 17:59:35.063: INFO: Waiting for pod pod-subpath-test-projected-p2fs to disappear
May 11 17:59:35.129: INFO: Pod pod-subpath-test-projected-p2fs no longer exists
STEP: Deleting pod pod-subpath-test-projected-p2fs
May 11 17:59:35.129: INFO: Deleting pod "pod-subpath-test-projected-p2fs" in namespace "subpath-329"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:59:35.198: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-329" for this suite.


• [SLOW TEST:26.036 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":-1,"completed":10,"skipped":224,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
May 11 17:59:24.771: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 11 17:59:24.771: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 create -f -'
May 11 17:59:25.584: INFO: stderr: ""
May 11 17:59:25.584: INFO: stdout: "service/agnhost-replica created\n"
May 11 17:59:25.585: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 11 17:59:25.585: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 create -f -'
May 11 17:59:26.465: INFO: stderr: ""
May 11 17:59:26.465: INFO: stdout: "service/agnhost-primary created\n"
May 11 17:59:26.465: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 11 17:59:26.465: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 create -f -'
May 11 17:59:26.974: INFO: stderr: ""
May 11 17:59:26.974: INFO: stdout: "service/frontend created\n"
May 11 17:59:26.974: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 11 17:59:26.974: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 create -f -'
May 11 17:59:27.465: INFO: stderr: ""
May 11 17:59:27.465: INFO: stdout: "deployment.apps/frontend created\n"
May 11 17:59:27.465: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 11 17:59:27.466: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 create -f -'
May 11 17:59:27.958: INFO: stderr: ""
May 11 17:59:27.958: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 11 17:59:27.958: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 11 17:59:27.959: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 create -f -'
May 11 17:59:28.774: INFO: stderr: ""
May 11 17:59:28.774: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 11 17:59:28.774: INFO: Waiting for all frontend pods to be Running.
May 11 17:59:33.874: INFO: Waiting for frontend to serve content.
May 11 17:59:33.964: INFO: Trying to add a new entry to the guestbook.
May 11 17:59:34.052: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 11 17:59:34.131: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 delete --grace-period=0 --force -f -'
May 11 17:59:34.491: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 17:59:34.491: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 11 17:59:34.491: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 delete --grace-period=0 --force -f -'
May 11 17:59:34.847: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 17:59:34.847: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 11 17:59:34.847: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 delete --grace-period=0 --force -f -'
May 11 17:59:35.184: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 17:59:35.184: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 11 17:59:35.185: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 delete --grace-period=0 --force -f -'
May 11 17:59:35.509: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 17:59:35.509: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 11 17:59:35.509: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 delete --grace-period=0 --force -f -'
May 11 17:59:35.833: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 17:59:35.833: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 11 17:59:35.833: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3596 delete --grace-period=0 --force -f -'
May 11 17:59:36.168: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 17:59:36.168: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:59:36.168: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3596" for this suite.


• [SLOW TEST:12.006 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":-1,"completed":16,"skipped":248,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:59:45.080: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-9979" for this suite.


• [SLOW TEST:8.898 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":-1,"completed":17,"skipped":267,"failed":0}

SS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:59:50.295: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-3651" for this suite.


• [SLOW TEST:5.215 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":-1,"completed":18,"skipped":269,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 17:59:51.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352791, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352791, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352791, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352791, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 17:59:54.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352791, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352791, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352791, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352791, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 17:59:57.124: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4846-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 17:59:58.907: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-5461" for this suite.
STEP: Destroying namespace "webhook-5461-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:9.095 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":-1,"completed":19,"skipped":313,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
May 11 18:00:00.121: INFO: Waiting up to 5m0s for pod "var-expansion-e9255d57-11af-4c27-a3ac-90d8d088ba1f" in namespace "var-expansion-9382" to be "Succeeded or Failed"
May 11 18:00:00.201: INFO: Pod "var-expansion-e9255d57-11af-4c27-a3ac-90d8d088ba1f": Phase="Pending", Reason="", readiness=false. Elapsed: 79.733096ms
May 11 18:00:02.269: INFO: Pod "var-expansion-e9255d57-11af-4c27-a3ac-90d8d088ba1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.148212132s
May 11 18:00:04.355: INFO: Pod "var-expansion-e9255d57-11af-4c27-a3ac-90d8d088ba1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.233334239s
STEP: Saw pod success
May 11 18:00:04.355: INFO: Pod "var-expansion-e9255d57-11af-4c27-a3ac-90d8d088ba1f" satisfied condition "Succeeded or Failed"
May 11 18:00:04.434: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod var-expansion-e9255d57-11af-4c27-a3ac-90d8d088ba1f container dapi-container: <nil>
STEP: delete the pod
May 11 18:00:04.631: INFO: Waiting for pod var-expansion-e9255d57-11af-4c27-a3ac-90d8d088ba1f to disappear
May 11 18:00:04.699: INFO: Pod var-expansion-e9255d57-11af-4c27-a3ac-90d8d088ba1f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:00:04.699: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-9382" for this suite.


• [SLOW TEST:5.277 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":-1,"completed":20,"skipped":329,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:00:05.459: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a72d187-6b20-4d51-a5c4-694b2f51e2d8" in namespace "projected-8610" to be "Succeeded or Failed"
May 11 18:00:05.529: INFO: Pod "downwardapi-volume-2a72d187-6b20-4d51-a5c4-694b2f51e2d8": Phase="Pending", Reason="", readiness=false. Elapsed: 69.617431ms
May 11 18:00:07.597: INFO: Pod "downwardapi-volume-2a72d187-6b20-4d51-a5c4-694b2f51e2d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138029087s
May 11 18:00:09.666: INFO: Pod "downwardapi-volume-2a72d187-6b20-4d51-a5c4-694b2f51e2d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.206692944s
STEP: Saw pod success
May 11 18:00:09.666: INFO: Pod "downwardapi-volume-2a72d187-6b20-4d51-a5c4-694b2f51e2d8" satisfied condition "Succeeded or Failed"
May 11 18:00:09.734: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod downwardapi-volume-2a72d187-6b20-4d51-a5c4-694b2f51e2d8 container client-container: <nil>
STEP: delete the pod
May 11 18:00:09.893: INFO: Waiting for pod downwardapi-volume-2a72d187-6b20-4d51-a5c4-694b2f51e2d8 to disappear
May 11 18:00:09.961: INFO: Pod downwardapi-volume-2a72d187-6b20-4d51-a5c4-694b2f51e2d8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:00:09.961: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8610" for this suite.


• [SLOW TEST:5.248 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":21,"skipped":346,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 17:59:35.881: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-35e83ef0-b323-47a1-8053-fbe4a381b913
STEP: Creating secret with name s-test-opt-upd-e0947e35-29e4-4363-959f-6328bdbfad37
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-35e83ef0-b323-47a1-8053-fbe4a381b913
STEP: Updating secret s-test-opt-upd-e0947e35-29e4-4363-959f-6328bdbfad37
STEP: Creating secret with name s-test-opt-create-46accf32-b43e-424d-bd1b-ad904d5da75a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:00:49.274: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9735" for this suite.


• [SLOW TEST:74.060 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":11,"skipped":252,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:00:54.358: INFO: Waiting up to 5m0s for pod "client-envvars-fe3ad240-b8a5-4f3d-b98d-b1966ebdff48" in namespace "pods-2018" to be "Succeeded or Failed"
May 11 18:00:54.424: INFO: Pod "client-envvars-fe3ad240-b8a5-4f3d-b98d-b1966ebdff48": Phase="Pending", Reason="", readiness=false. Elapsed: 65.755026ms
May 11 18:00:56.491: INFO: Pod "client-envvars-fe3ad240-b8a5-4f3d-b98d-b1966ebdff48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132756402s
May 11 18:00:58.560: INFO: Pod "client-envvars-fe3ad240-b8a5-4f3d-b98d-b1966ebdff48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.20178502s
STEP: Saw pod success
May 11 18:00:58.560: INFO: Pod "client-envvars-fe3ad240-b8a5-4f3d-b98d-b1966ebdff48" satisfied condition "Succeeded or Failed"
May 11 18:00:58.626: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod client-envvars-fe3ad240-b8a5-4f3d-b98d-b1966ebdff48 container env3cont: <nil>
STEP: delete the pod
May 11 18:00:58.775: INFO: Waiting for pod client-envvars-fe3ad240-b8a5-4f3d-b98d-b1966ebdff48 to disappear
May 11 18:00:58.844: INFO: Pod client-envvars-fe3ad240-b8a5-4f3d-b98d-b1966ebdff48 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:00:58.844: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-2018" for this suite.


• [SLOW TEST:9.548 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":289,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-b742455a-1f75-4189-a7de-952375b4f56f in namespace container-probe-6920
May 11 17:58:34.104: INFO: Started pod liveness-b742455a-1f75-4189-a7de-952375b4f56f in namespace container-probe-6920
STEP: checking the pod's current state and verifying that restartCount is present
May 11 17:58:34.170: INFO: Initial restart count of pod liveness-b742455a-1f75-4189-a7de-952375b4f56f is 0
May 11 17:58:44.578: INFO: Restart count of pod container-probe-6920/liveness-b742455a-1f75-4189-a7de-952375b4f56f is now 1 (10.407765345s elapsed)
May 11 17:59:05.261: INFO: Restart count of pod container-probe-6920/liveness-b742455a-1f75-4189-a7de-952375b4f56f is now 2 (31.090702956s elapsed)
May 11 17:59:25.949: INFO: Restart count of pod container-probe-6920/liveness-b742455a-1f75-4189-a7de-952375b4f56f is now 3 (51.778690684s elapsed)
May 11 17:59:44.560: INFO: Restart count of pod container-probe-6920/liveness-b742455a-1f75-4189-a7de-952375b4f56f is now 4 (1m10.389791131s elapsed)
May 11 18:00:59.123: INFO: Restart count of pod container-probe-6920/liveness-b742455a-1f75-4189-a7de-952375b4f56f is now 5 (2m24.952639629s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:00:59.199: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-6920" for this suite.


• [SLOW TEST:149.930 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":-1,"completed":5,"skipped":87,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 11 18:00:59.612: INFO: Waiting up to 5m0s for pod "pod-e1b76e29-100c-4be7-b985-05ec89317cb1" in namespace "emptydir-4604" to be "Succeeded or Failed"
May 11 18:00:59.681: INFO: Pod "pod-e1b76e29-100c-4be7-b985-05ec89317cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 69.192059ms
May 11 18:01:01.747: INFO: Pod "pod-e1b76e29-100c-4be7-b985-05ec89317cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.135801671s
May 11 18:01:03.814: INFO: Pod "pod-e1b76e29-100c-4be7-b985-05ec89317cb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.202530939s
STEP: Saw pod success
May 11 18:01:03.814: INFO: Pod "pod-e1b76e29-100c-4be7-b985-05ec89317cb1" satisfied condition "Succeeded or Failed"
May 11 18:01:03.880: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-e1b76e29-100c-4be7-b985-05ec89317cb1 container test-container: <nil>
STEP: delete the pod
May 11 18:01:04.032: INFO: Waiting for pod pod-e1b76e29-100c-4be7-b985-05ec89317cb1 to disappear
May 11 18:01:04.101: INFO: Pod pod-e1b76e29-100c-4be7-b985-05ec89317cb1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:04.101: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4604" for this suite.


• [SLOW TEST:5.183 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":13,"skipped":331,"failed":0}

S
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:00:59.964: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3690b3a7-4e3c-4459-832e-6b6dfe6e1873" in namespace "downward-api-7824" to be "Succeeded or Failed"
May 11 18:01:00.032: INFO: Pod "downwardapi-volume-3690b3a7-4e3c-4459-832e-6b6dfe6e1873": Phase="Pending", Reason="", readiness=false. Elapsed: 67.931535ms
May 11 18:01:02.104: INFO: Pod "downwardapi-volume-3690b3a7-4e3c-4459-832e-6b6dfe6e1873": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13997544s
May 11 18:01:04.172: INFO: Pod "downwardapi-volume-3690b3a7-4e3c-4459-832e-6b6dfe6e1873": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.207410483s
STEP: Saw pod success
May 11 18:01:04.172: INFO: Pod "downwardapi-volume-3690b3a7-4e3c-4459-832e-6b6dfe6e1873" satisfied condition "Succeeded or Failed"
May 11 18:01:04.239: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downwardapi-volume-3690b3a7-4e3c-4459-832e-6b6dfe6e1873 container client-container: <nil>
STEP: delete the pod
May 11 18:01:04.397: INFO: Waiting for pod downwardapi-volume-3690b3a7-4e3c-4459-832e-6b6dfe6e1873 to disappear
May 11 18:01:04.479: INFO: Pod downwardapi-volume-3690b3a7-4e3c-4459-832e-6b6dfe6e1873 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:04.479: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-7824" for this suite.


• [SLOW TEST:5.224 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":-1,"completed":6,"skipped":118,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:05.984: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3302" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":-1,"completed":7,"skipped":124,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:01:04.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-499baafe-8048-47ce-b7fb-9596d617650c" in namespace "projected-9812" to be "Succeeded or Failed"
May 11 18:01:04.859: INFO: Pod "downwardapi-volume-499baafe-8048-47ce-b7fb-9596d617650c": Phase="Pending", Reason="", readiness=false. Elapsed: 68.620795ms
May 11 18:01:06.938: INFO: Pod "downwardapi-volume-499baafe-8048-47ce-b7fb-9596d617650c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.147586655s
May 11 18:01:09.007: INFO: Pod "downwardapi-volume-499baafe-8048-47ce-b7fb-9596d617650c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.216655457s
STEP: Saw pod success
May 11 18:01:09.007: INFO: Pod "downwardapi-volume-499baafe-8048-47ce-b7fb-9596d617650c" satisfied condition "Succeeded or Failed"
May 11 18:01:09.073: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod downwardapi-volume-499baafe-8048-47ce-b7fb-9596d617650c container client-container: <nil>
STEP: delete the pod
May 11 18:01:09.219: INFO: Waiting for pod downwardapi-volume-499baafe-8048-47ce-b7fb-9596d617650c to disappear
May 11 18:01:09.285: INFO: Pod downwardapi-volume-499baafe-8048-47ce-b7fb-9596d617650c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:09.285: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9812" for this suite.


• [SLOW TEST:5.181 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":14,"skipped":332,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":-1,"completed":2,"skipped":28,"failed":0}
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-7e4d3a93-d7df-4517-a3c2-60092046d46a in namespace container-probe-6228
May 11 17:57:14.955: INFO: Started pod busybox-7e4d3a93-d7df-4517-a3c2-60092046d46a in namespace container-probe-6228
STEP: checking the pod's current state and verifying that restartCount is present
May 11 17:57:15.027: INFO: Initial restart count of pod busybox-7e4d3a93-d7df-4517-a3c2-60092046d46a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:16.960: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-6228" for this suite.


• [SLOW TEST:250.857 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":-1,"completed":3,"skipped":28,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 11 18:01:10.058: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:17.079: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-9521" for this suite.


• [SLOW TEST:7.774 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":-1,"completed":15,"skipped":367,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:17.760: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-1276" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

•
------------------------------
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":-1,"completed":16,"skipped":391,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 11 18:01:18.399: INFO: Waiting up to 5m0s for pod "pod-60c46dbd-9ba5-470d-a230-36cc023a5b1e" in namespace "emptydir-9804" to be "Succeeded or Failed"
May 11 18:01:18.468: INFO: Pod "pod-60c46dbd-9ba5-470d-a230-36cc023a5b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 68.686918ms
May 11 18:01:20.534: INFO: Pod "pod-60c46dbd-9ba5-470d-a230-36cc023a5b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134481998s
May 11 18:01:22.603: INFO: Pod "pod-60c46dbd-9ba5-470d-a230-36cc023a5b1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.203738635s
STEP: Saw pod success
May 11 18:01:22.603: INFO: Pod "pod-60c46dbd-9ba5-470d-a230-36cc023a5b1e" satisfied condition "Succeeded or Failed"
May 11 18:01:22.668: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-60c46dbd-9ba5-470d-a230-36cc023a5b1e container test-container: <nil>
STEP: delete the pod
May 11 18:01:22.814: INFO: Waiting for pod pod-60c46dbd-9ba5-470d-a230-36cc023a5b1e to disappear
May 11 18:01:22.880: INFO: Pod pod-60c46dbd-9ba5-470d-a230-36cc023a5b1e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:22.880: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-9804" for this suite.


• [SLOW TEST:5.168 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":17,"skipped":404,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2005
STEP: creating service affinity-clusterip-transition in namespace services-2005
STEP: creating replication controller affinity-clusterip-transition in namespace services-2005
I0511 18:01:06.853647   45016 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-2005, replica count: 3
I0511 18:01:09.953991   45016 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:01:12.954147   45016 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 18:01:13.092: INFO: Creating new exec pod
May 11 18:01:18.370: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2005 exec execpod-affinitybdx54 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
May 11 18:01:19.164: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 11 18:01:19.164: INFO: stdout: ""
May 11 18:01:19.164: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2005 exec execpod-affinitybdx54 -- /bin/sh -x -c nc -zv -t -w 2 172.30.64.104 80'
May 11 18:01:19.914: INFO: stderr: "+ nc -zv -t -w 2 172.30.64.104 80\nConnection to 172.30.64.104 80 port [tcp/http] succeeded!\n"
May 11 18:01:19.914: INFO: stdout: ""
May 11 18:01:20.054: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2005 exec execpod-affinitybdx54 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.64.104:80/ ; done'
May 11 18:01:20.989: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n"
May 11 18:01:20.989: INFO: stdout: "\naffinity-clusterip-transition-4qxdx\naffinity-clusterip-transition-jbvkg\naffinity-clusterip-transition-4qxdx\naffinity-clusterip-transition-4qxdx\naffinity-clusterip-transition-4qxdx\naffinity-clusterip-transition-4qxdx\naffinity-clusterip-transition-4qxdx\naffinity-clusterip-transition-4qxdx\naffinity-clusterip-transition-4qxdx\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-jbvkg\naffinity-clusterip-transition-jbvkg\naffinity-clusterip-transition-4qxdx\naffinity-clusterip-transition-jbvkg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-jbvkg"
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-4qxdx
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-jbvkg
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-4qxdx
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-4qxdx
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-4qxdx
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-4qxdx
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-4qxdx
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-4qxdx
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-4qxdx
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-jbvkg
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-jbvkg
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-4qxdx
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-jbvkg
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:20.989: INFO: Received response from host: affinity-clusterip-transition-jbvkg
May 11 18:01:21.134: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2005 exec execpod-affinitybdx54 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.64.104:80/ ; done'
May 11 18:01:21.957: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.64.104:80/\n"
May 11 18:01:21.957: INFO: stdout: "\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg\naffinity-clusterip-transition-5mzhg"
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Received response from host: affinity-clusterip-transition-5mzhg
May 11 18:01:21.957: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2005, will wait for the garbage collector to delete the pods
May 11 18:01:22.275: INFO: Deleting ReplicationController affinity-clusterip-transition took: 74.230964ms
May 11 18:01:22.375: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.189384ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:28.767: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-2005" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:22.771 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":-1,"completed":8,"skipped":128,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-cg94
STEP: Creating a pod to test atomic-volume-subpath
May 11 18:01:23.721: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cg94" in namespace "subpath-1644" to be "Succeeded or Failed"
May 11 18:01:23.786: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Pending", Reason="", readiness=false. Elapsed: 65.069773ms
May 11 18:01:25.852: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131009549s
May 11 18:01:27.919: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Running", Reason="", readiness=true. Elapsed: 4.197672681s
May 11 18:01:29.985: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Running", Reason="", readiness=true. Elapsed: 6.263177042s
May 11 18:01:32.050: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Running", Reason="", readiness=true. Elapsed: 8.32871671s
May 11 18:01:34.450: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Running", Reason="", readiness=true. Elapsed: 10.729037073s
May 11 18:01:36.520: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Running", Reason="", readiness=true. Elapsed: 12.798460359s
May 11 18:01:38.586: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Running", Reason="", readiness=true. Elapsed: 14.864177553s
May 11 18:01:40.655: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Running", Reason="", readiness=true. Elapsed: 16.933317222s
May 11 18:01:42.724: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Running", Reason="", readiness=true. Elapsed: 19.002720167s
May 11 18:01:44.790: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Running", Reason="", readiness=true. Elapsed: 21.068893743s
May 11 18:01:46.859: INFO: Pod "pod-subpath-test-configmap-cg94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.137833635s
STEP: Saw pod success
May 11 18:01:46.859: INFO: Pod "pod-subpath-test-configmap-cg94" satisfied condition "Succeeded or Failed"
May 11 18:01:46.925: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-subpath-test-configmap-cg94 container test-container-subpath-configmap-cg94: <nil>
STEP: delete the pod
May 11 18:01:47.075: INFO: Waiting for pod pod-subpath-test-configmap-cg94 to disappear
May 11 18:01:47.141: INFO: Pod pod-subpath-test-configmap-cg94 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cg94
May 11 18:01:47.141: INFO: Deleting pod "pod-subpath-test-configmap-cg94" in namespace "subpath-1644"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:47.209: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-1644" for this suite.


• [SLOW TEST:24.564 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":-1,"completed":18,"skipped":423,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9241
STEP: creating service affinity-nodeport in namespace services-9241
STEP: creating replication controller affinity-nodeport in namespace services-9241
I0511 18:01:17.730709   45012 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-9241, replica count: 3
I0511 18:01:20.830997   45012 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:01:23.831166   45012 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 18:01:24.135: INFO: Creating new exec pod
May 11 18:01:29.600: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-9241 exec execpod-affinity5dgc7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
May 11 18:01:30.374: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 11 18:01:30.374: INFO: stdout: ""
May 11 18:01:30.375: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-9241 exec execpod-affinity5dgc7 -- /bin/sh -x -c nc -zv -t -w 2 172.30.242.38 80'
May 11 18:01:31.115: INFO: stderr: "+ nc -zv -t -w 2 172.30.242.38 80\nConnection to 172.30.242.38 80 port [tcp/http] succeeded!\n"
May 11 18:01:31.115: INFO: stdout: ""
May 11 18:01:31.115: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-9241 exec execpod-affinity5dgc7 -- /bin/sh -x -c nc -zv -t -w 2 10.0.128.72 30696'
May 11 18:01:31.873: INFO: stderr: "+ nc -zv -t -w 2 10.0.128.72 30696\nConnection to 10.0.128.72 30696 port [tcp/30696] succeeded!\n"
May 11 18:01:31.873: INFO: stdout: ""
May 11 18:01:31.873: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-9241 exec execpod-affinity5dgc7 -- /bin/sh -x -c nc -zv -t -w 2 10.0.143.31 30696'
May 11 18:01:32.649: INFO: stderr: "+ nc -zv -t -w 2 10.0.143.31 30696\nConnection to 10.0.143.31 30696 port [tcp/30696] succeeded!\n"
May 11 18:01:32.649: INFO: stdout: ""
May 11 18:01:32.650: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-9241 exec execpod-affinity5dgc7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.128.72:30696/ ; done'
May 11 18:01:33.551: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30696/\n"
May 11 18:01:33.552: INFO: stdout: "\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6\naffinity-nodeport-kczp6"
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Received response from host: affinity-nodeport-kczp6
May 11 18:01:33.552: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9241, will wait for the garbage collector to delete the pods
May 11 18:01:33.864: INFO: Deleting ReplicationController affinity-nodeport took: 71.713719ms
May 11 18:01:33.964: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.184872ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:48.757: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-9241" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:31.776 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":-1,"completed":4,"skipped":70,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 11 18:01:51.480: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:51.621: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-6195" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":-1,"completed":19,"skipped":438,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-161
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 11 18:01:29.372: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
May 11 18:01:29.816: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 11 18:01:31.886: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 11 18:01:33.887: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:01:35.883: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:01:37.883: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:01:39.883: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:01:41.882: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 11 18:01:42.016: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 11 18:01:44.083: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 11 18:01:46.084: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 11 18:01:48.083: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 11 18:01:50.083: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 11 18:01:50.219: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 11 18:01:54.567: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 11 18:01:54.567: INFO: Breadth first check of 10.129.2.88 on host 10.0.128.72...
May 11 18:01:54.633: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.56:9080/dial?request=hostname&protocol=http&host=10.129.2.88&port=8080&tries=1'] Namespace:pod-network-test-161 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:01:55.171: INFO: Waiting for responses: map[]
May 11 18:01:55.171: INFO: reached 10.129.2.88 after 0/1 tries
May 11 18:01:55.171: INFO: Breadth first check of 10.131.0.45 on host 10.0.143.31...
May 11 18:01:55.238: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.56:9080/dial?request=hostname&protocol=http&host=10.131.0.45&port=8080&tries=1'] Namespace:pod-network-test-161 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:01:55.742: INFO: Waiting for responses: map[]
May 11 18:01:55.743: INFO: reached 10.131.0.45 after 0/1 tries
May 11 18:01:55.743: INFO: Breadth first check of 10.128.2.55 on host 10.0.198.156...
May 11 18:01:55.814: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.56:9080/dial?request=hostname&protocol=http&host=10.128.2.55&port=8080&tries=1'] Namespace:pod-network-test-161 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:01:56.303: INFO: Waiting for responses: map[]
May 11 18:01:56.303: INFO: reached 10.128.2.55 after 0/1 tries
May 11 18:01:56.303: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:56.303: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-161" for this suite.


• [SLOW TEST:27.537 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":-1,"completed":9,"skipped":130,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-7321
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7321 to expose endpoints map[]
May 11 18:01:49.629: INFO: successfully validated that service multi-endpoint-test in namespace services-7321 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7321
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7321 to expose endpoints map[pod1:[100]]
May 11 18:01:53.041: INFO: successfully validated that service multi-endpoint-test in namespace services-7321 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7321
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7321 to expose endpoints map[pod1:[100] pod2:[101]]
May 11 18:01:56.509: INFO: successfully validated that service multi-endpoint-test in namespace services-7321 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7321
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7321 to expose endpoints map[pod2:[101]]
May 11 18:01:56.859: INFO: successfully validated that service multi-endpoint-test in namespace services-7321 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7321
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7321 to expose endpoints map[]
May 11 18:01:57.137: INFO: successfully validated that service multi-endpoint-test in namespace services-7321 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:57.214: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-7321" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:8.451 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 11 18:01:52.408: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:57.163: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-9118" for this suite.


• [SLOW TEST:5.473 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":-1,"completed":20,"skipped":445,"failed":0}

SSSSSSSS
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":-1,"completed":5,"skipped":78,"failed":0}
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
May 11 18:01:57.809: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-184 api-versions'
May 11 18:01:58.176: INFO: stderr: ""
May 11 18:01:58.176: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd-publish-openapi-test-common-group.example.com/v6\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetwork.openshift.io/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:01:58.177: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-184" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":-1,"completed":6,"skipped":78,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 11 18:01:58.841: INFO: Waiting up to 5m0s for pod "downward-api-b0c9ee3d-85cc-4cb8-bd20-7d988fa0f3d9" in namespace "downward-api-2140" to be "Succeeded or Failed"
May 11 18:01:58.907: INFO: Pod "downward-api-b0c9ee3d-85cc-4cb8-bd20-7d988fa0f3d9": Phase="Pending", Reason="", readiness=false. Elapsed: 65.086256ms
May 11 18:02:00.972: INFO: Pod "downward-api-b0c9ee3d-85cc-4cb8-bd20-7d988fa0f3d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130336293s
May 11 18:02:03.037: INFO: Pod "downward-api-b0c9ee3d-85cc-4cb8-bd20-7d988fa0f3d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.195358142s
STEP: Saw pod success
May 11 18:02:03.037: INFO: Pod "downward-api-b0c9ee3d-85cc-4cb8-bd20-7d988fa0f3d9" satisfied condition "Succeeded or Failed"
May 11 18:02:03.103: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod downward-api-b0c9ee3d-85cc-4cb8-bd20-7d988fa0f3d9 container dapi-container: <nil>
STEP: delete the pod
May 11 18:02:03.274: INFO: Waiting for pod downward-api-b0c9ee3d-85cc-4cb8-bd20-7d988fa0f3d9 to disappear
May 11 18:02:03.340: INFO: Pod downward-api-b0c9ee3d-85cc-4cb8-bd20-7d988fa0f3d9 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:03.340: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2140" for this suite.


• [SLOW TEST:5.182 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":-1,"completed":7,"skipped":151,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
May 11 18:02:01.936: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-4901 pod-service-account-0eeb0438-dc88-4cab-9a35-b23677510986 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 11 18:02:02.786: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-4901 pod-service-account-0eeb0438-dc88-4cab-9a35-b23677510986 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 11 18:02:03.549: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-4901 pod-service-account-0eeb0438-dc88-4cab-9a35-b23677510986 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:04.360: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-4901" for this suite.


• [SLOW TEST:8.059 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":-1,"completed":10,"skipped":142,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:01:57.967: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-70ced5b6-2199-4169-a3a9-de2e9e384a8f" in namespace "security-context-test-8225" to be "Succeeded or Failed"
May 11 18:01:58.039: INFO: Pod "alpine-nnp-false-70ced5b6-2199-4169-a3a9-de2e9e384a8f": Phase="Pending", Reason="", readiness=false. Elapsed: 71.528401ms
May 11 18:02:00.104: INFO: Pod "alpine-nnp-false-70ced5b6-2199-4169-a3a9-de2e9e384a8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137180968s
May 11 18:02:02.174: INFO: Pod "alpine-nnp-false-70ced5b6-2199-4169-a3a9-de2e9e384a8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206680745s
May 11 18:02:04.239: INFO: Pod "alpine-nnp-false-70ced5b6-2199-4169-a3a9-de2e9e384a8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.272270006s
May 11 18:02:04.239: INFO: Pod "alpine-nnp-false-70ced5b6-2199-4169-a3a9-de2e9e384a8f" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:04.318: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-8225" for this suite.


• [SLOW TEST:7.156 seconds]
[k8s.io] Security Context
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when creating containers with AllowPrivilegeEscalation
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":21,"skipped":453,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:08.744: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-7265" for this suite.


• [SLOW TEST:5.347 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a read only busybox container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":8,"skipped":231,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:02:05.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0811399-4f0f-4d3c-aee5-d2e9b11f8713" in namespace "downward-api-4324" to be "Succeeded or Failed"
May 11 18:02:05.287: INFO: Pod "downwardapi-volume-b0811399-4f0f-4d3c-aee5-d2e9b11f8713": Phase="Pending", Reason="", readiness=false. Elapsed: 65.532477ms
May 11 18:02:07.354: INFO: Pod "downwardapi-volume-b0811399-4f0f-4d3c-aee5-d2e9b11f8713": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13262959s
May 11 18:02:09.423: INFO: Pod "downwardapi-volume-b0811399-4f0f-4d3c-aee5-d2e9b11f8713": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.201998317s
STEP: Saw pod success
May 11 18:02:09.423: INFO: Pod "downwardapi-volume-b0811399-4f0f-4d3c-aee5-d2e9b11f8713" satisfied condition "Succeeded or Failed"
May 11 18:02:09.488: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downwardapi-volume-b0811399-4f0f-4d3c-aee5-d2e9b11f8713 container client-container: <nil>
STEP: delete the pod
May 11 18:02:09.706: INFO: Waiting for pod downwardapi-volume-b0811399-4f0f-4d3c-aee5-d2e9b11f8713 to disappear
May 11 18:02:09.771: INFO: Pod downwardapi-volume-b0811399-4f0f-4d3c-aee5-d2e9b11f8713 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:09.771: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4324" for this suite.


• [SLOW TEST:5.382 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":-1,"completed":22,"skipped":494,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:13.195: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-4498" for this suite.


• [SLOW TEST:8.805 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":-1,"completed":11,"skipped":158,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 11 18:02:10.549: INFO: Waiting up to 5m0s for pod "pod-17631ad5-6f5e-44ba-abd2-6835711771e2" in namespace "emptydir-5028" to be "Succeeded or Failed"
May 11 18:02:10.614: INFO: Pod "pod-17631ad5-6f5e-44ba-abd2-6835711771e2": Phase="Pending", Reason="", readiness=false. Elapsed: 65.18123ms
May 11 18:02:12.681: INFO: Pod "pod-17631ad5-6f5e-44ba-abd2-6835711771e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132376773s
May 11 18:02:14.752: INFO: Pod "pod-17631ad5-6f5e-44ba-abd2-6835711771e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.203061639s
STEP: Saw pod success
May 11 18:02:14.752: INFO: Pod "pod-17631ad5-6f5e-44ba-abd2-6835711771e2" satisfied condition "Succeeded or Failed"
May 11 18:02:14.821: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-17631ad5-6f5e-44ba-abd2-6835711771e2 container test-container: <nil>
STEP: delete the pod
May 11 18:02:14.993: INFO: Waiting for pod pod-17631ad5-6f5e-44ba-abd2-6835711771e2 to disappear
May 11 18:02:15.060: INFO: Pod pod-17631ad5-6f5e-44ba-abd2-6835711771e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:15.060: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5028" for this suite.


• [SLOW TEST:5.257 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":23,"skipped":501,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-786a60ef-7c9f-4081-b1f4-282675fd8ee5
STEP: Creating secret with name secret-projected-all-test-volume-a6de2aeb-f49c-4ed0-91b3-c810d1f9bcba
STEP: Creating a pod to test Check all projections for projected volume plugin
May 11 18:02:14.395: INFO: Waiting up to 5m0s for pod "projected-volume-ca209213-fe59-4ab0-af8f-8be6a43e1e11" in namespace "projected-3081" to be "Succeeded or Failed"
May 11 18:02:14.463: INFO: Pod "projected-volume-ca209213-fe59-4ab0-af8f-8be6a43e1e11": Phase="Pending", Reason="", readiness=false. Elapsed: 67.509082ms
May 11 18:02:16.530: INFO: Pod "projected-volume-ca209213-fe59-4ab0-af8f-8be6a43e1e11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13474798s
May 11 18:02:18.602: INFO: Pod "projected-volume-ca209213-fe59-4ab0-af8f-8be6a43e1e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.207010155s
STEP: Saw pod success
May 11 18:02:18.602: INFO: Pod "projected-volume-ca209213-fe59-4ab0-af8f-8be6a43e1e11" satisfied condition "Succeeded or Failed"
May 11 18:02:18.669: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod projected-volume-ca209213-fe59-4ab0-af8f-8be6a43e1e11 container projected-all-volume-test: <nil>
STEP: delete the pod
May 11 18:02:18.821: INFO: Waiting for pod projected-volume-ca209213-fe59-4ab0-af8f-8be6a43e1e11 to disappear
May 11 18:02:19.356: INFO: Pod projected-volume-ca209213-fe59-4ab0-af8f-8be6a43e1e11 no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:19.356: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3081" for this suite.


• [SLOW TEST:6.151 seconds]
[sig-storage] Projected combined
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":171,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 11 18:00:18.126: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
May 11 18:01:11.844: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:19.748: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2667" for this suite.


• [SLOW TEST:129.749 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":-1,"completed":22,"skipped":389,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
May 11 18:02:15.824: INFO: Waiting up to 5m0s for pod "pod-0d5df692-a36d-4340-b888-48b55c9c494e" in namespace "emptydir-207" to be "Succeeded or Failed"
May 11 18:02:15.890: INFO: Pod "pod-0d5df692-a36d-4340-b888-48b55c9c494e": Phase="Pending", Reason="", readiness=false. Elapsed: 65.451976ms
May 11 18:02:17.956: INFO: Pod "pod-0d5df692-a36d-4340-b888-48b55c9c494e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131312835s
May 11 18:02:20.024: INFO: Pod "pod-0d5df692-a36d-4340-b888-48b55c9c494e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.200114246s
STEP: Saw pod success
May 11 18:02:20.025: INFO: Pod "pod-0d5df692-a36d-4340-b888-48b55c9c494e" satisfied condition "Succeeded or Failed"
May 11 18:02:20.090: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-0d5df692-a36d-4340-b888-48b55c9c494e container test-container: <nil>
STEP: delete the pod
May 11 18:02:20.243: INFO: Waiting for pod pod-0d5df692-a36d-4340-b888-48b55c9c494e to disappear
May 11 18:02:20.308: INFO: Pod pod-0d5df692-a36d-4340-b888-48b55c9c494e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:20.308: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-207" for this suite.


• [SLOW TEST:5.246 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":24,"skipped":510,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Lease
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:20.900: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "lease-test-1131" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":-1,"completed":13,"skipped":214,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0511 18:02:20.508166   45012 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W0511 18:02:20.508201   45012 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0511 18:02:20.508210   45012 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0511 18:02:20.508218   45012 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 11 18:02:20.508: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 11 18:02:20.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-24hzh" in namespace "gc-1137"
May 11 18:02:20.591: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zlrj" in namespace "gc-1137"
May 11 18:02:20.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-4csmg" in namespace "gc-1137"
May 11 18:02:20.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-5p56d" in namespace "gc-1137"
May 11 18:02:20.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pjtm" in namespace "gc-1137"
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:20.897: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-1137" for this suite.


• [SLOW TEST:12.132 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":-1,"completed":9,"skipped":268,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
May 11 18:02:21.179: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-285 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:21.568: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-285" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":-1,"completed":25,"skipped":579,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-bebdc819-c4f0-41d5-ae25-dcc701a9adfe
STEP: Creating a pod to test consume secrets
May 11 18:02:21.846: INFO: Waiting up to 5m0s for pod "pod-secrets-755f07a0-4aa2-475f-ac45-5f1bae46055e" in namespace "secrets-2217" to be "Succeeded or Failed"
May 11 18:02:22.187: INFO: Pod "pod-secrets-755f07a0-4aa2-475f-ac45-5f1bae46055e": Phase="Pending", Reason="", readiness=false. Elapsed: 67.480868ms
May 11 18:02:24.271: INFO: Pod "pod-secrets-755f07a0-4aa2-475f-ac45-5f1bae46055e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.151582379s
May 11 18:02:26.529: INFO: Pod "pod-secrets-755f07a0-4aa2-475f-ac45-5f1bae46055e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.409150117s
STEP: Saw pod success
May 11 18:02:26.529: INFO: Pod "pod-secrets-755f07a0-4aa2-475f-ac45-5f1bae46055e" satisfied condition "Succeeded or Failed"
May 11 18:02:26.596: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-secrets-755f07a0-4aa2-475f-ac45-5f1bae46055e container secret-volume-test: <nil>
STEP: delete the pod
May 11 18:02:27.068: INFO: Waiting for pod pod-secrets-755f07a0-4aa2-475f-ac45-5f1bae46055e to disappear
May 11 18:02:27.134: INFO: Pod pod-secrets-755f07a0-4aa2-475f-ac45-5f1bae46055e no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:27.134: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-2217" for this suite.


• [SLOW TEST:6.344 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":-1,"completed":14,"skipped":231,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:02:21.299: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352941, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352941, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352941, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352941, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:02:23.364: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352941, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352941, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352941, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352941, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:02:26.442: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:27.199: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-218" for this suite.
STEP: Destroying namespace "webhook-218-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:7.810 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":-1,"completed":23,"skipped":406,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-a0a8196e-20dd-4a2c-8a73-8483516297ed
STEP: Creating a pod to test consume secrets
May 11 18:02:22.202: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fbe2eee3-9fe1-4730-ba2f-c10ec041ef2c" in namespace "projected-2980" to be "Succeeded or Failed"
May 11 18:02:22.333: INFO: Pod "pod-projected-secrets-fbe2eee3-9fe1-4730-ba2f-c10ec041ef2c": Phase="Pending", Reason="", readiness=false. Elapsed: 69.016251ms
May 11 18:02:25.343: INFO: Pod "pod-projected-secrets-fbe2eee3-9fe1-4730-ba2f-c10ec041ef2c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079005116s
May 11 18:02:27.410: INFO: Pod "pod-projected-secrets-fbe2eee3-9fe1-4730-ba2f-c10ec041ef2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.146033743s
STEP: Saw pod success
May 11 18:02:27.410: INFO: Pod "pod-projected-secrets-fbe2eee3-9fe1-4730-ba2f-c10ec041ef2c" satisfied condition "Succeeded or Failed"
May 11 18:02:27.476: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-projected-secrets-fbe2eee3-9fe1-4730-ba2f-c10ec041ef2c container projected-secret-volume-test: <nil>
STEP: delete the pod
May 11 18:02:27.655: INFO: Waiting for pod pod-projected-secrets-fbe2eee3-9fe1-4730-ba2f-c10ec041ef2c to disappear
May 11 18:02:27.725: INFO: Pod pod-projected-secrets-fbe2eee3-9fe1-4730-ba2f-c10ec041ef2c no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:27.725: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2980" for this suite.


• [SLOW TEST:6.863 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":10,"skipped":275,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 11 18:02:24.176: INFO: Waiting up to 5m0s for pod "downward-api-42850ad0-eaf9-443b-b616-c1aa94676569" in namespace "downward-api-5987" to be "Succeeded or Failed"
May 11 18:02:24.242: INFO: Pod "downward-api-42850ad0-eaf9-443b-b616-c1aa94676569": Phase="Pending", Reason="", readiness=false. Elapsed: 65.612867ms
May 11 18:02:26.527: INFO: Pod "downward-api-42850ad0-eaf9-443b-b616-c1aa94676569": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351223061s
May 11 18:02:28.596: INFO: Pod "downward-api-42850ad0-eaf9-443b-b616-c1aa94676569": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.419969848s
STEP: Saw pod success
May 11 18:02:28.596: INFO: Pod "downward-api-42850ad0-eaf9-443b-b616-c1aa94676569" satisfied condition "Succeeded or Failed"
May 11 18:02:28.663: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downward-api-42850ad0-eaf9-443b-b616-c1aa94676569 container dapi-container: <nil>
STEP: delete the pod
May 11 18:02:28.814: INFO: Waiting for pod downward-api-42850ad0-eaf9-443b-b616-c1aa94676569 to disappear
May 11 18:02:28.880: INFO: Pod downward-api-42850ad0-eaf9-443b-b616-c1aa94676569 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:28.880: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5987" for this suite.


• [SLOW TEST:6.931 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":-1,"completed":26,"skipped":594,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:29.082: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-2448" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":-1,"completed":24,"skipped":411,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
May 11 18:02:28.498: INFO: Waiting up to 5m0s for pod "var-expansion-f9bbeffc-c147-489b-9f5e-5a987bfa5103" in namespace "var-expansion-7535" to be "Succeeded or Failed"
May 11 18:02:28.567: INFO: Pod "var-expansion-f9bbeffc-c147-489b-9f5e-5a987bfa5103": Phase="Pending", Reason="", readiness=false. Elapsed: 68.595729ms
May 11 18:02:30.632: INFO: Pod "var-expansion-f9bbeffc-c147-489b-9f5e-5a987bfa5103": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133874399s
May 11 18:02:32.697: INFO: Pod "var-expansion-f9bbeffc-c147-489b-9f5e-5a987bfa5103": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.198973087s
STEP: Saw pod success
May 11 18:02:32.698: INFO: Pod "var-expansion-f9bbeffc-c147-489b-9f5e-5a987bfa5103" satisfied condition "Succeeded or Failed"
May 11 18:02:32.763: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod var-expansion-f9bbeffc-c147-489b-9f5e-5a987bfa5103 container dapi-container: <nil>
STEP: delete the pod
May 11 18:02:32.911: INFO: Waiting for pod var-expansion-f9bbeffc-c147-489b-9f5e-5a987bfa5103 to disappear
May 11 18:02:32.982: INFO: Pod var-expansion-f9bbeffc-c147-489b-9f5e-5a987bfa5103 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:32.982: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-7535" for this suite.


• [SLOW TEST:5.161 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":-1,"completed":11,"skipped":356,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0511 18:02:36.143128   45014 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W0511 18:02:36.143171   45014 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0511 18:02:36.143178   45014 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0511 18:02:36.143183   45014 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 11 18:02:36.143: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:36.143: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-1220" for this suite.


• [SLOW TEST:7.269 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":-1,"completed":27,"skipped":599,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:02:30.527: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352950, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352950, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352950, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352950, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:02:32.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352950, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352950, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352950, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352950, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:02:35.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:36.128: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-5479" for this suite.
STEP: Destroying namespace "webhook-5479-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:7.487 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":-1,"completed":25,"skipped":433,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:37.507: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6227" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":-1,"completed":12,"skipped":363,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:37.920: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "podtemplate-825" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":-1,"completed":26,"skipped":449,"failed":0}

S
------------------------------
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:02:36.909: INFO: Creating deployment "test-recreate-deployment"
May 11 18:02:36.989: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 11 18:02:37.192: INFO: Waiting deployment "test-recreate-deployment" to complete
May 11 18:02:37.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352957, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352957, loc:(*time.Location)(0x7bb6980)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-786dd7c454\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352957, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352957, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
May 11 18:02:39.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352957, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352957, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352957, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352957, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:02:41.325: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 11 18:02:41.465: INFO: Updating deployment test-recreate-deployment
May 11 18:02:41.465: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 11 18:02:41.747: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-13 /apis/apps/v1/namespaces/deployment-13/deployments/test-recreate-deployment 02abf019-b0b3-48dd-a5af-00c084a5a864 51136 2 2021-05-11 18:02:36 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-11 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-11 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0006683f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-11 18:02:41 +0000 UTC,LastTransitionTime:2021-05-11 18:02:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-05-11 18:02:41 +0000 UTC,LastTransitionTime:2021-05-11 18:02:37 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 11 18:02:41.818: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-13 /apis/apps/v1/namespaces/deployment-13/replicasets/test-recreate-deployment-f79dd4667 9b6ed9f2-043a-4bf9-8a25-682254716b0d 51134 1 2021-05-11 18:02:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 02abf019-b0b3-48dd-a5af-00c084a5a864 0xc0007a2790 0xc0007a2791}] []  [{kube-controller-manager Update apps/v1 2021-05-11 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02abf019-b0b3-48dd-a5af-00c084a5a864\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007a2818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 11 18:02:41.818: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 11 18:02:41.818: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-13 /apis/apps/v1/namespaces/deployment-13/replicasets/test-recreate-deployment-786dd7c454 188a7b04-19a0-458f-9489-9d0eb9cd9173 51124 2 2021-05-11 18:02:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 02abf019-b0b3-48dd-a5af-00c084a5a864 0xc0007a24f7 0xc0007a24f8}] []  [{kube-controller-manager Update apps/v1 2021-05-11 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02abf019-b0b3-48dd-a5af-00c084a5a864\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007a2628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 11 18:02:41.891: INFO: Pod "test-recreate-deployment-f79dd4667-cw2rv" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-cw2rv test-recreate-deployment-f79dd4667- deployment-13 /api/v1/namespaces/deployment-13/pods/test-recreate-deployment-f79dd4667-cw2rv d5f7715d-d563-4d0a-a2b3-1c885446cc2c 51135 0 2021-05-11 18:02:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 9b6ed9f2-043a-4bf9-8a25-682254716b0d 0xc0007a2f47 0xc0007a2f48}] []  [{kube-controller-manager Update v1 2021-05-11 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b6ed9f2-043a-4bf9-8a25-682254716b0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:02:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zbnxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zbnxv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zbnxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vft5l,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:02:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:,StartTime:2021-05-11 18:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:41.891: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-13" for this suite.


• [SLOW TEST:5.758 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":-1,"completed":28,"skipped":626,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 11 18:02:42.742: INFO: Waiting up to 5m0s for pod "downward-api-af74ab3b-8f02-4375-a894-236d74b86139" in namespace "downward-api-7027" to be "Succeeded or Failed"
May 11 18:02:42.808: INFO: Pod "downward-api-af74ab3b-8f02-4375-a894-236d74b86139": Phase="Pending", Reason="", readiness=false. Elapsed: 66.324448ms
May 11 18:02:44.897: INFO: Pod "downward-api-af74ab3b-8f02-4375-a894-236d74b86139": Phase="Pending", Reason="", readiness=false. Elapsed: 2.154893315s
May 11 18:02:46.964: INFO: Pod "downward-api-af74ab3b-8f02-4375-a894-236d74b86139": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.22255449s
STEP: Saw pod success
May 11 18:02:46.964: INFO: Pod "downward-api-af74ab3b-8f02-4375-a894-236d74b86139" satisfied condition "Succeeded or Failed"
May 11 18:02:47.030: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downward-api-af74ab3b-8f02-4375-a894-236d74b86139 container dapi-container: <nil>
STEP: delete the pod
May 11 18:02:47.178: INFO: Waiting for pod downward-api-af74ab3b-8f02-4375-a894-236d74b86139 to disappear
May 11 18:02:47.247: INFO: Pod downward-api-af74ab3b-8f02-4375-a894-236d74b86139 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:47.247: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-7027" for this suite.


• [SLOW TEST:5.297 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":-1,"completed":29,"skipped":659,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:02:49.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352968, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352968, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352968, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352968, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:02:51.106: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352968, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352968, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352968, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352968, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:02:54.218: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:54.744: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-2282" for this suite.
STEP: Destroying namespace "webhook-2282-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:7.644 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":-1,"completed":30,"skipped":664,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:02:55.940: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-6104" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":-1,"completed":31,"skipped":672,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 11 18:02:56.542: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:06.726: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-903" for this suite.


• [SLOW TEST:10.842 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":-1,"completed":32,"skipped":679,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 11 18:02:38.495: INFO: PodSpec: initContainers in spec.initContainers
May 11 18:03:25.792: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9bb86256-5bfd-49d9-b90d-063469d2797d", GenerateName:"", Namespace:"init-container-7614", SelfLink:"/api/v1/namespaces/init-container-7614/pods/pod-init-9bb86256-5bfd-49d9-b90d-063469d2797d", UID:"7b065401-3ea4-4534-8fab-8deba88d6fa3", ResourceVersion:"52200", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63756352958, loc:(*time.Location)(0x7bb6980)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"495397801"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.108\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.108\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002003bc0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002003be0)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002003c00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002003c40)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002003c60), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002003cc0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-wcpn9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc006787780), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wcpn9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc001869c20), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wcpn9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc001869d40), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wcpn9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc001869bc0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003f84750), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-128-72.us-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000771880), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-68nmj"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003f84910)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003f849f0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003f84a6c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003f84b50), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002207ae0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352958, loc:(*time.Location)(0x7bb6980)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352958, loc:(*time.Location)(0x7bb6980)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352958, loc:(*time.Location)(0x7bb6980)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756352958, loc:(*time.Location)(0x7bb6980)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.128.72", PodIP:"10.129.2.108", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.129.2.108"}}, StartTime:(*v1.Time)(0xc002003ce0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007719d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000771ab0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://f2fb830f1c994c9874542425347a13b01cf1aba9f9666212f3ebf5feac87ee13", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002003d40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002003d20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc003f84c7f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:25.793: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-7614" for this suite.


• [SLOW TEST:47.911 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":-1,"completed":27,"skipped":450,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 11 18:02:28.164: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-a 4cad7f8a-0100-418d-991c-1d3c170bf9dc 50212 0 2021-05-11 18:02:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-11 18:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:02:28.164: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-a 4cad7f8a-0100-418d-991c-1d3c170bf9dc 50212 0 2021-05-11 18:02:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-11 18:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 11 18:02:38.316: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-a 4cad7f8a-0100-418d-991c-1d3c170bf9dc 51033 0 2021-05-11 18:02:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-11 18:02:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:02:38.316: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-a 4cad7f8a-0100-418d-991c-1d3c170bf9dc 51033 0 2021-05-11 18:02:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-11 18:02:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 11 18:02:48.454: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-a 4cad7f8a-0100-418d-991c-1d3c170bf9dc 51513 0 2021-05-11 18:02:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-11 18:02:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:02:48.454: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-a 4cad7f8a-0100-418d-991c-1d3c170bf9dc 51513 0 2021-05-11 18:02:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-11 18:02:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 11 18:02:58.528: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-a 4cad7f8a-0100-418d-991c-1d3c170bf9dc 51785 0 2021-05-11 18:02:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-11 18:02:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:02:58.529: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-a 4cad7f8a-0100-418d-991c-1d3c170bf9dc 51785 0 2021-05-11 18:02:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-11 18:02:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 11 18:03:08.600: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-b adb0fd11-e0c9-4911-922b-638ea448d5de 52015 0 2021-05-11 18:03:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-11 18:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:03:08.600: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-b adb0fd11-e0c9-4911-922b-638ea448d5de 52015 0 2021-05-11 18:03:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-11 18:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 11 18:03:18.672: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-b adb0fd11-e0c9-4911-922b-638ea448d5de 52146 0 2021-05-11 18:03:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-11 18:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:03:18.672: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8262 /api/v1/namespaces/watch-8262/configmaps/e2e-watch-test-configmap-b adb0fd11-e0c9-4911-922b-638ea448d5de 52146 0 2021-05-11 18:03:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-11 18:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:28.673: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-8262" for this suite.


• [SLOW TEST:61.533 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":-1,"completed":15,"skipped":238,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-73c830be-d852-4778-b3a9-aaab75cb522c
STEP: Creating a pod to test consume secrets
May 11 18:03:26.540: INFO: Waiting up to 5m0s for pod "pod-secrets-57c1ae55-2ac4-447f-bd72-0a663776ad9a" in namespace "secrets-1082" to be "Succeeded or Failed"
May 11 18:03:26.607: INFO: Pod "pod-secrets-57c1ae55-2ac4-447f-bd72-0a663776ad9a": Phase="Pending", Reason="", readiness=false. Elapsed: 67.654192ms
May 11 18:03:28.672: INFO: Pod "pod-secrets-57c1ae55-2ac4-447f-bd72-0a663776ad9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132712635s
May 11 18:03:30.738: INFO: Pod "pod-secrets-57c1ae55-2ac4-447f-bd72-0a663776ad9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.198945276s
STEP: Saw pod success
May 11 18:03:30.739: INFO: Pod "pod-secrets-57c1ae55-2ac4-447f-bd72-0a663776ad9a" satisfied condition "Succeeded or Failed"
May 11 18:03:30.808: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-secrets-57c1ae55-2ac4-447f-bd72-0a663776ad9a container secret-volume-test: <nil>
STEP: delete the pod
May 11 18:03:30.963: INFO: Waiting for pod pod-secrets-57c1ae55-2ac4-447f-bd72-0a663776ad9a to disappear
May 11 18:03:31.031: INFO: Pod pod-secrets-57c1ae55-2ac4-447f-bd72-0a663776ad9a no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:31.031: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-1082" for this suite.


• [SLOW TEST:5.244 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":28,"skipped":462,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:31.910: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2475" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":-1,"completed":29,"skipped":508,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3838
STEP: creating service affinity-nodeport-transition in namespace services-3838
STEP: creating replication controller affinity-nodeport-transition in namespace services-3838
I0511 18:03:07.535925   45014 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-3838, replica count: 3
I0511 18:03:10.636350   45014 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:03:13.636554   45014 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 18:03:13.945: INFO: Creating new exec pod
May 11 18:03:19.395: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3838 exec execpod-affinityglhcg -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 11 18:03:20.184: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 11 18:03:20.184: INFO: stdout: ""
May 11 18:03:20.185: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3838 exec execpod-affinityglhcg -- /bin/sh -x -c nc -zv -t -w 2 172.30.103.93 80'
May 11 18:03:21.017: INFO: stderr: "+ nc -zv -t -w 2 172.30.103.93 80\nConnection to 172.30.103.93 80 port [tcp/http] succeeded!\n"
May 11 18:03:21.017: INFO: stdout: ""
May 11 18:03:21.017: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3838 exec execpod-affinityglhcg -- /bin/sh -x -c nc -zv -t -w 2 10.0.143.31 30323'
May 11 18:03:21.795: INFO: stderr: "+ nc -zv -t -w 2 10.0.143.31 30323\nConnection to 10.0.143.31 30323 port [tcp/30323] succeeded!\n"
May 11 18:03:21.795: INFO: stdout: ""
May 11 18:03:21.795: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3838 exec execpod-affinityglhcg -- /bin/sh -x -c nc -zv -t -w 2 10.0.128.72 30323'
May 11 18:03:22.598: INFO: stderr: "+ nc -zv -t -w 2 10.0.128.72 30323\nConnection to 10.0.128.72 30323 port [tcp/30323] succeeded!\n"
May 11 18:03:22.598: INFO: stdout: ""
May 11 18:03:22.735: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3838 exec execpod-affinityglhcg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.128.72:30323/ ; done'
May 11 18:03:23.636: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n"
May 11 18:03:23.637: INFO: stdout: "\naffinity-nodeport-transition-59zkv\naffinity-nodeport-transition-z6qs5\naffinity-nodeport-transition-z6qs5\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-59zkv\naffinity-nodeport-transition-z6qs5\naffinity-nodeport-transition-z6qs5\naffinity-nodeport-transition-z6qs5\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-z6qs5\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-59zkv\naffinity-nodeport-transition-59zkv"
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-59zkv
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-z6qs5
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-z6qs5
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-59zkv
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-z6qs5
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-z6qs5
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-z6qs5
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-z6qs5
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-59zkv
May 11 18:03:23.637: INFO: Received response from host: affinity-nodeport-transition-59zkv
May 11 18:03:23.770: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3838 exec execpod-affinityglhcg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.128.72:30323/ ; done'
May 11 18:03:24.736: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:30323/\n"
May 11 18:03:24.736: INFO: stdout: "\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw\naffinity-nodeport-transition-kxqpw"
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Received response from host: affinity-nodeport-transition-kxqpw
May 11 18:03:24.736: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3838, will wait for the garbage collector to delete the pods
May 11 18:03:25.049: INFO: Deleting ReplicationController affinity-nodeport-transition took: 71.516021ms
May 11 18:03:25.150: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.173047ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:38.739: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-3838" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:31.943 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":-1,"completed":33,"skipped":690,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating first CR 
May 11 18:02:38.801: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-11T18:02:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-11T18:02:38Z]] name:name1 resourceVersion:51068 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4afda28b-4bc7-4b0e-9ddd-a326ac1c9c27] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 11 18:02:48.870: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-11T18:02:48Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-11T18:02:48Z]] name:name2 resourceVersion:51556 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:b798b9bc-04af-4107-8625-ece46c1630f2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 11 18:02:58.942: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-11T18:02:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-11T18:02:58Z]] name:name1 resourceVersion:51789 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4afda28b-4bc7-4b0e-9ddd-a326ac1c9c27] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 11 18:03:09.010: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-11T18:02:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-11T18:03:08Z]] name:name2 resourceVersion:52018 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:b798b9bc-04af-4107-8625-ece46c1630f2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 11 18:03:19.085: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-11T18:02:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-11T18:02:58Z]] name:name1 resourceVersion:52148 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4afda28b-4bc7-4b0e-9ddd-a326ac1c9c27] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 11 18:03:29.154: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-11T18:02:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-11T18:03:08Z]] name:name2 resourceVersion:52305 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:b798b9bc-04af-4107-8625-ece46c1630f2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:39.293: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-watch-5116" for this suite.


• [SLOW TEST:61.776 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":-1,"completed":13,"skipped":367,"failed":0}

SS
------------------------------
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:40.534: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-3340" for this suite.

•
------------------------------
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":-1,"completed":34,"skipped":698,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:03:40.006: INFO: Waiting up to 5m0s for pod "downwardapi-volume-830ac531-ff53-4a22-b005-b01e7004f340" in namespace "downward-api-1573" to be "Succeeded or Failed"
May 11 18:03:40.071: INFO: Pod "downwardapi-volume-830ac531-ff53-4a22-b005-b01e7004f340": Phase="Pending", Reason="", readiness=false. Elapsed: 65.440902ms
May 11 18:03:42.137: INFO: Pod "downwardapi-volume-830ac531-ff53-4a22-b005-b01e7004f340": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130986518s
May 11 18:03:44.202: INFO: Pod "downwardapi-volume-830ac531-ff53-4a22-b005-b01e7004f340": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.196390165s
STEP: Saw pod success
May 11 18:03:44.202: INFO: Pod "downwardapi-volume-830ac531-ff53-4a22-b005-b01e7004f340" satisfied condition "Succeeded or Failed"
May 11 18:03:44.268: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod downwardapi-volume-830ac531-ff53-4a22-b005-b01e7004f340 container client-container: <nil>
STEP: delete the pod
May 11 18:03:44.422: INFO: Waiting for pod downwardapi-volume-830ac531-ff53-4a22-b005-b01e7004f340 to disappear
May 11 18:03:44.487: INFO: Pod downwardapi-volume-830ac531-ff53-4a22-b005-b01e7004f340 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:44.487: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-1573" for this suite.


• [SLOW TEST:5.169 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":14,"skipped":369,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-9dbc3385-774b-45c0-ad37-4c1d0e2056b3
STEP: Creating a pod to test consume configMaps
May 11 18:03:41.251: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-06870c61-119d-44da-af05-4d343143ee63" in namespace "projected-4519" to be "Succeeded or Failed"
May 11 18:03:41.316: INFO: Pod "pod-projected-configmaps-06870c61-119d-44da-af05-4d343143ee63": Phase="Pending", Reason="", readiness=false. Elapsed: 65.243889ms
May 11 18:03:43.384: INFO: Pod "pod-projected-configmaps-06870c61-119d-44da-af05-4d343143ee63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133442164s
May 11 18:03:45.450: INFO: Pod "pod-projected-configmaps-06870c61-119d-44da-af05-4d343143ee63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.199746256s
STEP: Saw pod success
May 11 18:03:45.450: INFO: Pod "pod-projected-configmaps-06870c61-119d-44da-af05-4d343143ee63" satisfied condition "Succeeded or Failed"
May 11 18:03:45.516: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-projected-configmaps-06870c61-119d-44da-af05-4d343143ee63 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:03:45.661: INFO: Waiting for pod pod-projected-configmaps-06870c61-119d-44da-af05-4d343143ee63 to disappear
May 11 18:03:45.730: INFO: Pod pod-projected-configmaps-06870c61-119d-44da-af05-4d343143ee63 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:45.730: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4519" for this suite.


• [SLOW TEST:5.301 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":-1,"completed":35,"skipped":740,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-api-machinery] server version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
May 11 18:03:46.469: INFO: Major version: 1
STEP: Confirm minor version
May 11 18:03:46.469: INFO: cleanMinorVersion: 20
May 11 18:03:46.469: INFO: Minor version: 20+
[AfterEach] [sig-api-machinery] server version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:46.469: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "server-version-2296" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":-1,"completed":36,"skipped":743,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
May 11 18:03:47.113: INFO: Waiting up to 5m0s for pod "client-containers-11b0b80a-f168-4e95-9471-d7438e74771e" in namespace "containers-6478" to be "Succeeded or Failed"
May 11 18:03:47.179: INFO: Pod "client-containers-11b0b80a-f168-4e95-9471-d7438e74771e": Phase="Pending", Reason="", readiness=false. Elapsed: 65.935571ms
May 11 18:03:49.246: INFO: Pod "client-containers-11b0b80a-f168-4e95-9471-d7438e74771e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133149014s
May 11 18:03:51.313: INFO: Pod "client-containers-11b0b80a-f168-4e95-9471-d7438e74771e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.199294509s
STEP: Saw pod success
May 11 18:03:51.313: INFO: Pod "client-containers-11b0b80a-f168-4e95-9471-d7438e74771e" satisfied condition "Succeeded or Failed"
May 11 18:03:51.378: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod client-containers-11b0b80a-f168-4e95-9471-d7438e74771e container agnhost-container: <nil>
STEP: delete the pod
May 11 18:03:51.528: INFO: Waiting for pod client-containers-11b0b80a-f168-4e95-9471-d7438e74771e to disappear
May 11 18:03:51.594: INFO: Pod client-containers-11b0b80a-f168-4e95-9471-d7438e74771e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:51.594: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-6478" for this suite.


• [SLOW TEST:5.205 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":-1,"completed":37,"skipped":779,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:03:45.219: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 11 18:03:49.356: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 11 18:03:53.897: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-969 /apis/apps/v1/namespaces/deployment-969/deployments/test-cleanup-deployment 8ccb7249-5ee1-475b-8481-09cbbc187cb6 53127 1 2021-05-11 18:03:49 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-05-11 18:03:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-11 18:03:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001825308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-11 18:03:49 +0000 UTC,LastTransitionTime:2021-05-11 18:03:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-05-11 18:03:52 +0000 UTC,LastTransitionTime:2021-05-11 18:03:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 11 18:03:53.965: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-969 /apis/apps/v1/namespaces/deployment-969/replicasets/test-cleanup-deployment-685c4f8568 9e06273e-637e-4b52-bc6b-4c731a0bec7e 53106 1 2021-05-11 18:03:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8ccb7249-5ee1-475b-8481-09cbbc187cb6 0xc000f97677 0xc000f97678}] []  [{kube-controller-manager Update apps/v1 2021-05-11 18:03:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ccb7249-5ee1-475b-8481-09cbbc187cb6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000f97708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 11 18:03:54.031: INFO: Pod "test-cleanup-deployment-685c4f8568-fzz8r" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-fzz8r test-cleanup-deployment-685c4f8568- deployment-969 /api/v1/namespaces/deployment-969/pods/test-cleanup-deployment-685c4f8568-fzz8r 47ddad68-bad8-4dcd-878f-0f4e4f0db683 53103 0 2021-05-11 18:03:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.77"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.77"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 9e06273e-637e-4b52-bc6b-4c731a0bec7e 0xc001825697 0xc001825698}] []  [{kube-controller-manager Update v1 2021-05-11 18:03:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e06273e-637e-4b52-bc6b-4c731a0bec7e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:03:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:03:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q4z6g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q4z6g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q4z6g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c44,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-l45dj,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:03:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:03:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:03:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:03:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:10.128.2.77,StartTime:2021-05-11 18:03:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:03:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://803fc10e7b7360bff174ad229b3270ae4cb44caf489c1a2fa361b5ec9ae7306e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:54.032: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-969" for this suite.


• [SLOW TEST:9.541 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":-1,"completed":15,"skipped":382,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:03:54.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08950566-5e19-402d-b293-9c90138caa3e" in namespace "projected-7094" to be "Succeeded or Failed"
May 11 18:03:54.812: INFO: Pod "downwardapi-volume-08950566-5e19-402d-b293-9c90138caa3e": Phase="Pending", Reason="", readiness=false. Elapsed: 65.41685ms
May 11 18:03:56.878: INFO: Pod "downwardapi-volume-08950566-5e19-402d-b293-9c90138caa3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131042478s
May 11 18:03:58.943: INFO: Pod "downwardapi-volume-08950566-5e19-402d-b293-9c90138caa3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.1968983s
STEP: Saw pod success
May 11 18:03:58.943: INFO: Pod "downwardapi-volume-08950566-5e19-402d-b293-9c90138caa3e" satisfied condition "Succeeded or Failed"
May 11 18:03:59.012: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod downwardapi-volume-08950566-5e19-402d-b293-9c90138caa3e container client-container: <nil>
STEP: delete the pod
May 11 18:03:59.159: INFO: Waiting for pod downwardapi-volume-08950566-5e19-402d-b293-9c90138caa3e to disappear
May 11 18:03:59.223: INFO: Pod downwardapi-volume-08950566-5e19-402d-b293-9c90138caa3e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:59.224: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7094" for this suite.


• [SLOW TEST:5.168 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":16,"skipped":392,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:03:59.526: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-5139" for this suite.


• [SLOW TEST:7.921 seconds]
[sig-api-machinery] ResourceQuota
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":-1,"completed":38,"skipped":788,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
May 11 18:04:00.386: INFO: Waiting up to 5m0s for pod "client-containers-86e467e7-1757-4b83-9c44-95e006c2c4e9" in namespace "containers-8978" to be "Succeeded or Failed"
May 11 18:04:00.451: INFO: Pod "client-containers-86e467e7-1757-4b83-9c44-95e006c2c4e9": Phase="Pending", Reason="", readiness=false. Elapsed: 65.124886ms
May 11 18:04:02.517: INFO: Pod "client-containers-86e467e7-1757-4b83-9c44-95e006c2c4e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130925716s
May 11 18:04:04.583: INFO: Pod "client-containers-86e467e7-1757-4b83-9c44-95e006c2c4e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.196516939s
STEP: Saw pod success
May 11 18:04:04.583: INFO: Pod "client-containers-86e467e7-1757-4b83-9c44-95e006c2c4e9" satisfied condition "Succeeded or Failed"
May 11 18:04:04.648: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod client-containers-86e467e7-1757-4b83-9c44-95e006c2c4e9 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:04:04.792: INFO: Waiting for pod client-containers-86e467e7-1757-4b83-9c44-95e006c2c4e9 to disappear
May 11 18:04:04.857: INFO: Pod client-containers-86e467e7-1757-4b83-9c44-95e006c2c4e9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:04.857: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-8978" for this suite.


• [SLOW TEST:5.183 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":-1,"completed":39,"skipped":798,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 11 18:03:36.693: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6100 PodName:var-expansion-fcf4620c-f893-472c-b623-523cb201d196 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: test for file in mounted path
May 11 18:03:37.269: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6100 PodName:var-expansion-fcf4620c-f893-472c-b623-523cb201d196 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
STEP: updating the annotation value
May 11 18:03:38.431: INFO: Successfully updated pod "var-expansion-fcf4620c-f893-472c-b623-523cb201d196"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 11 18:03:38.496: INFO: Deleting pod "var-expansion-fcf4620c-f893-472c-b623-523cb201d196" in namespace "var-expansion-6100"
May 11 18:03:38.568: INFO: Wait up to 5m0s for pod "var-expansion-fcf4620c-f893-472c-b623-523cb201d196" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:12.697: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-6100" for this suite.


• [SLOW TEST:40.877 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":-1,"completed":30,"skipped":553,"failed":0}

SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-a673be60-61a3-4709-af33-d611cf087d3c
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:13.360: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-6286" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":-1,"completed":31,"skipped":573,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-06f2df9e-22d7-4213-9ebf-a323c04b8caf
STEP: Creating a pod to test consume secrets
May 11 18:04:14.013: INFO: Waiting up to 5m0s for pod "pod-secrets-a147322b-34cf-4778-aa5e-31cb04c07e80" in namespace "secrets-5760" to be "Succeeded or Failed"
May 11 18:04:14.078: INFO: Pod "pod-secrets-a147322b-34cf-4778-aa5e-31cb04c07e80": Phase="Pending", Reason="", readiness=false. Elapsed: 64.228022ms
May 11 18:04:16.142: INFO: Pod "pod-secrets-a147322b-34cf-4778-aa5e-31cb04c07e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129002676s
May 11 18:04:18.208: INFO: Pod "pod-secrets-a147322b-34cf-4778-aa5e-31cb04c07e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.194195677s
STEP: Saw pod success
May 11 18:04:18.208: INFO: Pod "pod-secrets-a147322b-34cf-4778-aa5e-31cb04c07e80" satisfied condition "Succeeded or Failed"
May 11 18:04:18.282: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-secrets-a147322b-34cf-4778-aa5e-31cb04c07e80 container secret-volume-test: <nil>
STEP: delete the pod
May 11 18:04:18.446: INFO: Waiting for pod pod-secrets-a147322b-34cf-4778-aa5e-31cb04c07e80 to disappear
May 11 18:04:18.510: INFO: Pod pod-secrets-a147322b-34cf-4778-aa5e-31cb04c07e80 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:18.510: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-5760" for this suite.


• [SLOW TEST:5.254 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":32,"skipped":597,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4982.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4982.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 18:04:20.355: INFO: DNS probes using dns-4982/dns-test-4c4b2395-c8ec-4391-8dce-bcae7e5071db succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:20.434: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-4982" for this suite.


• [SLOW TEST:15.534 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":-1,"completed":40,"skipped":820,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
May 11 18:04:01.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:04:03.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:04:05.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:04:07.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:04:09.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:04:11.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:04:13.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:04:15.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353040, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:04:17.933: INFO: Waited 363.063176ms for the sample-apiserver to be ready to handle requests.
I0511 18:04:19.472912   45012 request.go:655] Throttling request took 1.049835954s, request: GET:https://api.ci-op-4m5ks5p8-9de3b.origin-ci-int-aws.dev.rhcloud.com:6443/apis/project.openshift.io/v1?timeout=32s
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:21.930: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "aggregator-7833" for this suite.


• [SLOW TEST:22.584 seconds]
[sig-api-machinery] Aggregator
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":-1,"completed":17,"skipped":409,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-75b4ddf2-0a40-40cd-b305-d4610d9519e8
STEP: Creating a pod to test consume configMaps
May 11 18:04:22.595: INFO: Waiting up to 5m0s for pod "pod-configmaps-48727d27-8c78-4d5b-b27a-2eacb7d2db4f" in namespace "configmap-2440" to be "Succeeded or Failed"
May 11 18:04:22.664: INFO: Pod "pod-configmaps-48727d27-8c78-4d5b-b27a-2eacb7d2db4f": Phase="Pending", Reason="", readiness=false. Elapsed: 68.385187ms
May 11 18:04:24.737: INFO: Pod "pod-configmaps-48727d27-8c78-4d5b-b27a-2eacb7d2db4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.142101862s
May 11 18:04:26.808: INFO: Pod "pod-configmaps-48727d27-8c78-4d5b-b27a-2eacb7d2db4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.212279631s
STEP: Saw pod success
May 11 18:04:26.808: INFO: Pod "pod-configmaps-48727d27-8c78-4d5b-b27a-2eacb7d2db4f" satisfied condition "Succeeded or Failed"
May 11 18:04:26.873: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-configmaps-48727d27-8c78-4d5b-b27a-2eacb7d2db4f container agnhost-container: <nil>
STEP: delete the pod
May 11 18:04:27.022: INFO: Waiting for pod pod-configmaps-48727d27-8c78-4d5b-b27a-2eacb7d2db4f to disappear
May 11 18:04:27.087: INFO: Pod pod-configmaps-48727d27-8c78-4d5b-b27a-2eacb7d2db4f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:27.087: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2440" for this suite.


• [SLOW TEST:5.256 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":-1,"completed":18,"skipped":422,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:04:21.209: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2747cb85-f4e1-4c6b-a80c-b5601fa41e07
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-2747cb85-f4e1-4c6b-a80c-b5601fa41e07
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:27.899: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3459" for this suite.


• [SLOW TEST:7.442 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":41,"skipped":831,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-de8d24e9-eaf1-44bc-82b0-f49cf876fa09
STEP: Creating a pod to test consume configMaps
May 11 18:04:28.655: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f5071af0-2f5b-4084-8b15-936c86f85952" in namespace "projected-7029" to be "Succeeded or Failed"
May 11 18:04:28.725: INFO: Pod "pod-projected-configmaps-f5071af0-2f5b-4084-8b15-936c86f85952": Phase="Pending", Reason="", readiness=false. Elapsed: 69.861418ms
May 11 18:04:30.790: INFO: Pod "pod-projected-configmaps-f5071af0-2f5b-4084-8b15-936c86f85952": Phase="Pending", Reason="", readiness=false. Elapsed: 2.135170205s
May 11 18:04:32.856: INFO: Pod "pod-projected-configmaps-f5071af0-2f5b-4084-8b15-936c86f85952": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.200827602s
STEP: Saw pod success
May 11 18:04:32.856: INFO: Pod "pod-projected-configmaps-f5071af0-2f5b-4084-8b15-936c86f85952" satisfied condition "Succeeded or Failed"
May 11 18:04:32.922: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-projected-configmaps-f5071af0-2f5b-4084-8b15-936c86f85952 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:04:33.074: INFO: Waiting for pod pod-projected-configmaps-f5071af0-2f5b-4084-8b15-936c86f85952 to disappear
May 11 18:04:33.139: INFO: Pod pod-projected-configmaps-f5071af0-2f5b-4084-8b15-936c86f85952 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:33.139: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7029" for this suite.


• [SLOW TEST:5.234 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2449.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2449.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2449.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2449.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2449.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2449.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 18:04:34.513: INFO: DNS probes using dns-2449/dns-test-6e883cba-a349-4a7a-8616-bb732c899f45 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:34.591: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-2449" for this suite.


• [SLOW TEST:7.496 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":-1,"completed":19,"skipped":439,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":42,"skipped":845,"failed":0}
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:04:33.876: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3c391fa8-f46c-4a04-9aa4-67914da7183f" in namespace "security-context-test-7559" to be "Succeeded or Failed"
May 11 18:04:33.942: INFO: Pod "busybox-readonly-false-3c391fa8-f46c-4a04-9aa4-67914da7183f": Phase="Pending", Reason="", readiness=false. Elapsed: 66.222016ms
May 11 18:04:36.008: INFO: Pod "busybox-readonly-false-3c391fa8-f46c-4a04-9aa4-67914da7183f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132130654s
May 11 18:04:38.074: INFO: Pod "busybox-readonly-false-3c391fa8-f46c-4a04-9aa4-67914da7183f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.197893029s
May 11 18:04:38.074: INFO: Pod "busybox-readonly-false-3c391fa8-f46c-4a04-9aa4-67914da7183f" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:38.074: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-7559" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":-1,"completed":43,"skipped":845,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2666
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2666
STEP: creating replication controller externalsvc in namespace services-2666
I0511 18:04:19.348960   45011 runners.go:190] Created replication controller with name: externalsvc, namespace: services-2666, replica count: 2
I0511 18:04:22.449259   45011 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:04:25.449435   45011 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 11 18:04:25.683: INFO: Creating new exec pod
May 11 18:04:29.999: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2666 exec execpodmwrph -- /bin/sh -x -c nslookup nodeport-service.services-2666.svc.cluster.local'
May 11 18:04:30.779: INFO: stderr: "+ nslookup nodeport-service.services-2666.svc.cluster.local\n"
May 11 18:04:30.779: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-2666.svc.cluster.local\tcanonical name = externalsvc.services-2666.svc.cluster.local.\nName:\texternalsvc.services-2666.svc.cluster.local\nAddress: 172.30.231.183\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2666, will wait for the garbage collector to delete the pods
May 11 18:04:31.015: INFO: Deleting ReplicationController externalsvc took: 68.428834ms
May 11 18:04:31.115: INFO: Terminating ReplicationController externalsvc pods took: 100.150774ms
May 11 18:04:38.805: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:38.881: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-2666" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:20.345 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":-1,"completed":33,"skipped":622,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-6ab771ad-3259-484d-8731-c881dfe9860b
STEP: Creating a pod to test consume configMaps
May 11 18:04:35.350: INFO: Waiting up to 5m0s for pod "pod-configmaps-e504efdb-4410-4bca-8767-cad956b25902" in namespace "configmap-8360" to be "Succeeded or Failed"
May 11 18:04:35.419: INFO: Pod "pod-configmaps-e504efdb-4410-4bca-8767-cad956b25902": Phase="Pending", Reason="", readiness=false. Elapsed: 68.347281ms
May 11 18:04:37.484: INFO: Pod "pod-configmaps-e504efdb-4410-4bca-8767-cad956b25902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133415287s
May 11 18:04:39.549: INFO: Pod "pod-configmaps-e504efdb-4410-4bca-8767-cad956b25902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.198599328s
STEP: Saw pod success
May 11 18:04:39.549: INFO: Pod "pod-configmaps-e504efdb-4410-4bca-8767-cad956b25902" satisfied condition "Succeeded or Failed"
May 11 18:04:39.618: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-configmaps-e504efdb-4410-4bca-8767-cad956b25902 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:04:39.768: INFO: Waiting for pod pod-configmaps-e504efdb-4410-4bca-8767-cad956b25902 to disappear
May 11 18:04:39.834: INFO: Pod pod-configmaps-e504efdb-4410-4bca-8767-cad956b25902 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:39.834: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-8360" for this suite.


• [SLOW TEST:5.219 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":20,"skipped":475,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:04:38.760: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff9d317e-dc67-4a2b-8c88-4135477fbc7b" in namespace "projected-1339" to be "Succeeded or Failed"
May 11 18:04:38.825: INFO: Pod "downwardapi-volume-ff9d317e-dc67-4a2b-8c88-4135477fbc7b": Phase="Pending", Reason="", readiness=false. Elapsed: 65.658523ms
May 11 18:04:40.892: INFO: Pod "downwardapi-volume-ff9d317e-dc67-4a2b-8c88-4135477fbc7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132336381s
May 11 18:04:42.961: INFO: Pod "downwardapi-volume-ff9d317e-dc67-4a2b-8c88-4135477fbc7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.201603479s
STEP: Saw pod success
May 11 18:04:42.961: INFO: Pod "downwardapi-volume-ff9d317e-dc67-4a2b-8c88-4135477fbc7b" satisfied condition "Succeeded or Failed"
May 11 18:04:43.027: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod downwardapi-volume-ff9d317e-dc67-4a2b-8c88-4135477fbc7b container client-container: <nil>
STEP: delete the pod
May 11 18:04:43.178: INFO: Waiting for pod downwardapi-volume-ff9d317e-dc67-4a2b-8c88-4135477fbc7b to disappear
May 11 18:04:43.245: INFO: Pod downwardapi-volume-ff9d317e-dc67-4a2b-8c88-4135477fbc7b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:43.245: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1339" for this suite.


• [SLOW TEST:5.169 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":-1,"completed":44,"skipped":857,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 11 18:04:39.569: INFO: Waiting up to 5m0s for pod "pod-6f69c960-bda6-4570-8c8c-ce7b9b0cfb57" in namespace "emptydir-7685" to be "Succeeded or Failed"
May 11 18:04:39.633: INFO: Pod "pod-6f69c960-bda6-4570-8c8c-ce7b9b0cfb57": Phase="Pending", Reason="", readiness=false. Elapsed: 64.193018ms
May 11 18:04:41.698: INFO: Pod "pod-6f69c960-bda6-4570-8c8c-ce7b9b0cfb57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129200796s
May 11 18:04:43.773: INFO: Pod "pod-6f69c960-bda6-4570-8c8c-ce7b9b0cfb57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.203949176s
STEP: Saw pod success
May 11 18:04:43.773: INFO: Pod "pod-6f69c960-bda6-4570-8c8c-ce7b9b0cfb57" satisfied condition "Succeeded or Failed"
May 11 18:04:43.838: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-6f69c960-bda6-4570-8c8c-ce7b9b0cfb57 container test-container: <nil>
STEP: delete the pod
May 11 18:04:44.005: INFO: Waiting for pod pod-6f69c960-bda6-4570-8c8c-ce7b9b0cfb57 to disappear
May 11 18:04:44.069: INFO: Pod pod-6f69c960-bda6-4570-8c8c-ce7b9b0cfb57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:44.069: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-7685" for this suite.


• [SLOW TEST:5.222 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":34,"skipped":653,"failed":0}
[BeforeEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
May 11 18:04:44.804: INFO: created test-podtemplate-1
May 11 18:04:44.870: INFO: created test-podtemplate-2
May 11 18:04:44.936: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 11 18:04:45.002: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 11 18:04:45.092: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:45.160: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "podtemplate-7830" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":-1,"completed":35,"skipped":653,"failed":0}
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 11 18:04:45.725: INFO: Waiting up to 5m0s for pod "pod-f37700a0-ccc6-425d-92d1-e8050c22256a" in namespace "emptydir-5832" to be "Succeeded or Failed"
May 11 18:04:45.789: INFO: Pod "pod-f37700a0-ccc6-425d-92d1-e8050c22256a": Phase="Pending", Reason="", readiness=false. Elapsed: 64.293554ms
May 11 18:04:47.854: INFO: Pod "pod-f37700a0-ccc6-425d-92d1-e8050c22256a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129167602s
May 11 18:04:49.919: INFO: Pod "pod-f37700a0-ccc6-425d-92d1-e8050c22256a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.193784454s
STEP: Saw pod success
May 11 18:04:49.919: INFO: Pod "pod-f37700a0-ccc6-425d-92d1-e8050c22256a" satisfied condition "Succeeded or Failed"
May 11 18:04:49.986: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-f37700a0-ccc6-425d-92d1-e8050c22256a container test-container: <nil>
STEP: delete the pod
May 11 18:04:50.132: INFO: Waiting for pod pod-f37700a0-ccc6-425d-92d1-e8050c22256a to disappear
May 11 18:04:50.196: INFO: Pod pod-f37700a0-ccc6-425d-92d1-e8050c22256a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:50.196: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5832" for this suite.


• [SLOW TEST:5.157 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":36,"skipped":653,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 11 18:04:52.619: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 11 18:04:52.685: INFO: Pod pod-with-poststart-http-hook still exists
May 11 18:04:54.685: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 11 18:04:54.751: INFO: Pod pod-with-poststart-http-hook still exists
May 11 18:04:56.685: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 11 18:04:56.751: INFO: Pod pod-with-poststart-http-hook still exists
May 11 18:04:58.685: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 11 18:04:58.751: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:04:58.751: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9760" for this suite.


• [SLOW TEST:15.484 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":-1,"completed":45,"skipped":868,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 11 18:05:04.342: INFO: Successfully updated pod "annotationupdate8c5ff384-f268-4b61-acd2-6964b9e50cad"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:06.659: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-221" for this suite.


• [SLOW TEST:8.082 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":-1,"completed":46,"skipped":878,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9160.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9160.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9160.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9160.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 18:05:03.215: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-0e01708f-4b7d-481a-be27-d557471da1f5: the server could not find the requested resource (get pods dns-test-0e01708f-4b7d-481a-be27-d557471da1f5)
May 11 18:05:03.285: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-0e01708f-4b7d-481a-be27-d557471da1f5: the server could not find the requested resource (get pods dns-test-0e01708f-4b7d-481a-be27-d557471da1f5)
May 11 18:05:03.352: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-0e01708f-4b7d-481a-be27-d557471da1f5: the server could not find the requested resource (get pods dns-test-0e01708f-4b7d-481a-be27-d557471da1f5)
May 11 18:05:03.622: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-0e01708f-4b7d-481a-be27-d557471da1f5: the server could not find the requested resource (get pods dns-test-0e01708f-4b7d-481a-be27-d557471da1f5)
May 11 18:05:03.758: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-0e01708f-4b7d-481a-be27-d557471da1f5: the server could not find the requested resource (get pods dns-test-0e01708f-4b7d-481a-be27-d557471da1f5)
May 11 18:05:03.961: INFO: Lookups using dns-9160/dns-test-0e01708f-4b7d-481a-be27-d557471da1f5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9160.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local jessie_udp@dns-test-service-2.dns-9160.svc.cluster.local]

May 11 18:05:09.772: INFO: DNS probes using dns-9160/dns-test-0e01708f-4b7d-481a-be27-d557471da1f5 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:09.928: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-9160" for this suite.


• [SLOW TEST:19.717 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":-1,"completed":37,"skipped":669,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-c9ecec04-ae60-4337-8019-2c376fba0492
STEP: Creating a pod to test consume configMaps
May 11 18:05:10.679: INFO: Waiting up to 5m0s for pod "pod-configmaps-a035eafc-ccdc-41d6-80e6-e336a24f787a" in namespace "configmap-2032" to be "Succeeded or Failed"
May 11 18:05:10.744: INFO: Pod "pod-configmaps-a035eafc-ccdc-41d6-80e6-e336a24f787a": Phase="Pending", Reason="", readiness=false. Elapsed: 64.484052ms
May 11 18:05:12.809: INFO: Pod "pod-configmaps-a035eafc-ccdc-41d6-80e6-e336a24f787a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12925863s
May 11 18:05:14.874: INFO: Pod "pod-configmaps-a035eafc-ccdc-41d6-80e6-e336a24f787a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.194418061s
STEP: Saw pod success
May 11 18:05:14.874: INFO: Pod "pod-configmaps-a035eafc-ccdc-41d6-80e6-e336a24f787a" satisfied condition "Succeeded or Failed"
May 11 18:05:14.938: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-configmaps-a035eafc-ccdc-41d6-80e6-e336a24f787a container configmap-volume-test: <nil>
STEP: delete the pod
May 11 18:05:15.084: INFO: Waiting for pod pod-configmaps-a035eafc-ccdc-41d6-80e6-e336a24f787a to disappear
May 11 18:05:15.152: INFO: Pod pod-configmaps-a035eafc-ccdc-41d6-80e6-e336a24f787a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:15.152: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2032" for this suite.


• [SLOW TEST:5.208 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":-1,"completed":38,"skipped":686,"failed":0}

SSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:05:11.887: INFO: Deleting pod "var-expansion-4862f4d6-0cd9-476f-a57b-11dd7c6519c7" in namespace "var-expansion-934"
May 11 18:05:11.959: INFO: Wait up to 5m0s for pod "var-expansion-4862f4d6-0cd9-476f-a57b-11dd7c6519c7" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:20.091: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-934" for this suite.


• [SLOW TEST:13.012 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":-1,"completed":47,"skipped":885,"failed":0}

SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 11 18:05:20.573: INFO: Successfully updated pod "adopt-release-62z4v"
STEP: Checking that the Job readopts the Pod
May 11 18:05:20.573: INFO: Waiting up to 15m0s for pod "adopt-release-62z4v" in namespace "job-9376" to be "adopted"
May 11 18:05:20.638: INFO: Pod "adopt-release-62z4v": Phase="Running", Reason="", readiness=true. Elapsed: 64.199262ms
May 11 18:05:20.638: INFO: Pod "adopt-release-62z4v" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 11 18:05:21.276: INFO: Successfully updated pod "adopt-release-62z4v"
STEP: Checking that the Job releases the Pod
May 11 18:05:21.277: INFO: Waiting up to 15m0s for pod "adopt-release-62z4v" in namespace "job-9376" to be "released"
May 11 18:05:21.344: INFO: Pod "adopt-release-62z4v": Phase="Running", Reason="", readiness=true. Elapsed: 67.58781ms
May 11 18:05:21.344: INFO: Pod "adopt-release-62z4v" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:21.344: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-9376" for this suite.


• [SLOW TEST:6.192 seconds]
[sig-apps] Job
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":-1,"completed":39,"skipped":690,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-a3c3c1ee-a2bc-4b14-93ff-565e95a21428
STEP: Creating a pod to test consume configMaps
May 11 18:05:20.851: INFO: Waiting up to 5m0s for pod "pod-configmaps-eab848ab-04af-4ef5-b738-32e37ee3f690" in namespace "configmap-9972" to be "Succeeded or Failed"
May 11 18:05:20.916: INFO: Pod "pod-configmaps-eab848ab-04af-4ef5-b738-32e37ee3f690": Phase="Pending", Reason="", readiness=false. Elapsed: 65.498078ms
May 11 18:05:22.982: INFO: Pod "pod-configmaps-eab848ab-04af-4ef5-b738-32e37ee3f690": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13145231s
May 11 18:05:25.052: INFO: Pod "pod-configmaps-eab848ab-04af-4ef5-b738-32e37ee3f690": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.200672566s
STEP: Saw pod success
May 11 18:05:25.318: INFO: Pod "pod-configmaps-eab848ab-04af-4ef5-b738-32e37ee3f690" satisfied condition "Succeeded or Failed"
May 11 18:05:25.385: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-configmaps-eab848ab-04af-4ef5-b738-32e37ee3f690 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:05:25.535: INFO: Waiting for pod pod-configmaps-eab848ab-04af-4ef5-b738-32e37ee3f690 to disappear
May 11 18:05:25.601: INFO: Pod pod-configmaps-eab848ab-04af-4ef5-b738-32e37ee3f690 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:25.601: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-9972" for this suite.


• [SLOW TEST:5.500 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":48,"skipped":905,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:05:27.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353127, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353127, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353127, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353127, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:05:29.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353127, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353127, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353127, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353127, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:05:32.770: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:33.683: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-3869" for this suite.
STEP: Destroying namespace "webhook-3869-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:8.497 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":-1,"completed":49,"skipped":917,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 11 18:03:37.552: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
May 11 18:04:30.210: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:36.235: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5293" for this suite.


• [SLOW TEST:127.524 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":-1,"completed":16,"skipped":256,"failed":0}

SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
May 11 18:05:36.868: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-22 proxy --unix-socket=/tmp/kubectl-proxy-unix482183700/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:36.916: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-22" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":-1,"completed":17,"skipped":277,"failed":0}
[BeforeEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:37.993: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-5517" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":-1,"completed":18,"skipped":277,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 11 18:05:34.981: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 11 18:05:34.981: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 11 18:05:34.981: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 11 18:05:34.981: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 11 18:05:34.981: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 11 18:05:34.981: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 11 18:05:34.981: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 11 18:05:34.981: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 11 18:05:37.503: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 11 18:05:37.503: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 11 18:05:37.959: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 11 18:05:38.095: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 0
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 2
May 11 18:05:38.159: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 2
May 11 18:05:38.221: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 2
May 11 18:05:38.221: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 2
May 11 18:05:38.221: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 2
May 11 18:05:38.221: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 2
May 11 18:05:38.221: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 2
May 11 18:05:38.221: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 2
May 11 18:05:38.221: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1
STEP: listing Deployments
May 11 18:05:38.299: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 11 18:05:38.444: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 11 18:05:38.578: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 11 18:05:38.578: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 11 18:05:38.578: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 11 18:05:38.578: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 11 18:05:38.578: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 11 18:05:38.578: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 11 18:05:41.914: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1
May 11 18:05:41.914: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1
May 11 18:05:41.914: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1
May 11 18:05:41.914: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1
May 11 18:05:41.914: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1
May 11 18:05:41.914: INFO: observed Deployment test-deployment in namespace deployment-5247 with ReadyReplicas 1
STEP: deleting the Deployment
May 11 18:05:42.055: INFO: observed event type MODIFIED
May 11 18:05:42.055: INFO: observed event type MODIFIED
May 11 18:05:42.055: INFO: observed event type MODIFIED
May 11 18:05:42.055: INFO: observed event type MODIFIED
May 11 18:05:42.055: INFO: observed event type MODIFIED
May 11 18:05:42.055: INFO: observed event type MODIFIED
May 11 18:05:42.056: INFO: observed event type MODIFIED
May 11 18:05:42.056: INFO: observed event type MODIFIED
May 11 18:05:42.056: INFO: observed event type MODIFIED
May 11 18:05:42.056: INFO: observed event type MODIFIED
May 11 18:05:42.056: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 11 18:05:42.122: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:42.187: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-5247" for this suite.


• [SLOW TEST:8.007 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":-1,"completed":50,"skipped":934,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:05:38.619: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a5db001-3409-4c47-8850-ffb2ce55900e" in namespace "projected-8783" to be "Succeeded or Failed"
May 11 18:05:38.685: INFO: Pod "downwardapi-volume-7a5db001-3409-4c47-8850-ffb2ce55900e": Phase="Pending", Reason="", readiness=false. Elapsed: 66.557327ms
May 11 18:05:40.752: INFO: Pod "downwardapi-volume-7a5db001-3409-4c47-8850-ffb2ce55900e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133555533s
May 11 18:05:42.820: INFO: Pod "downwardapi-volume-7a5db001-3409-4c47-8850-ffb2ce55900e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.201094223s
STEP: Saw pod success
May 11 18:05:42.820: INFO: Pod "downwardapi-volume-7a5db001-3409-4c47-8850-ffb2ce55900e" satisfied condition "Succeeded or Failed"
May 11 18:05:42.886: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downwardapi-volume-7a5db001-3409-4c47-8850-ffb2ce55900e container client-container: <nil>
STEP: delete the pod
May 11 18:05:43.037: INFO: Waiting for pod downwardapi-volume-7a5db001-3409-4c47-8850-ffb2ce55900e to disappear
May 11 18:05:43.103: INFO: Pod downwardapi-volume-7a5db001-3409-4c47-8850-ffb2ce55900e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:43.103: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8783" for this suite.


• [SLOW TEST:5.179 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":19,"skipped":323,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 11 18:05:42.822: INFO: Waiting up to 5m0s for pod "pod-e20241b8-7894-4811-ad37-38cf6626dd16" in namespace "emptydir-2171" to be "Succeeded or Failed"
May 11 18:05:42.890: INFO: Pod "pod-e20241b8-7894-4811-ad37-38cf6626dd16": Phase="Pending", Reason="", readiness=false. Elapsed: 68.729294ms
May 11 18:05:44.968: INFO: Pod "pod-e20241b8-7894-4811-ad37-38cf6626dd16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.146171763s
May 11 18:05:47.038: INFO: Pod "pod-e20241b8-7894-4811-ad37-38cf6626dd16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.216609038s
STEP: Saw pod success
May 11 18:05:47.038: INFO: Pod "pod-e20241b8-7894-4811-ad37-38cf6626dd16" satisfied condition "Succeeded or Failed"
May 11 18:05:47.111: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-e20241b8-7894-4811-ad37-38cf6626dd16 container test-container: <nil>
STEP: delete the pod
May 11 18:05:47.267: INFO: Waiting for pod pod-e20241b8-7894-4811-ad37-38cf6626dd16 to disappear
May 11 18:05:47.332: INFO: Pod pod-e20241b8-7894-4811-ad37-38cf6626dd16 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:47.332: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-2171" for this suite.


• [SLOW TEST:5.194 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":51,"skipped":941,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-rql9
STEP: Creating a pod to test atomic-volume-subpath
May 11 18:05:22.154: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-rql9" in namespace "subpath-9383" to be "Succeeded or Failed"
May 11 18:05:22.220: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Pending", Reason="", readiness=false. Elapsed: 65.667809ms
May 11 18:05:24.288: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133846805s
May 11 18:05:26.352: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Running", Reason="", readiness=true. Elapsed: 4.198093783s
May 11 18:05:28.417: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Running", Reason="", readiness=true. Elapsed: 6.263166985s
May 11 18:05:30.486: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Running", Reason="", readiness=true. Elapsed: 8.331263884s
May 11 18:05:32.550: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Running", Reason="", readiness=true. Elapsed: 10.395955778s
May 11 18:05:34.618: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Running", Reason="", readiness=true. Elapsed: 12.46379635s
May 11 18:05:36.688: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Running", Reason="", readiness=true. Elapsed: 14.533762202s
May 11 18:05:38.753: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Running", Reason="", readiness=true. Elapsed: 16.598395367s
May 11 18:05:40.817: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Running", Reason="", readiness=true. Elapsed: 18.662738245s
May 11 18:05:42.883: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Running", Reason="", readiness=true. Elapsed: 20.728692791s
May 11 18:05:44.949: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Running", Reason="", readiness=true. Elapsed: 22.794683351s
May 11 18:05:47.014: INFO: Pod "pod-subpath-test-secret-rql9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.859342573s
STEP: Saw pod success
May 11 18:05:47.014: INFO: Pod "pod-subpath-test-secret-rql9" satisfied condition "Succeeded or Failed"
May 11 18:05:47.083: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-subpath-test-secret-rql9 container test-container-subpath-secret-rql9: <nil>
STEP: delete the pod
May 11 18:05:47.241: INFO: Waiting for pod pod-subpath-test-secret-rql9 to disappear
May 11 18:05:47.305: INFO: Pod pod-subpath-test-secret-rql9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-rql9
May 11 18:05:47.305: INFO: Deleting pod "pod-subpath-test-secret-rql9" in namespace "subpath-9383"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:47.369: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-9383" for this suite.


• [SLOW TEST:26.022 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
S
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":-1,"completed":40,"skipped":698,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 11 18:05:51.435: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:51.574: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-9392" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":-1,"completed":41,"skipped":732,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
May 11 18:05:52.292: INFO: created test-event-1
May 11 18:05:52.358: INFO: created test-event-2
May 11 18:05:52.424: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 11 18:05:52.493: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 11 18:05:52.577: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:05:52.641: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-4156" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":-1,"completed":42,"skipped":776,"failed":0}

SSSS
------------------------------
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-ssg7d in namespace proxy-7340
I0511 18:05:43.920654   45016 runners.go:190] Created replication controller with name: proxy-service-ssg7d, namespace: proxy-7340, replica count: 1
I0511 18:05:45.021123   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:05:46.021295   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:05:47.021473   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 18:05:48.021640   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 18:05:49.021784   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 18:05:50.021930   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 18:05:51.022075   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 18:05:52.022227   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 18:05:53.022426   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 18:05:54.022601   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 18:05:55.022751   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 18:05:56.022927   45016 runners.go:190] proxy-service-ssg7d Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 18:05:56.093: INFO: setup took 12.321623619s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 11 18:05:56.165: INFO: (0) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 71.526456ms)
May 11 18:05:56.165: INFO: (0) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 71.671135ms)
May 11 18:05:56.165: INFO: (0) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 71.785068ms)
May 11 18:05:56.166: INFO: (0) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 72.385675ms)
May 11 18:05:56.167: INFO: (0) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 73.188396ms)
May 11 18:05:56.229: INFO: (0) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 135.693666ms)
May 11 18:05:56.229: INFO: (0) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 135.628461ms)
May 11 18:05:56.229: INFO: (0) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 135.73347ms)
May 11 18:05:56.229: INFO: (0) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 135.738051ms)
May 11 18:05:56.229: INFO: (0) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 135.673073ms)
May 11 18:05:56.229: INFO: (0) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 135.738235ms)
May 11 18:05:56.229: INFO: (0) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 135.696875ms)
May 11 18:05:56.229: INFO: (0) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 135.704064ms)
May 11 18:05:56.229: INFO: (0) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 135.803407ms)
May 11 18:05:56.230: INFO: (0) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 136.549914ms)
May 11 18:05:56.230: INFO: (0) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 136.655229ms)
May 11 18:05:56.301: INFO: (1) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 70.921784ms)
May 11 18:05:56.302: INFO: (1) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 71.949112ms)
May 11 18:05:56.302: INFO: (1) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 72.147543ms)
May 11 18:05:56.302: INFO: (1) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 72.117412ms)
May 11 18:05:56.303: INFO: (1) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 72.681919ms)
May 11 18:05:56.303: INFO: (1) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 72.857396ms)
May 11 18:05:56.303: INFO: (1) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.876148ms)
May 11 18:05:56.303: INFO: (1) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.958479ms)
May 11 18:05:56.304: INFO: (1) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 73.63942ms)
May 11 18:05:56.305: INFO: (1) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 74.594939ms)
May 11 18:05:56.305: INFO: (1) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 74.900046ms)
May 11 18:05:56.305: INFO: (1) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 74.907775ms)
May 11 18:05:56.365: INFO: (1) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 135.097293ms)
May 11 18:05:56.365: INFO: (1) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 135.217251ms)
May 11 18:05:56.365: INFO: (1) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 135.138723ms)
May 11 18:05:56.365: INFO: (1) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 135.152717ms)
May 11 18:05:56.447: INFO: (2) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 81.738138ms)
May 11 18:05:56.448: INFO: (2) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 82.619428ms)
May 11 18:05:56.448: INFO: (2) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 82.618722ms)
May 11 18:05:56.451: INFO: (2) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 85.309458ms)
May 11 18:05:56.453: INFO: (2) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 87.691837ms)
May 11 18:05:56.454: INFO: (2) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 88.834597ms)
May 11 18:05:56.457: INFO: (2) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 91.073076ms)
May 11 18:05:56.457: INFO: (2) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 91.745195ms)
May 11 18:05:56.457: INFO: (2) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 91.932773ms)
May 11 18:05:56.457: INFO: (2) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 91.96204ms)
May 11 18:05:56.458: INFO: (2) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 92.031201ms)
May 11 18:05:56.458: INFO: (2) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 92.058876ms)
May 11 18:05:56.458: INFO: (2) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 92.141718ms)
May 11 18:05:56.460: INFO: (2) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 94.283615ms)
May 11 18:05:56.460: INFO: (2) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 94.76272ms)
May 11 18:05:56.462: INFO: (2) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 96.686337ms)
May 11 18:05:56.546: INFO: (3) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 83.264532ms)
May 11 18:05:56.549: INFO: (3) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 86.957908ms)
May 11 18:05:56.549: INFO: (3) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 86.980443ms)
May 11 18:05:56.549: INFO: (3) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 87.124005ms)
May 11 18:05:56.549: INFO: (3) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 87.063648ms)
May 11 18:05:56.552: INFO: (3) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 89.517412ms)
May 11 18:05:56.552: INFO: (3) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 89.610651ms)
May 11 18:05:56.552: INFO: (3) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 89.530229ms)
May 11 18:05:56.552: INFO: (3) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 89.574532ms)
May 11 18:05:56.556: INFO: (3) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 94.058611ms)
May 11 18:05:56.559: INFO: (3) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 96.527131ms)
May 11 18:05:56.561: INFO: (3) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 98.723943ms)
May 11 18:05:56.561: INFO: (3) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 98.844212ms)
May 11 18:05:56.564: INFO: (3) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 101.256416ms)
May 11 18:05:56.564: INFO: (3) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 101.217698ms)
May 11 18:05:56.564: INFO: (3) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 101.24904ms)
May 11 18:05:56.636: INFO: (4) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 72.011972ms)
May 11 18:05:56.636: INFO: (4) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 72.562757ms)
May 11 18:05:56.636: INFO: (4) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.737556ms)
May 11 18:05:56.637: INFO: (4) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 73.094842ms)
May 11 18:05:56.639: INFO: (4) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 75.33193ms)
May 11 18:05:56.640: INFO: (4) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 76.121821ms)
May 11 18:05:56.640: INFO: (4) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 76.184688ms)
May 11 18:05:56.641: INFO: (4) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 76.815499ms)
May 11 18:05:56.646: INFO: (4) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 82.657022ms)
May 11 18:05:56.646: INFO: (4) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 82.684341ms)
May 11 18:05:56.647: INFO: (4) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 82.86297ms)
May 11 18:05:56.647: INFO: (4) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 82.905709ms)
May 11 18:05:56.647: INFO: (4) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 82.925829ms)
May 11 18:05:56.647: INFO: (4) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 83.18412ms)
May 11 18:05:56.647: INFO: (4) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 83.393216ms)
May 11 18:05:56.647: INFO: (4) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 83.422194ms)
May 11 18:05:56.718: INFO: (5) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 70.350191ms)
May 11 18:05:56.719: INFO: (5) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 72.038741ms)
May 11 18:05:56.719: INFO: (5) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.155527ms)
May 11 18:05:56.719: INFO: (5) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 72.099575ms)
May 11 18:05:56.720: INFO: (5) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 72.449382ms)
May 11 18:05:56.720: INFO: (5) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 72.410641ms)
May 11 18:05:56.720: INFO: (5) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 72.934283ms)
May 11 18:05:56.720: INFO: (5) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 73.104104ms)
May 11 18:05:56.721: INFO: (5) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 73.510807ms)
May 11 18:05:56.721: INFO: (5) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 73.94673ms)
May 11 18:05:56.722: INFO: (5) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 74.374414ms)
May 11 18:05:56.722: INFO: (5) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 74.394627ms)
May 11 18:05:56.722: INFO: (5) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 74.884999ms)
May 11 18:05:56.723: INFO: (5) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 75.680917ms)
May 11 18:05:56.723: INFO: (5) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 76.125069ms)
May 11 18:05:56.725: INFO: (5) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 78.170583ms)
May 11 18:05:56.795: INFO: (6) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 69.761209ms)
May 11 18:05:56.796: INFO: (6) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 70.386481ms)
May 11 18:05:56.796: INFO: (6) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 70.715742ms)
May 11 18:05:56.797: INFO: (6) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 71.043896ms)
May 11 18:05:56.797: INFO: (6) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 71.190737ms)
May 11 18:05:56.797: INFO: (6) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 71.733747ms)
May 11 18:05:56.797: INFO: (6) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 71.863409ms)
May 11 18:05:56.798: INFO: (6) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 72.456774ms)
May 11 18:05:56.798: INFO: (6) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 72.590171ms)
May 11 18:05:56.798: INFO: (6) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 72.740591ms)
May 11 18:05:56.799: INFO: (6) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 73.10202ms)
May 11 18:05:56.799: INFO: (6) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 73.673836ms)
May 11 18:05:56.800: INFO: (6) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 74.579158ms)
May 11 18:05:56.801: INFO: (6) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 75.451366ms)
May 11 18:05:56.802: INFO: (6) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 76.114173ms)
May 11 18:05:56.802: INFO: (6) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 76.264874ms)
May 11 18:05:56.874: INFO: (7) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 71.677302ms)
May 11 18:05:56.874: INFO: (7) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 71.632565ms)
May 11 18:05:56.876: INFO: (7) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 73.82741ms)
May 11 18:05:56.876: INFO: (7) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 74.061149ms)
May 11 18:05:56.877: INFO: (7) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 75.301708ms)
May 11 18:05:56.877: INFO: (7) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 75.482837ms)
May 11 18:05:56.877: INFO: (7) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 75.426782ms)
May 11 18:05:56.878: INFO: (7) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 75.709015ms)
May 11 18:05:56.878: INFO: (7) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 75.875751ms)
May 11 18:05:56.878: INFO: (7) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 76.067256ms)
May 11 18:05:56.878: INFO: (7) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 76.209421ms)
May 11 18:05:56.879: INFO: (7) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 77.05462ms)
May 11 18:05:56.879: INFO: (7) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 77.580854ms)
May 11 18:05:56.880: INFO: (7) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 77.919211ms)
May 11 18:05:56.881: INFO: (7) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 78.592064ms)
May 11 18:05:56.883: INFO: (7) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 81.511332ms)
May 11 18:05:56.955: INFO: (8) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 71.037266ms)
May 11 18:05:56.955: INFO: (8) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 71.317647ms)
May 11 18:05:56.955: INFO: (8) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 71.334406ms)
May 11 18:05:56.955: INFO: (8) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 71.777852ms)
May 11 18:05:56.956: INFO: (8) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 72.307931ms)
May 11 18:05:56.956: INFO: (8) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 72.310154ms)
May 11 18:05:56.956: INFO: (8) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 72.383966ms)
May 11 18:05:56.956: INFO: (8) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 72.365201ms)
May 11 18:05:56.957: INFO: (8) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 73.056803ms)
May 11 18:05:56.957: INFO: (8) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 73.419088ms)
May 11 18:05:56.957: INFO: (8) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 73.492209ms)
May 11 18:05:56.957: INFO: (8) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 73.761005ms)
May 11 18:05:56.958: INFO: (8) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 74.025621ms)
May 11 18:05:56.959: INFO: (8) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 75.002473ms)
May 11 18:05:56.960: INFO: (8) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 76.276205ms)
May 11 18:05:56.962: INFO: (8) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 78.567018ms)
May 11 18:05:57.032: INFO: (9) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 70.144176ms)
May 11 18:05:57.033: INFO: (9) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 70.407261ms)
May 11 18:05:57.034: INFO: (9) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 72.109707ms)
May 11 18:05:57.034: INFO: (9) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.213449ms)
May 11 18:05:57.035: INFO: (9) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 72.615467ms)
May 11 18:05:57.035: INFO: (9) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 72.771296ms)
May 11 18:05:57.035: INFO: (9) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 72.768523ms)
May 11 18:05:57.036: INFO: (9) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 73.738349ms)
May 11 18:05:57.036: INFO: (9) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 74.018084ms)
May 11 18:05:57.036: INFO: (9) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 73.994576ms)
May 11 18:05:57.037: INFO: (9) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 74.957055ms)
May 11 18:05:57.038: INFO: (9) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 75.772471ms)
May 11 18:05:57.038: INFO: (9) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 75.752284ms)
May 11 18:05:57.039: INFO: (9) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 76.824574ms)
May 11 18:05:57.040: INFO: (9) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 77.978919ms)
May 11 18:05:57.040: INFO: (9) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 78.111505ms)
May 11 18:05:57.111: INFO: (10) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 70.661249ms)
May 11 18:05:57.111: INFO: (10) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 71.017199ms)
May 11 18:05:57.112: INFO: (10) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 71.193946ms)
May 11 18:05:57.112: INFO: (10) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 71.554021ms)
May 11 18:05:57.112: INFO: (10) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 71.793241ms)
May 11 18:05:57.112: INFO: (10) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 71.827302ms)
May 11 18:05:57.112: INFO: (10) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 72.10553ms)
May 11 18:05:57.112: INFO: (10) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 72.123061ms)
May 11 18:05:57.113: INFO: (10) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 72.709576ms)
May 11 18:05:57.113: INFO: (10) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 73.061466ms)
May 11 18:05:57.114: INFO: (10) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 73.27791ms)
May 11 18:05:57.114: INFO: (10) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 73.495671ms)
May 11 18:05:57.115: INFO: (10) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 74.796734ms)
May 11 18:05:57.116: INFO: (10) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 75.231246ms)
May 11 18:05:57.116: INFO: (10) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 75.478336ms)
May 11 18:05:57.118: INFO: (10) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 77.935555ms)
May 11 18:05:57.189: INFO: (11) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 71.002511ms)
May 11 18:05:57.189: INFO: (11) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 71.026648ms)
May 11 18:05:57.189: INFO: (11) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 71.090853ms)
May 11 18:05:57.190: INFO: (11) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 71.346514ms)
May 11 18:05:57.190: INFO: (11) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 71.943641ms)
May 11 18:05:57.191: INFO: (11) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.602373ms)
May 11 18:05:57.191: INFO: (11) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 72.635394ms)
May 11 18:05:57.191: INFO: (11) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 72.818014ms)
May 11 18:05:57.191: INFO: (11) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 72.816284ms)
May 11 18:05:57.191: INFO: (11) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 72.855097ms)
May 11 18:05:57.191: INFO: (11) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.922095ms)
May 11 18:05:57.192: INFO: (11) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 73.209083ms)
May 11 18:05:57.193: INFO: (11) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 74.652476ms)
May 11 18:05:57.193: INFO: (11) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 74.615ms)
May 11 18:05:57.194: INFO: (11) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 75.253976ms)
May 11 18:05:57.196: INFO: (11) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 77.719889ms)
May 11 18:05:57.268: INFO: (12) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 71.939865ms)
May 11 18:05:57.269: INFO: (12) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 72.826043ms)
May 11 18:05:57.269: INFO: (12) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 73.034257ms)
May 11 18:05:57.270: INFO: (12) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 73.753942ms)
May 11 18:05:57.270: INFO: (12) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 73.909749ms)
May 11 18:05:57.271: INFO: (12) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 74.671827ms)
May 11 18:05:57.271: INFO: (12) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 75.199481ms)
May 11 18:05:57.272: INFO: (12) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 75.730969ms)
May 11 18:05:57.272: INFO: (12) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 75.90516ms)
May 11 18:05:57.273: INFO: (12) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 76.385229ms)
May 11 18:05:57.273: INFO: (12) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 76.65269ms)
May 11 18:05:57.273: INFO: (12) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 76.914267ms)
May 11 18:05:57.275: INFO: (12) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 78.932849ms)
May 11 18:05:57.275: INFO: (12) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 78.977888ms)
May 11 18:05:57.275: INFO: (12) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 79.192001ms)
May 11 18:05:57.276: INFO: (12) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 79.647352ms)
May 11 18:05:57.347: INFO: (13) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 70.653716ms)
May 11 18:05:57.347: INFO: (13) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 70.958461ms)
May 11 18:05:57.349: INFO: (13) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 72.531846ms)
May 11 18:05:57.349: INFO: (13) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 72.646289ms)
May 11 18:05:57.349: INFO: (13) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 72.892371ms)
May 11 18:05:57.349: INFO: (13) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 72.881384ms)
May 11 18:05:57.349: INFO: (13) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.880822ms)
May 11 18:05:57.349: INFO: (13) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.879024ms)
May 11 18:05:57.349: INFO: (13) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 73.151353ms)
May 11 18:05:57.349: INFO: (13) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 73.23538ms)
May 11 18:05:57.353: INFO: (13) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 76.423753ms)
May 11 18:05:57.353: INFO: (13) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 76.474117ms)
May 11 18:05:57.353: INFO: (13) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 76.569121ms)
May 11 18:05:57.354: INFO: (13) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 78.225737ms)
May 11 18:05:57.354: INFO: (13) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 78.378421ms)
May 11 18:05:57.355: INFO: (13) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 78.732073ms)
May 11 18:05:57.425: INFO: (14) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 70.226302ms)
May 11 18:05:57.425: INFO: (14) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 70.180796ms)
May 11 18:05:57.425: INFO: (14) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 70.160245ms)
May 11 18:05:57.425: INFO: (14) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 70.204379ms)
May 11 18:05:57.426: INFO: (14) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 70.868784ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 77.668917ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 77.977891ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 78.028494ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 78.119058ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 78.090881ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 78.063715ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 78.07868ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 78.03403ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 78.084176ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 78.164927ms)
May 11 18:05:57.433: INFO: (14) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 78.139893ms)
May 11 18:05:57.505: INFO: (15) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.110902ms)
May 11 18:05:57.505: INFO: (15) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 72.125203ms)
May 11 18:05:57.505: INFO: (15) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.17941ms)
May 11 18:05:57.505: INFO: (15) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 72.182676ms)
May 11 18:05:57.505: INFO: (15) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 72.326578ms)
May 11 18:05:57.506: INFO: (15) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 72.799936ms)
May 11 18:05:57.508: INFO: (15) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 74.8091ms)
May 11 18:05:57.508: INFO: (15) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 74.768576ms)
May 11 18:05:57.508: INFO: (15) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 74.845641ms)
May 11 18:05:57.508: INFO: (15) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 74.762544ms)
May 11 18:05:57.508: INFO: (15) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 74.768575ms)
May 11 18:05:57.509: INFO: (15) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 75.657981ms)
May 11 18:05:57.509: INFO: (15) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 75.871404ms)
May 11 18:05:57.509: INFO: (15) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 75.884704ms)
May 11 18:05:57.510: INFO: (15) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 77.005355ms)
May 11 18:05:57.510: INFO: (15) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 77.218998ms)
May 11 18:05:57.583: INFO: (16) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 72.372119ms)
May 11 18:05:57.583: INFO: (16) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 72.369189ms)
May 11 18:05:57.583: INFO: (16) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 72.60799ms)
May 11 18:05:57.583: INFO: (16) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.699995ms)
May 11 18:05:57.583: INFO: (16) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 72.652312ms)
May 11 18:05:57.583: INFO: (16) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 72.839419ms)
May 11 18:05:57.583: INFO: (16) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 72.924234ms)
May 11 18:05:57.583: INFO: (16) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.8484ms)
May 11 18:05:57.584: INFO: (16) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 73.367637ms)
May 11 18:05:57.584: INFO: (16) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 74.093442ms)
May 11 18:05:57.587: INFO: (16) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 76.304612ms)
May 11 18:05:57.587: INFO: (16) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 76.344661ms)
May 11 18:05:57.587: INFO: (16) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 76.448633ms)
May 11 18:05:57.591: INFO: (16) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 80.367386ms)
May 11 18:05:57.596: INFO: (16) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 85.210811ms)
May 11 18:05:57.596: INFO: (16) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 85.178626ms)
May 11 18:05:57.675: INFO: (17) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 79.076163ms)
May 11 18:05:57.675: INFO: (17) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 79.153647ms)
May 11 18:05:57.675: INFO: (17) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 79.269789ms)
May 11 18:05:57.675: INFO: (17) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 79.414551ms)
May 11 18:05:57.675: INFO: (17) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 79.468167ms)
May 11 18:05:57.680: INFO: (17) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 84.744413ms)
May 11 18:05:57.682: INFO: (17) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 86.062561ms)
May 11 18:05:57.682: INFO: (17) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 86.061842ms)
May 11 18:05:57.687: INFO: (17) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 90.863104ms)
May 11 18:05:57.687: INFO: (17) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 90.917792ms)
May 11 18:05:57.693: INFO: (17) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 97.316099ms)
May 11 18:05:57.693: INFO: (17) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 97.37233ms)
May 11 18:05:57.693: INFO: (17) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 97.498565ms)
May 11 18:05:57.693: INFO: (17) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 97.730142ms)
May 11 18:05:57.697: INFO: (17) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 100.851006ms)
May 11 18:05:57.707: INFO: (17) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 111.675092ms)
May 11 18:05:57.792: INFO: (18) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 84.78339ms)
May 11 18:05:57.793: INFO: (18) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 85.074969ms)
May 11 18:05:57.793: INFO: (18) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 85.431836ms)
May 11 18:05:57.793: INFO: (18) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 85.482535ms)
May 11 18:05:57.793: INFO: (18) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 85.568792ms)
May 11 18:05:57.794: INFO: (18) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 86.944634ms)
May 11 18:05:57.796: INFO: (18) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 88.850402ms)
May 11 18:05:57.797: INFO: (18) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 89.107335ms)
May 11 18:05:57.799: INFO: (18) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 91.972506ms)
May 11 18:05:57.800: INFO: (18) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 92.124863ms)
May 11 18:05:57.800: INFO: (18) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 92.174199ms)
May 11 18:05:57.800: INFO: (18) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 92.167468ms)
May 11 18:05:57.800: INFO: (18) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 92.327054ms)
May 11 18:05:57.800: INFO: (18) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 92.373038ms)
May 11 18:05:57.804: INFO: (18) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 96.300458ms)
May 11 18:05:57.807: INFO: (18) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 99.759157ms)
May 11 18:05:57.880: INFO: (19) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 72.425004ms)
May 11 18:05:57.880: INFO: (19) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 72.464125ms)
May 11 18:05:57.880: INFO: (19) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">test<... (200; 72.711824ms)
May 11 18:05:57.880: INFO: (19) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:460/proxy/: tls baz (200; 72.667294ms)
May 11 18:05:57.880: INFO: (19) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:462/proxy/: tls qux (200; 72.959701ms)
May 11 18:05:57.881: INFO: (19) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname1/proxy/: foo (200; 73.268784ms)
May 11 18:05:57.881: INFO: (19) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:160/proxy/: foo (200; 73.771773ms)
May 11 18:05:57.881: INFO: (19) /api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/http:proxy-service-ssg7d-vlk9w:1080/proxy/rewriteme">... (200; 73.795695ms)
May 11 18:05:57.892: INFO: (19) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname1/proxy/: tls baz (200; 84.924138ms)
May 11 18:05:57.893: INFO: (19) /api/v1/namespaces/proxy-7340/services/https:proxy-service-ssg7d:tlsportname2/proxy/: tls qux (200; 85.360938ms)
May 11 18:05:57.893: INFO: (19) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname2/proxy/: bar (200; 85.37911ms)
May 11 18:05:57.893: INFO: (19) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w/proxy/rewriteme">test</a> (200; 85.50418ms)
May 11 18:05:57.893: INFO: (19) /api/v1/namespaces/proxy-7340/pods/proxy-service-ssg7d-vlk9w:162/proxy/: bar (200; 85.778262ms)
May 11 18:05:57.893: INFO: (19) /api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/: <a href="/api/v1/namespaces/proxy-7340/pods/https:proxy-service-ssg7d-vlk9w:443/proxy/tlsrewritem... (200; 85.9083ms)
May 11 18:05:57.911: INFO: (19) /api/v1/namespaces/proxy-7340/services/proxy-service-ssg7d:portname1/proxy/: foo (200; 103.50438ms)
May 11 18:05:57.911: INFO: (19) /api/v1/namespaces/proxy-7340/services/http:proxy-service-ssg7d:portname2/proxy/: bar (200; 103.641111ms)
STEP: deleting ReplicationController proxy-service-ssg7d in namespace proxy-7340, will wait for the garbage collector to delete the pods
May 11 18:05:58.160: INFO: Deleting ReplicationController proxy-service-ssg7d took: 74.855228ms
May 11 18:05:58.260: INFO: Terminating ReplicationController proxy-service-ssg7d pods took: 100.199092ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:08.460: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-7340" for this suite.


• [SLOW TEST:25.335 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":-1,"completed":20,"skipped":331,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:10.102: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-3007" for this suite.


• [SLOW TEST:17.581 seconds]
[sig-api-machinery] ResourceQuota
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":-1,"completed":43,"skipped":780,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:19.234: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-2445" for this suite.


• [SLOW TEST:10.739 seconds]
[sig-apps] Job
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":-1,"completed":21,"skipped":355,"failed":0}
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:06:19.924: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8558856-28a9-42c3-9e9b-f741f50a04f4" in namespace "projected-6755" to be "Succeeded or Failed"
May 11 18:06:19.991: INFO: Pod "downwardapi-volume-c8558856-28a9-42c3-9e9b-f741f50a04f4": Phase="Pending", Reason="", readiness=false. Elapsed: 66.539245ms
May 11 18:06:22.058: INFO: Pod "downwardapi-volume-c8558856-28a9-42c3-9e9b-f741f50a04f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133093138s
May 11 18:06:24.125: INFO: Pod "downwardapi-volume-c8558856-28a9-42c3-9e9b-f741f50a04f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.200065379s
STEP: Saw pod success
May 11 18:06:24.125: INFO: Pod "downwardapi-volume-c8558856-28a9-42c3-9e9b-f741f50a04f4" satisfied condition "Succeeded or Failed"
May 11 18:06:24.191: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod downwardapi-volume-c8558856-28a9-42c3-9e9b-f741f50a04f4 container client-container: <nil>
STEP: delete the pod
May 11 18:06:24.342: INFO: Waiting for pod downwardapi-volume-c8558856-28a9-42c3-9e9b-f741f50a04f4 to disappear
May 11 18:06:24.412: INFO: Pod downwardapi-volume-c8558856-28a9-42c3-9e9b-f741f50a04f4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:24.412: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6755" for this suite.


• [SLOW TEST:5.188 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":-1,"completed":22,"skipped":355,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6824
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6824
STEP: Creating statefulset with conflicting port in namespace statefulset-6824
STEP: Waiting until pod test-pod will start running in namespace statefulset-6824
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6824
May 11 18:06:15.275: INFO: Observed stateful pod in namespace: statefulset-6824, name: ss-0, uid: 3de583ab-2d80-44f2-b976-d834bafade85, status phase: Pending. Waiting for statefulset controller to delete.
May 11 18:06:15.275: INFO: Observed stateful pod in namespace: statefulset-6824, name: ss-0, uid: 3de583ab-2d80-44f2-b976-d834bafade85, status phase: Failed. Waiting for statefulset controller to delete.
May 11 18:06:15.275: INFO: Observed stateful pod in namespace: statefulset-6824, name: ss-0, uid: 3de583ab-2d80-44f2-b976-d834bafade85, status phase: Failed. Waiting for statefulset controller to delete.
May 11 18:06:15.275: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6824
STEP: Removing pod with conflicting port in namespace statefulset-6824
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6824 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 11 18:06:19.547: INFO: Deleting all statefulset in ns statefulset-6824
May 11 18:06:19.615: INFO: Scaling statefulset ss to 0
May 11 18:06:29.884: INFO: Waiting for statefulset status.replicas updated to 0
May 11 18:06:29.951: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:30.148: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-6824" for this suite.


• [SLOW TEST:20.007 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":-1,"completed":44,"skipped":804,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 11 18:06:33.722: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 11 18:06:33.788: INFO: Pod pod-with-prestop-http-hook still exists
May 11 18:06:35.788: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 11 18:06:35.855: INFO: Pod pod-with-prestop-http-hook still exists
May 11 18:06:37.788: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 11 18:06:37.856: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:37.928: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2261" for this suite.


• [SLOW TEST:13.471 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":-1,"completed":23,"skipped":384,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:06:30.821: INFO: Creating ReplicaSet my-hostname-basic-48ee9511-7ce1-4bf4-93ed-3824b7515362
May 11 18:06:30.954: INFO: Pod name my-hostname-basic-48ee9511-7ce1-4bf4-93ed-3824b7515362: Found 1 pods out of 1
May 11 18:06:30.954: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-48ee9511-7ce1-4bf4-93ed-3824b7515362" is running
May 11 18:06:35.086: INFO: Pod "my-hostname-basic-48ee9511-7ce1-4bf4-93ed-3824b7515362-zmbbw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-11 18:06:30 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-11 18:06:30 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-48ee9511-7ce1-4bf4-93ed-3824b7515362]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-11 18:06:30 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-48ee9511-7ce1-4bf4-93ed-3824b7515362]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-11 18:06:30 +0000 UTC Reason: Message:}])
May 11 18:06:35.086: INFO: Trying to dial the pod
May 11 18:06:40.287: INFO: Controller my-hostname-basic-48ee9511-7ce1-4bf4-93ed-3824b7515362: Got expected result from replica 1 [my-hostname-basic-48ee9511-7ce1-4bf4-93ed-3824b7515362-zmbbw]: "my-hostname-basic-48ee9511-7ce1-4bf4-93ed-3824b7515362-zmbbw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:40.287: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-614" for this suite.


• [SLOW TEST:10.044 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":-1,"completed":45,"skipped":858,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5596
I0511 18:06:38.641138   45016 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5596, replica count: 1
I0511 18:06:39.741479   45016 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:06:40.741645   45016 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:06:41.741821   45016 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 18:06:41.919: INFO: Created: latency-svc-2p6dw
May 11 18:06:41.931: INFO: Got endpoints: latency-svc-2p6dw [89.582945ms]
May 11 18:06:42.022: INFO: Created: latency-svc-lpctp
May 11 18:06:42.031: INFO: Got endpoints: latency-svc-lpctp [99.465029ms]
May 11 18:06:42.033: INFO: Created: latency-svc-qzm7x
May 11 18:06:42.042: INFO: Got endpoints: latency-svc-qzm7x [110.429493ms]
May 11 18:06:42.067: INFO: Created: latency-svc-x9nps
May 11 18:06:42.067: INFO: Created: latency-svc-ld45s
May 11 18:06:42.069: INFO: Got endpoints: latency-svc-ld45s [137.471312ms]
May 11 18:06:42.069: INFO: Got endpoints: latency-svc-x9nps [137.524871ms]
May 11 18:06:42.087: INFO: Created: latency-svc-rqg4j
May 11 18:06:42.087: INFO: Created: latency-svc-65czs
May 11 18:06:42.087: INFO: Got endpoints: latency-svc-65czs [155.29459ms]
May 11 18:06:42.094: INFO: Got endpoints: latency-svc-rqg4j [162.820073ms]
May 11 18:06:42.097: INFO: Created: latency-svc-s2cs2
May 11 18:06:42.102: INFO: Created: latency-svc-d25cw
May 11 18:06:42.105: INFO: Got endpoints: latency-svc-s2cs2 [173.800372ms]
May 11 18:06:42.106: INFO: Created: latency-svc-sb8mw
May 11 18:06:42.110: INFO: Got endpoints: latency-svc-d25cw [178.743362ms]
May 11 18:06:42.131: INFO: Created: latency-svc-rmzh9
May 11 18:06:42.131: INFO: Got endpoints: latency-svc-rmzh9 [199.723314ms]
May 11 18:06:42.131: INFO: Got endpoints: latency-svc-sb8mw [199.775859ms]
May 11 18:06:42.132: INFO: Created: latency-svc-2ztw5
May 11 18:06:42.143: INFO: Got endpoints: latency-svc-2ztw5 [211.546627ms]
May 11 18:06:42.143: INFO: Created: latency-svc-dgr7w
May 11 18:06:42.151: INFO: Created: latency-svc-wrqwj
May 11 18:06:42.153: INFO: Got endpoints: latency-svc-dgr7w [221.408812ms]
May 11 18:06:42.163: INFO: Got endpoints: latency-svc-wrqwj [231.729847ms]
May 11 18:06:42.169: INFO: Created: latency-svc-fhldl
May 11 18:06:42.178: INFO: Got endpoints: latency-svc-fhldl [247.276977ms]
May 11 18:06:42.179: INFO: Created: latency-svc-4nk8k
May 11 18:06:42.182: INFO: Got endpoints: latency-svc-4nk8k [251.081091ms]
May 11 18:06:42.190: INFO: Created: latency-svc-bhx4h
May 11 18:06:42.199: INFO: Created: latency-svc-6jhbd
May 11 18:06:42.199: INFO: Got endpoints: latency-svc-bhx4h [168.03988ms]
May 11 18:06:42.211: INFO: Got endpoints: latency-svc-6jhbd [168.788194ms]
May 11 18:06:42.211: INFO: Created: latency-svc-6j2jj
May 11 18:06:42.219: INFO: Got endpoints: latency-svc-6j2jj [150.479744ms]
May 11 18:06:42.226: INFO: Created: latency-svc-wsvr6
May 11 18:06:42.230: INFO: Created: latency-svc-hcqqr
May 11 18:06:42.231: INFO: Got endpoints: latency-svc-wsvr6 [162.640366ms]
May 11 18:06:42.238: INFO: Created: latency-svc-88lsr
May 11 18:06:42.240: INFO: Got endpoints: latency-svc-hcqqr [153.037658ms]
May 11 18:06:42.248: INFO: Got endpoints: latency-svc-88lsr [153.348487ms]
May 11 18:06:42.254: INFO: Created: latency-svc-ln7wb
May 11 18:06:42.262: INFO: Got endpoints: latency-svc-ln7wb [156.313069ms]
May 11 18:06:42.262: INFO: Created: latency-svc-s9phf
May 11 18:06:42.269: INFO: Created: latency-svc-j6p77
May 11 18:06:42.271: INFO: Got endpoints: latency-svc-s9phf [160.576846ms]
May 11 18:06:42.275: INFO: Created: latency-svc-pvz52
May 11 18:06:42.282: INFO: Got endpoints: latency-svc-j6p77 [150.823006ms]
May 11 18:06:42.287: INFO: Got endpoints: latency-svc-pvz52 [155.577828ms]
May 11 18:06:42.288: INFO: Created: latency-svc-m9gmf
May 11 18:06:42.297: INFO: Got endpoints: latency-svc-m9gmf [154.231744ms]
May 11 18:06:42.305: INFO: Created: latency-svc-b7ps7
May 11 18:06:42.310: INFO: Created: latency-svc-mkvjx
May 11 18:06:42.315: INFO: Created: latency-svc-nwnw7
May 11 18:06:42.315: INFO: Got endpoints: latency-svc-b7ps7 [162.054667ms]
May 11 18:06:42.320: INFO: Got endpoints: latency-svc-mkvjx [156.460832ms]
May 11 18:06:42.327: INFO: Created: latency-svc-msmlg
May 11 18:06:42.327: INFO: Got endpoints: latency-svc-nwnw7 [148.928924ms]
May 11 18:06:42.336: INFO: Created: latency-svc-n4b7r
May 11 18:06:42.337: INFO: Got endpoints: latency-svc-msmlg [154.526103ms]
May 11 18:06:42.341: INFO: Got endpoints: latency-svc-n4b7r [142.451055ms]
May 11 18:06:42.342: INFO: Created: latency-svc-9w9fj
May 11 18:06:42.349: INFO: Created: latency-svc-nswmc
May 11 18:06:42.349: INFO: Got endpoints: latency-svc-9w9fj [138.860597ms]
May 11 18:06:42.357: INFO: Got endpoints: latency-svc-nswmc [137.376281ms]
May 11 18:06:42.360: INFO: Created: latency-svc-2gg5q
May 11 18:06:42.369: INFO: Got endpoints: latency-svc-2gg5q [137.211768ms]
May 11 18:06:42.374: INFO: Created: latency-svc-tmrq5
May 11 18:06:42.374: INFO: Created: latency-svc-tf6c2
May 11 18:06:42.379: INFO: Created: latency-svc-c9v4s
May 11 18:06:42.389: INFO: Got endpoints: latency-svc-tf6c2 [140.899286ms]
May 11 18:06:42.389: INFO: Got endpoints: latency-svc-tmrq5 [148.908458ms]
May 11 18:06:42.389: INFO: Got endpoints: latency-svc-c9v4s [127.522225ms]
May 11 18:06:42.391: INFO: Created: latency-svc-kspsc
May 11 18:06:42.401: INFO: Got endpoints: latency-svc-kspsc [129.841233ms]
May 11 18:06:42.402: INFO: Created: latency-svc-nsnpb
May 11 18:06:42.407: INFO: Created: latency-svc-ldclp
May 11 18:06:42.410: INFO: Got endpoints: latency-svc-nsnpb [128.061451ms]
May 11 18:06:42.414: INFO: Created: latency-svc-4z6gj
May 11 18:06:42.418: INFO: Got endpoints: latency-svc-ldclp [131.67973ms]
May 11 18:06:42.424: INFO: Created: latency-svc-jh56t
May 11 18:06:42.435: INFO: Got endpoints: latency-svc-4z6gj [98.03342ms]
May 11 18:06:42.436: INFO: Got endpoints: latency-svc-jh56t [138.85647ms]
May 11 18:06:42.441: INFO: Created: latency-svc-k66wj
May 11 18:06:42.445: INFO: Created: latency-svc-fjs4s
May 11 18:06:42.450: INFO: Created: latency-svc-6ljrk
May 11 18:06:42.450: INFO: Got endpoints: latency-svc-k66wj [135.041972ms]
May 11 18:06:42.461: INFO: Got endpoints: latency-svc-6ljrk [133.079584ms]
May 11 18:06:42.464: INFO: Got endpoints: latency-svc-fjs4s [144.686823ms]
May 11 18:06:42.472: INFO: Created: latency-svc-5nzmd
May 11 18:06:42.480: INFO: Created: latency-svc-2tqxn
May 11 18:06:42.481: INFO: Got endpoints: latency-svc-5nzmd [139.825172ms]
May 11 18:06:42.485: INFO: Created: latency-svc-qnfxk
May 11 18:06:42.487: INFO: Got endpoints: latency-svc-2tqxn [137.643518ms]
May 11 18:06:42.491: INFO: Created: latency-svc-jjl59
May 11 18:06:42.506: INFO: Created: latency-svc-vll5p
May 11 18:06:42.507: INFO: Got endpoints: latency-svc-jjl59 [137.828089ms]
May 11 18:06:42.507: INFO: Got endpoints: latency-svc-qnfxk [149.865099ms]
May 11 18:06:42.516: INFO: Got endpoints: latency-svc-vll5p [127.064176ms]
May 11 18:06:42.520: INFO: Created: latency-svc-rpl28
May 11 18:06:42.525: INFO: Created: latency-svc-7mdtv
May 11 18:06:42.532: INFO: Got endpoints: latency-svc-rpl28 [143.664212ms]
May 11 18:06:42.533: INFO: Created: latency-svc-ftq9g
May 11 18:06:42.537: INFO: Got endpoints: latency-svc-7mdtv [148.12191ms]
May 11 18:06:42.545: INFO: Created: latency-svc-m8sr5
May 11 18:06:42.546: INFO: Got endpoints: latency-svc-ftq9g [145.620349ms]
May 11 18:06:42.555: INFO: Created: latency-svc-njvwf
May 11 18:06:42.560: INFO: Got endpoints: latency-svc-m8sr5 [149.57766ms]
May 11 18:06:42.563: INFO: Got endpoints: latency-svc-njvwf [144.404749ms]
May 11 18:06:42.566: INFO: Created: latency-svc-gcp7g
May 11 18:06:42.571: INFO: Created: latency-svc-r7hz2
May 11 18:06:42.576: INFO: Got endpoints: latency-svc-gcp7g [140.682141ms]
May 11 18:06:42.592: INFO: Got endpoints: latency-svc-r7hz2 [155.473925ms]
May 11 18:06:42.593: INFO: Created: latency-svc-vbd5w
May 11 18:06:42.600: INFO: Created: latency-svc-hjj4g
May 11 18:06:42.603: INFO: Got endpoints: latency-svc-vbd5w [152.611756ms]
May 11 18:06:42.611: INFO: Got endpoints: latency-svc-hjj4g [150.160654ms]
May 11 18:06:42.618: INFO: Created: latency-svc-tjhq9
May 11 18:06:42.626: INFO: Got endpoints: latency-svc-tjhq9 [161.669601ms]
May 11 18:06:42.626: INFO: Created: latency-svc-kjp7n
May 11 18:06:42.639: INFO: Got endpoints: latency-svc-kjp7n [157.701679ms]
May 11 18:06:42.644: INFO: Created: latency-svc-ldxbb
May 11 18:06:42.649: INFO: Created: latency-svc-c6zbm
May 11 18:06:42.651: INFO: Got endpoints: latency-svc-ldxbb [163.746068ms]
May 11 18:06:42.657: INFO: Got endpoints: latency-svc-c6zbm [150.335089ms]
May 11 18:06:42.659: INFO: Created: latency-svc-ptf58
May 11 18:06:42.665: INFO: Created: latency-svc-5tvd4
May 11 18:06:42.669: INFO: Got endpoints: latency-svc-ptf58 [162.022566ms]
May 11 18:06:42.675: INFO: Got endpoints: latency-svc-5tvd4 [159.129573ms]
May 11 18:06:42.678: INFO: Created: latency-svc-6pspk
May 11 18:06:42.687: INFO: Created: latency-svc-w2ddd
May 11 18:06:42.691: INFO: Got endpoints: latency-svc-6pspk [158.72434ms]
May 11 18:06:42.694: INFO: Created: latency-svc-fvlcw
May 11 18:06:42.695: INFO: Got endpoints: latency-svc-w2ddd [157.791092ms]
May 11 18:06:42.699: INFO: Created: latency-svc-6s72c
May 11 18:06:42.711: INFO: Got endpoints: latency-svc-fvlcw [164.813964ms]
May 11 18:06:42.711: INFO: Got endpoints: latency-svc-6s72c [151.78626ms]
May 11 18:06:42.713: INFO: Created: latency-svc-s5rh5
May 11 18:06:42.722: INFO: Created: latency-svc-mpxz4
May 11 18:06:42.727: INFO: Created: latency-svc-hlmfr
May 11 18:06:42.728: INFO: Got endpoints: latency-svc-s5rh5 [165.225408ms]
May 11 18:06:42.731: INFO: Got endpoints: latency-svc-mpxz4 [154.628866ms]
May 11 18:06:42.735: INFO: Created: latency-svc-tkzr4
May 11 18:06:42.746: INFO: Got endpoints: latency-svc-hlmfr [154.302182ms]
May 11 18:06:42.752: INFO: Got endpoints: latency-svc-tkzr4 [149.240532ms]
May 11 18:06:42.754: INFO: Created: latency-svc-c9d9w
May 11 18:06:42.766: INFO: Got endpoints: latency-svc-c9d9w [155.601927ms]
May 11 18:06:42.769: INFO: Created: latency-svc-6jdrx
May 11 18:06:42.773: INFO: Created: latency-svc-2v74c
May 11 18:06:42.778: INFO: Got endpoints: latency-svc-6jdrx [152.210105ms]
May 11 18:06:42.784: INFO: Got endpoints: latency-svc-2v74c [145.173541ms]
May 11 18:06:42.784: INFO: Created: latency-svc-psmnn
May 11 18:06:42.790: INFO: Created: latency-svc-6djnm
May 11 18:06:42.792: INFO: Got endpoints: latency-svc-psmnn [141.482549ms]
May 11 18:06:42.801: INFO: Got endpoints: latency-svc-6djnm [144.430568ms]
May 11 18:06:42.803: INFO: Created: latency-svc-n56kn
May 11 18:06:42.808: INFO: Created: latency-svc-cwwnn
May 11 18:06:42.809: INFO: Got endpoints: latency-svc-n56kn [140.799668ms]
May 11 18:06:42.815: INFO: Created: latency-svc-4nbrj
May 11 18:06:42.820: INFO: Got endpoints: latency-svc-cwwnn [145.051367ms]
May 11 18:06:42.822: INFO: Got endpoints: latency-svc-4nbrj [130.49745ms]
May 11 18:06:42.825: INFO: Created: latency-svc-vm48f
May 11 18:06:42.833: INFO: Created: latency-svc-mqr99
May 11 18:06:42.833: INFO: Got endpoints: latency-svc-vm48f [138.156212ms]
May 11 18:06:42.843: INFO: Created: latency-svc-qc9cv
May 11 18:06:42.843: INFO: Got endpoints: latency-svc-mqr99 [131.751921ms]
May 11 18:06:42.850: INFO: Created: latency-svc-fbh64
May 11 18:06:42.855: INFO: Got endpoints: latency-svc-qc9cv [143.655181ms]
May 11 18:06:42.859: INFO: Got endpoints: latency-svc-fbh64 [130.889823ms]
May 11 18:06:42.860: INFO: Created: latency-svc-zckd7
May 11 18:06:42.867: INFO: Got endpoints: latency-svc-zckd7 [136.538164ms]
May 11 18:06:42.873: INFO: Created: latency-svc-2xfcr
May 11 18:06:42.879: INFO: Got endpoints: latency-svc-2xfcr [132.750998ms]
May 11 18:06:42.885: INFO: Created: latency-svc-q4jl4
May 11 18:06:42.887: INFO: Created: latency-svc-hnczh
May 11 18:06:42.891: INFO: Got endpoints: latency-svc-q4jl4 [139.475865ms]
May 11 18:06:42.897: INFO: Got endpoints: latency-svc-hnczh [130.534182ms]
May 11 18:06:42.900: INFO: Created: latency-svc-lz2kp
May 11 18:06:42.907: INFO: Got endpoints: latency-svc-lz2kp [128.751812ms]
May 11 18:06:42.908: INFO: Created: latency-svc-j2n2g
May 11 18:06:42.915: INFO: Created: latency-svc-nw5x2
May 11 18:06:42.916: INFO: Got endpoints: latency-svc-j2n2g [132.031587ms]
May 11 18:06:42.918: INFO: Created: latency-svc-6v2sx
May 11 18:06:42.929: INFO: Got endpoints: latency-svc-6v2sx [127.068078ms]
May 11 18:06:42.929: INFO: Got endpoints: latency-svc-nw5x2 [136.095776ms]
May 11 18:06:42.930: INFO: Created: latency-svc-5v64h
May 11 18:06:42.937: INFO: Created: latency-svc-xk4ps
May 11 18:06:42.940: INFO: Got endpoints: latency-svc-5v64h [129.997824ms]
May 11 18:06:42.946: INFO: Got endpoints: latency-svc-xk4ps [126.195709ms]
May 11 18:06:42.951: INFO: Created: latency-svc-gb99t
May 11 18:06:42.956: INFO: Created: latency-svc-krdl7
May 11 18:06:42.960: INFO: Got endpoints: latency-svc-gb99t [137.835946ms]
May 11 18:06:42.966: INFO: Got endpoints: latency-svc-krdl7 [132.706795ms]
May 11 18:06:42.967: INFO: Created: latency-svc-s8nqk
May 11 18:06:42.990: INFO: Got endpoints: latency-svc-s8nqk [146.617599ms]
May 11 18:06:42.990: INFO: Created: latency-svc-7s8ml
May 11 18:06:42.995: INFO: Created: latency-svc-jzcv7
May 11 18:06:43.001: INFO: Got endpoints: latency-svc-7s8ml [145.945291ms]
May 11 18:06:43.002: INFO: Created: latency-svc-nqvg8
May 11 18:06:43.006: INFO: Got endpoints: latency-svc-jzcv7 [146.997951ms]
May 11 18:06:43.010: INFO: Got endpoints: latency-svc-nqvg8 [142.703245ms]
May 11 18:06:43.012: INFO: Created: latency-svc-jvv7c
May 11 18:06:43.021: INFO: Got endpoints: latency-svc-jvv7c [142.414974ms]
May 11 18:06:43.026: INFO: Created: latency-svc-dnnrp
May 11 18:06:43.033: INFO: Created: latency-svc-jn7rt
May 11 18:06:43.036: INFO: Got endpoints: latency-svc-dnnrp [144.675279ms]
May 11 18:06:43.036: INFO: Created: latency-svc-j8dnq
May 11 18:06:43.044: INFO: Got endpoints: latency-svc-jn7rt [146.514462ms]
May 11 18:06:43.047: INFO: Got endpoints: latency-svc-j8dnq [130.980771ms]
May 11 18:06:43.052: INFO: Created: latency-svc-ms47d
May 11 18:06:43.059: INFO: Got endpoints: latency-svc-ms47d [151.910155ms]
May 11 18:06:43.065: INFO: Created: latency-svc-8h2q5
May 11 18:06:43.069: INFO: Got endpoints: latency-svc-8h2q5 [140.113551ms]
May 11 18:06:43.069: INFO: Created: latency-svc-v9g5x
May 11 18:06:43.080: INFO: Got endpoints: latency-svc-v9g5x [151.738326ms]
May 11 18:06:43.081: INFO: Created: latency-svc-8qsw8
May 11 18:06:43.091: INFO: Got endpoints: latency-svc-8qsw8 [151.842746ms]
May 11 18:06:43.094: INFO: Created: latency-svc-9d9st
May 11 18:06:43.104: INFO: Got endpoints: latency-svc-9d9st [157.352825ms]
May 11 18:06:43.105: INFO: Created: latency-svc-tch7t
May 11 18:06:43.109: INFO: Created: latency-svc-trp9t
May 11 18:06:43.116: INFO: Got endpoints: latency-svc-tch7t [156.545892ms]
May 11 18:06:43.118: INFO: Created: latency-svc-shlvv
May 11 18:06:43.122: INFO: Got endpoints: latency-svc-trp9t [156.401999ms]
May 11 18:06:43.132: INFO: Got endpoints: latency-svc-shlvv [142.509696ms]
May 11 18:06:43.137: INFO: Created: latency-svc-ppr6p
May 11 18:06:43.149: INFO: Got endpoints: latency-svc-ppr6p [147.903563ms]
May 11 18:06:43.154: INFO: Created: latency-svc-2r4lk
May 11 18:06:43.158: INFO: Created: latency-svc-nmtxn
May 11 18:06:43.161: INFO: Got endpoints: latency-svc-2r4lk [154.734668ms]
May 11 18:06:43.170: INFO: Created: latency-svc-svg6j
May 11 18:06:43.172: INFO: Got endpoints: latency-svc-nmtxn [162.070038ms]
May 11 18:06:43.179: INFO: Got endpoints: latency-svc-svg6j [157.308251ms]
May 11 18:06:43.184: INFO: Created: latency-svc-h5x84
May 11 18:06:43.196: INFO: Got endpoints: latency-svc-h5x84 [159.512454ms]
May 11 18:06:43.196: INFO: Created: latency-svc-dmw5h
May 11 18:06:43.202: INFO: Created: latency-svc-q25lb
May 11 18:06:43.216: INFO: Created: latency-svc-g6qmt
May 11 18:06:43.216: INFO: Got endpoints: latency-svc-dmw5h [171.862724ms]
May 11 18:06:43.217: INFO: Got endpoints: latency-svc-q25lb [169.453193ms]
May 11 18:06:43.227: INFO: Got endpoints: latency-svc-g6qmt [168.257397ms]
May 11 18:06:43.231: INFO: Created: latency-svc-lvk7f
May 11 18:06:43.242: INFO: Created: latency-svc-fwgqb
May 11 18:06:43.249: INFO: Created: latency-svc-wzqms
May 11 18:06:43.253: INFO: Created: latency-svc-2s2cd
May 11 18:06:43.268: INFO: Created: latency-svc-7zb54
May 11 18:06:43.275: INFO: Got endpoints: latency-svc-lvk7f [206.199247ms]
May 11 18:06:43.275: INFO: Got endpoints: latency-svc-fwgqb [194.621777ms]
May 11 18:06:43.280: INFO: Created: latency-svc-skw6c
May 11 18:06:43.280: INFO: Got endpoints: latency-svc-2s2cd [176.753968ms]
May 11 18:06:43.284: INFO: Got endpoints: latency-svc-7zb54 [167.816747ms]
May 11 18:06:43.284: INFO: Got endpoints: latency-svc-wzqms [192.503154ms]
May 11 18:06:43.285: INFO: Created: latency-svc-xh5lg
May 11 18:06:43.291: INFO: Got endpoints: latency-svc-skw6c [168.781455ms]
May 11 18:06:43.294: INFO: Created: latency-svc-xnth2
May 11 18:06:43.298: INFO: Got endpoints: latency-svc-xh5lg [166.214575ms]
May 11 18:06:43.305: INFO: Got endpoints: latency-svc-xnth2 [155.402641ms]
May 11 18:06:43.307: INFO: Created: latency-svc-x7hl7
May 11 18:06:43.312: INFO: Created: latency-svc-4kmbl
May 11 18:06:43.316: INFO: Got endpoints: latency-svc-x7hl7 [155.145993ms]
May 11 18:06:43.323: INFO: Got endpoints: latency-svc-4kmbl [151.372548ms]
May 11 18:06:43.324: INFO: Created: latency-svc-wn7qh
May 11 18:06:43.330: INFO: Created: latency-svc-f6fxg
May 11 18:06:43.334: INFO: Got endpoints: latency-svc-wn7qh [155.270564ms]
May 11 18:06:43.343: INFO: Got endpoints: latency-svc-f6fxg [147.477753ms]
May 11 18:06:43.344: INFO: Created: latency-svc-p98f2
May 11 18:06:43.352: INFO: Got endpoints: latency-svc-p98f2 [135.45116ms]
May 11 18:06:43.361: INFO: Created: latency-svc-nxdvt
May 11 18:06:43.364: INFO: Created: latency-svc-k9vzz
May 11 18:06:43.370: INFO: Got endpoints: latency-svc-nxdvt [154.466595ms]
May 11 18:06:43.372: INFO: Created: latency-svc-kc9xk
May 11 18:06:43.373: INFO: Got endpoints: latency-svc-k9vzz [145.927034ms]
May 11 18:06:43.383: INFO: Created: latency-svc-w65lv
May 11 18:06:43.385: INFO: Got endpoints: latency-svc-kc9xk [110.356834ms]
May 11 18:06:43.393: INFO: Got endpoints: latency-svc-w65lv [118.140107ms]
May 11 18:06:43.397: INFO: Created: latency-svc-w2hxp
May 11 18:06:43.404: INFO: Got endpoints: latency-svc-w2hxp [123.058155ms]
May 11 18:06:43.404: INFO: Created: latency-svc-kslq8
May 11 18:06:43.409: INFO: Created: latency-svc-tcwn7
May 11 18:06:43.414: INFO: Got endpoints: latency-svc-kslq8 [130.003092ms]
May 11 18:06:43.419: INFO: Got endpoints: latency-svc-tcwn7 [135.088412ms]
May 11 18:06:43.421: INFO: Created: latency-svc-8r2zs
May 11 18:06:43.429: INFO: Got endpoints: latency-svc-8r2zs [137.909766ms]
May 11 18:06:43.432: INFO: Created: latency-svc-q4x7w
May 11 18:06:43.437: INFO: Got endpoints: latency-svc-q4x7w [138.755461ms]
May 11 18:06:43.442: INFO: Created: latency-svc-j5998
May 11 18:06:43.445: INFO: Created: latency-svc-6xdr2
May 11 18:06:43.451: INFO: Got endpoints: latency-svc-j5998 [146.221929ms]
May 11 18:06:43.454: INFO: Got endpoints: latency-svc-6xdr2 [137.611366ms]
May 11 18:06:43.456: INFO: Created: latency-svc-k6xhj
May 11 18:06:43.464: INFO: Created: latency-svc-fswfc
May 11 18:06:43.466: INFO: Got endpoints: latency-svc-k6xhj [142.251422ms]
May 11 18:06:43.473: INFO: Got endpoints: latency-svc-fswfc [138.90829ms]
May 11 18:06:43.477: INFO: Created: latency-svc-25d2w
May 11 18:06:43.496: INFO: Created: latency-svc-4khzv
May 11 18:06:43.507: INFO: Got endpoints: latency-svc-4khzv [154.620697ms]
May 11 18:06:43.507: INFO: Got endpoints: latency-svc-25d2w [163.68885ms]
May 11 18:06:43.514: INFO: Created: latency-svc-8dwnz
May 11 18:06:43.528: INFO: Created: latency-svc-gwnxc
May 11 18:06:43.529: INFO: Got endpoints: latency-svc-8dwnz [159.349911ms]
May 11 18:06:43.533: INFO: Created: latency-svc-ljp82
May 11 18:06:43.548: INFO: Got endpoints: latency-svc-ljp82 [162.750996ms]
May 11 18:06:43.548: INFO: Got endpoints: latency-svc-gwnxc [174.856692ms]
May 11 18:06:43.566: INFO: Created: latency-svc-ktfvx
May 11 18:06:43.585: INFO: Created: latency-svc-w4nzm
May 11 18:06:43.591: INFO: Got endpoints: latency-svc-ktfvx [197.3257ms]
May 11 18:06:43.593: INFO: Got endpoints: latency-svc-w4nzm [189.79778ms]
May 11 18:06:43.595: INFO: Created: latency-svc-hz6xn
May 11 18:06:43.600: INFO: Created: latency-svc-tw8bf
May 11 18:06:43.605: INFO: Got endpoints: latency-svc-hz6xn [190.47834ms]
May 11 18:06:43.607: INFO: Created: latency-svc-rptfk
May 11 18:06:43.610: INFO: Got endpoints: latency-svc-tw8bf [190.822435ms]
May 11 18:06:43.617: INFO: Got endpoints: latency-svc-rptfk [187.19443ms]
May 11 18:06:43.619: INFO: Created: latency-svc-wv67j
May 11 18:06:43.627: INFO: Created: latency-svc-mr9qk
May 11 18:06:43.629: INFO: Got endpoints: latency-svc-wv67j [192.032645ms]
May 11 18:06:43.636: INFO: Got endpoints: latency-svc-mr9qk [185.072655ms]
May 11 18:06:43.640: INFO: Created: latency-svc-7wwk8
May 11 18:06:43.645: INFO: Created: latency-svc-x766l
May 11 18:06:43.650: INFO: Got endpoints: latency-svc-7wwk8 [195.767621ms]
May 11 18:06:43.655: INFO: Got endpoints: latency-svc-x766l [189.667877ms]
May 11 18:06:43.657: INFO: Created: latency-svc-ddnz7
May 11 18:06:43.666: INFO: Got endpoints: latency-svc-ddnz7 [192.792108ms]
May 11 18:06:43.670: INFO: Created: latency-svc-xdxhv
May 11 18:06:43.675: INFO: Created: latency-svc-pjcdp
May 11 18:06:43.678: INFO: Got endpoints: latency-svc-xdxhv [171.095562ms]
May 11 18:06:43.681: INFO: Created: latency-svc-zlnvz
May 11 18:06:43.685: INFO: Got endpoints: latency-svc-pjcdp [178.088904ms]
May 11 18:06:43.690: INFO: Got endpoints: latency-svc-zlnvz [160.957237ms]
May 11 18:06:43.695: INFO: Created: latency-svc-sqzxw
May 11 18:06:43.702: INFO: Created: latency-svc-qpcwr
May 11 18:06:43.704: INFO: Got endpoints: latency-svc-sqzxw [155.659106ms]
May 11 18:06:43.709: INFO: Created: latency-svc-fbscg
May 11 18:06:43.711: INFO: Got endpoints: latency-svc-qpcwr [162.577726ms]
May 11 18:06:43.713: INFO: Created: latency-svc-hk6wc
May 11 18:06:43.722: INFO: Got endpoints: latency-svc-fbscg [130.93084ms]
May 11 18:06:43.722: INFO: Got endpoints: latency-svc-hk6wc [128.649141ms]
May 11 18:06:43.726: INFO: Created: latency-svc-hkz8p
May 11 18:06:43.734: INFO: Created: latency-svc-xpl6z
May 11 18:06:43.735: INFO: Got endpoints: latency-svc-hkz8p [130.329573ms]
May 11 18:06:43.739: INFO: Created: latency-svc-8sk6d
May 11 18:06:43.741: INFO: Got endpoints: latency-svc-xpl6z [130.565106ms]
May 11 18:06:43.743: INFO: Created: latency-svc-rwvkb
May 11 18:06:43.749: INFO: Got endpoints: latency-svc-8sk6d [132.79617ms]
May 11 18:06:43.757: INFO: Created: latency-svc-6rms4
May 11 18:06:43.757: INFO: Got endpoints: latency-svc-rwvkb [127.280514ms]
May 11 18:06:43.762: INFO: Created: latency-svc-qlftb
May 11 18:06:43.770: INFO: Got endpoints: latency-svc-6rms4 [133.66934ms]
May 11 18:06:43.772: INFO: Got endpoints: latency-svc-qlftb [122.652185ms]
May 11 18:06:43.775: INFO: Created: latency-svc-jk7x5
May 11 18:06:43.779: INFO: Created: latency-svc-bcrgk
May 11 18:06:43.791: INFO: Got endpoints: latency-svc-jk7x5 [135.880904ms]
May 11 18:06:43.792: INFO: Created: latency-svc-lz6x2
May 11 18:06:43.797: INFO: Got endpoints: latency-svc-bcrgk [131.096405ms]
May 11 18:06:43.802: INFO: Got endpoints: latency-svc-lz6x2 [124.057083ms]
May 11 18:06:43.809: INFO: Created: latency-svc-769ps
May 11 18:06:43.810: INFO: Created: latency-svc-7m29d
May 11 18:06:43.816: INFO: Created: latency-svc-kjhdh
May 11 18:06:43.817: INFO: Got endpoints: latency-svc-769ps [131.913997ms]
May 11 18:06:43.828: INFO: Created: latency-svc-sq7vk
May 11 18:06:43.828: INFO: Got endpoints: latency-svc-7m29d [137.717468ms]
May 11 18:06:43.829: INFO: Got endpoints: latency-svc-kjhdh [125.115684ms]
May 11 18:06:43.836: INFO: Created: latency-svc-wlk27
May 11 18:06:43.837: INFO: Got endpoints: latency-svc-sq7vk [125.799929ms]
May 11 18:06:43.842: INFO: Created: latency-svc-gdg87
May 11 18:06:43.844: INFO: Got endpoints: latency-svc-wlk27 [122.253964ms]
May 11 18:06:43.851: INFO: Created: latency-svc-665wt
May 11 18:06:43.852: INFO: Got endpoints: latency-svc-gdg87 [129.746128ms]
May 11 18:06:43.861: INFO: Got endpoints: latency-svc-665wt [125.854008ms]
May 11 18:06:43.863: INFO: Created: latency-svc-525lb
May 11 18:06:43.871: INFO: Got endpoints: latency-svc-525lb [130.022949ms]
May 11 18:06:43.871: INFO: Created: latency-svc-6gkt7
May 11 18:06:43.877: INFO: Created: latency-svc-tf47p
May 11 18:06:43.883: INFO: Got endpoints: latency-svc-6gkt7 [133.27154ms]
May 11 18:06:43.883: INFO: Created: latency-svc-ctd9t
May 11 18:06:43.887: INFO: Got endpoints: latency-svc-tf47p [130.714245ms]
May 11 18:06:43.894: INFO: Got endpoints: latency-svc-ctd9t [124.276653ms]
May 11 18:06:43.897: INFO: Created: latency-svc-b7gfv
May 11 18:06:43.905: INFO: Created: latency-svc-56xgp
May 11 18:06:43.908: INFO: Got endpoints: latency-svc-b7gfv [135.285251ms]
May 11 18:06:43.910: INFO: Got endpoints: latency-svc-56xgp [118.654787ms]
May 11 18:06:43.912: INFO: Created: latency-svc-fxlsp
May 11 18:06:43.917: INFO: Created: latency-svc-xlqm6
May 11 18:06:43.927: INFO: Got endpoints: latency-svc-fxlsp [129.920698ms]
May 11 18:06:43.933: INFO: Got endpoints: latency-svc-xlqm6 [130.658914ms]
May 11 18:06:43.933: INFO: Created: latency-svc-86hws
May 11 18:06:43.939: INFO: Got endpoints: latency-svc-86hws [121.691743ms]
May 11 18:06:43.950: INFO: Created: latency-svc-2ghn9
May 11 18:06:43.954: INFO: Created: latency-svc-2dbcj
May 11 18:06:43.955: INFO: Got endpoints: latency-svc-2ghn9 [125.340183ms]
May 11 18:06:43.960: INFO: Created: latency-svc-mlbh6
May 11 18:06:43.966: INFO: Got endpoints: latency-svc-2dbcj [137.665147ms]
May 11 18:06:43.970: INFO: Got endpoints: latency-svc-mlbh6 [133.194053ms]
May 11 18:06:43.977: INFO: Created: latency-svc-htd2p
May 11 18:06:43.985: INFO: Created: latency-svc-vc8bp
May 11 18:06:43.986: INFO: Got endpoints: latency-svc-htd2p [142.449406ms]
May 11 18:06:43.990: INFO: Got endpoints: latency-svc-vc8bp [138.476082ms]
May 11 18:06:43.993: INFO: Created: latency-svc-p2dnv
May 11 18:06:44.002: INFO: Got endpoints: latency-svc-p2dnv [140.624036ms]
May 11 18:06:44.002: INFO: Latencies: [98.03342ms 99.465029ms 110.356834ms 110.429493ms 118.140107ms 118.654787ms 121.691743ms 122.253964ms 122.652185ms 123.058155ms 124.057083ms 124.276653ms 125.115684ms 125.340183ms 125.799929ms 125.854008ms 126.195709ms 127.064176ms 127.068078ms 127.280514ms 127.522225ms 128.061451ms 128.649141ms 128.751812ms 129.746128ms 129.841233ms 129.920698ms 129.997824ms 130.003092ms 130.022949ms 130.329573ms 130.49745ms 130.534182ms 130.565106ms 130.658914ms 130.714245ms 130.889823ms 130.93084ms 130.980771ms 131.096405ms 131.67973ms 131.751921ms 131.913997ms 132.031587ms 132.706795ms 132.750998ms 132.79617ms 133.079584ms 133.194053ms 133.27154ms 133.66934ms 135.041972ms 135.088412ms 135.285251ms 135.45116ms 135.880904ms 136.095776ms 136.538164ms 137.211768ms 137.376281ms 137.471312ms 137.524871ms 137.611366ms 137.643518ms 137.665147ms 137.717468ms 137.828089ms 137.835946ms 137.909766ms 138.156212ms 138.476082ms 138.755461ms 138.85647ms 138.860597ms 138.90829ms 139.475865ms 139.825172ms 140.113551ms 140.624036ms 140.682141ms 140.799668ms 140.899286ms 141.482549ms 142.251422ms 142.414974ms 142.449406ms 142.451055ms 142.509696ms 142.703245ms 143.655181ms 143.664212ms 144.404749ms 144.430568ms 144.675279ms 144.686823ms 145.051367ms 145.173541ms 145.620349ms 145.927034ms 145.945291ms 146.221929ms 146.514462ms 146.617599ms 146.997951ms 147.477753ms 147.903563ms 148.12191ms 148.908458ms 148.928924ms 149.240532ms 149.57766ms 149.865099ms 150.160654ms 150.335089ms 150.479744ms 150.823006ms 151.372548ms 151.738326ms 151.78626ms 151.842746ms 151.910155ms 152.210105ms 152.611756ms 153.037658ms 153.348487ms 154.231744ms 154.302182ms 154.466595ms 154.526103ms 154.620697ms 154.628866ms 154.734668ms 155.145993ms 155.270564ms 155.29459ms 155.402641ms 155.473925ms 155.577828ms 155.601927ms 155.659106ms 156.313069ms 156.401999ms 156.460832ms 156.545892ms 157.308251ms 157.352825ms 157.701679ms 157.791092ms 158.72434ms 159.129573ms 159.349911ms 159.512454ms 160.576846ms 160.957237ms 161.669601ms 162.022566ms 162.054667ms 162.070038ms 162.577726ms 162.640366ms 162.750996ms 162.820073ms 163.68885ms 163.746068ms 164.813964ms 165.225408ms 166.214575ms 167.816747ms 168.03988ms 168.257397ms 168.781455ms 168.788194ms 169.453193ms 171.095562ms 171.862724ms 173.800372ms 174.856692ms 176.753968ms 178.088904ms 178.743362ms 185.072655ms 187.19443ms 189.667877ms 189.79778ms 190.47834ms 190.822435ms 192.032645ms 192.503154ms 192.792108ms 194.621777ms 195.767621ms 197.3257ms 199.723314ms 199.775859ms 206.199247ms 211.546627ms 221.408812ms 231.729847ms 247.276977ms 251.081091ms]
May 11 18:06:44.002: INFO: 50 %ile: 146.221929ms
May 11 18:06:44.002: INFO: 90 %ile: 185.072655ms
May 11 18:06:44.002: INFO: 99 %ile: 247.276977ms
May 11 18:06:44.002: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:44.002: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svc-latency-5596" for this suite.


• [SLOW TEST:5.982 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":-1,"completed":24,"skipped":396,"failed":0}

SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
May 11 18:06:40.894: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-8339 create -f -'
May 11 18:06:41.709: INFO: stderr: ""
May 11 18:06:41.709: INFO: stdout: "pod/pause created\n"
May 11 18:06:41.709: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 11 18:06:41.709: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8339" to be "running and ready"
May 11 18:06:41.773: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 63.947453ms
May 11 18:06:43.842: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133185336s
May 11 18:06:45.910: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.200979213s
May 11 18:06:45.910: INFO: Pod "pause" satisfied condition "running and ready"
May 11 18:06:45.910: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
May 11 18:06:45.910: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-8339 label pods pause testing-label=testing-label-value'
May 11 18:06:46.668: INFO: stderr: ""
May 11 18:06:46.668: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 11 18:06:46.669: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-8339 get pod pause -L testing-label'
May 11 18:06:46.929: INFO: stderr: ""
May 11 18:06:46.929: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 11 18:06:46.929: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-8339 label pods pause testing-label-'
May 11 18:06:47.272: INFO: stderr: ""
May 11 18:06:47.272: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 11 18:06:47.272: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-8339 get pod pause -L testing-label'
May 11 18:06:47.522: INFO: stderr: ""
May 11 18:06:47.522: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
[AfterEach] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
May 11 18:06:47.522: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-8339 delete --grace-period=0 --force -f -'
May 11 18:06:47.855: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 18:06:47.855: INFO: stdout: "pod \"pause\" force deleted\n"
May 11 18:06:47.855: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-8339 get rc,svc -l name=pause --no-headers'
May 11 18:06:48.189: INFO: stderr: "No resources found in kubectl-8339 namespace.\n"
May 11 18:06:48.189: INFO: stdout: ""
May 11 18:06:48.189: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-8339 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 11 18:06:48.444: INFO: stderr: ""
May 11 18:06:48.444: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:48.444: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-8339" for this suite.


• [SLOW TEST:8.134 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":-1,"completed":46,"skipped":871,"failed":0}

SSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:53.393: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-8569" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":-1,"completed":47,"skipped":876,"failed":0}

S
------------------------------
[BeforeEach] [sig-network] Ingress API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 11 18:06:54.571: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 11 18:06:54.701: INFO: starting watch
STEP: patching
STEP: updating
May 11 18:06:54.915: INFO: waiting for watch events with expected annotations
May 11 18:06:54.915: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:55.465: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "ingress-8656" for this suite.

•
------------------------------
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":-1,"completed":48,"skipped":877,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-6448
STEP: creating replication controller nodeport-test in namespace services-6448
I0511 18:06:44.738131   45016 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6448, replica count: 2
I0511 18:06:47.838425   45016 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:06:50.838614   45016 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 18:06:50.838: INFO: Creating new exec pod
May 11 18:06:56.306: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-6448 exec execpodlcgvh -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 11 18:06:57.093: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 11 18:06:57.093: INFO: stdout: ""
May 11 18:06:57.093: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-6448 exec execpodlcgvh -- /bin/sh -x -c nc -zv -t -w 2 172.30.0.211 80'
May 11 18:06:57.880: INFO: stderr: "+ nc -zv -t -w 2 172.30.0.211 80\nConnection to 172.30.0.211 80 port [tcp/http] succeeded!\n"
May 11 18:06:57.880: INFO: stdout: ""
May 11 18:06:57.880: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-6448 exec execpodlcgvh -- /bin/sh -x -c nc -zv -t -w 2 10.0.198.156 30830'
May 11 18:06:58.630: INFO: stderr: "+ nc -zv -t -w 2 10.0.198.156 30830\nConnection to 10.0.198.156 30830 port [tcp/30830] succeeded!\n"
May 11 18:06:58.630: INFO: stdout: ""
May 11 18:06:58.630: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-6448 exec execpodlcgvh -- /bin/sh -x -c nc -zv -t -w 2 10.0.143.31 30830'
May 11 18:06:59.390: INFO: stderr: "+ nc -zv -t -w 2 10.0.143.31 30830\nConnection to 10.0.143.31 30830 port [tcp/30830] succeeded!\n"
May 11 18:06:59.390: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:06:59.390: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-6448" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:15.423 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":-1,"completed":25,"skipped":416,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:06:56.027: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd918e0c-6bd0-4729-b567-06acfcaee648" in namespace "downward-api-3826" to be "Succeeded or Failed"
May 11 18:06:56.092: INFO: Pod "downwardapi-volume-bd918e0c-6bd0-4729-b567-06acfcaee648": Phase="Pending", Reason="", readiness=false. Elapsed: 64.336104ms
May 11 18:06:58.161: INFO: Pod "downwardapi-volume-bd918e0c-6bd0-4729-b567-06acfcaee648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13367897s
May 11 18:07:00.225: INFO: Pod "downwardapi-volume-bd918e0c-6bd0-4729-b567-06acfcaee648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.197633727s
STEP: Saw pod success
May 11 18:07:00.225: INFO: Pod "downwardapi-volume-bd918e0c-6bd0-4729-b567-06acfcaee648" satisfied condition "Succeeded or Failed"
May 11 18:07:00.293: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downwardapi-volume-bd918e0c-6bd0-4729-b567-06acfcaee648 container client-container: <nil>
STEP: delete the pod
May 11 18:07:00.450: INFO: Waiting for pod downwardapi-volume-bd918e0c-6bd0-4729-b567-06acfcaee648 to disappear
May 11 18:07:00.513: INFO: Pod downwardapi-volume-bd918e0c-6bd0-4729-b567-06acfcaee648 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:00.513: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-3826" for this suite.


• [SLOW TEST:5.160 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":-1,"completed":49,"skipped":884,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-937.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-937.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-937.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-937.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-937.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-937.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 18:07:06.050: INFO: DNS probes using dns-937/dns-test-cafa3601-c02a-49da-8b17-c03896ddf4ac succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:06.215: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-937" for this suite.


• [SLOW TEST:5.643 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":-1,"completed":50,"skipped":925,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 11 18:07:07.663: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353227, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353227, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353227, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353227, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:07:09.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353227, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353227, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353227, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353227, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:07:12.808: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:14.227: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-webhook-1247" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137


• [SLOW TEST:8.302 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":-1,"completed":51,"skipped":934,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 11 18:07:15.318: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-31 create -f -'
May 11 18:07:16.142: INFO: stderr: ""
May 11 18:07:16.142: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 11 18:07:17.206: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 18:07:17.206: INFO: Found 0 / 1
May 11 18:07:18.207: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 18:07:18.207: INFO: Found 0 / 1
May 11 18:07:19.206: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 18:07:19.206: INFO: Found 1 / 1
May 11 18:07:19.207: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 11 18:07:19.271: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 18:07:19.271: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 11 18:07:19.271: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-31 patch pod agnhost-primary-nj2z7 -p {"metadata":{"annotations":{"x":"y"}}}'
May 11 18:07:19.604: INFO: stderr: ""
May 11 18:07:19.604: INFO: stdout: "pod/agnhost-primary-nj2z7 patched\n"
STEP: checking annotations
May 11 18:07:19.668: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 18:07:19.668: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:19.668: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-31" for this suite.


• [SLOW TEST:5.077 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1466
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":-1,"completed":52,"skipped":979,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:07:21.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353241, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353241, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353241, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353241, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:07:23.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353241, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353241, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353241, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353241, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:07:26.611: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:27.746: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-5857" for this suite.
STEP: Destroying namespace "webhook-5857-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:8.486 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":-1,"completed":53,"skipped":993,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-7987
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 11 18:07:00.023: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
May 11 18:07:00.472: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 11 18:07:02.540: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 11 18:07:04.539: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:06.539: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:08.540: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:10.539: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:12.543: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:14.539: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:16.545: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:18.543: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:20.549: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 11 18:07:20.686: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 11 18:07:20.822: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 11 18:07:25.394: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 11 18:07:25.395: INFO: Going to poll 10.129.2.159 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 11 18:07:25.461: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.159 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7987 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:07:26.964: INFO: Found all 1 expected endpoints: [netserver-0]
May 11 18:07:26.964: INFO: Going to poll 10.131.0.55 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 11 18:07:27.036: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.0.55 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7987 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:07:28.545: INFO: Found all 1 expected endpoints: [netserver-1]
May 11 18:07:28.546: INFO: Going to poll 10.128.2.98 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 11 18:07:28.613: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.98 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7987 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:07:30.174: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:30.174: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-7987" for this suite.


• [SLOW TEST:30.758 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":26,"skipped":430,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 11 18:07:30.885: INFO: Waiting up to 5m0s for pod "pod-3fe210ae-2723-47b4-ab05-a3cdb9e19f03" in namespace "emptydir-4601" to be "Succeeded or Failed"
May 11 18:07:30.955: INFO: Pod "pod-3fe210ae-2723-47b4-ab05-a3cdb9e19f03": Phase="Pending", Reason="", readiness=false. Elapsed: 69.902456ms
May 11 18:07:33.023: INFO: Pod "pod-3fe210ae-2723-47b4-ab05-a3cdb9e19f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138085217s
May 11 18:07:35.091: INFO: Pod "pod-3fe210ae-2723-47b4-ab05-a3cdb9e19f03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.205501966s
STEP: Saw pod success
May 11 18:07:35.091: INFO: Pod "pod-3fe210ae-2723-47b4-ab05-a3cdb9e19f03" satisfied condition "Succeeded or Failed"
May 11 18:07:35.162: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-3fe210ae-2723-47b4-ab05-a3cdb9e19f03 container test-container: <nil>
STEP: delete the pod
May 11 18:07:35.321: INFO: Waiting for pod pod-3fe210ae-2723-47b4-ab05-a3cdb9e19f03 to disappear
May 11 18:07:35.400: INFO: Pod pod-3fe210ae-2723-47b4-ab05-a3cdb9e19f03 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:35.400: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4601" for this suite.


• [SLOW TEST:5.204 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":27,"skipped":440,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:42.709: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2475" for this suite.


• [SLOW TEST:7.279 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":-1,"completed":28,"skipped":462,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:43.844: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-7084" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":-1,"completed":29,"skipped":476,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:07:44.411: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f690acdc-84e8-4ddb-8bcf-c989b78fb551" in namespace "projected-6311" to be "Succeeded or Failed"
May 11 18:07:44.477: INFO: Pod "downwardapi-volume-f690acdc-84e8-4ddb-8bcf-c989b78fb551": Phase="Pending", Reason="", readiness=false. Elapsed: 66.519038ms
May 11 18:07:46.548: INFO: Pod "downwardapi-volume-f690acdc-84e8-4ddb-8bcf-c989b78fb551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137021601s
May 11 18:07:48.618: INFO: Pod "downwardapi-volume-f690acdc-84e8-4ddb-8bcf-c989b78fb551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.207571295s
STEP: Saw pod success
May 11 18:07:48.618: INFO: Pod "downwardapi-volume-f690acdc-84e8-4ddb-8bcf-c989b78fb551" satisfied condition "Succeeded or Failed"
May 11 18:07:48.686: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod downwardapi-volume-f690acdc-84e8-4ddb-8bcf-c989b78fb551 container client-container: <nil>
STEP: delete the pod
May 11 18:07:48.847: INFO: Waiting for pod downwardapi-volume-f690acdc-84e8-4ddb-8bcf-c989b78fb551 to disappear
May 11 18:07:48.914: INFO: Pod downwardapi-volume-f690acdc-84e8-4ddb-8bcf-c989b78fb551 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:48.914: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6311" for this suite.


• [SLOW TEST:5.190 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":-1,"completed":30,"skipped":479,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3121
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 11 18:07:29.003: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
May 11 18:07:29.449: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 11 18:07:31.513: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 11 18:07:33.518: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:35.514: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:37.514: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:39.514: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:41.513: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:43.514: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:45.515: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:07:47.514: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 11 18:07:47.648: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 11 18:07:47.776: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 11 18:07:52.119: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 11 18:07:52.119: INFO: Breadth first check of 10.129.2.165 on host 10.0.128.72...
May 11 18:07:52.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.129.2.168:9080/dial?request=hostname&protocol=udp&host=10.129.2.165&port=8081&tries=1'] Namespace:pod-network-test-3121 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:07:52.694: INFO: Waiting for responses: map[]
May 11 18:07:52.694: INFO: reached 10.129.2.165 after 0/1 tries
May 11 18:07:52.694: INFO: Breadth first check of 10.131.0.56 on host 10.0.143.31...
May 11 18:07:52.758: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.129.2.168:9080/dial?request=hostname&protocol=udp&host=10.131.0.56&port=8081&tries=1'] Namespace:pod-network-test-3121 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:07:53.229: INFO: Waiting for responses: map[]
May 11 18:07:53.229: INFO: reached 10.131.0.56 after 0/1 tries
May 11 18:07:53.229: INFO: Breadth first check of 10.128.2.100 on host 10.0.198.156...
May 11 18:07:53.293: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.129.2.168:9080/dial?request=hostname&protocol=udp&host=10.128.2.100&port=8081&tries=1'] Namespace:pod-network-test-3121 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:07:53.789: INFO: Waiting for responses: map[]
May 11 18:07:53.789: INFO: reached 10.128.2.100 after 0/1 tries
May 11 18:07:53.789: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:53.789: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-3121" for this suite.


• [SLOW TEST:25.483 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":-1,"completed":54,"skipped":1101,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:07:49.526: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3476 create -f -'
May 11 18:07:50.343: INFO: stderr: ""
May 11 18:07:50.343: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 11 18:07:50.343: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3476 create -f -'
May 11 18:07:51.162: INFO: stderr: ""
May 11 18:07:51.162: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 11 18:07:52.229: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 18:07:52.229: INFO: Found 0 / 1
May 11 18:07:53.232: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 18:07:53.232: INFO: Found 0 / 1
May 11 18:07:54.236: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 18:07:54.236: INFO: Found 1 / 1
May 11 18:07:54.236: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 11 18:07:54.303: INFO: Selector matched 1 pods for map[app:agnhost]
May 11 18:07:54.303: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 11 18:07:54.303: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3476 describe pod agnhost-primary-jqkh7'
May 11 18:07:54.717: INFO: stderr: ""
May 11 18:07:54.717: INFO: stdout: "Name:         agnhost-primary-jqkh7\nNamespace:    kubectl-3476\nPriority:     0\nNode:         ip-10-0-198-156.us-west-1.compute.internal/10.0.198.156\nStart Time:   Tue, 11 May 2021 18:07:50 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.128.2.102\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.128.2.102\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           10.128.2.102\nIPs:\n  IP:           10.128.2.102\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://84e56f31d30dfb9cc22037fdb2e7c34cdd103fe142d0b9c610962f2e16962356\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 11 May 2021 18:07:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qq5vr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-qq5vr:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-qq5vr\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       4s    default-scheduler  Successfully assigned kubectl-3476/agnhost-primary-jqkh7 to ip-10-0-198-156.us-west-1.compute.internal\n  Normal  AddedInterface  2s    multus             Add eth0 [10.128.2.102/23]\n  Normal  Pulled          2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         2s    kubelet            Started container agnhost-primary\n"
May 11 18:07:54.717: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3476 describe rc agnhost-primary'
May 11 18:07:55.181: INFO: stderr: ""
May 11 18:07:55.181: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3476\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-jqkh7\n"
May 11 18:07:55.181: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3476 describe service agnhost-primary'
May 11 18:07:55.638: INFO: stderr: ""
May 11 18:07:55.638: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3476\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.30.109.169\nIPs:               172.30.109.169\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.128.2.102:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 11 18:07:55.840: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3476 describe node ip-10-0-128-72.us-west-1.compute.internal'
May 11 18:07:56.719: INFO: stderr: ""
May 11 18:07:56.719: INFO: stdout: "Name:               ip-10-0-128-72.us-west-1.compute.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m4.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-1\n                    failure-domain.beta.kubernetes.io/zone=us-west-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-128-72\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=m4.xlarge\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=us-west-1a\n                    topology.kubernetes.io/region=us-west-1\n                    topology.kubernetes.io/zone=us-west-1a\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0acb7b555136ee2d1\"}\n                    machine.openshift.io/machine: openshift-machine-api/ci-op-4m5ks5p8-9de3b-tjwdx-worker-us-west-1a-v2hxs\n                    machineconfiguration.openshift.io/currentConfig: rendered-worker-3bf99b7aac7bd55d894c1cad224cc5be\n                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-3bf99b7aac7bd55d894c1cad224cc5be\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 11 May 2021 17:10:08 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-128-72.us-west-1.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 11 May 2021 18:07:53 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 11 May 2021 18:07:13 +0000   Tue, 11 May 2021 17:10:08 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 11 May 2021 18:07:13 +0000   Tue, 11 May 2021 17:10:08 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 11 May 2021 18:07:13 +0000   Tue, 11 May 2021 17:10:08 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 11 May 2021 18:07:13 +0000   Tue, 11 May 2021 17:11:18 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.128.72\n  Hostname:     ip-10-0-128-72.us-west-1.compute.internal\n  InternalDNS:  ip-10-0-128-72.us-west-1.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         4\n  ephemeral-storage:           125293548Ki\n  example.com/fakecpu:         1k\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16408560Ki\n  pods:                        250\n  scheduling.k8s.io/foo:       3\nAllocatable:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         3500m\n  ephemeral-storage:           114396791822\n  example.com/fakecpu:         1k\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      15257584Ki\n  pods:                        250\n  scheduling.k8s.io/foo:       3\nSystem Info:\n  Machine ID:                             1ffc88467f4c46e5a90f6fe78db288a7\n  System UUID:                            ec2e6e48-e6ad-eaf3-8108-e8efe5a84a4c\n  Boot ID:                                18a8c2aa-9d49-4f58-a353-15da43efa4f5\n  Kernel Version:                         4.18.0-240.22.1.el8_3.x86_64\n  OS Image:                               Red Hat Enterprise Linux CoreOS 47.83.202105070244-0 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.20.2-11.rhaos4.7.git704b03d.el8\n  Kubelet Version:                        v1.20.0+75370d3-1066\n  Kube-Proxy Version:                     v1.20.0+75370d3-1066\nProviderID:                               aws:///us-west-1a/i-0acb7b555136ee2d1\nNon-terminated Pods:                      (15 in total)\n  Namespace                               Name                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                   ------------  ----------  ---------------  -------------  ---\n  configmap-7780                          pod-configmaps-968f00a3-1003-434f-a8c3-71da560297c0    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  openshift-cluster-csi-drivers           aws-ebs-csi-driver-node-d96jg                          30m (0%)      0 (0%)      150Mi (1%)       0 (0%)         57m\n  openshift-cluster-node-tuning-operator  tuned-j6rbx                                            10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         57m\n  openshift-dns                           dns-default-hxb5r                                      65m (1%)      0 (0%)      131Mi (0%)       0 (0%)         57m\n  openshift-image-registry                node-ca-7m6gk                                          10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         57m\n  openshift-ingress-canary                ingress-canary-vspp4                                   10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         24m\n  openshift-machine-config-operator       machine-config-daemon-s2xtz                            40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         57m\n  openshift-monitoring                    node-exporter-m2lc8                                    9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         57m\n  openshift-multus                        multus-wgjrz                                           10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         57m\n  openshift-multus                        network-metrics-daemon-rn9fw                           20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         57m\n  openshift-network-diagnostics           network-check-target-9pvtg                             10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         57m\n  openshift-sdn                           sdn-24ltc                                              110m (3%)     0 (0%)      220Mi (1%)       0 (0%)         57m\n  pod-network-test-3121                   netserver-0                                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         27s\n  pod-network-test-3121                   test-container-pod                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         9s\n  statefulset-709                         ss2-1                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         68s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests     Limits\n  --------                    --------     ------\n  cpu                         324m (9%)    0 (0%)\n  memory                      1176Mi (7%)  0 (0%)\n  ephemeral-storage           0 (0%)       0 (0%)\n  hugepages-1Gi               0 (0%)       0 (0%)\n  hugepages-2Mi               0 (0%)       0 (0%)\n  attachable-volumes-aws-ebs  0            0\n  example.com/fakecpu         0            0\n  scheduling.k8s.io/foo       0            0\nEvents:\n  Type    Reason                   Age                From     Message\n  ----    ------                   ----               ----     -------\n  Normal  Starting                 57m                kubelet  Starting kubelet.\n  Normal  NodeHasSufficientMemory  57m (x2 over 57m)  kubelet  Node ip-10-0-128-72.us-west-1.compute.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    57m (x2 over 57m)  kubelet  Node ip-10-0-128-72.us-west-1.compute.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     57m (x2 over 57m)  kubelet  Node ip-10-0-128-72.us-west-1.compute.internal status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  57m                kubelet  Updated Node Allocatable limit across pods\n  Normal  NodeReady                56m                kubelet  Node ip-10-0-128-72.us-west-1.compute.internal status is now: NodeReady\n"
May 11 18:07:56.719: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-3476 describe namespace kubectl-3476'
May 11 18:07:57.194: INFO: stderr: ""
May 11 18:07:57.195: INFO: stdout: "Name:         kubectl-3476\nLabels:       e2e-framework=kubectl\n              e2e-run=2ac0dcc8-a89b-41d2-a717-2316c693ebf2\nAnnotations:  openshift.io/sa.scc.mcs: s0:c50,c45\n              openshift.io/sa.scc.supplemental-groups: 1002540000/10000\n              openshift.io/sa.scc.uid-range: 1002540000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:57.195: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3476" for this suite.


• [SLOW TEST:8.276 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1090
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":-1,"completed":31,"skipped":483,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-7780/configmap-test-4c31096f-f394-4cef-9572-e1a8cd95cd97
STEP: Creating a pod to test consume configMaps
May 11 18:07:54.559: INFO: Waiting up to 5m0s for pod "pod-configmaps-968f00a3-1003-434f-a8c3-71da560297c0" in namespace "configmap-7780" to be "Succeeded or Failed"
May 11 18:07:54.623: INFO: Pod "pod-configmaps-968f00a3-1003-434f-a8c3-71da560297c0": Phase="Pending", Reason="", readiness=false. Elapsed: 64.299862ms
May 11 18:07:56.688: INFO: Pod "pod-configmaps-968f00a3-1003-434f-a8c3-71da560297c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.128957835s
May 11 18:07:58.752: INFO: Pod "pod-configmaps-968f00a3-1003-434f-a8c3-71da560297c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.193742837s
STEP: Saw pod success
May 11 18:07:58.752: INFO: Pod "pod-configmaps-968f00a3-1003-434f-a8c3-71da560297c0" satisfied condition "Succeeded or Failed"
May 11 18:07:58.817: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-configmaps-968f00a3-1003-434f-a8c3-71da560297c0 container env-test: <nil>
STEP: delete the pod
May 11 18:07:58.964: INFO: Waiting for pod pod-configmaps-968f00a3-1003-434f-a8c3-71da560297c0 to disappear
May 11 18:07:59.028: INFO: Pod pod-configmaps-968f00a3-1003-434f-a8c3-71da560297c0 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:07:59.028: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-7780" for this suite.


• [SLOW TEST:5.207 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":-1,"completed":55,"skipped":1114,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 11 18:04:51.400: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 11 18:06:01.767: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
May 11 18:06:55.188: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:01.662: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2391" for this suite.


• [SLOW TEST:201.801 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":-1,"completed":21,"skipped":511,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-4876/secret-test-03fa1cb9-2898-4f00-bd5f-1efca8f4d5ac
STEP: Creating a pod to test consume secrets
May 11 18:07:59.833: INFO: Waiting up to 5m0s for pod "pod-configmaps-0784051e-88ef-43c5-9bd4-e774b1db9a8a" in namespace "secrets-4876" to be "Succeeded or Failed"
May 11 18:07:59.897: INFO: Pod "pod-configmaps-0784051e-88ef-43c5-9bd4-e774b1db9a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 64.465153ms
May 11 18:08:01.961: INFO: Pod "pod-configmaps-0784051e-88ef-43c5-9bd4-e774b1db9a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.128500551s
May 11 18:08:04.025: INFO: Pod "pod-configmaps-0784051e-88ef-43c5-9bd4-e774b1db9a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.19275639s
STEP: Saw pod success
May 11 18:08:04.025: INFO: Pod "pod-configmaps-0784051e-88ef-43c5-9bd4-e774b1db9a8a" satisfied condition "Succeeded or Failed"
May 11 18:08:04.090: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-configmaps-0784051e-88ef-43c5-9bd4-e774b1db9a8a container env-test: <nil>
STEP: delete the pod
May 11 18:08:04.248: INFO: Waiting for pod pod-configmaps-0784051e-88ef-43c5-9bd4-e774b1db9a8a to disappear
May 11 18:08:04.313: INFO: Pod pod-configmaps-0784051e-88ef-43c5-9bd4-e774b1db9a8a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:04.313: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-4876" for this suite.


• [SLOW TEST:5.207 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":-1,"completed":56,"skipped":1176,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-c3f4c011-71b8-4fdc-b837-fe6b65830aab
STEP: Creating a pod to test consume configMaps
May 11 18:08:02.419: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-922d1373-350e-458e-ae24-9cfba1b3e692" in namespace "projected-882" to be "Succeeded or Failed"
May 11 18:08:02.493: INFO: Pod "pod-projected-configmaps-922d1373-350e-458e-ae24-9cfba1b3e692": Phase="Pending", Reason="", readiness=false. Elapsed: 73.651098ms
May 11 18:08:04.562: INFO: Pod "pod-projected-configmaps-922d1373-350e-458e-ae24-9cfba1b3e692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.142343933s
May 11 18:08:06.627: INFO: Pod "pod-projected-configmaps-922d1373-350e-458e-ae24-9cfba1b3e692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.207796583s
STEP: Saw pod success
May 11 18:08:06.627: INFO: Pod "pod-projected-configmaps-922d1373-350e-458e-ae24-9cfba1b3e692" satisfied condition "Succeeded or Failed"
May 11 18:08:06.695: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-projected-configmaps-922d1373-350e-458e-ae24-9cfba1b3e692 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:08:06.851: INFO: Waiting for pod pod-projected-configmaps-922d1373-350e-458e-ae24-9cfba1b3e692 to disappear
May 11 18:08:06.917: INFO: Pod pod-projected-configmaps-922d1373-350e-458e-ae24-9cfba1b3e692 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:06.917: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-882" for this suite.


• [SLOW TEST:5.245 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":-1,"completed":22,"skipped":516,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-9f6ce976-7fe9-458d-9a79-af140f50bd7d
STEP: Creating a pod to test consume secrets
May 11 18:08:05.076: INFO: Waiting up to 5m0s for pod "pod-secrets-fdcdc66b-ad84-42ca-a455-a57d5129fae9" in namespace "secrets-2487" to be "Succeeded or Failed"
May 11 18:08:05.140: INFO: Pod "pod-secrets-fdcdc66b-ad84-42ca-a455-a57d5129fae9": Phase="Pending", Reason="", readiness=false. Elapsed: 64.191473ms
May 11 18:08:07.204: INFO: Pod "pod-secrets-fdcdc66b-ad84-42ca-a455-a57d5129fae9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.128396507s
May 11 18:08:09.269: INFO: Pod "pod-secrets-fdcdc66b-ad84-42ca-a455-a57d5129fae9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.193033506s
STEP: Saw pod success
May 11 18:08:09.269: INFO: Pod "pod-secrets-fdcdc66b-ad84-42ca-a455-a57d5129fae9" satisfied condition "Succeeded or Failed"
May 11 18:08:09.333: INFO: Trying to get logs from node ip-10-0-143-31.us-west-1.compute.internal pod pod-secrets-fdcdc66b-ad84-42ca-a455-a57d5129fae9 container secret-volume-test: <nil>
STEP: delete the pod
May 11 18:08:09.489: INFO: Waiting for pod pod-secrets-fdcdc66b-ad84-42ca-a455-a57d5129fae9 to disappear
May 11 18:08:09.553: INFO: Pod pod-secrets-fdcdc66b-ad84-42ca-a455-a57d5129fae9 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:09.553: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-2487" for this suite.


• [SLOW TEST:5.213 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":57,"skipped":1184,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
May 11 18:08:10.142: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-517 cluster-info'
May 11 18:08:10.393: INFO: stderr: ""
May 11 18:08:10.393: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.ci-op-4m5ks5p8-9de3b.origin-ci-int-aws.dev.rhcloud.com:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:10.393: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-517" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":-1,"completed":58,"skipped":1189,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-8615ad4b-f05d-4cb8-8fd7-e47417c9922b
STEP: Creating a pod to test consume configMaps
May 11 18:08:11.082: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3274e39b-a313-4823-966a-7fcc4663f2ca" in namespace "projected-9766" to be "Succeeded or Failed"
May 11 18:08:11.147: INFO: Pod "pod-projected-configmaps-3274e39b-a313-4823-966a-7fcc4663f2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 64.259616ms
May 11 18:08:13.212: INFO: Pod "pod-projected-configmaps-3274e39b-a313-4823-966a-7fcc4663f2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129970728s
May 11 18:08:15.277: INFO: Pod "pod-projected-configmaps-3274e39b-a313-4823-966a-7fcc4663f2ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.194487659s
STEP: Saw pod success
May 11 18:08:15.277: INFO: Pod "pod-projected-configmaps-3274e39b-a313-4823-966a-7fcc4663f2ca" satisfied condition "Succeeded or Failed"
May 11 18:08:15.345: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-projected-configmaps-3274e39b-a313-4823-966a-7fcc4663f2ca container agnhost-container: <nil>
STEP: delete the pod
May 11 18:08:15.492: INFO: Waiting for pod pod-projected-configmaps-3274e39b-a313-4823-966a-7fcc4663f2ca to disappear
May 11 18:08:15.556: INFO: Pod pod-projected-configmaps-3274e39b-a313-4823-966a-7fcc4663f2ca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:15.556: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9766" for this suite.


• [SLOW TEST:5.204 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":59,"skipped":1200,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6520.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6520.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6520.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6520.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 18:08:04.531: INFO: DNS probes using dns-test-84ebd5cc-1ef3-470b-bf30-aab50a5b8ef3 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6520.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6520.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6520.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6520.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 18:08:11.182: INFO: DNS probes using dns-test-2333a75a-60d4-4e12-a3fb-62ae9dec19c9 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6520.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6520.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6520.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6520.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 18:08:15.923: INFO: DNS probes using dns-test-b5f622a0-ddf1-4919-b344-79fcd66a23a3 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:16.110: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-6520" for this suite.


• [SLOW TEST:18.684 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":-1,"completed":32,"skipped":645,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-scheduling] LimitRange
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 11 18:08:16.874: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 11 18:08:17.009: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 11 18:08:17.009: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 11 18:08:17.160: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 11 18:08:17.160: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 11 18:08:17.304: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 11 18:08:17.304: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 11 18:08:24.879: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:24.958: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "limitrange-2715" for this suite.


• [SLOW TEST:8.827 seconds]
[sig-scheduling] LimitRange
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":-1,"completed":33,"skipped":664,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:27.685: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-6078" for this suite.


• [SLOW TEST:12.089 seconds]
[sig-api-machinery] ResourceQuota
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":-1,"completed":60,"skipped":1224,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:39.803: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-8588" for this suite.


• [SLOW TEST:12.077 seconds]
[sig-api-machinery] ResourceQuota
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":-1,"completed":61,"skipped":1251,"failed":0}

SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:40.551: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-2836" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

•
------------------------------
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":-1,"completed":62,"skipped":1272,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-709
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 11 18:05:48.206: INFO: Found 1 stateful pods, waiting for 3
May 11 18:05:58.282: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:05:58.282: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:05:58.282: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 11 18:06:08.279: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:06:08.279: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:06:08.279: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:06:08.475: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-709 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 11 18:06:09.237: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 11 18:06:09.237: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 11 18:06:09.237: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 11 18:06:19.653: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 11 18:06:19.854: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-709 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:06:20.590: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 11 18:06:20.590: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 11 18:06:20.590: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 11 18:06:30.990: INFO: Waiting for StatefulSet statefulset-709/ss2 to complete update
May 11 18:06:30.990: INFO: Waiting for Pod statefulset-709/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 11 18:06:30.990: INFO: Waiting for Pod statefulset-709/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 11 18:06:41.124: INFO: Waiting for StatefulSet statefulset-709/ss2 to complete update
May 11 18:06:41.124: INFO: Waiting for Pod statefulset-709/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 11 18:06:41.124: INFO: Waiting for Pod statefulset-709/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 11 18:06:51.125: INFO: Waiting for StatefulSet statefulset-709/ss2 to complete update
May 11 18:06:51.125: INFO: Waiting for Pod statefulset-709/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 11 18:07:01.122: INFO: Waiting for StatefulSet statefulset-709/ss2 to complete update
May 11 18:07:01.123: INFO: Waiting for Pod statefulset-709/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 11 18:07:11.121: INFO: Waiting for StatefulSet statefulset-709/ss2 to complete update
STEP: Rolling back to a previous revision
May 11 18:07:21.124: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-709 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 11 18:07:21.919: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 11 18:07:21.919: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 11 18:07:21.919: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 11 18:07:32.333: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 11 18:07:32.534: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-709 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:07:33.282: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 11 18:07:33.282: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 11 18:07:33.283: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 11 18:07:43.679: INFO: Waiting for StatefulSet statefulset-709/ss2 to complete update
May 11 18:07:43.679: INFO: Waiting for Pod statefulset-709/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 11 18:07:43.679: INFO: Waiting for Pod statefulset-709/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 11 18:07:53.815: INFO: Waiting for StatefulSet statefulset-709/ss2 to complete update
May 11 18:07:53.815: INFO: Waiting for Pod statefulset-709/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 11 18:07:53.815: INFO: Waiting for Pod statefulset-709/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 11 18:08:03.811: INFO: Waiting for StatefulSet statefulset-709/ss2 to complete update
May 11 18:08:03.811: INFO: Waiting for Pod statefulset-709/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 11 18:08:13.810: INFO: Deleting all statefulset in ns statefulset-709
May 11 18:08:13.880: INFO: Scaling statefulset ss2 to 0
May 11 18:08:44.145: INFO: Waiting for statefulset status.replicas updated to 0
May 11 18:08:44.211: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:44.419: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-709" for this suite.


• [SLOW TEST:177.040 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":-1,"completed":52,"skipped":966,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
May 11 18:08:45.434: INFO: Pod pod-hostip-58641ef8-5f91-4e3a-af29-bba9262c0282 has hostIP: 10.0.128.72
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:45.434: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-5095" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":-1,"completed":63,"skipped":1323,"failed":0}

S
------------------------------
[BeforeEach] [sig-network] IngressClass API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 11 18:08:46.608: INFO: starting watch
STEP: patching
STEP: updating
May 11 18:08:46.808: INFO: waiting for watch events with expected annotations
May 11 18:08:46.808: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:47.153: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "ingressclass-8476" for this suite.

•
------------------------------
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":-1,"completed":64,"skipped":1324,"failed":0}

S
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1645
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1645
STEP: creating replication controller externalsvc in namespace services-1645
I0511 18:08:25.875344   45016 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1645, replica count: 2
I0511 18:08:28.975685   45016 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 11 18:08:29.192: INFO: Creating new exec pod
May 11 18:08:33.407: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-1645 exec execpodccwg9 -- /bin/sh -x -c nslookup clusterip-service.services-1645.svc.cluster.local'
May 11 18:08:34.156: INFO: stderr: "+ nslookup clusterip-service.services-1645.svc.cluster.local\n"
May 11 18:08:34.156: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-1645.svc.cluster.local\tcanonical name = externalsvc.services-1645.svc.cluster.local.\nName:\texternalsvc.services-1645.svc.cluster.local\nAddress: 172.30.123.124\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1645, will wait for the garbage collector to delete the pods
May 11 18:08:34.397: INFO: Deleting ReplicationController externalsvc took: 71.032474ms
May 11 18:08:34.497: INFO: Terminating ReplicationController externalsvc pods took: 100.210026ms
May 11 18:08:48.783: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:48.861: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-1645" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:23.878 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":-1,"completed":34,"skipped":678,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-acf75292-36ed-43bc-8c64-d1ef5838e254
May 11 18:08:49.618: INFO: Pod name my-hostname-basic-acf75292-36ed-43bc-8c64-d1ef5838e254: Found 1 pods out of 1
May 11 18:08:49.618: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-acf75292-36ed-43bc-8c64-d1ef5838e254" are running
May 11 18:08:53.751: INFO: Pod "my-hostname-basic-acf75292-36ed-43bc-8c64-d1ef5838e254-4kz2n" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-11 18:08:49 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-11 18:08:49 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-acf75292-36ed-43bc-8c64-d1ef5838e254]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-11 18:08:49 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-acf75292-36ed-43bc-8c64-d1ef5838e254]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-11 18:08:49 +0000 UTC Reason: Message:}])
May 11 18:08:53.752: INFO: Trying to dial the pod
May 11 18:08:58.960: INFO: Controller my-hostname-basic-acf75292-36ed-43bc-8c64-d1ef5838e254: Got expected result from replica 1 [my-hostname-basic-acf75292-36ed-43bc-8c64-d1ef5838e254-4kz2n]: "my-hostname-basic-acf75292-36ed-43bc-8c64-d1ef5838e254-4kz2n", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:08:58.960: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-1672" for this suite.


• [SLOW TEST:10.087 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":-1,"completed":35,"skipped":681,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 11 18:08:59.603: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-9451 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
May 11 18:08:59.882: INFO: stderr: ""
May 11 18:08:59.882: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
May 11 18:08:59.948: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-9451 delete pods e2e-test-httpd-pod'
May 11 18:09:02.753: INFO: stderr: ""
May 11 18:09:02.753: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:09:02.753: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9451" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":-1,"completed":36,"skipped":709,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-9192
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9192
STEP: Deleting pre-stop pod
May 11 18:09:03.240: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:09:03.315: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "prestop-9192" for this suite.


• [SLOW TEST:16.279 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":-1,"completed":65,"skipped":1325,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-7d6c2910-56c3-490d-ad80-b541e5f90730
STEP: Creating a pod to test consume configMaps
May 11 18:09:04.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-773a2ef5-1087-48e5-a094-74735f1ee418" in namespace "configmap-4714" to be "Succeeded or Failed"
May 11 18:09:04.157: INFO: Pod "pod-configmaps-773a2ef5-1087-48e5-a094-74735f1ee418": Phase="Pending", Reason="", readiness=false. Elapsed: 64.31683ms
May 11 18:09:06.225: INFO: Pod "pod-configmaps-773a2ef5-1087-48e5-a094-74735f1ee418": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131978819s
May 11 18:09:08.289: INFO: Pod "pod-configmaps-773a2ef5-1087-48e5-a094-74735f1ee418": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.196320389s
STEP: Saw pod success
May 11 18:09:08.289: INFO: Pod "pod-configmaps-773a2ef5-1087-48e5-a094-74735f1ee418" satisfied condition "Succeeded or Failed"
May 11 18:09:08.354: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-configmaps-773a2ef5-1087-48e5-a094-74735f1ee418 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:09:08.507: INFO: Waiting for pod pod-configmaps-773a2ef5-1087-48e5-a094-74735f1ee418 to disappear
May 11 18:09:08.572: INFO: Pod pod-configmaps-773a2ef5-1087-48e5-a094-74735f1ee418 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:09:08.572: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-4714" for this suite.


• [SLOW TEST:5.216 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":66,"skipped":1352,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-2493
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 11 18:08:45.057: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
May 11 18:08:45.498: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 11 18:08:47.567: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 11 18:08:49.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:08:51.569: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:08:53.568: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:08:55.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:08:57.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:08:59.568: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:09:01.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:09:03.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 11 18:09:05.564: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 11 18:09:05.699: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 11 18:09:05.830: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 11 18:09:10.421: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 11 18:09:10.421: INFO: Going to poll 10.129.2.179 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 11 18:09:10.486: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.179:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2493 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:10.997: INFO: Found all 1 expected endpoints: [netserver-0]
May 11 18:09:10.997: INFO: Going to poll 10.131.0.59 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 11 18:09:11.066: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.0.59:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2493 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:11.583: INFO: Found all 1 expected endpoints: [netserver-1]
May 11 18:09:11.583: INFO: Going to poll 10.128.2.109 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 11 18:09:11.651: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.109:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2493 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:12.154: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:09:12.154: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-2493" for this suite.


• [SLOW TEST:27.690 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":53,"skipped":998,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3407 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3407;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3407 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3407;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3407.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3407.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3407.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3407.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3407.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3407.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3407.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3407.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3407.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3407.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3407.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3407.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3407.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 37.99.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.99.37_udp@PTR;check="$$(dig +tcp +noall +answer +search 37.99.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.99.37_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3407 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3407;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3407 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3407;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3407.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3407.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3407.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3407.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3407.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3407.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3407.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3407.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3407.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3407.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3407.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3407.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3407.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 37.99.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.99.37_udp@PTR;check="$$(dig +tcp +noall +answer +search 37.99.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.99.37_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 18:09:13.677: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:13.741: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:13.806: INFO: Unable to read wheezy_udp@dns-test-service.dns-3407 from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:13.871: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3407 from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:13.936: INFO: Unable to read wheezy_udp@dns-test-service.dns-3407.svc from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:14.001: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3407.svc from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:14.066: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3407.svc from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:14.596: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:14.660: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:14.725: INFO: Unable to read jessie_udp@dns-test-service.dns-3407 from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:14.793: INFO: Unable to read jessie_tcp@dns-test-service.dns-3407 from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:14.858: INFO: Unable to read jessie_udp@dns-test-service.dns-3407.svc from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:14.989: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3407.svc from pod dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485: the server could not find the requested resource (get pods dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485)
May 11 18:09:15.452: INFO: Lookups using dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3407 wheezy_tcp@dns-test-service.dns-3407 wheezy_udp@dns-test-service.dns-3407.svc wheezy_tcp@dns-test-service.dns-3407.svc wheezy_udp@_http._tcp.dns-test-service.dns-3407.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3407 jessie_tcp@dns-test-service.dns-3407 jessie_udp@dns-test-service.dns-3407.svc jessie_udp@_http._tcp.dns-test-service.dns-3407.svc]

May 11 18:09:22.302: INFO: DNS probes using dns-3407/dns-test-9edfa002-9e65-49a7-ab67-4064d4f86485 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:09:22.544: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-3407" for this suite.


• [SLOW TEST:13.961 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":-1,"completed":67,"skipped":1363,"failed":0}

S
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 11 18:09:12.826: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 create -f -'
May 11 18:09:13.629: INFO: stderr: ""
May 11 18:09:13.629: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 11 18:09:13.629: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 11 18:09:13.897: INFO: stderr: ""
May 11 18:09:13.897: INFO: stdout: "update-demo-nautilus-cbvpr update-demo-nautilus-jph6h "
May 11 18:09:13.897: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-cbvpr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 18:09:14.161: INFO: stderr: ""
May 11 18:09:14.161: INFO: stdout: ""
May 11 18:09:14.161: INFO: update-demo-nautilus-cbvpr is created but not running
May 11 18:09:19.161: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 11 18:09:19.481: INFO: stderr: ""
May 11 18:09:19.481: INFO: stdout: "update-demo-nautilus-cbvpr update-demo-nautilus-jph6h "
May 11 18:09:19.481: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-cbvpr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 18:09:19.739: INFO: stderr: ""
May 11 18:09:19.739: INFO: stdout: "true"
May 11 18:09:19.739: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-cbvpr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 11 18:09:19.997: INFO: stderr: ""
May 11 18:09:19.997: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 18:09:19.997: INFO: validating pod update-demo-nautilus-cbvpr
May 11 18:09:20.065: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 18:09:20.065: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 18:09:20.065: INFO: update-demo-nautilus-cbvpr is verified up and running
May 11 18:09:20.065: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-jph6h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 18:09:20.318: INFO: stderr: ""
May 11 18:09:20.318: INFO: stdout: "true"
May 11 18:09:20.318: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-jph6h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 11 18:09:20.574: INFO: stderr: ""
May 11 18:09:20.574: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 18:09:20.574: INFO: validating pod update-demo-nautilus-jph6h
May 11 18:09:20.648: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 18:09:20.648: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 18:09:20.648: INFO: update-demo-nautilus-jph6h is verified up and running
STEP: scaling down the replication controller
May 11 18:09:20.652: INFO: scanned /alabama for discovery docs: <nil>
May 11 18:09:20.652: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 11 18:09:21.057: INFO: stderr: ""
May 11 18:09:21.057: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 11 18:09:21.057: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 11 18:09:21.378: INFO: stderr: ""
May 11 18:09:21.379: INFO: stdout: "update-demo-nautilus-cbvpr update-demo-nautilus-jph6h "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 11 18:09:26.379: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 11 18:09:26.695: INFO: stderr: ""
May 11 18:09:26.695: INFO: stdout: "update-demo-nautilus-cbvpr update-demo-nautilus-jph6h "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 11 18:09:31.696: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 11 18:09:32.016: INFO: stderr: ""
May 11 18:09:32.016: INFO: stdout: "update-demo-nautilus-cbvpr update-demo-nautilus-jph6h "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 11 18:09:37.017: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 11 18:09:37.282: INFO: stderr: ""
May 11 18:09:37.283: INFO: stdout: "update-demo-nautilus-jph6h "
May 11 18:09:37.283: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-jph6h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 18:09:37.531: INFO: stderr: ""
May 11 18:09:37.531: INFO: stdout: "true"
May 11 18:09:37.531: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-jph6h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 11 18:09:37.791: INFO: stderr: ""
May 11 18:09:37.791: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 18:09:37.792: INFO: validating pod update-demo-nautilus-jph6h
May 11 18:09:37.864: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 18:09:37.864: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 18:09:37.864: INFO: update-demo-nautilus-jph6h is verified up and running
STEP: scaling up the replication controller
May 11 18:09:37.867: INFO: scanned /alabama for discovery docs: <nil>
May 11 18:09:37.867: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 11 18:09:38.267: INFO: stderr: ""
May 11 18:09:38.267: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 11 18:09:38.267: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 11 18:09:38.578: INFO: stderr: ""
May 11 18:09:38.578: INFO: stdout: "update-demo-nautilus-jph6h update-demo-nautilus-vgjsh "
May 11 18:09:38.578: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-jph6h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 18:09:38.840: INFO: stderr: ""
May 11 18:09:38.840: INFO: stdout: "true"
May 11 18:09:38.840: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-jph6h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 11 18:09:39.093: INFO: stderr: ""
May 11 18:09:39.093: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 18:09:39.093: INFO: validating pod update-demo-nautilus-jph6h
May 11 18:09:39.161: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 18:09:39.161: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 18:09:39.161: INFO: update-demo-nautilus-jph6h is verified up and running
May 11 18:09:39.161: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-vgjsh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 18:09:39.421: INFO: stderr: ""
May 11 18:09:39.421: INFO: stdout: ""
May 11 18:09:39.421: INFO: update-demo-nautilus-vgjsh is created but not running
May 11 18:09:44.421: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 11 18:09:44.735: INFO: stderr: ""
May 11 18:09:44.735: INFO: stdout: "update-demo-nautilus-jph6h update-demo-nautilus-vgjsh "
May 11 18:09:44.735: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-jph6h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 18:09:44.986: INFO: stderr: ""
May 11 18:09:44.986: INFO: stdout: "true"
May 11 18:09:44.986: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-jph6h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 11 18:09:45.234: INFO: stderr: ""
May 11 18:09:45.234: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 18:09:45.234: INFO: validating pod update-demo-nautilus-jph6h
May 11 18:09:45.306: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 18:09:45.306: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 18:09:45.306: INFO: update-demo-nautilus-jph6h is verified up and running
May 11 18:09:45.306: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-vgjsh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 11 18:09:45.556: INFO: stderr: ""
May 11 18:09:45.556: INFO: stdout: "true"
May 11 18:09:45.556: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods update-demo-nautilus-vgjsh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 11 18:09:45.816: INFO: stderr: ""
May 11 18:09:45.816: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 18:09:45.816: INFO: validating pod update-demo-nautilus-vgjsh
May 11 18:09:45.884: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 18:09:45.884: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 18:09:45.884: INFO: update-demo-nautilus-vgjsh is verified up and running
STEP: using delete to clean up resources
May 11 18:09:45.884: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 delete --grace-period=0 --force -f -'
May 11 18:09:46.207: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 18:09:46.207: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 11 18:09:46.207: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get rc,svc -l name=update-demo --no-headers'
May 11 18:09:46.539: INFO: stderr: "No resources found in kubectl-283 namespace.\n"
May 11 18:09:46.539: INFO: stdout: ""
May 11 18:09:46.539: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-283 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 11 18:09:46.856: INFO: stderr: ""
May 11 18:09:46.856: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:09:46.856: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-283" for this suite.


• [SLOW TEST:34.621 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":-1,"completed":54,"skipped":1045,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 11 18:09:54.038: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9778 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:54.525: INFO: Exec stderr: ""
May 11 18:09:54.525: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9778 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:55.020: INFO: Exec stderr: ""
May 11 18:09:55.020: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9778 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:55.481: INFO: Exec stderr: ""
May 11 18:09:55.481: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9778 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:55.946: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 11 18:09:55.946: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9778 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:56.403: INFO: Exec stderr: ""
May 11 18:09:56.403: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9778 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:56.867: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 11 18:09:56.867: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9778 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:57.353: INFO: Exec stderr: ""
May 11 18:09:57.353: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9778 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:57.873: INFO: Exec stderr: ""
May 11 18:09:57.873: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9778 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:58.351: INFO: Exec stderr: ""
May 11 18:09:58.351: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9778 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:09:58.841: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:09:58.841: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9778" for this suite.


• [SLOW TEST:11.952 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":55,"skipped":1069,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:09:03.463: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-b34d8a5c-4003-47d5-bb0e-5735b92990cf
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-b34d8a5c-4003-47d5-bb0e-5735b92990cf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:18.458: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-9898" for this suite.


• [SLOW TEST:75.673 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":37,"skipped":737,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:10:00.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:10:02.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:10:04.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353400, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:10:07.899: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:19.057: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-9981" for this suite.
STEP: Destroying namespace "webhook-9981-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:20.667 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":-1,"completed":56,"skipped":1071,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
May 11 18:10:19.114: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-5384 create -f -'
May 11 18:10:19.949: INFO: stderr: ""
May 11 18:10:19.949: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 11 18:10:19.949: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-5384 diff -f -'
May 11 18:10:20.750: INFO: rc: 1
May 11 18:10:20.750: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-5384 delete -f -'
May 11 18:10:21.081: INFO: stderr: ""
May 11 18:10:21.081: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:21.081: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5384" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":-1,"completed":38,"skipped":768,"failed":0}

S
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:23.193: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-501" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":-1,"completed":39,"skipped":769,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-acb3d12a-57c7-459a-ba75-29ae33e1f7ac
STEP: Creating a pod to test consume secrets
May 11 18:10:20.287: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b90c8116-9b31-44f1-a5c5-4e9f45e44ccf" in namespace "projected-4434" to be "Succeeded or Failed"
May 11 18:10:20.354: INFO: Pod "pod-projected-secrets-b90c8116-9b31-44f1-a5c5-4e9f45e44ccf": Phase="Pending", Reason="", readiness=false. Elapsed: 66.531566ms
May 11 18:10:22.425: INFO: Pod "pod-projected-secrets-b90c8116-9b31-44f1-a5c5-4e9f45e44ccf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13818247s
May 11 18:10:24.492: INFO: Pod "pod-projected-secrets-b90c8116-9b31-44f1-a5c5-4e9f45e44ccf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.2044538s
STEP: Saw pod success
May 11 18:10:24.492: INFO: Pod "pod-projected-secrets-b90c8116-9b31-44f1-a5c5-4e9f45e44ccf" satisfied condition "Succeeded or Failed"
May 11 18:10:24.558: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-projected-secrets-b90c8116-9b31-44f1-a5c5-4e9f45e44ccf container secret-volume-test: <nil>
STEP: delete the pod
May 11 18:10:24.712: INFO: Waiting for pod pod-projected-secrets-b90c8116-9b31-44f1-a5c5-4e9f45e44ccf to disappear
May 11 18:10:24.781: INFO: Pod pod-projected-secrets-b90c8116-9b31-44f1-a5c5-4e9f45e44ccf no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:24.781: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4434" for this suite.


• [SLOW TEST:5.241 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":-1,"completed":57,"skipped":1089,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:09:30.546: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 11 18:10:15.546: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-86 --namespace=crd-publish-openapi-86 create -f -'
May 11 18:10:16.881: INFO: stderr: ""
May 11 18:10:16.881: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 11 18:10:16.881: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-86 --namespace=crd-publish-openapi-86 delete e2e-test-crd-publish-openapi-4111-crds test-cr'
May 11 18:10:17.212: INFO: stderr: ""
May 11 18:10:17.212: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 11 18:10:17.212: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-86 --namespace=crd-publish-openapi-86 apply -f -'
May 11 18:10:18.173: INFO: stderr: ""
May 11 18:10:18.173: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 11 18:10:18.174: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-86 --namespace=crd-publish-openapi-86 delete e2e-test-crd-publish-openapi-4111-crds test-cr'
May 11 18:10:18.501: INFO: stderr: ""
May 11 18:10:18.501: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 11 18:10:18.502: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-86 explain e2e-test-crd-publish-openapi-4111-crds'
May 11 18:10:18.933: INFO: stderr: ""
May 11 18:10:18.933: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4111-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:27.299: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-86" for this suite.


• [SLOW TEST:64.756 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":-1,"completed":68,"skipped":1364,"failed":0}

S
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:10:23.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76866821-c5fe-4d4f-a832-66121844373a" in namespace "projected-1963" to be "Succeeded or Failed"
May 11 18:10:24.023: INFO: Pod "downwardapi-volume-76866821-c5fe-4d4f-a832-66121844373a": Phase="Pending", Reason="", readiness=false. Elapsed: 70.488408ms
May 11 18:10:26.090: INFO: Pod "downwardapi-volume-76866821-c5fe-4d4f-a832-66121844373a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137434239s
May 11 18:10:28.157: INFO: Pod "downwardapi-volume-76866821-c5fe-4d4f-a832-66121844373a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.204286411s
STEP: Saw pod success
May 11 18:10:28.157: INFO: Pod "downwardapi-volume-76866821-c5fe-4d4f-a832-66121844373a" satisfied condition "Succeeded or Failed"
May 11 18:10:28.225: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downwardapi-volume-76866821-c5fe-4d4f-a832-66121844373a container client-container: <nil>
STEP: delete the pod
May 11 18:10:28.380: INFO: Waiting for pod downwardapi-volume-76866821-c5fe-4d4f-a832-66121844373a to disappear
May 11 18:10:28.450: INFO: Pod downwardapi-volume-76866821-c5fe-4d4f-a832-66121844373a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:28.450: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1963" for this suite.


• [SLOW TEST:5.249 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":-1,"completed":40,"skipped":773,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:10:26.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353426, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353426, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353426, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353426, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:10:28.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353426, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353426, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353426, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353426, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:10:31.464: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5454-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:32.284: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-9309" for this suite.
STEP: Destroying namespace "webhook-9309-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:7.888 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":-1,"completed":58,"skipped":1093,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 11 18:10:37.792: INFO: &Pod{ObjectMeta:{send-events-6e6bd749-a12c-4685-b061-59f01a5636ef  events-3893 /api/v1/namespaces/events-3893/pods/send-events-6e6bd749-a12c-4685-b061-59f01a5636ef 246d7a47-8347-4ac3-bc7e-2e86a83620ab 66719 0 2021-05-11 18:10:33 +0000 UTC <nil> <nil> map[name:foo time:382336645] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.2.194"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.2.194"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-05-11 18:10:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:10:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:10:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n4pdq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n4pdq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n4pdq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-f8kw7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:10:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:10:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:10:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:10:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:10.129.2.194,StartTime:2021-05-11 18:10:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:10:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://760b0aed23e06c8046ae4e2863a88c4ded7e0b99fb2da7b1813ed636656abed6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 11 18:10:39.859: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 11 18:10:41.925: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:41.996: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-3893" for this suite.


• [SLOW TEST:9.288 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":-1,"completed":59,"skipped":1115,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
May 11 18:10:29.084: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-9134 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 11 18:10:29.362: INFO: stderr: ""
May 11 18:10:29.362: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
May 11 18:10:29.362: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 11 18:10:29.362: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9134" to be "running and ready, or succeeded"
May 11 18:10:29.428: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 66.268838ms
May 11 18:10:31.496: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133515873s
May 11 18:10:33.569: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.206930252s
May 11 18:10:33.569: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 11 18:10:33.569: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 11 18:10:33.569: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-9134 logs logs-generator logs-generator'
May 11 18:10:34.002: INFO: stderr: ""
May 11 18:10:34.002: INFO: stdout: "I0511 18:10:31.618865       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/vtc 414\nI0511 18:10:31.818993       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/x5q 268\nI0511 18:10:32.018994       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/zx9j 556\nI0511 18:10:32.219007       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/b5pd 394\nI0511 18:10:32.419027       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/zmt 440\nI0511 18:10:32.618987       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/x85 564\nI0511 18:10:32.818991       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/pwh 544\nI0511 18:10:33.018990       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/mxb 334\nI0511 18:10:33.218927       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/cqnk 535\nI0511 18:10:33.418994       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/n76l 384\nI0511 18:10:33.618985       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/sd6 262\nI0511 18:10:33.819025       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/vcl 574\n"
STEP: limiting log lines
May 11 18:10:34.002: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-9134 logs logs-generator logs-generator --tail=1'
May 11 18:10:34.387: INFO: stderr: ""
May 11 18:10:34.387: INFO: stdout: "I0511 18:10:34.218971       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/pjl 546\n"
May 11 18:10:34.387: INFO: got output "I0511 18:10:34.218971       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/pjl 546\n"
STEP: limiting log bytes
May 11 18:10:34.387: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-9134 logs logs-generator logs-generator --limit-bytes=1'
May 11 18:10:34.729: INFO: stderr: ""
May 11 18:10:34.729: INFO: stdout: "I"
May 11 18:10:34.729: INFO: got output "I"
STEP: exposing timestamps
May 11 18:10:34.729: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-9134 logs logs-generator logs-generator --tail=1 --timestamps'
May 11 18:10:35.074: INFO: stderr: ""
May 11 18:10:35.074: INFO: stdout: "2021-05-11T18:10:35.019010457Z I0511 18:10:35.018952       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/8hkt 450\n"
May 11 18:10:35.074: INFO: got output "2021-05-11T18:10:35.019010457Z I0511 18:10:35.018952       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/8hkt 450\n"
STEP: restricting to a time range
May 11 18:10:37.574: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-9134 logs logs-generator logs-generator --since=1s'
May 11 18:10:37.916: INFO: stderr: ""
May 11 18:10:37.916: INFO: stdout: "I0511 18:10:37.018986       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/rgks 312\nI0511 18:10:37.218987       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/tmg 216\nI0511 18:10:37.418990       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/zpvw 585\nI0511 18:10:37.618992       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/tgnf 307\nI0511 18:10:37.818987       1 logs_generator.go:76] 31 GET /api/v1/namespaces/kube-system/pods/x8q7 499\n"
May 11 18:10:37.916: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-9134 logs logs-generator logs-generator --since=24h'
May 11 18:10:38.242: INFO: stderr: ""
May 11 18:10:38.242: INFO: stdout: "I0511 18:10:31.618865       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/vtc 414\nI0511 18:10:31.818993       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/x5q 268\nI0511 18:10:32.018994       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/zx9j 556\nI0511 18:10:32.219007       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/b5pd 394\nI0511 18:10:32.419027       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/zmt 440\nI0511 18:10:32.618987       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/x85 564\nI0511 18:10:32.818991       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/pwh 544\nI0511 18:10:33.018990       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/mxb 334\nI0511 18:10:33.218927       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/cqnk 535\nI0511 18:10:33.418994       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/n76l 384\nI0511 18:10:33.618985       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/sd6 262\nI0511 18:10:33.819025       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/vcl 574\nI0511 18:10:34.018991       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/l6pj 204\nI0511 18:10:34.218971       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/pjl 546\nI0511 18:10:34.418962       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bbs 449\nI0511 18:10:34.618978       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/9rf2 492\nI0511 18:10:34.818980       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/bp9l 310\nI0511 18:10:35.018952       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/8hkt 450\nI0511 18:10:35.219009       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/646 382\nI0511 18:10:35.418986       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/j7l 300\nI0511 18:10:35.619008       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/zqc 421\nI0511 18:10:35.818979       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/rr28 233\nI0511 18:10:36.018975       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/b5h 463\nI0511 18:10:36.218991       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/z8j 255\nI0511 18:10:36.418988       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/cml 380\nI0511 18:10:36.618984       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/ttgm 424\nI0511 18:10:36.818952       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/hvt8 553\nI0511 18:10:37.018986       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/rgks 312\nI0511 18:10:37.218987       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/tmg 216\nI0511 18:10:37.418990       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/zpvw 585\nI0511 18:10:37.618992       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/tgnf 307\nI0511 18:10:37.818987       1 logs_generator.go:76] 31 GET /api/v1/namespaces/kube-system/pods/x8q7 499\nI0511 18:10:38.018993       1 logs_generator.go:76] 32 POST /api/v1/namespaces/default/pods/csrz 563\n"
[AfterEach] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
May 11 18:10:38.242: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-9134 delete pod logs-generator'
May 11 18:10:48.352: INFO: stderr: ""
May 11 18:10:48.352: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:48.352: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9134" for this suite.


• [SLOW TEST:19.870 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":-1,"completed":41,"skipped":791,"failed":0}

SSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
May 11 18:10:08.453: INFO: Successfully updated pod "var-expansion-8d1233b7-e05f-44ee-91f0-da120be394a6"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 11 18:10:10.588: INFO: Deleting pod "var-expansion-8d1233b7-e05f-44ee-91f0-da120be394a6" in namespace "var-expansion-3989"
May 11 18:10:10.656: INFO: Wait up to 5m0s for pod "var-expansion-8d1233b7-e05f-44ee-91f0-da120be394a6" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:48.791: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-3989" for this suite.


• [SLOW TEST:161.868 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":-1,"completed":23,"skipped":527,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 11 18:10:49.701: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:49.867: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-7251" for this suite.

•
------------------------------
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":-1,"completed":24,"skipped":531,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-9njg
STEP: Creating a pod to test atomic-volume-subpath
May 11 18:10:28.119: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9njg" in namespace "subpath-2586" to be "Succeeded or Failed"
May 11 18:10:28.193: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Pending", Reason="", readiness=false. Elapsed: 74.269335ms
May 11 18:10:30.262: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.142717765s
May 11 18:10:32.331: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Running", Reason="", readiness=true. Elapsed: 4.212413312s
May 11 18:10:34.397: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Running", Reason="", readiness=true. Elapsed: 6.278313668s
May 11 18:10:36.465: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Running", Reason="", readiness=true. Elapsed: 8.346468826s
May 11 18:10:38.537: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Running", Reason="", readiness=true. Elapsed: 10.418429091s
May 11 18:10:40.605: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Running", Reason="", readiness=true. Elapsed: 12.486282226s
May 11 18:10:42.670: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Running", Reason="", readiness=true. Elapsed: 14.550717099s
May 11 18:10:44.738: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Running", Reason="", readiness=true. Elapsed: 16.61937963s
May 11 18:10:46.803: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Running", Reason="", readiness=true. Elapsed: 18.683765969s
May 11 18:10:48.871: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Running", Reason="", readiness=true. Elapsed: 20.751716679s
May 11 18:10:50.935: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Running", Reason="", readiness=true. Elapsed: 22.81623983s
May 11 18:10:53.000: INFO: Pod "pod-subpath-test-downwardapi-9njg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.880708971s
STEP: Saw pod success
May 11 18:10:53.000: INFO: Pod "pod-subpath-test-downwardapi-9njg" satisfied condition "Succeeded or Failed"
May 11 18:10:53.067: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-subpath-test-downwardapi-9njg container test-container-subpath-downwardapi-9njg: <nil>
STEP: delete the pod
May 11 18:10:53.212: INFO: Waiting for pod pod-subpath-test-downwardapi-9njg to disappear
May 11 18:10:53.276: INFO: Pod pod-subpath-test-downwardapi-9njg no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-9njg
May 11 18:10:53.276: INFO: Deleting pod "pod-subpath-test-downwardapi-9njg" in namespace "subpath-2586"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:53.340: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-2586" for this suite.


• [SLOW TEST:26.025 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":-1,"completed":69,"skipped":1365,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
May 11 18:10:54.023: INFO: Waiting up to 5m0s for pod "test-pod-3e3453b4-6d6b-48bd-97d3-959edd7ad704" in namespace "svcaccounts-489" to be "Succeeded or Failed"
May 11 18:10:54.087: INFO: Pod "test-pod-3e3453b4-6d6b-48bd-97d3-959edd7ad704": Phase="Pending", Reason="", readiness=false. Elapsed: 63.962827ms
May 11 18:10:56.151: INFO: Pod "test-pod-3e3453b4-6d6b-48bd-97d3-959edd7ad704": Phase="Pending", Reason="", readiness=false. Elapsed: 2.128655582s
May 11 18:10:58.216: INFO: Pod "test-pod-3e3453b4-6d6b-48bd-97d3-959edd7ad704": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.193521099s
STEP: Saw pod success
May 11 18:10:58.216: INFO: Pod "test-pod-3e3453b4-6d6b-48bd-97d3-959edd7ad704" satisfied condition "Succeeded or Failed"
May 11 18:10:58.280: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod test-pod-3e3453b4-6d6b-48bd-97d3-959edd7ad704 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:10:58.430: INFO: Waiting for pod test-pod-3e3453b4-6d6b-48bd-97d3-959edd7ad704 to disappear
May 11 18:10:58.497: INFO: Pod test-pod-3e3453b4-6d6b-48bd-97d3-959edd7ad704 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:10:58.498: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-489" for this suite.


• [SLOW TEST:5.148 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":-1,"completed":70,"skipped":1369,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:10:59.217: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-2e33e0f1-e9e4-47a4-b15c-cd7b1d774390
STEP: Creating configMap with name cm-test-opt-upd-c8fa4f1f-7c3d-40be-9789-ae75ae16fd65
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2e33e0f1-e9e4-47a4-b15c-cd7b1d774390
STEP: Updating configmap cm-test-opt-upd-c8fa4f1f-7c3d-40be-9789-ae75ae16fd65
STEP: Creating configMap with name cm-test-opt-create-438e2579-3514-49fd-ba1f-edeee651ad3d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:11:06.313: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2953" for this suite.


• [SLOW TEST:7.755 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":71,"skipped":1411,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 11 18:11:06.965: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-856 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 11 18:11:07.246: INFO: stderr: ""
May 11 18:11:07.246: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 11 18:11:12.346: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-856 get pod e2e-test-httpd-pod -o json'
May 11 18:11:12.613: INFO: stderr: ""
May 11 18:11:12.613: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.2.117\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.2.117\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2021-05-11T18:11:07Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {\n                            \".\": {},\n                            \"f:seLinuxOptions\": {\n                                \"f:level\": {}\n                            }\n                        },\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-11T18:11:07Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:k8s.v1.cni.cncf.io/network-status\": {},\n                            \"f:k8s.v1.cni.cncf.io/networks-status\": {}\n                        }\n                    }\n                },\n                \"manager\": \"multus\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-11T18:11:09Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.128.2.117\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-11T18:11:10Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-856\",\n        \"resourceVersion\": \"67455\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-856/pods/e2e-test-httpd-pod\",\n        \"uid\": \"e39cd3f4-3ce4-4bf3-9f49-0bb01a35e467\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-4hr4v\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-ztjk6\"\n            }\n        ],\n        \"nodeName\": \"ip-10-0-198-156.us-west-1.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c55,c5\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-4hr4v\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-4hr4v\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-11T18:11:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-11T18:11:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-11T18:11:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-11T18:11:07Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://8a7ed919a381b01f8e9cf7762ebb56216d4d1d39c5c02621039e2650e75f4486\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-11T18:11:10Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.198.156\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.128.2.117\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.128.2.117\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-11T18:11:07Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 11 18:11:12.613: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-856 replace -f -'
May 11 18:11:13.450: INFO: stderr: ""
May 11 18:11:13.450: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
May 11 18:11:13.514: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-856 delete pods e2e-test-httpd-pod'
May 11 18:11:18.651: INFO: stderr: ""
May 11 18:11:18.651: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:11:18.651: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-856" for this suite.


• [SLOW TEST:12.335 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":-1,"completed":72,"skipped":1414,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1496, will wait for the garbage collector to delete the pods
May 11 18:10:46.998: INFO: Deleting Job.batch foo took: 70.973619ms
May 11 18:10:47.098: INFO: Terminating Job.batch foo pods took: 100.193301ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:11:28.467: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-1496" for this suite.


• [SLOW TEST:46.471 seconds]
[sig-apps] Job
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":-1,"completed":60,"skipped":1119,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 11 18:11:29.310: INFO: observed Pod pod-test in namespace pods-345 in phase Pending conditions []
May 11 18:11:29.310: INFO: observed Pod pod-test in namespace pods-345 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:29 +0000 UTC  }]
May 11 18:11:29.310: INFO: observed Pod pod-test in namespace pods-345 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:29 +0000 UTC  }]
May 11 18:11:31.685: INFO: observed Pod pod-test in namespace pods-345 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:29 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 11 18:11:32.554: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 11 18:11:32.898: INFO: observed event type ADDED
May 11 18:11:32.898: INFO: observed event type MODIFIED
May 11 18:11:32.898: INFO: observed event type MODIFIED
May 11 18:11:32.898: INFO: observed event type MODIFIED
May 11 18:11:32.898: INFO: observed event type MODIFIED
May 11 18:11:32.898: INFO: observed event type MODIFIED
May 11 18:11:32.898: INFO: observed event type MODIFIED
May 11 18:11:32.898: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:11:32.898: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-345" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":-1,"completed":61,"skipped":1126,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:11:33.037: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-4468" for this suite.


• [SLOW TEST:14.371 seconds]
[sig-api-machinery] ResourceQuota
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":-1,"completed":73,"skipped":1423,"failed":0}

SSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:11:37.911: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-2175" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":-1,"completed":74,"skipped":1428,"failed":0}

SS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:11:50.519: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-6332" for this suite.


• [SLOW TEST:60.744 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:10:56.401: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 11 18:11:41.401: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-794 --namespace=crd-publish-openapi-794 create -f -'
May 11 18:11:42.718: INFO: stderr: ""
May 11 18:11:42.718: INFO: stdout: "e2e-test-crd-publish-openapi-5045-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 11 18:11:42.718: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-794 --namespace=crd-publish-openapi-794 delete e2e-test-crd-publish-openapi-5045-crds test-cr'
May 11 18:11:43.047: INFO: stderr: ""
May 11 18:11:43.047: INFO: stdout: "e2e-test-crd-publish-openapi-5045-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 11 18:11:43.047: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-794 --namespace=crd-publish-openapi-794 apply -f -'
May 11 18:11:44.069: INFO: stderr: ""
May 11 18:11:44.069: INFO: stdout: "e2e-test-crd-publish-openapi-5045-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 11 18:11:44.069: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-794 --namespace=crd-publish-openapi-794 delete e2e-test-crd-publish-openapi-5045-crds test-cr'
May 11 18:11:44.406: INFO: stderr: ""
May 11 18:11:44.406: INFO: stdout: "e2e-test-crd-publish-openapi-5045-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 11 18:11:44.406: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-794 explain e2e-test-crd-publish-openapi-5045-crds'
May 11 18:11:44.843: INFO: stderr: ""
May 11 18:11:44.843: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5045-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:11:52.263: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-794" for this suite.


• [SLOW TEST:63.898 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":-1,"completed":42,"skipped":796,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3272
STEP: creating service affinity-clusterip in namespace services-3272
STEP: creating replication controller affinity-clusterip in namespace services-3272
I0511 18:11:38.641524   45011 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-3272, replica count: 3
I0511 18:11:41.741854   45011 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:11:44.742051   45011 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 18:11:44.874: INFO: Creating new exec pod
May 11 18:11:50.160: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3272 exec execpod-affinity99zgc -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
May 11 18:11:50.923: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 11 18:11:50.923: INFO: stdout: ""
May 11 18:11:50.923: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3272 exec execpod-affinity99zgc -- /bin/sh -x -c nc -zv -t -w 2 172.30.18.141 80'
May 11 18:11:51.732: INFO: stderr: "+ nc -zv -t -w 2 172.30.18.141 80\nConnection to 172.30.18.141 80 port [tcp/http] succeeded!\n"
May 11 18:11:51.732: INFO: stdout: ""
May 11 18:11:51.732: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-3272 exec execpod-affinity99zgc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.18.141:80/ ; done'
May 11 18:11:52.612: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.141:80/\n"
May 11 18:11:52.612: INFO: stdout: "\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm\naffinity-clusterip-8fpnm"
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Received response from host: affinity-clusterip-8fpnm
May 11 18:11:52.612: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3272, will wait for the garbage collector to delete the pods
May 11 18:11:52.936: INFO: Deleting ReplicationController affinity-clusterip took: 71.534135ms
May 11 18:11:53.136: INFO: Terminating ReplicationController affinity-clusterip pods took: 200.176519ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:08.826: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-3272" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:30.912 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":-1,"completed":75,"skipped":1430,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:12:09.427: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 11 18:12:10.892: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:10.956: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-4243" for this suite.

•
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":-1,"completed":76,"skipped":1437,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-5885
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5885 to expose endpoints map[]
May 11 18:12:11.833: INFO: successfully validated that service endpoint-test2 in namespace services-5885 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5885
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5885 to expose endpoints map[pod1:[80]]
May 11 18:12:16.228: INFO: successfully validated that service endpoint-test2 in namespace services-5885 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-5885
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5885 to expose endpoints map[pod1:[80] pod2:[80]]
May 11 18:12:19.694: INFO: successfully validated that service endpoint-test2 in namespace services-5885 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-5885
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5885 to expose endpoints map[pod2:[80]]
May 11 18:12:20.028: INFO: successfully validated that service endpoint-test2 in namespace services-5885 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-5885
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5885 to expose endpoints map[]
May 11 18:12:20.299: INFO: successfully validated that service endpoint-test2 in namespace services-5885 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:20.387: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-5885" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:9.406 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":-1,"completed":77,"skipped":1454,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 11 18:12:21.110: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:21.321: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-1120" for this suite.

•
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":-1,"completed":78,"skipped":1458,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-689acc0e-5d00-4f25-ad0d-7d18f07ecd53
STEP: Creating a pod to test consume configMaps
May 11 18:12:21.959: INFO: Waiting up to 5m0s for pod "pod-configmaps-bafeeb14-3bae-47ce-b5f9-6fc60b0c7de0" in namespace "configmap-8237" to be "Succeeded or Failed"
May 11 18:12:22.023: INFO: Pod "pod-configmaps-bafeeb14-3bae-47ce-b5f9-6fc60b0c7de0": Phase="Pending", Reason="", readiness=false. Elapsed: 63.960303ms
May 11 18:12:24.088: INFO: Pod "pod-configmaps-bafeeb14-3bae-47ce-b5f9-6fc60b0c7de0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129223184s
May 11 18:12:26.156: INFO: Pod "pod-configmaps-bafeeb14-3bae-47ce-b5f9-6fc60b0c7de0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.197227311s
STEP: Saw pod success
May 11 18:12:26.156: INFO: Pod "pod-configmaps-bafeeb14-3bae-47ce-b5f9-6fc60b0c7de0" satisfied condition "Succeeded or Failed"
May 11 18:12:26.220: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-configmaps-bafeeb14-3bae-47ce-b5f9-6fc60b0c7de0 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:12:26.372: INFO: Waiting for pod pod-configmaps-bafeeb14-3bae-47ce-b5f9-6fc60b0c7de0 to disappear
May 11 18:12:26.436: INFO: Pod pod-configmaps-bafeeb14-3bae-47ce-b5f9-6fc60b0c7de0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:26.436: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-8237" for this suite.


• [SLOW TEST:5.210 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":79,"skipped":1474,"failed":0}

SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9331
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-9331
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9331
May 11 18:11:33.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
May 11 18:11:43.705: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 11 18:11:43.773: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-9331 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 11 18:11:44.531: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 11 18:11:44.531: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 11 18:11:44.531: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 11 18:11:44.600: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 11 18:11:54.666: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 11 18:11:54.666: INFO: Waiting for statefulset status.replicas updated to 0
May 11 18:11:54.936: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999698s
May 11 18:11:56.002: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.929044203s
May 11 18:11:57.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.863154826s
May 11 18:11:58.138: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.794425573s
May 11 18:11:59.208: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.727345108s
May 11 18:12:00.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.657428318s
May 11 18:12:01.340: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.591585965s
May 11 18:12:02.406: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.525556115s
May 11 18:12:03.475: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.459584128s
May 11 18:12:04.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 390.300441ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9331
May 11 18:12:05.607: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-9331 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:12:06.340: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 11 18:12:06.340: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 11 18:12:06.340: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 11 18:12:06.340: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-9331 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:12:07.086: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 11 18:12:07.086: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 11 18:12:07.086: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 11 18:12:07.086: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-9331 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:12:07.834: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 11 18:12:07.834: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 11 18:12:07.834: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 11 18:12:07.900: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:12:07.900: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:12:07.900: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 11 18:12:07.969: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-9331 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 11 18:12:08.692: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 11 18:12:08.692: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 11 18:12:08.692: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 11 18:12:08.692: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-9331 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 11 18:12:09.483: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 11 18:12:09.483: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 11 18:12:09.483: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 11 18:12:09.483: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-9331 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 11 18:12:10.214: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 11 18:12:10.214: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 11 18:12:10.214: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 11 18:12:10.214: INFO: Waiting for statefulset status.replicas updated to 0
May 11 18:12:10.279: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 11 18:12:20.412: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 11 18:12:20.412: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 11 18:12:20.412: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 11 18:12:20.619: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May 11 18:12:20.619: INFO: ss-0  ip-10-0-128-72.us-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  }]
May 11 18:12:20.619: INFO: ss-1  ip-10-0-198-156.us-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  }]
May 11 18:12:20.619: INFO: ss-2  ip-10-0-143-31.us-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  }]
May 11 18:12:20.619: INFO: 
May 11 18:12:20.619: INFO: StatefulSet ss has not reached scale 0, at 3
May 11 18:12:21.684: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May 11 18:12:21.684: INFO: ss-0  ip-10-0-128-72.us-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  }]
May 11 18:12:21.684: INFO: ss-1  ip-10-0-198-156.us-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  }]
May 11 18:12:21.685: INFO: ss-2  ip-10-0-143-31.us-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  }]
May 11 18:12:21.685: INFO: 
May 11 18:12:21.685: INFO: StatefulSet ss has not reached scale 0, at 3
May 11 18:12:22.751: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May 11 18:12:22.751: INFO: ss-0  ip-10-0-128-72.us-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  }]
May 11 18:12:22.751: INFO: ss-1  ip-10-0-198-156.us-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  }]
May 11 18:12:22.751: INFO: 
May 11 18:12:22.751: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 18:12:23.817: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May 11 18:12:23.817: INFO: ss-0  ip-10-0-128-72.us-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  }]
May 11 18:12:23.817: INFO: ss-1  ip-10-0-198-156.us-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  }]
May 11 18:12:23.817: INFO: 
May 11 18:12:23.817: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 18:12:24.883: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May 11 18:12:24.883: INFO: ss-0  ip-10-0-128-72.us-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  }]
May 11 18:12:24.883: INFO: ss-1  ip-10-0-198-156.us-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  }]
May 11 18:12:24.883: INFO: 
May 11 18:12:24.883: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 18:12:25.949: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May 11 18:12:25.949: INFO: ss-0  ip-10-0-128-72.us-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  }]
May 11 18:12:25.949: INFO: ss-1  ip-10-0-198-156.us-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  }]
May 11 18:12:25.949: INFO: 
May 11 18:12:25.949: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 18:12:27.020: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May 11 18:12:27.020: INFO: ss-0  ip-10-0-128-72.us-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  }]
May 11 18:12:27.020: INFO: ss-1  ip-10-0-198-156.us-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  }]
May 11 18:12:27.020: INFO: 
May 11 18:12:27.020: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 18:12:28.086: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May 11 18:12:28.086: INFO: ss-0  ip-10-0-128-72.us-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:33 +0000 UTC  }]
May 11 18:12:28.086: INFO: ss-1  ip-10-0-198-156.us-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:12:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-11 18:11:54 +0000 UTC  }]
May 11 18:12:28.086: INFO: 
May 11 18:12:28.086: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 18:12:29.152: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.46259462s
May 11 18:12:30.221: INFO: Verifying statefulset ss doesn't scale past 0 for another 396.826264ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9331
May 11 18:12:31.287: INFO: Scaling statefulset ss to 0
May 11 18:12:31.487: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 11 18:12:31.552: INFO: Deleting all statefulset in ns statefulset-9331
May 11 18:12:31.621: INFO: Scaling statefulset ss to 0
May 11 18:12:31.830: INFO: Waiting for statefulset status.replicas updated to 0
May 11 18:12:31.895: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:32.103: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-9331" for this suite.


• [SLOW TEST:59.259 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":-1,"completed":62,"skipped":1129,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 11 18:12:27.053: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2338 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 11 18:12:27.326: INFO: stderr: ""
May 11 18:12:27.326: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 11 18:12:27.326: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2338 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
May 11 18:12:28.200: INFO: stderr: ""
May 11 18:12:28.200: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
May 11 18:12:28.264: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=kubectl-2338 delete pods e2e-test-httpd-pod'
May 11 18:12:38.351: INFO: stderr: ""
May 11 18:12:38.351: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:38.351: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2338" for this suite.


• [SLOW TEST:11.886 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":-1,"completed":80,"skipped":1494,"failed":0}

SS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:48.454: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-3721" for this suite.


• [SLOW TEST:9.974 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":-1,"completed":81,"skipped":1496,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-e5930c79-3690-439c-b7df-fe780753c916
STEP: Creating a pod to test consume secrets
May 11 18:12:49.093: INFO: Waiting up to 5m0s for pod "pod-secrets-0e5cdb9e-5fdd-4fa9-9446-4ff545954f36" in namespace "secrets-3639" to be "Succeeded or Failed"
May 11 18:12:49.161: INFO: Pod "pod-secrets-0e5cdb9e-5fdd-4fa9-9446-4ff545954f36": Phase="Pending", Reason="", readiness=false. Elapsed: 67.766398ms
May 11 18:12:51.239: INFO: Pod "pod-secrets-0e5cdb9e-5fdd-4fa9-9446-4ff545954f36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.146165792s
May 11 18:12:53.304: INFO: Pod "pod-secrets-0e5cdb9e-5fdd-4fa9-9446-4ff545954f36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.210963575s
STEP: Saw pod success
May 11 18:12:53.304: INFO: Pod "pod-secrets-0e5cdb9e-5fdd-4fa9-9446-4ff545954f36" satisfied condition "Succeeded or Failed"
May 11 18:12:53.368: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-secrets-0e5cdb9e-5fdd-4fa9-9446-4ff545954f36 container secret-volume-test: <nil>
STEP: delete the pod
May 11 18:12:53.596: INFO: Waiting for pod pod-secrets-0e5cdb9e-5fdd-4fa9-9446-4ff545954f36 to disappear
May 11 18:12:53.664: INFO: Pod pod-secrets-0e5cdb9e-5fdd-4fa9-9446-4ff545954f36 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:53.664: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-3639" for this suite.


• [SLOW TEST:5.300 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":82,"skipped":1511,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":-1,"completed":25,"skipped":560,"failed":0}
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:11:58.751: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 11 18:12:43.751: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-5574 --namespace=crd-publish-openapi-5574 create -f -'
May 11 18:12:44.817: INFO: stderr: ""
May 11 18:12:44.817: INFO: stdout: "e2e-test-crd-publish-openapi-2988-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 11 18:12:44.817: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-5574 --namespace=crd-publish-openapi-5574 delete e2e-test-crd-publish-openapi-2988-crds test-foo'
May 11 18:12:45.175: INFO: stderr: ""
May 11 18:12:45.175: INFO: stdout: "e2e-test-crd-publish-openapi-2988-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 11 18:12:45.175: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-5574 --namespace=crd-publish-openapi-5574 apply -f -'
May 11 18:12:46.119: INFO: stderr: ""
May 11 18:12:46.119: INFO: stdout: "e2e-test-crd-publish-openapi-2988-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 11 18:12:46.119: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-5574 --namespace=crd-publish-openapi-5574 delete e2e-test-crd-publish-openapi-2988-crds test-foo'
May 11 18:12:46.451: INFO: stderr: ""
May 11 18:12:46.451: INFO: stdout: "e2e-test-crd-publish-openapi-2988-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl explain works to explain CR properties
May 11 18:12:46.451: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-5574 explain e2e-test-crd-publish-openapi-2988-crds'
May 11 18:12:47.201: INFO: stderr: ""
May 11 18:12:47.201: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2988-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 11 18:12:47.201: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-5574 explain e2e-test-crd-publish-openapi-2988-crds.metadata'
May 11 18:12:47.964: INFO: stderr: ""
May 11 18:12:47.964: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2988-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 11 18:12:47.964: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-5574 explain e2e-test-crd-publish-openapi-2988-crds.spec'
May 11 18:12:48.713: INFO: stderr: ""
May 11 18:12:48.713: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2988-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 11 18:12:48.713: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-5574 explain e2e-test-crd-publish-openapi-2988-crds.spec.bars'
May 11 18:12:49.146: INFO: stderr: ""
May 11 18:12:49.146: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2988-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 11 18:12:49.147: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-5574 explain e2e-test-crd-publish-openapi-2988-crds.spec.bars2'
May 11 18:12:49.885: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:57.901: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5574" for this suite.


• [SLOW TEST:67.380 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":-1,"completed":26,"skipped":560,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-163c4b9e-4a0c-4a53-882b-23b7248eaa90
STEP: Creating a pod to test consume configMaps
May 11 18:12:54.438: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-90f7ee3f-0eb9-4d3f-ab7b-91bd514a35a2" in namespace "projected-5042" to be "Succeeded or Failed"
May 11 18:12:54.507: INFO: Pod "pod-projected-configmaps-90f7ee3f-0eb9-4d3f-ab7b-91bd514a35a2": Phase="Pending", Reason="", readiness=false. Elapsed: 68.877971ms
May 11 18:12:56.572: INFO: Pod "pod-projected-configmaps-90f7ee3f-0eb9-4d3f-ab7b-91bd514a35a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133660399s
May 11 18:12:58.644: INFO: Pod "pod-projected-configmaps-90f7ee3f-0eb9-4d3f-ab7b-91bd514a35a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.205772558s
STEP: Saw pod success
May 11 18:12:58.644: INFO: Pod "pod-projected-configmaps-90f7ee3f-0eb9-4d3f-ab7b-91bd514a35a2" satisfied condition "Succeeded or Failed"
May 11 18:12:58.709: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-projected-configmaps-90f7ee3f-0eb9-4d3f-ab7b-91bd514a35a2 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:12:58.854: INFO: Waiting for pod pod-projected-configmaps-90f7ee3f-0eb9-4d3f-ab7b-91bd514a35a2 to disappear
May 11 18:12:58.918: INFO: Pod pod-projected-configmaps-90f7ee3f-0eb9-4d3f-ab7b-91bd514a35a2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:12:58.918: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5042" for this suite.


• [SLOW TEST:5.222 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":83,"skipped":1536,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:12:02.824: INFO: sleeping 45 seconds before running the actual tests, we hope that during all API servers converge during that window, see "https://github.com/kubernetes/kubernetes/pull/90452" for more
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 11 18:12:47.824: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-7949 --namespace=crd-publish-openapi-7949 create -f -'
May 11 18:12:48.689: INFO: stderr: ""
May 11 18:12:48.689: INFO: stdout: "e2e-test-crd-publish-openapi-8444-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 11 18:12:48.689: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-7949 --namespace=crd-publish-openapi-7949 delete e2e-test-crd-publish-openapi-8444-crds test-cr'
May 11 18:12:49.020: INFO: stderr: ""
May 11 18:12:49.020: INFO: stdout: "e2e-test-crd-publish-openapi-8444-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 11 18:12:49.020: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-7949 --namespace=crd-publish-openapi-7949 apply -f -'
May 11 18:12:49.661: INFO: stderr: ""
May 11 18:12:49.661: INFO: stdout: "e2e-test-crd-publish-openapi-8444-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 11 18:12:49.661: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-7949 --namespace=crd-publish-openapi-7949 delete e2e-test-crd-publish-openapi-8444-crds test-cr'
May 11 18:12:49.993: INFO: stderr: ""
May 11 18:12:49.993: INFO: stdout: "e2e-test-crd-publish-openapi-8444-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 11 18:12:49.993: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=crd-publish-openapi-7949 explain e2e-test-crd-publish-openapi-8444-crds'
May 11 18:12:50.751: INFO: stderr: ""
May 11 18:12:50.751: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8444-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:13:01.680: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7949" for this suite.


• [SLOW TEST:69.409 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":-1,"completed":43,"skipped":805,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-18e98cc7-7099-47b9-8f86-3363be7a2430
STEP: Creating a pod to test consume secrets
May 11 18:12:59.763: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-df6b3dec-36f2-4b32-8c87-0de03dc3e802" in namespace "projected-9744" to be "Succeeded or Failed"
May 11 18:12:59.828: INFO: Pod "pod-projected-secrets-df6b3dec-36f2-4b32-8c87-0de03dc3e802": Phase="Pending", Reason="", readiness=false. Elapsed: 64.479364ms
May 11 18:13:01.894: INFO: Pod "pod-projected-secrets-df6b3dec-36f2-4b32-8c87-0de03dc3e802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131040348s
May 11 18:13:03.964: INFO: Pod "pod-projected-secrets-df6b3dec-36f2-4b32-8c87-0de03dc3e802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.200573193s
STEP: Saw pod success
May 11 18:13:03.964: INFO: Pod "pod-projected-secrets-df6b3dec-36f2-4b32-8c87-0de03dc3e802" satisfied condition "Succeeded or Failed"
May 11 18:13:04.028: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-projected-secrets-df6b3dec-36f2-4b32-8c87-0de03dc3e802 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 11 18:13:04.170: INFO: Waiting for pod pod-projected-secrets-df6b3dec-36f2-4b32-8c87-0de03dc3e802 to disappear
May 11 18:13:04.243: INFO: Pod pod-projected-secrets-df6b3dec-36f2-4b32-8c87-0de03dc3e802 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:13:04.243: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9744" for this suite.


• [SLOW TEST:5.221 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":84,"skipped":1617,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:13:02.305: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 11 18:13:02.440: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 11 18:13:06.577: INFO: Creating deployment "test-rolling-update-deployment"
May 11 18:13:06.647: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 11 18:13:06.780: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 11 18:13:06.847: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353586, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353586, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353586, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353586, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:13:08.918: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353586, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353586, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353586, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353586, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:13:10.914: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 11 18:13:11.114: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-889 /apis/apps/v1/namespaces/deployment-889/deployments/test-rolling-update-deployment b814daf7-343f-421a-a7e1-a7ca64ee33d3 70206 1 2021-05-11 18:13:06 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-11 18:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-11 18:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0089b91e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-11 18:13:06 +0000 UTC,LastTransitionTime:2021-05-11 18:13:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-05-11 18:13:09 +0000 UTC,LastTransitionTime:2021-05-11 18:13:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 11 18:13:11.185: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-889 /apis/apps/v1/namespaces/deployment-889/replicasets/test-rolling-update-deployment-6b6bf9df46 eddbd757-c7f6-414b-bc9f-df47968088cb 70194 1 2021-05-11 18:13:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b814daf7-343f-421a-a7e1-a7ca64ee33d3 0xc0089b9e27 0xc0089b9e28}] []  [{kube-controller-manager Update apps/v1 2021-05-11 18:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b814daf7-343f-421a-a7e1-a7ca64ee33d3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0089b9f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 11 18:13:11.185: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 11 18:13:11.185: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-889 /apis/apps/v1/namespaces/deployment-889/replicasets/test-rolling-update-controller cd46b57b-263b-45cd-8f8f-81b55fc5ab11 70204 2 2021-05-11 18:13:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b814daf7-343f-421a-a7e1-a7ca64ee33d3 0xc0089b9ab7 0xc0089b9ab8}] []  [{e2e.test Update apps/v1 2021-05-11 18:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-11 18:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b814daf7-343f-421a-a7e1-a7ca64ee33d3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0089b9d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 11 18:13:11.252: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-7728c" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-7728c test-rolling-update-deployment-6b6bf9df46- deployment-889 /api/v1/namespaces/deployment-889/pods/test-rolling-update-deployment-6b6bf9df46-7728c fdcc9b03-793b-4d6e-9248-8ddca5f68f68 70193 0 2021-05-11 18:13:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.2.213"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.2.213"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 eddbd757-c7f6-414b-bc9f-df47968088cb 0xc00beaeb37 0xc00beaeb38}] []  [{kube-controller-manager Update v1 2021-05-11 18:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eddbd757-c7f6-414b-bc9f-df47968088cb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jx2qg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jx2qg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jx2qg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c56,c45,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6h8s2,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:13:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:13:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:10.129.2.213,StartTime:2021-05-11 18:13:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:13:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://cc5ca9eaed6a87fe8b8445a1dd4a03502f0a72084ca34d9d1ceea16dd98f1ff3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:13:11.252: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-889" for this suite.


• [SLOW TEST:9.554 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":-1,"completed":44,"skipped":817,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:13:16.312: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-5655" for this suite.


• [SLOW TEST:5.039 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":-1,"completed":45,"skipped":829,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:13:45.500: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-638" for this suite.


• [SLOW TEST:29.168 seconds]
[sig-api-machinery] ResourceQuota
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":-1,"completed":46,"skipped":844,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:13:47.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353626, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353626, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353626, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353626, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:13:49.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353626, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353626, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353626, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353626, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:13:52.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:13:52.964: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-8646" for this suite.
STEP: Destroying namespace "webhook-8646-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:7.841 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":-1,"completed":47,"skipped":869,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 11 18:14:02.744: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 18:14:02.814: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 18:14:04.814: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 18:14:04.881: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 18:14:06.814: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 18:14:06.884: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 18:14:08.814: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 18:14:08.881: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:14:08.950: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-30" for this suite.


• [SLOW TEST:15.468 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":-1,"completed":48,"skipped":948,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2632
May 11 18:13:00.800: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2632 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 11 18:13:01.542: INFO: rc: 7
May 11 18:13:01.618: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 11 18:13:01.683: INFO: Pod kube-proxy-mode-detector no longer exists
May 11 18:13:01.683: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2632 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-2632
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2632
I0511 18:13:01.834237   45012 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2632, replica count: 3
I0511 18:13:04.934482   45012 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 18:13:07.934663   45012 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 18:13:08.259: INFO: Creating new exec pod
May 11 18:13:13.731: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2632 exec execpod-affinitytnszv -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
May 11 18:13:14.505: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 11 18:13:14.505: INFO: stdout: ""
May 11 18:13:14.505: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2632 exec execpod-affinitytnszv -- /bin/sh -x -c nc -zv -t -w 2 172.30.87.0 80'
May 11 18:13:15.245: INFO: stderr: "+ nc -zv -t -w 2 172.30.87.0 80\nConnection to 172.30.87.0 80 port [tcp/http] succeeded!\n"
May 11 18:13:15.245: INFO: stdout: ""
May 11 18:13:15.245: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2632 exec execpod-affinitytnszv -- /bin/sh -x -c nc -zv -t -w 2 10.0.198.156 31905'
May 11 18:13:15.977: INFO: stderr: "+ nc -zv -t -w 2 10.0.198.156 31905\nConnection to 10.0.198.156 31905 port [tcp/31905] succeeded!\n"
May 11 18:13:15.977: INFO: stdout: ""
May 11 18:13:15.978: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2632 exec execpod-affinitytnszv -- /bin/sh -x -c nc -zv -t -w 2 10.0.128.72 31905'
May 11 18:13:16.728: INFO: stderr: "+ nc -zv -t -w 2 10.0.128.72 31905\nConnection to 10.0.128.72 31905 port [tcp/31905] succeeded!\n"
May 11 18:13:16.728: INFO: stdout: ""
May 11 18:13:16.728: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2632 exec execpod-affinitytnszv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.128.72:31905/ ; done'
May 11 18:13:17.676: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n"
May 11 18:13:17.676: INFO: stdout: "\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x\naffinity-nodeport-timeout-b286x"
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Received response from host: affinity-nodeport-timeout-b286x
May 11 18:13:17.676: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2632 exec execpod-affinitytnszv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.72:31905/'
May 11 18:13:18.441: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n"
May 11 18:13:18.441: INFO: stdout: "affinity-nodeport-timeout-b286x"
May 11 18:13:38.442: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2632 exec execpod-affinitytnszv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.72:31905/'
May 11 18:13:39.207: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n"
May 11 18:13:39.207: INFO: stdout: "affinity-nodeport-timeout-b286x"
May 11 18:13:59.207: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=services-2632 exec execpod-affinitytnszv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.72:31905/'
May 11 18:13:59.984: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.72:31905/\n"
May 11 18:13:59.985: INFO: stdout: "affinity-nodeport-timeout-288c6"
May 11 18:13:59.985: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2632, will wait for the garbage collector to delete the pods
May 11 18:14:00.303: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 76.742209ms
May 11 18:14:00.403: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.203581ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:14:12.303: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-2632" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749


• [SLOW TEST:74.388 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":-1,"completed":27,"skipped":566,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 11 18:14:13.122: INFO: Waiting up to 5m0s for pod "pod-08cdba68-a482-4a2e-a036-e3824aafe5ee" in namespace "emptydir-2593" to be "Succeeded or Failed"
May 11 18:14:13.190: INFO: Pod "pod-08cdba68-a482-4a2e-a036-e3824aafe5ee": Phase="Pending", Reason="", readiness=false. Elapsed: 68.217299ms
May 11 18:14:15.255: INFO: Pod "pod-08cdba68-a482-4a2e-a036-e3824aafe5ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133354445s
May 11 18:14:17.321: INFO: Pod "pod-08cdba68-a482-4a2e-a036-e3824aafe5ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.198874142s
STEP: Saw pod success
May 11 18:14:17.321: INFO: Pod "pod-08cdba68-a482-4a2e-a036-e3824aafe5ee" satisfied condition "Succeeded or Failed"
May 11 18:14:17.391: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-08cdba68-a482-4a2e-a036-e3824aafe5ee container test-container: <nil>
STEP: delete the pod
May 11 18:14:17.539: INFO: Waiting for pod pod-08cdba68-a482-4a2e-a036-e3824aafe5ee to disappear
May 11 18:14:17.604: INFO: Pod pod-08cdba68-a482-4a2e-a036-e3824aafe5ee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:14:17.604: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-2593" for this suite.


• [SLOW TEST:5.158 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":28,"skipped":606,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:14:26.636: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-3431" for this suite.


• [SLOW TEST:17.648 seconds]
[sig-api-machinery] ResourceQuota
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":-1,"completed":49,"skipped":973,"failed":0}
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0511 18:14:27.725157   45016 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W0511 18:14:27.725221   45016 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0511 18:14:27.725228   45016 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0511 18:14:27.725234   45016 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 11 18:14:27.725: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:14:27.725: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-4388" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":-1,"completed":50,"skipped":973,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-993d4e1f-c2b4-4864-8d10-8404588993ca in namespace container-probe-4851
May 11 18:14:22.420: INFO: Started pod busybox-993d4e1f-c2b4-4864-8d10-8404588993ca in namespace container-probe-4851
STEP: checking the pod's current state and verifying that restartCount is present
May 11 18:14:22.485: INFO: Initial restart count of pod busybox-993d4e1f-c2b4-4864-8d10-8404588993ca is 0
May 11 18:15:14.217: INFO: Restart count of pod container-probe-4851/busybox-993d4e1f-c2b4-4864-8d10-8404588993ca is now 1 (51.73193585s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:15:14.292: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-4851" for this suite.


• [SLOW TEST:56.689 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":-1,"completed":29,"skipped":609,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:15:14.997: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-4ed9e568-cc12-4ab3-a0e1-73981747f931
STEP: Creating configMap with name cm-test-opt-upd-61eb4a0f-2f10-4147-aa24-5e864bc4c794
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4ed9e568-cc12-4ab3-a0e1-73981747f931
STEP: Updating configmap cm-test-opt-upd-61eb4a0f-2f10-4147-aa24-5e864bc4c794
STEP: Creating configMap with name cm-test-opt-create-aa432f92-9278-41cb-8718-6af89e97c6e4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:15:24.204: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-4829" for this suite.


• [SLOW TEST:9.865 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":30,"skipped":633,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:15:24.878: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-2ac50039-10f4-4e6a-8ae2-2c801e313051" in namespace "security-context-test-9307" to be "Succeeded or Failed"
May 11 18:15:24.943: INFO: Pod "busybox-privileged-false-2ac50039-10f4-4e6a-8ae2-2c801e313051": Phase="Pending", Reason="", readiness=false. Elapsed: 65.239973ms
May 11 18:15:27.008: INFO: Pod "busybox-privileged-false-2ac50039-10f4-4e6a-8ae2-2c801e313051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130609402s
May 11 18:15:29.074: INFO: Pod "busybox-privileged-false-2ac50039-10f4-4e6a-8ae2-2c801e313051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.196425308s
May 11 18:15:29.074: INFO: Pod "busybox-privileged-false-2ac50039-10f4-4e6a-8ae2-2c801e313051" satisfied condition "Succeeded or Failed"
May 11 18:15:29.142: INFO: Got logs for pod "busybox-privileged-false-2ac50039-10f4-4e6a-8ae2-2c801e313051": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:15:29.142: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-9307" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":31,"skipped":639,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 11 18:15:30.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353730, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353730, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353730, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353730, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:15:32.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353730, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353730, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353730, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353730, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:15:35.994: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:15:37.714: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-webhook-2776" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137


• [SLOW TEST:8.984 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":-1,"completed":32,"skipped":661,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
May 11 18:15:39.710: INFO: created pod pod-service-account-defaultsa
May 11 18:15:39.710: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 11 18:15:39.811: INFO: created pod pod-service-account-mountsa
May 11 18:15:39.811: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 11 18:15:39.889: INFO: created pod pod-service-account-nomountsa
May 11 18:15:39.889: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 11 18:15:39.971: INFO: created pod pod-service-account-defaultsa-mountspec
May 11 18:15:39.971: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 11 18:15:40.047: INFO: created pod pod-service-account-mountsa-mountspec
May 11 18:15:40.047: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 11 18:15:40.166: INFO: created pod pod-service-account-nomountsa-mountspec
May 11 18:15:40.166: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 11 18:15:40.266: INFO: created pod pod-service-account-defaultsa-nomountspec
May 11 18:15:40.266: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 11 18:15:40.379: INFO: created pod pod-service-account-mountsa-nomountspec
May 11 18:15:40.379: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 11 18:15:40.471: INFO: created pod pod-service-account-nomountsa-nomountspec
May 11 18:15:40.471: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:15:40.471: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-8906" for this suite.

•
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":-1,"completed":33,"skipped":683,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:15:42.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:15:44.099: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:15:46.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353741, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:15:49.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 11 18:15:53.598: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=webhook-5344 attach --namespace=webhook-5344 to-be-attached-pod -i -c=container1'
May 11 18:15:54.087: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:15:54.159: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-5344" for this suite.
STEP: Destroying namespace "webhook-5344-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:14.109 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":-1,"completed":34,"skipped":693,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:06.662: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-78" for this suite.


• [SLOW TEST:12.117 seconds]
[sig-api-machinery] ResourceQuota
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":-1,"completed":35,"skipped":700,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:16:07.357: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9feb2fa0-6ceb-4c66-aa3a-506523a97f07" in namespace "security-context-test-8478" to be "Succeeded or Failed"
May 11 18:16:07.422: INFO: Pod "busybox-user-65534-9feb2fa0-6ceb-4c66-aa3a-506523a97f07": Phase="Pending", Reason="", readiness=false. Elapsed: 65.123522ms
May 11 18:16:09.488: INFO: Pod "busybox-user-65534-9feb2fa0-6ceb-4c66-aa3a-506523a97f07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130741078s
May 11 18:16:11.553: INFO: Pod "busybox-user-65534-9feb2fa0-6ceb-4c66-aa3a-506523a97f07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.195981849s
May 11 18:16:11.553: INFO: Pod "busybox-user-65534-9feb2fa0-6ceb-4c66-aa3a-506523a97f07" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:11.553: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-8478" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":36,"skipped":712,"failed":0}

S
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-732baf83-0801-4ad5-89a6-ee9077590d46
STEP: Creating a pod to test consume configMaps
May 11 18:16:12.299: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d601586c-d9c3-4642-b6f0-8378c7fd31dc" in namespace "projected-5358" to be "Succeeded or Failed"
May 11 18:16:12.364: INFO: Pod "pod-projected-configmaps-d601586c-d9c3-4642-b6f0-8378c7fd31dc": Phase="Pending", Reason="", readiness=false. Elapsed: 65.113994ms
May 11 18:16:14.429: INFO: Pod "pod-projected-configmaps-d601586c-d9c3-4642-b6f0-8378c7fd31dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130566967s
May 11 18:16:16.495: INFO: Pod "pod-projected-configmaps-d601586c-d9c3-4642-b6f0-8378c7fd31dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.195861158s
STEP: Saw pod success
May 11 18:16:16.495: INFO: Pod "pod-projected-configmaps-d601586c-d9c3-4642-b6f0-8378c7fd31dc" satisfied condition "Succeeded or Failed"
May 11 18:16:16.560: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-projected-configmaps-d601586c-d9c3-4642-b6f0-8378c7fd31dc container agnhost-container: <nil>
STEP: delete the pod
May 11 18:16:16.702: INFO: Waiting for pod pod-projected-configmaps-d601586c-d9c3-4642-b6f0-8378c7fd31dc to disappear
May 11 18:16:16.767: INFO: Pod pod-projected-configmaps-d601586c-d9c3-4642-b6f0-8378c7fd31dc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:16.767: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5358" for this suite.


• [SLOW TEST:5.206 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":37,"skipped":713,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6520
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 11 18:14:28.490: INFO: Found 1 stateful pods, waiting for 3
May 11 18:14:38.562: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:14:38.562: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:14:38.562: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 11 18:14:48.566: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:14:48.566: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:14:48.566: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 11 18:14:48.916: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 11 18:14:49.199: INFO: Updating stateful set ss2
May 11 18:14:49.336: INFO: Waiting for Pod statefulset-6520/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 11 18:14:59.477: INFO: Waiting for Pod statefulset-6520/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May 11 18:15:09.693: INFO: Found 2 stateful pods, waiting for 3
May 11 18:15:19.760: INFO: Found 2 stateful pods, waiting for 3
May 11 18:15:29.760: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:15:29.760: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:15:29.760: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 11 18:15:30.044: INFO: Updating stateful set ss2
May 11 18:15:30.180: INFO: Waiting for Pod statefulset-6520/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 11 18:15:40.478: INFO: Updating stateful set ss2
May 11 18:15:40.615: INFO: Waiting for StatefulSet statefulset-6520/ss2 to complete update
May 11 18:15:40.615: INFO: Waiting for Pod statefulset-6520/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 11 18:15:50.752: INFO: Waiting for StatefulSet statefulset-6520/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 11 18:16:00.752: INFO: Deleting all statefulset in ns statefulset-6520
May 11 18:16:00.818: INFO: Scaling statefulset ss2 to 0
May 11 18:16:21.094: INFO: Waiting for statefulset status.replicas updated to 0
May 11 18:16:21.164: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:21.373: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-6520" for this suite.


• [SLOW TEST:113.752 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":-1,"completed":51,"skipped":985,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-ca3a0056-a1d3-42a3-8ddb-a619b8e07f2a
STEP: Creating a pod to test consume configMaps
May 11 18:16:17.515: INFO: Waiting up to 5m0s for pod "pod-configmaps-a98bbc2d-4677-4343-8081-d869e6e76724" in namespace "configmap-8822" to be "Succeeded or Failed"
May 11 18:16:17.580: INFO: Pod "pod-configmaps-a98bbc2d-4677-4343-8081-d869e6e76724": Phase="Pending", Reason="", readiness=false. Elapsed: 64.764306ms
May 11 18:16:19.645: INFO: Pod "pod-configmaps-a98bbc2d-4677-4343-8081-d869e6e76724": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129830946s
May 11 18:16:21.710: INFO: Pod "pod-configmaps-a98bbc2d-4677-4343-8081-d869e6e76724": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.195121573s
STEP: Saw pod success
May 11 18:16:21.710: INFO: Pod "pod-configmaps-a98bbc2d-4677-4343-8081-d869e6e76724" satisfied condition "Succeeded or Failed"
May 11 18:16:21.778: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-configmaps-a98bbc2d-4677-4343-8081-d869e6e76724 container agnhost-container: <nil>
STEP: delete the pod
May 11 18:16:21.961: INFO: Waiting for pod pod-configmaps-a98bbc2d-4677-4343-8081-d869e6e76724 to disappear
May 11 18:16:22.028: INFO: Pod pod-configmaps-a98bbc2d-4677-4343-8081-d869e6e76724 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:22.028: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-8822" for this suite.


• [SLOW TEST:5.254 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":-1,"completed":38,"skipped":717,"failed":0}

S
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:22.773: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-1966" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":-1,"completed":39,"skipped":718,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-d1575f17-dc72-4e2f-8ae6-9a18c5f134d0
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:23.350: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-9123" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":-1,"completed":40,"skipped":750,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 11 18:16:22.078: INFO: Waiting up to 5m0s for pod "downward-api-fe984a1c-18c8-47e5-976a-701190d424fb" in namespace "downward-api-4666" to be "Succeeded or Failed"
May 11 18:16:22.146: INFO: Pod "downward-api-fe984a1c-18c8-47e5-976a-701190d424fb": Phase="Pending", Reason="", readiness=false. Elapsed: 67.718875ms
May 11 18:16:24.214: INFO: Pod "downward-api-fe984a1c-18c8-47e5-976a-701190d424fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134977683s
May 11 18:16:26.281: INFO: Pod "downward-api-fe984a1c-18c8-47e5-976a-701190d424fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.202604635s
STEP: Saw pod success
May 11 18:16:26.281: INFO: Pod "downward-api-fe984a1c-18c8-47e5-976a-701190d424fb" satisfied condition "Succeeded or Failed"
May 11 18:16:26.351: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downward-api-fe984a1c-18c8-47e5-976a-701190d424fb container dapi-container: <nil>
STEP: delete the pod
May 11 18:16:26.506: INFO: Waiting for pod downward-api-fe984a1c-18c8-47e5-976a-701190d424fb to disappear
May 11 18:16:26.572: INFO: Pod downward-api-fe984a1c-18c8-47e5-976a-701190d424fb no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:26.572: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4666" for this suite.


• [SLOW TEST:5.197 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":-1,"completed":52,"skipped":991,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:16:24.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353784, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353784, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353784, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353784, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:16:26.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353784, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353784, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353784, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353784, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:16:30.052: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:30.424: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-6456" for this suite.
STEP: Destroying namespace "webhook-6456-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:7.511 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":-1,"completed":41,"skipped":797,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:16:27.284: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76227b47-4359-4944-8efc-99f7251dc564" in namespace "projected-1235" to be "Succeeded or Failed"
May 11 18:16:27.354: INFO: Pod "downwardapi-volume-76227b47-4359-4944-8efc-99f7251dc564": Phase="Pending", Reason="", readiness=false. Elapsed: 70.137319ms
May 11 18:16:29.421: INFO: Pod "downwardapi-volume-76227b47-4359-4944-8efc-99f7251dc564": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137132424s
May 11 18:16:31.492: INFO: Pod "downwardapi-volume-76227b47-4359-4944-8efc-99f7251dc564": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.207945435s
STEP: Saw pod success
May 11 18:16:31.492: INFO: Pod "downwardapi-volume-76227b47-4359-4944-8efc-99f7251dc564" satisfied condition "Succeeded or Failed"
May 11 18:16:31.560: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downwardapi-volume-76227b47-4359-4944-8efc-99f7251dc564 container client-container: <nil>
STEP: delete the pod
May 11 18:16:31.712: INFO: Waiting for pod downwardapi-volume-76227b47-4359-4944-8efc-99f7251dc564 to disappear
May 11 18:16:31.782: INFO: Pod downwardapi-volume-76227b47-4359-4944-8efc-99f7251dc564 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:31.782: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1235" for this suite.


• [SLOW TEST:5.189 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":-1,"completed":53,"skipped":1000,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 11 18:16:36.494: INFO: Successfully updated pod "pod-update-a1e54921-731a-4b42-a7c8-be95666a98f0"
STEP: verifying the updated pod is in kubernetes
May 11 18:16:36.624: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:36.624: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-4355" for this suite.


• [SLOW TEST:5.731 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":-1,"completed":42,"skipped":813,"failed":0}

SS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
May 11 18:16:32.519: INFO: Waiting up to 5m0s for pod "var-expansion-581a605d-eb06-49d1-9e80-2ba5ea984221" in namespace "var-expansion-9160" to be "Succeeded or Failed"
May 11 18:16:32.586: INFO: Pod "var-expansion-581a605d-eb06-49d1-9e80-2ba5ea984221": Phase="Pending", Reason="", readiness=false. Elapsed: 66.494895ms
May 11 18:16:34.653: INFO: Pod "var-expansion-581a605d-eb06-49d1-9e80-2ba5ea984221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133296984s
May 11 18:16:36.719: INFO: Pod "var-expansion-581a605d-eb06-49d1-9e80-2ba5ea984221": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.200230652s
STEP: Saw pod success
May 11 18:16:36.719: INFO: Pod "var-expansion-581a605d-eb06-49d1-9e80-2ba5ea984221" satisfied condition "Succeeded or Failed"
May 11 18:16:36.786: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod var-expansion-581a605d-eb06-49d1-9e80-2ba5ea984221 container dapi-container: <nil>
STEP: delete the pod
May 11 18:16:36.937: INFO: Waiting for pod var-expansion-581a605d-eb06-49d1-9e80-2ba5ea984221 to disappear
May 11 18:16:37.013: INFO: Pod var-expansion-581a605d-eb06-49d1-9e80-2ba5ea984221 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:37.013: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-9160" for this suite.


• [SLOW TEST:5.181 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":-1,"completed":54,"skipped":1033,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 11 18:16:37.642: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3515 /api/v1/namespaces/watch-3515/configmaps/e2e-watch-test-label-changed 28a35e3c-02d3-4413-a45c-8f9ea3de7d70 74209 0 2021-05-11 18:16:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-11 18:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:16:37.642: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3515 /api/v1/namespaces/watch-3515/configmaps/e2e-watch-test-label-changed 28a35e3c-02d3-4413-a45c-8f9ea3de7d70 74248 0 2021-05-11 18:16:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-11 18:16:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:16:37.642: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3515 /api/v1/namespaces/watch-3515/configmaps/e2e-watch-test-label-changed 28a35e3c-02d3-4413-a45c-8f9ea3de7d70 74253 0 2021-05-11 18:16:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-11 18:16:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 11 18:16:48.110: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3515 /api/v1/namespaces/watch-3515/configmaps/e2e-watch-test-label-changed 28a35e3c-02d3-4413-a45c-8f9ea3de7d70 74402 0 2021-05-11 18:16:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-11 18:16:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:16:48.110: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3515 /api/v1/namespaces/watch-3515/configmaps/e2e-watch-test-label-changed 28a35e3c-02d3-4413-a45c-8f9ea3de7d70 74405 0 2021-05-11 18:16:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-11 18:16:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:16:48.110: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3515 /api/v1/namespaces/watch-3515/configmaps/e2e-watch-test-label-changed 28a35e3c-02d3-4413-a45c-8f9ea3de7d70 74406 0 2021-05-11 18:16:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-11 18:16:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:16:48.110: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-3515" for this suite.


• [SLOW TEST:11.478 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":-1,"completed":43,"skipped":815,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:16:37.778: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Pending, waiting for it to be Running (with Ready = true)
May 11 18:16:39.849: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Pending, waiting for it to be Running (with Ready = true)
May 11 18:16:41.845: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = false)
May 11 18:16:43.849: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = false)
May 11 18:16:45.846: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = false)
May 11 18:16:47.845: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = false)
May 11 18:16:49.848: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = false)
May 11 18:16:51.848: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = false)
May 11 18:16:53.845: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = false)
May 11 18:16:55.845: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = false)
May 11 18:16:57.846: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = false)
May 11 18:16:59.845: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = false)
May 11 18:17:01.845: INFO: The status of Pod test-webserver-0c15cd04-bf0f-46bd-a8d9-f82075e8501c is Running (Ready = true)
May 11 18:17:01.915: INFO: Container started at 2021-05-11 18:16:40 +0000 UTC, pod became ready at 2021-05-11 18:17:01 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:01.915: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-7699" for this suite.


• [SLOW TEST:24.898 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":-1,"completed":55,"skipped":1042,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-e05c28e4-ca81-4ad5-903d-086fc827ca38 in namespace container-probe-6136
May 11 18:13:09.092: INFO: Started pod test-webserver-e05c28e4-ca81-4ad5-903d-086fc827ca38 in namespace container-probe-6136
STEP: checking the pod's current state and verifying that restartCount is present
May 11 18:13:09.156: INFO: Initial restart count of pod test-webserver-e05c28e4-ca81-4ad5-903d-086fc827ca38 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:10.938: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-6136" for this suite.


• [SLOW TEST:246.640 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":-1,"completed":85,"skipped":1644,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-w8mj
STEP: Creating a pod to test atomic-volume-subpath
May 11 18:16:48.921: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w8mj" in namespace "subpath-7950" to be "Succeeded or Failed"
May 11 18:16:48.986: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Pending", Reason="", readiness=false. Elapsed: 65.105362ms
May 11 18:16:51.052: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130649015s
May 11 18:16:53.117: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Running", Reason="", readiness=true. Elapsed: 4.19616613s
May 11 18:16:55.186: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Running", Reason="", readiness=true. Elapsed: 6.264653295s
May 11 18:16:57.253: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Running", Reason="", readiness=true. Elapsed: 8.331537529s
May 11 18:16:59.320: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Running", Reason="", readiness=true. Elapsed: 10.398605794s
May 11 18:17:01.385: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Running", Reason="", readiness=true. Elapsed: 12.463779097s
May 11 18:17:03.454: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Running", Reason="", readiness=true. Elapsed: 14.532394272s
May 11 18:17:05.519: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Running", Reason="", readiness=true. Elapsed: 16.597981545s
May 11 18:17:07.588: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Running", Reason="", readiness=true. Elapsed: 18.666479294s
May 11 18:17:09.654: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Running", Reason="", readiness=true. Elapsed: 20.732510284s
May 11 18:17:11.719: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Running", Reason="", readiness=true. Elapsed: 22.798090311s
May 11 18:17:13.788: INFO: Pod "pod-subpath-test-configmap-w8mj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.866514624s
STEP: Saw pod success
May 11 18:17:13.788: INFO: Pod "pod-subpath-test-configmap-w8mj" satisfied condition "Succeeded or Failed"
May 11 18:17:13.853: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-subpath-test-configmap-w8mj container test-container-subpath-configmap-w8mj: <nil>
STEP: delete the pod
May 11 18:17:13.996: INFO: Waiting for pod pod-subpath-test-configmap-w8mj to disappear
May 11 18:17:14.061: INFO: Pod pod-subpath-test-configmap-w8mj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w8mj
May 11 18:17:14.061: INFO: Deleting pod "pod-subpath-test-configmap-w8mj" in namespace "subpath-7950"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:14.132: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-7950" for this suite.


• [SLOW TEST:26.013 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":-1,"completed":44,"skipped":821,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:17:06.766: INFO: Deleting pod "var-expansion-d97149ab-b59f-4434-b123-18ace94ad72b" in namespace "var-expansion-9024"
May 11 18:17:06.839: INFO: Wait up to 5m0s for pod "var-expansion-d97149ab-b59f-4434-b123-18ace94ad72b" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:18.974: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-9024" for this suite.


• [SLOW TEST:17.020 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":-1,"completed":56,"skipped":1053,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
May 11 18:17:20.142: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:17:20.483: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353840, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353840, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353840, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353840, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:17:22.553: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353840, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353840, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353840, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353840, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:17:25.628: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:27.043: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-6894" for this suite.
STEP: Destroying namespace "webhook-6894-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:8.387 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":-1,"completed":57,"skipped":1059,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:17:12.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:17:14.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:17:16.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353832, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:17:19.598: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:33.086: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-1037" for this suite.
STEP: Destroying namespace "webhook-1037-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:22.521 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":-1,"completed":86,"skipped":1651,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0511 18:17:34.746563   45011 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W0511 18:17:34.746602   45011 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0511 18:17:34.746607   45011 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0511 18:17:34.746612   45011 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 11 18:17:34.746: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:34.746: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-9471" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":-1,"completed":87,"skipped":1669,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:17:29.068: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353848, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353848, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353848, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353848, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:17:31.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353848, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353848, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353848, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353848, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:17:34.216: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:35.010: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-563" for this suite.
STEP: Destroying namespace "webhook-563-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:7.949 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":-1,"completed":58,"skipped":1069,"failed":0}

SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:17:35.299: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2c7518f-a845-4689-8a4f-f39457e724d5" in namespace "downward-api-6827" to be "Succeeded or Failed"
May 11 18:17:35.364: INFO: Pod "downwardapi-volume-c2c7518f-a845-4689-8a4f-f39457e724d5": Phase="Pending", Reason="", readiness=false. Elapsed: 64.589267ms
May 11 18:17:37.432: INFO: Pod "downwardapi-volume-c2c7518f-a845-4689-8a4f-f39457e724d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133426351s
May 11 18:17:39.497: INFO: Pod "downwardapi-volume-c2c7518f-a845-4689-8a4f-f39457e724d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.197812255s
STEP: Saw pod success
May 11 18:17:39.497: INFO: Pod "downwardapi-volume-c2c7518f-a845-4689-8a4f-f39457e724d5" satisfied condition "Succeeded or Failed"
May 11 18:17:39.561: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downwardapi-volume-c2c7518f-a845-4689-8a4f-f39457e724d5 container client-container: <nil>
STEP: delete the pod
May 11 18:17:39.705: INFO: Waiting for pod downwardapi-volume-c2c7518f-a845-4689-8a4f-f39457e724d5 to disappear
May 11 18:17:39.769: INFO: Pod downwardapi-volume-c2c7518f-a845-4689-8a4f-f39457e724d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:39.769: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6827" for this suite.


• [SLOW TEST:5.245 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":88,"skipped":1678,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 11 18:17:36.042: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:44.110: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-2755" for this suite.


• [SLOW TEST:8.680 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":-1,"completed":59,"skipped":1090,"failed":0}

SSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:44.198: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-1699" for this suite.


• [SLOW TEST:30.042 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":-1,"completed":45,"skipped":838,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:17:41.493: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353861, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353861, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353861, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353861, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:17:43.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353861, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353861, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353861, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353861, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:17:46.631: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:47.966: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-2304" for this suite.
STEP: Destroying namespace "webhook-2304-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:8.374 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":-1,"completed":89,"skipped":1687,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:49.406: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9450" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":-1,"completed":90,"skipped":1750,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 11 18:17:49.753: INFO: Successfully updated pod "labelsupdated2d80b59-b62a-43ec-ab9c-75881bb44291"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:51.905: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8216" for this suite.


• [SLOW TEST:7.795 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":-1,"completed":60,"skipped":1095,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 11 18:17:52.613: INFO: Waiting up to 5m0s for pod "pod-ae2f5721-a0a1-4698-b629-5469b43f499a" in namespace "emptydir-5160" to be "Succeeded or Failed"
May 11 18:17:52.681: INFO: Pod "pod-ae2f5721-a0a1-4698-b629-5469b43f499a": Phase="Pending", Reason="", readiness=false. Elapsed: 67.780098ms
May 11 18:17:54.748: INFO: Pod "pod-ae2f5721-a0a1-4698-b629-5469b43f499a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134783528s
May 11 18:17:56.823: INFO: Pod "pod-ae2f5721-a0a1-4698-b629-5469b43f499a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.210231641s
STEP: Saw pod success
May 11 18:17:56.823: INFO: Pod "pod-ae2f5721-a0a1-4698-b629-5469b43f499a" satisfied condition "Succeeded or Failed"
May 11 18:17:56.894: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-ae2f5721-a0a1-4698-b629-5469b43f499a container test-container: <nil>
STEP: delete the pod
May 11 18:17:57.054: INFO: Waiting for pod pod-ae2f5721-a0a1-4698-b629-5469b43f499a to disappear
May 11 18:17:57.121: INFO: Pod pod-ae2f5721-a0a1-4698-b629-5469b43f499a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:57.121: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5160" for this suite.


• [SLOW TEST:5.199 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 11 18:17:53.521: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 18:17:53.588: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 18:17:55.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 18:17:55.655: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 18:17:57.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 18:17:57.656: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 18:17:59.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 18:17:59.654: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:17:59.654: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2918" for this suite.


• [SLOW TEST:15.436 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":-1,"completed":46,"skipped":852,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":61,"skipped":1101,"failed":0}
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:17:57.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91f288c6-b825-4a17-82ef-f32d6d1e3f4f" in namespace "downward-api-5921" to be "Succeeded or Failed"
May 11 18:17:57.892: INFO: Pod "downwardapi-volume-91f288c6-b825-4a17-82ef-f32d6d1e3f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 66.876667ms
May 11 18:17:59.962: INFO: Pod "downwardapi-volume-91f288c6-b825-4a17-82ef-f32d6d1e3f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137040946s
May 11 18:18:02.032: INFO: Pod "downwardapi-volume-91f288c6-b825-4a17-82ef-f32d6d1e3f4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.207193715s
STEP: Saw pod success
May 11 18:18:02.033: INFO: Pod "downwardapi-volume-91f288c6-b825-4a17-82ef-f32d6d1e3f4f" satisfied condition "Succeeded or Failed"
May 11 18:18:02.099: INFO: Trying to get logs from node ip-10-0-143-31.us-west-1.compute.internal pod downwardapi-volume-91f288c6-b825-4a17-82ef-f32d6d1e3f4f container client-container: <nil>
STEP: delete the pod
May 11 18:18:02.259: INFO: Waiting for pod downwardapi-volume-91f288c6-b825-4a17-82ef-f32d6d1e3f4f to disappear
May 11 18:18:02.326: INFO: Pod downwardapi-volume-91f288c6-b825-4a17-82ef-f32d6d1e3f4f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:02.326: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5921" for this suite.


• [SLOW TEST:5.204 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":-1,"completed":62,"skipped":1101,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2413.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2413.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2413.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2413.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2413.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2413.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2413.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2413.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2413.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 157.234.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.234.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.234.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.234.157_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2413.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2413.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2413.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2413.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2413.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2413.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2413.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2413.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2413.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 157.234.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.234.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.234.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.234.157_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 18:17:54.504: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc.cluster.local from pod dns-2413/dns-test-d2a9b294-e6e8-462e-b183-f58cff545789: the server could not find the requested resource (get pods dns-test-d2a9b294-e6e8-462e-b183-f58cff545789)
May 11 18:17:54.641: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2413.svc.cluster.local from pod dns-2413/dns-test-d2a9b294-e6e8-462e-b183-f58cff545789: the server could not find the requested resource (get pods dns-test-d2a9b294-e6e8-462e-b183-f58cff545789)
May 11 18:17:55.180: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc.cluster.local from pod dns-2413/dns-test-d2a9b294-e6e8-462e-b183-f58cff545789: the server could not find the requested resource (get pods dns-test-d2a9b294-e6e8-462e-b183-f58cff545789)
May 11 18:17:55.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2413.svc.cluster.local from pod dns-2413/dns-test-d2a9b294-e6e8-462e-b183-f58cff545789: the server could not find the requested resource (get pods dns-test-d2a9b294-e6e8-462e-b183-f58cff545789)
May 11 18:17:55.817: INFO: Lookups using dns-2413/dns-test-d2a9b294-e6e8-462e-b183-f58cff545789 failed for: [wheezy_udp@dns-test-service.dns-2413.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2413.svc.cluster.local jessie_udp@dns-test-service.dns-2413.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2413.svc.cluster.local]

May 11 18:18:02.159: INFO: DNS probes using dns-2413/dns-test-d2a9b294-e6e8-462e-b183-f58cff545789 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:02.403: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-2413" for this suite.


• [SLOW TEST:13.080 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":-1,"completed":91,"skipped":1773,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:03.409: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-4761" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":-1,"completed":63,"skipped":1107,"failed":0}

S
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 11 18:18:04.838: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:04.984: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-6773" for this suite.


• [SLOW TEST:5.215 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":-1,"completed":47,"skipped":939,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-126a8c4f-3aec-4fe9-a7d3-3315bff9c9ad
STEP: Creating a pod to test consume secrets
May 11 18:18:04.043: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d4874f7a-83ed-4967-991b-0dd3332f0b5a" in namespace "projected-2994" to be "Succeeded or Failed"
May 11 18:18:04.113: INFO: Pod "pod-projected-secrets-d4874f7a-83ed-4967-991b-0dd3332f0b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 69.936464ms
May 11 18:18:06.184: INFO: Pod "pod-projected-secrets-d4874f7a-83ed-4967-991b-0dd3332f0b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.141351109s
May 11 18:18:08.251: INFO: Pod "pod-projected-secrets-d4874f7a-83ed-4967-991b-0dd3332f0b5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.208134422s
STEP: Saw pod success
May 11 18:18:08.251: INFO: Pod "pod-projected-secrets-d4874f7a-83ed-4967-991b-0dd3332f0b5a" satisfied condition "Succeeded or Failed"
May 11 18:18:08.317: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod pod-projected-secrets-d4874f7a-83ed-4967-991b-0dd3332f0b5a container projected-secret-volume-test: <nil>
STEP: delete the pod
May 11 18:18:08.486: INFO: Waiting for pod pod-projected-secrets-d4874f7a-83ed-4967-991b-0dd3332f0b5a to disappear
May 11 18:18:08.557: INFO: Pod pod-projected-secrets-d4874f7a-83ed-4967-991b-0dd3332f0b5a no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:08.557: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2994" for this suite.


• [SLOW TEST:5.275 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":64,"skipped":1108,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
May 11 18:18:05.687: INFO: Waiting up to 5m0s for pod "pod-f0888745-1eaf-4589-882a-0d72866a0c46" in namespace "emptydir-1474" to be "Succeeded or Failed"
May 11 18:18:05.752: INFO: Pod "pod-f0888745-1eaf-4589-882a-0d72866a0c46": Phase="Pending", Reason="", readiness=false. Elapsed: 64.847601ms
May 11 18:18:07.821: INFO: Pod "pod-f0888745-1eaf-4589-882a-0d72866a0c46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134104048s
May 11 18:18:09.887: INFO: Pod "pod-f0888745-1eaf-4589-882a-0d72866a0c46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.19993797s
STEP: Saw pod success
May 11 18:18:09.887: INFO: Pod "pod-f0888745-1eaf-4589-882a-0d72866a0c46" satisfied condition "Succeeded or Failed"
May 11 18:18:09.953: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-f0888745-1eaf-4589-882a-0d72866a0c46 container test-container: <nil>
STEP: delete the pod
May 11 18:18:10.101: INFO: Waiting for pod pod-f0888745-1eaf-4589-882a-0d72866a0c46 to disappear
May 11 18:18:10.166: INFO: Pod pod-f0888745-1eaf-4589-882a-0d72866a0c46 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:10.166: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-1474" for this suite.


• [SLOW TEST:5.158 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":48,"skipped":951,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
May 11 18:18:10.915: INFO: created test-pod-1
May 11 18:18:10.988: INFO: created test-pod-2
May 11 18:18:11.065: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:11.377: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-4842" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":-1,"completed":49,"skipped":997,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:18:09.257: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c585b27-285e-4f25-b5f3-295b8afe4fd7" in namespace "downward-api-9790" to be "Succeeded or Failed"
May 11 18:18:09.330: INFO: Pod "downwardapi-volume-0c585b27-285e-4f25-b5f3-295b8afe4fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 72.207672ms
May 11 18:18:11.396: INFO: Pod "downwardapi-volume-0c585b27-285e-4f25-b5f3-295b8afe4fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138811437s
May 11 18:18:13.466: INFO: Pod "downwardapi-volume-0c585b27-285e-4f25-b5f3-295b8afe4fd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.209098484s
STEP: Saw pod success
May 11 18:18:13.466: INFO: Pod "downwardapi-volume-0c585b27-285e-4f25-b5f3-295b8afe4fd7" satisfied condition "Succeeded or Failed"
May 11 18:18:13.533: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod downwardapi-volume-0c585b27-285e-4f25-b5f3-295b8afe4fd7 container client-container: <nil>
STEP: delete the pod
May 11 18:18:13.680: INFO: Waiting for pod downwardapi-volume-0c585b27-285e-4f25-b5f3-295b8afe4fd7 to disappear
May 11 18:18:13.746: INFO: Pod downwardapi-volume-0c585b27-285e-4f25-b5f3-295b8afe4fd7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:13.747: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-9790" for this suite.


• [SLOW TEST:5.180 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":-1,"completed":65,"skipped":1110,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:18:11.921: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-cec08c45-056c-4947-8e10-1eed74468ca3
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:16.417: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-1364" for this suite.


• [SLOW TEST:5.151 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":50,"skipped":1005,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 11 18:18:19.423: INFO: Successfully updated pod "labelsupdated900168c-75d4-46da-b11f-4cccdd941864"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:21.561: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8996" for this suite.


• [SLOW TEST:7.753 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":-1,"completed":66,"skipped":1142,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 11 18:18:22.668: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-745 /api/v1/namespaces/watch-745/configmaps/e2e-watch-test-resource-version 874d01cb-58a3-46e3-b8a6-b2a0c6343ea7 77573 0 2021-05-11 18:18:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-11 18:18:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 11 18:18:22.668: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-745 /api/v1/namespaces/watch-745/configmaps/e2e-watch-test-resource-version 874d01cb-58a3-46e3-b8a6-b2a0c6343ea7 77574 0 2021-05-11 18:18:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-11 18:18:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:22.668: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-745" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":-1,"completed":67,"skipped":1153,"failed":0}

SS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:18:24.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353903, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353903, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353903, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353903, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:18:26.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353903, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353903, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353903, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353903, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:18:29.280: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2521-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:30.594: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-4453" for this suite.
STEP: Destroying namespace "webhook-4453-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:8.508 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":-1,"completed":68,"skipped":1155,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-57c1ddbf-804b-45f9-bcc1-6e4e6154a450
STEP: Creating a pod to test consume configMaps
May 11 18:18:31.845: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8a4039c2-5569-46c7-b5da-6d884327e413" in namespace "projected-3249" to be "Succeeded or Failed"
May 11 18:18:31.912: INFO: Pod "pod-projected-configmaps-8a4039c2-5569-46c7-b5da-6d884327e413": Phase="Pending", Reason="", readiness=false. Elapsed: 66.812509ms
May 11 18:18:33.979: INFO: Pod "pod-projected-configmaps-8a4039c2-5569-46c7-b5da-6d884327e413": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133855314s
May 11 18:18:36.047: INFO: Pod "pod-projected-configmaps-8a4039c2-5569-46c7-b5da-6d884327e413": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.201563628s
STEP: Saw pod success
May 11 18:18:36.047: INFO: Pod "pod-projected-configmaps-8a4039c2-5569-46c7-b5da-6d884327e413" satisfied condition "Succeeded or Failed"
May 11 18:18:36.118: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-projected-configmaps-8a4039c2-5569-46c7-b5da-6d884327e413 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 11 18:18:36.268: INFO: Waiting for pod pod-projected-configmaps-8a4039c2-5569-46c7-b5da-6d884327e413 to disappear
May 11 18:18:36.334: INFO: Pod pod-projected-configmaps-8a4039c2-5569-46c7-b5da-6d884327e413 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:36.334: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3249" for this suite.


• [SLOW TEST:5.275 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":-1,"completed":69,"skipped":1166,"failed":0}

S
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:41.473: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-7225" for this suite.


• [SLOW TEST:5.118 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":-1,"completed":70,"skipped":1167,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8249
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8249
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8249
May 11 18:12:32.977: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
May 11 18:12:43.044: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 11 18:12:43.112: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 11 18:12:43.884: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 11 18:12:43.884: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 11 18:12:43.884: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 11 18:12:43.950: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 11 18:12:54.016: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 11 18:12:54.016: INFO: Waiting for statefulset status.replicas updated to 0
May 11 18:12:54.288: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999654s
May 11 18:12:55.355: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.934423204s
May 11 18:12:56.421: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.866943354s
May 11 18:12:57.499: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.800815609s
May 11 18:12:58.573: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.723124501s
May 11 18:12:59.653: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.648985478s
May 11 18:13:00.719: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.56848404s
May 11 18:13:01.790: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.50265831s
May 11 18:13:02.855: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.432036107s
May 11 18:13:03.923: INFO: Verifying statefulset ss doesn't scale past 1 for another 366.411807ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8249
May 11 18:13:04.989: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:13:05.733: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 11 18:13:05.733: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 11 18:13:05.733: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 11 18:13:05.800: INFO: Found 1 stateful pods, waiting for 3
May 11 18:13:15.866: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:13:15.866: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 18:13:15.866: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 11 18:13:16.002: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 11 18:13:16.740: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 11 18:13:16.740: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 11 18:13:16.740: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 11 18:13:16.741: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 11 18:13:17.508: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 11 18:13:17.508: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 11 18:13:17.508: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 11 18:13:17.508: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 11 18:13:18.260: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 11 18:13:18.260: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 11 18:13:18.260: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 11 18:13:18.260: INFO: Waiting for statefulset status.replicas updated to 0
May 11 18:13:18.325: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 11 18:13:28.460: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 11 18:13:28.460: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 11 18:13:28.460: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 11 18:13:28.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999697s
May 11 18:13:29.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.934218728s
May 11 18:13:30.799: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.867400101s
May 11 18:13:31.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.801583527s
May 11 18:13:32.931: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.735114577s
May 11 18:13:33.998: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.668922112s
May 11 18:13:35.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.602326741s
May 11 18:13:36.133: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.532636579s
May 11 18:13:37.199: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.46681781s
May 11 18:13:38.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 400.756836ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8249
May 11 18:13:39.332: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:13:40.069: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 11 18:13:40.069: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 11 18:13:40.069: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 11 18:13:40.069: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:13:40.823: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 11 18:13:40.823: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 11 18:13:40.823: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 11 18:13:40.823: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:13:41.513: INFO: rc: 1
May 11 18:13:41.513: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container is not created or running

error:
exit status 1
May 11 18:13:51.513: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:13:51.852: INFO: rc: 1
May 11 18:13:51.852: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:14:01.852: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:14:02.175: INFO: rc: 1
May 11 18:14:02.175: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:14:12.175: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:14:12.496: INFO: rc: 1
May 11 18:14:12.496: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:14:22.496: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:14:22.819: INFO: rc: 1
May 11 18:14:22.819: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:14:32.819: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:14:33.139: INFO: rc: 1
May 11 18:14:33.140: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:14:43.140: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:14:43.453: INFO: rc: 1
May 11 18:14:43.453: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:14:53.453: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:14:53.798: INFO: rc: 1
May 11 18:14:53.798: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:15:03.799: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:15:04.137: INFO: rc: 1
May 11 18:15:04.137: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:15:14.137: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:15:14.461: INFO: rc: 1
May 11 18:15:14.461: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:15:24.461: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:15:24.778: INFO: rc: 1
May 11 18:15:24.779: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:15:34.779: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:15:35.110: INFO: rc: 1
May 11 18:15:35.110: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:15:45.110: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:15:45.429: INFO: rc: 1
May 11 18:15:45.429: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:15:55.429: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:15:55.751: INFO: rc: 1
May 11 18:15:55.751: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:16:05.751: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:16:06.079: INFO: rc: 1
May 11 18:16:06.079: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:16:16.079: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:16:16.394: INFO: rc: 1
May 11 18:16:16.394: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:16:26.394: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:16:26.713: INFO: rc: 1
May 11 18:16:26.713: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:16:36.714: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:16:37.050: INFO: rc: 1
May 11 18:16:37.050: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:16:47.050: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:16:47.373: INFO: rc: 1
May 11 18:16:47.373: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:16:57.373: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:16:57.719: INFO: rc: 1
May 11 18:16:57.719: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:17:07.720: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:17:08.065: INFO: rc: 1
May 11 18:17:08.065: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:17:18.066: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:17:18.397: INFO: rc: 1
May 11 18:17:18.397: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:17:28.397: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:17:28.737: INFO: rc: 1
May 11 18:17:28.737: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:17:38.737: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:17:39.076: INFO: rc: 1
May 11 18:17:39.076: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:17:49.076: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:17:49.410: INFO: rc: 1
May 11 18:17:49.410: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:17:59.410: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:17:59.728: INFO: rc: 1
May 11 18:17:59.728: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:18:09.728: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:18:10.045: INFO: rc: 1
May 11 18:18:10.045: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:18:20.045: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:18:20.370: INFO: rc: 1
May 11 18:18:20.370: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:18:30.370: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:18:30.696: INFO: rc: 1
May 11 18:18:30.696: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 11 18:18:40.696: INFO: Running '/go/src/k8s.io/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/kubeconfig-563040664 --namespace=statefulset-8249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 11 18:18:41.025: INFO: rc: 1
May 11 18:18:41.025: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
May 11 18:18:41.025: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 11 18:18:41.355: INFO: Deleting all statefulset in ns statefulset-8249
May 11 18:18:41.421: INFO: Scaling statefulset ss to 0
May 11 18:18:41.622: INFO: Waiting for statefulset status.replicas updated to 0
May 11 18:18:41.687: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:41.893: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-8249" for this suite.


• [SLOW TEST:369.777 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":-1,"completed":63,"skipped":1140,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0511 18:18:43.623478   45011 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W0511 18:18:43.623521   45011 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0511 18:18:43.623527   45011 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0511 18:18:43.623532   45011 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 11 18:18:43.623: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 11 18:18:43.623: INFO: Deleting pod "simpletest.rc-5vsrw" in namespace "gc-4309"
May 11 18:18:43.699: INFO: Deleting pod "simpletest.rc-cz9pg" in namespace "gc-4309"
May 11 18:18:43.776: INFO: Deleting pod "simpletest.rc-jhflv" in namespace "gc-4309"
May 11 18:18:43.852: INFO: Deleting pod "simpletest.rc-mxkj4" in namespace "gc-4309"
May 11 18:18:43.927: INFO: Deleting pod "simpletest.rc-nwzwk" in namespace "gc-4309"
May 11 18:18:44.002: INFO: Deleting pod "simpletest.rc-vs4kb" in namespace "gc-4309"
May 11 18:18:44.075: INFO: Deleting pod "simpletest.rc-wtfvz" in namespace "gc-4309"
May 11 18:18:44.151: INFO: Deleting pod "simpletest.rc-wzjfl" in namespace "gc-4309"
May 11 18:18:44.225: INFO: Deleting pod "simpletest.rc-xkgpq" in namespace "gc-4309"
May 11 18:18:44.301: INFO: Deleting pod "simpletest.rc-xsggc" in namespace "gc-4309"
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:44.382: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-4309" for this suite.


• [SLOW TEST:41.806 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":-1,"completed":92,"skipped":1789,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 11 18:18:42.685: INFO: Waiting up to 5m0s for pod "pod-05a3dc00-75d5-42c3-b765-700ff253d6b2" in namespace "emptydir-5096" to be "Succeeded or Failed"
May 11 18:18:42.750: INFO: Pod "pod-05a3dc00-75d5-42c3-b765-700ff253d6b2": Phase="Pending", Reason="", readiness=false. Elapsed: 65.570915ms
May 11 18:18:44.816: INFO: Pod "pod-05a3dc00-75d5-42c3-b765-700ff253d6b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131203563s
May 11 18:18:46.882: INFO: Pod "pod-05a3dc00-75d5-42c3-b765-700ff253d6b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.196696001s
STEP: Saw pod success
May 11 18:18:46.882: INFO: Pod "pod-05a3dc00-75d5-42c3-b765-700ff253d6b2" satisfied condition "Succeeded or Failed"
May 11 18:18:46.947: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-05a3dc00-75d5-42c3-b765-700ff253d6b2 container test-container: <nil>
STEP: delete the pod
May 11 18:18:47.094: INFO: Waiting for pod pod-05a3dc00-75d5-42c3-b765-700ff253d6b2 to disappear
May 11 18:18:47.159: INFO: Pod pod-05a3dc00-75d5-42c3-b765-700ff253d6b2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:47.159: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5096" for this suite.


• [SLOW TEST:5.144 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":64,"skipped":1158,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-499
[It] should have a working scale subresource [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-499
May 11 18:18:17.232: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
May 11 18:18:27.298: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 11 18:18:27.632: INFO: Deleting all statefulset in ns statefulset-499
May 11 18:18:27.697: INFO: Scaling statefulset ss to 0
May 11 18:18:47.967: INFO: Waiting for statefulset status.replicas updated to 0
May 11 18:18:48.033: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:48.241: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-499" for this suite.


• [SLOW TEST:31.806 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":-1,"completed":51,"skipped":1019,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 11 18:18:43.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353922, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353922, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353923, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353922, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:18:45.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353922, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353922, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353923, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353922, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 11 18:18:48.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:48.563: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-1944" for this suite.
STEP: Destroying namespace "webhook-1944-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101


• [SLOW TEST:7.345 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":-1,"completed":71,"skipped":1185,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-c70e6686-4015-46fc-8e5d-ad14182f5a58
STEP: Creating a pod to test consume secrets
May 11 18:18:45.069: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b25b1a7-9e89-4071-bc02-d9adfc9c5a0f" in namespace "projected-2335" to be "Succeeded or Failed"
May 11 18:18:45.136: INFO: Pod "pod-projected-secrets-1b25b1a7-9e89-4071-bc02-d9adfc9c5a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 67.677782ms
May 11 18:18:47.201: INFO: Pod "pod-projected-secrets-1b25b1a7-9e89-4071-bc02-d9adfc9c5a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13244549s
May 11 18:18:49.269: INFO: Pod "pod-projected-secrets-1b25b1a7-9e89-4071-bc02-d9adfc9c5a0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.200484353s
STEP: Saw pod success
May 11 18:18:49.269: INFO: Pod "pod-projected-secrets-1b25b1a7-9e89-4071-bc02-d9adfc9c5a0f" satisfied condition "Succeeded or Failed"
May 11 18:18:49.340: INFO: Trying to get logs from node ip-10-0-128-72.us-west-1.compute.internal pod pod-projected-secrets-1b25b1a7-9e89-4071-bc02-d9adfc9c5a0f container projected-secret-volume-test: <nil>
STEP: delete the pod
May 11 18:18:49.484: INFO: Waiting for pod pod-projected-secrets-1b25b1a7-9e89-4071-bc02-d9adfc9c5a0f to disappear
May 11 18:18:49.548: INFO: Pod pod-projected-secrets-1b25b1a7-9e89-4071-bc02-d9adfc9c5a0f no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:49.548: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2335" for this suite.


• [SLOW TEST:5.202 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":93,"skipped":1803,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 11 18:18:52.037: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8916 PodName:pod-sharedvolume-dc01c053-1c0d-436a-a1e7-b19532e67709 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 11 18:18:52.496: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:52.497: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-8916" for this suite.


• [SLOW TEST:5.335 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
May 11 18:18:49.642: INFO: Waiting up to 5m0s for pod "var-expansion-6d19cd24-78f3-420a-852a-5154767dbf88" in namespace "var-expansion-7840" to be "Succeeded or Failed"
May 11 18:18:49.709: INFO: Pod "var-expansion-6d19cd24-78f3-420a-852a-5154767dbf88": Phase="Pending", Reason="", readiness=false. Elapsed: 66.643198ms
May 11 18:18:51.779: INFO: Pod "var-expansion-6d19cd24-78f3-420a-852a-5154767dbf88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136761055s
May 11 18:18:53.850: INFO: Pod "var-expansion-6d19cd24-78f3-420a-852a-5154767dbf88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.207866935s
STEP: Saw pod success
May 11 18:18:53.850: INFO: Pod "var-expansion-6d19cd24-78f3-420a-852a-5154767dbf88" satisfied condition "Succeeded or Failed"
May 11 18:18:53.917: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod var-expansion-6d19cd24-78f3-420a-852a-5154767dbf88 container dapi-container: <nil>
STEP: delete the pod
May 11 18:18:54.075: INFO: Waiting for pod var-expansion-6d19cd24-78f3-420a-852a-5154767dbf88 to disappear
May 11 18:18:54.141: INFO: Pod var-expansion-6d19cd24-78f3-420a-852a-5154767dbf88 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:54.141: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-7840" for this suite.


• [SLOW TEST:5.200 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":-1,"completed":72,"skipped":1192,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 11 18:18:54.844: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf6eb063-ff89-4b3b-a36e-3237077e0db1" in namespace "downward-api-8601" to be "Succeeded or Failed"
May 11 18:18:54.910: INFO: Pod "downwardapi-volume-cf6eb063-ff89-4b3b-a36e-3237077e0db1": Phase="Pending", Reason="", readiness=false. Elapsed: 66.383604ms
May 11 18:18:56.981: INFO: Pod "downwardapi-volume-cf6eb063-ff89-4b3b-a36e-3237077e0db1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137020259s
May 11 18:18:59.048: INFO: Pod "downwardapi-volume-cf6eb063-ff89-4b3b-a36e-3237077e0db1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.204022747s
STEP: Saw pod success
May 11 18:18:59.048: INFO: Pod "downwardapi-volume-cf6eb063-ff89-4b3b-a36e-3237077e0db1" satisfied condition "Succeeded or Failed"
May 11 18:18:59.115: INFO: Trying to get logs from node ip-10-0-198-156.us-west-1.compute.internal pod downwardapi-volume-cf6eb063-ff89-4b3b-a36e-3237077e0db1 container client-container: <nil>
STEP: delete the pod
May 11 18:18:59.267: INFO: Waiting for pod downwardapi-volume-cf6eb063-ff89-4b3b-a36e-3237077e0db1 to disappear
May 11 18:18:59.334: INFO: Pod downwardapi-volume-cf6eb063-ff89-4b3b-a36e-3237077e0db1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:18:59.334: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8601" for this suite.


• [SLOW TEST:5.180 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":73,"skipped":1196,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 11 18:19:04.467: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:19:04.676: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-4091" for this suite.


• [SLOW TEST:5.309 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":-1,"completed":74,"skipped":1219,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":-1,"completed":65,"skipped":1161,"failed":0}
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:18:53.098: INFO: Creating deployment "webserver-deployment"
May 11 18:18:53.169: INFO: Waiting for observed generation 1
May 11 18:18:55.301: INFO: Waiting for all required pods to come up
May 11 18:18:55.436: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 11 18:18:59.569: INFO: Waiting for deployment "webserver-deployment" to complete
May 11 18:18:59.699: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 11 18:18:59.866: INFO: Updating deployment webserver-deployment
May 11 18:18:59.866: INFO: Waiting for observed generation 2
May 11 18:19:01.997: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 11 18:19:02.062: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 11 18:19:02.131: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 11 18:19:02.326: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 11 18:19:02.326: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 11 18:19:02.395: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 11 18:19:02.525: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 11 18:19:02.525: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 11 18:19:02.662: INFO: Updating deployment webserver-deployment
May 11 18:19:02.662: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 11 18:19:02.800: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 11 18:19:02.865: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 11 18:19:05.063: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4087 /apis/apps/v1/namespaces/deployment-4087/deployments/webserver-deployment 0c994db3-5acb-46e0-b1ed-b7e6caaffc2b 79384 3 2021-05-11 18:18:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-11 18:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-11 18:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0014d05f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-11 18:19:02 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-11 18:19:02 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 11 18:19:05.146: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-4087 /apis/apps/v1/namespaces/deployment-4087/replicasets/webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 79382 3 2021-05-11 18:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 0c994db3-5acb-46e0-b1ed-b7e6caaffc2b 0xc0014d09b7 0xc0014d09b8}] []  [{kube-controller-manager Update apps/v1 2021-05-11 18:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0c994db3-5acb-46e0-b1ed-b7e6caaffc2b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0014d0a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 11 18:19:05.146: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 11 18:19:05.146: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-4087 /apis/apps/v1/namespaces/deployment-4087/replicasets/webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 79331 3 2021-05-11 18:18:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 0c994db3-5acb-46e0-b1ed-b7e6caaffc2b 0xc0014d0aa7 0xc0014d0aa8}] []  [{kube-controller-manager Update apps/v1 2021-05-11 18:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0c994db3-5acb-46e0-b1ed-b7e6caaffc2b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0014d0b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 11 18:19:05.402: INFO: Pod "webserver-deployment-795d758f88-2nh8f" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2nh8f webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-2nh8f 50e77a29-1c9a-4be2-89bb-d28ce73c7ff0 79446 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.15"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.15"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc0014d1487 0xc0014d1488}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.403: INFO: Pod "webserver-deployment-795d758f88-6zs2d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6zs2d webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-6zs2d 3e4b4d23-bd45-4d17-aa5f-72b05fe965fa 79385 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc0014d1c07 0xc0014d1c08}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-31.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.143.31,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.403: INFO: Pod "webserver-deployment-795d758f88-c425z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c425z webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-c425z 9f1001f4-d207-4798-a9fb-21e6fd0c4dc6 79482 0 2021-05-11 18:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.77"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.77"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc001914097 0xc001914098}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:19:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-31.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.143.31,PodIP:10.131.0.77,StartTime:2021-05-11 18:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.403: INFO: Pod "webserver-deployment-795d758f88-f75jd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-f75jd webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-f75jd 94d8cc38-55a2-4d14-9f4d-4a6d019f4c7d 79380 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc001914297 0xc001914298}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.403: INFO: Pod "webserver-deployment-795d758f88-fxn48" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fxn48 webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-fxn48 226be2d2-ea3e-476f-b291-447dff1d91fd 79374 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc001914467 0xc001914468}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.403: INFO: Pod "webserver-deployment-795d758f88-hsr5f" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hsr5f webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-hsr5f 78c5cd0e-7166-4152-be11-72c8a1279ec6 79432 0 2021-05-11 18:19:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.162"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.162"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc001914627 0xc001914628}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:10.128.2.162,StartTime:2021-05-11 18:19:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.403: INFO: Pod "webserver-deployment-795d758f88-k8xf8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k8xf8 webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-k8xf8 a8ec0463-054b-4945-aa26-2e948f3f69f6 79373 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc001914827 0xc001914828}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-31.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.143.31,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.404: INFO: Pod "webserver-deployment-795d758f88-kfvnp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kfvnp webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-kfvnp 805af7c3-5993-4991-a67f-6703c74dd09d 79465 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.16"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.16"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc0019149e7 0xc0019149e8}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.404: INFO: Pod "webserver-deployment-795d758f88-l5ct7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-l5ct7 webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-l5ct7 0aad9387-1752-47df-aebf-e8b986a359cc 79368 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc001914bb7 0xc001914bb8}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.404: INFO: Pod "webserver-deployment-795d758f88-lpzg6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lpzg6 webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-lpzg6 f58c996f-27b7-4699-957b-b7009b13aa5a 79438 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.80"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.80"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc001914d77 0xc001914d78}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-31.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.143.31,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.404: INFO: Pod "webserver-deployment-795d758f88-mzh4b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mzh4b webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-mzh4b 4a5ec465-df32-43f3-8041-95b525b28c12 79403 0 2021-05-11 18:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.10"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.10"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc001914f47 0xc001914f48}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.3.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:10.129.3.10,StartTime:2021-05-11 18:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.3.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.404: INFO: Pod "webserver-deployment-795d758f88-qnpp7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qnpp7 webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-qnpp7 863ae482-2bd9-4b91-8d74-c3d818c1a863 79440 0 2021-05-11 18:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.160"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.160"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc001915147 0xc001915148}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:10.128.2.160,StartTime:2021-05-11 18:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.404: INFO: Pod "webserver-deployment-795d758f88-wss6v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wss6v webserver-deployment-795d758f88- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-795d758f88-wss6v c4f18107-ea4f-402a-87f8-a536f81edf2c 79262 0 2021-05-11 18:19:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.11"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.11"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 2e0dafe6-7d45-491b-a0d7-ab258e10acc0 0xc001915357 0xc001915358}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0dafe6-7d45-491b-a0d7-ab258e10acc0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:,StartTime:2021-05-11 18:19:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.405: INFO: Pod "webserver-deployment-dd94f59b7-4fsrd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4fsrd webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-4fsrd 8d48da1f-51bc-4db5-9d18-0a45f61d304c 79053 0 2021-05-11 18:18:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.158"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.158"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc001915557 0xc001915558}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:18:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:10.128.2.158,StartTime:2021-05-11 18:18:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://583e5d424e7012a906bd982e5af2dd8f048f46affae491a090c9914235bc2037,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.407: INFO: Pod "webserver-deployment-dd94f59b7-6ktt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6ktt8 webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-6ktt8 728c7c4e-0c2f-4fcb-a418-d3be36c78920 79436 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.14"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.14"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc001915727 0xc001915728}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.408: INFO: Pod "webserver-deployment-dd94f59b7-7d727" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7d727 webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-7d727 f2035bbb-b417-4942-b795-669364db3c33 79472 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.79"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.79"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc0019158d7 0xc0019158d8}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-31.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.143.31,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.408: INFO: Pod "webserver-deployment-dd94f59b7-7xkt6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7xkt6 webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-7xkt6 421dc461-a30c-4d80-aeeb-96c7c936b356 79397 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.163"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.163"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc001915a87 0xc001915a88}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.408: INFO: Pod "webserver-deployment-dd94f59b7-cnm9w" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cnm9w webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-cnm9w a234584b-70b4-467a-96b6-9d3bccdeaf11 79463 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.166"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.166"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc001915c47 0xc001915c48}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.408: INFO: Pod "webserver-deployment-dd94f59b7-gfqrs" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gfqrs webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-gfqrs 2757159a-677f-4b9f-91b6-d2bbfb3d1fd4 79026 0 2021-05-11 18:18:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.7"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.7"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc001915df7 0xc001915df8}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:18:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.3.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:10.129.3.7,StartTime:2021-05-11 18:18:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:18:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://20559dca100fe992e69427a3e041b5e7d22f37be1d135e4e721fa23395c7ac1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.3.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.408: INFO: Pod "webserver-deployment-dd94f59b7-jzs6q" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jzs6q webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-jzs6q 72c65a4d-d68a-4edf-8280-6a3e59146bc1 79471 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.13"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.13"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc001915fc7 0xc001915fc8}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.408: INFO: Pod "webserver-deployment-dd94f59b7-lvzjc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lvzjc webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-lvzjc d14d1b7e-d7f3-4be1-b9d2-0234b5da610d 79067 0 2021-05-11 18:18:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.75"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.75"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002014197 0xc002014198}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-31.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.143.31,PodIP:10.131.0.75,StartTime:2021-05-11 18:18:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://cb61af512ad9aa04eca0eb6df6a133ac24798c5780344a3ce2349a29a3e3e645,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.408: INFO: Pod "webserver-deployment-dd94f59b7-lwn2p" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lwn2p webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-lwn2p 4af715b0-a107-4e30-9160-4c8e8857857b 79047 0 2021-05-11 18:18:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.157"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.157"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002014367 0xc002014368}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:18:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:10.128.2.157,StartTime:2021-05-11 18:18:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://f4a7fea99240fe70a893d20506c27469680294e4ef8cf4d0c0a1dc73c873996c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.408: INFO: Pod "webserver-deployment-dd94f59b7-nhzz9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nhzz9 webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-nhzz9 cf07702d-7017-47cc-b21f-c4e110eeeefa 79064 0 2021-05-11 18:18:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.76"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.76"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002014547 0xc002014548}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:18:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-31.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.143.31,PodIP:10.131.0.76,StartTime:2021-05-11 18:18:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://7b282c5b1530f03a8fb0501a8c5e60bedc5d0bd2824b39ace8430a20989d7592,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.409: INFO: Pod "webserver-deployment-dd94f59b7-nr5j4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nr5j4 webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-nr5j4 7a14f885-a1d6-4a86-9d53-439d532e93e0 79467 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.168"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.168"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002014737 0xc002014738}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.409: INFO: Pod "webserver-deployment-dd94f59b7-pfjhr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pfjhr webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-pfjhr 2b3971de-c437-41fa-ade8-9170d23dad95 79318 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002014907 0xc002014908}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.409: INFO: Pod "webserver-deployment-dd94f59b7-pg76j" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pg76j webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-pg76j 487a9a63-f63c-459c-8114-ace224b1117c 79030 0 2021-05-11 18:18:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.8"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.8"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002014aa7 0xc002014aa8}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:18:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.3.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:10.129.3.8,StartTime:2021-05-11 18:18:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:18:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://5bfa19767cabae0bd69fda9522e08eda8e32c3a22268af3e3bd1d10064ce2edc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.3.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.409: INFO: Pod "webserver-deployment-dd94f59b7-rnfxr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rnfxr webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-rnfxr 69246170-5cc7-4d07-9335-c83a57ca0ce1 79356 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002014c77 0xc002014c78}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.409: INFO: Pod "webserver-deployment-dd94f59b7-rthmm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rthmm webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-rthmm bc0394c1-5e12-4a7d-a983-67845488ccad 79410 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.12"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.12"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002014e17 0xc002014e18}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.409: INFO: Pod "webserver-deployment-dd94f59b7-sbnlh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sbnlh webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-sbnlh 50c02e41-b624-419e-b509-ea006447f058 79412 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.81"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.81"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002014fc7 0xc002014fc8}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-31.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.143.31,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.409: INFO: Pod "webserver-deployment-dd94f59b7-tbqw4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tbqw4 webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-tbqw4 57006f64-480e-48b1-8718-5f554d3f5d2f 79428 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.165"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.165"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002015177 0xc002015178}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.409: INFO: Pod "webserver-deployment-dd94f59b7-vpnpb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vpnpb webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-vpnpb e4072ec4-b580-444e-9fe1-f0b7dc98202c 79494 0 2021-05-11 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.78"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.78"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc002015337 0xc002015338}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-31.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.143.31,PodIP:,StartTime:2021-05-11 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.410: INFO: Pod "webserver-deployment-dd94f59b7-vsrvh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vsrvh webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-vsrvh 2eb7b7cd-bee7-404b-9be1-2a624394269c 79007 0 2021-05-11 18:18:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.74"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.74"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc0020154e7 0xc0020154e8}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-11 18:18:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-05-11 18:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-31.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.143.31,PodIP:10.131.0.74,StartTime:2021-05-11 18:18:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:18:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://86e47b2c4571adb065d2e29ca846dcb5f08ab61db56633514e1266cd8140ac22,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 11 18:19:05.410: INFO: Pod "webserver-deployment-dd94f59b7-w7z9x" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w7z9x webserver-deployment-dd94f59b7- deployment-4087 /api/v1/namespaces/deployment-4087/pods/webserver-deployment-dd94f59b7-w7z9x bb2f07c7-a645-4731-9296-adac73000735 79034 0 2021-05-11 18:18:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.9"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.3.9"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 344a54a1-b7e5-41c3-8b21-0a580b6e6357 0xc0020156b7 0xc0020156b8}] []  [{kube-controller-manager Update v1 2021-05-11 18:18:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"344a54a1-b7e5-41c3-8b21-0a580b6e6357\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:18:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.3.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t24xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t24xv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t24xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-72.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c6pkg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:18:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.72,PodIP:10.129.3.9,StartTime:2021-05-11 18:18:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://22ae681bb8fed6f498f7fa2843b2625deebcf97be4cf3eef6cebdaef06a44ced,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.3.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:19:05.410: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-4087" for this suite.


• [SLOW TEST:12.777 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":-1,"completed":66,"skipped":1161,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:19:06.518: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-8233" for this suite.


• [SLOW TEST:18.215 seconds]
[sig-api-machinery] ResourceQuota
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":-1,"completed":52,"skipped":1059,"failed":0}

S
------------------------------
[BeforeEach] [sig-node] RuntimeClass
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 11 18:19:05.812: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 11 18:19:06.223: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:19:06.589: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "runtimeclass-6698" for this suite.

•SSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":-1,"completed":75,"skipped":1254,"failed":0}
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
May 11 18:19:07.240: INFO: Waiting up to 5m0s for pod "client-containers-ad17d81b-108c-474d-aded-078adbc6cd2c" in namespace "containers-9207" to be "Succeeded or Failed"
May 11 18:19:07.306: INFO: Pod "client-containers-ad17d81b-108c-474d-aded-078adbc6cd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 66.384119ms
May 11 18:19:09.376: INFO: Pod "client-containers-ad17d81b-108c-474d-aded-078adbc6cd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136711611s
May 11 18:19:11.444: INFO: Pod "client-containers-ad17d81b-108c-474d-aded-078adbc6cd2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.204361439s
STEP: Saw pod success
May 11 18:19:11.444: INFO: Pod "client-containers-ad17d81b-108c-474d-aded-078adbc6cd2c" satisfied condition "Succeeded or Failed"
May 11 18:19:11.511: INFO: Trying to get logs from node ip-10-0-143-31.us-west-1.compute.internal pod client-containers-ad17d81b-108c-474d-aded-078adbc6cd2c container agnhost-container: <nil>
STEP: delete the pod
May 11 18:19:11.659: INFO: Waiting for pod client-containers-ad17d81b-108c-474d-aded-078adbc6cd2c to disappear
May 11 18:19:11.730: INFO: Pod client-containers-ad17d81b-108c-474d-aded-078adbc6cd2c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:19:11.730: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-9207" for this suite.


• [SLOW TEST:5.182 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":-1,"completed":76,"skipped":1254,"failed":0}

S
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 11 18:19:11.014: INFO: Successfully updated pod "annotationupdateaf3b3171-078d-428d-95da-c6ae9da1c7b3"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:19:13.164: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6426" for this suite.


• [SLOW TEST:7.739 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":-1,"completed":67,"skipped":1188,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
May 11 18:19:13.482: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [sig-api-machinery] Discovery
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:19:12.881: INFO: Checking APIGroup: apiregistration.k8s.io
May 11 18:19:12.946: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 11 18:19:12.946: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 11 18:19:12.946: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 11 18:19:12.946: INFO: Checking APIGroup: apps
May 11 18:19:13.011: INFO: PreferredVersion.GroupVersion: apps/v1
May 11 18:19:13.011: INFO: Versions found [{apps/v1 v1}]
May 11 18:19:13.011: INFO: apps/v1 matches apps/v1
May 11 18:19:13.011: INFO: Checking APIGroup: events.k8s.io
May 11 18:19:13.076: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 11 18:19:13.076: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.076: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 11 18:19:13.076: INFO: Checking APIGroup: authentication.k8s.io
May 11 18:19:13.141: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 11 18:19:13.141: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.141: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 11 18:19:13.141: INFO: Checking APIGroup: authorization.k8s.io
May 11 18:19:13.206: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 11 18:19:13.206: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.206: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 11 18:19:13.206: INFO: Checking APIGroup: autoscaling
May 11 18:19:13.271: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 11 18:19:13.271: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 11 18:19:13.271: INFO: autoscaling/v1 matches autoscaling/v1
May 11 18:19:13.271: INFO: Checking APIGroup: batch
May 11 18:19:13.336: INFO: PreferredVersion.GroupVersion: batch/v1
May 11 18:19:13.336: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 11 18:19:13.336: INFO: batch/v1 matches batch/v1
May 11 18:19:13.336: INFO: Checking APIGroup: certificates.k8s.io
May 11 18:19:13.401: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 11 18:19:13.401: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.401: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 11 18:19:13.401: INFO: Checking APIGroup: networking.k8s.io
May 11 18:19:13.466: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 11 18:19:13.466: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.466: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 11 18:19:13.466: INFO: Checking APIGroup: extensions
May 11 18:19:13.532: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 11 18:19:13.532: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 11 18:19:13.532: INFO: extensions/v1beta1 matches extensions/v1beta1
May 11 18:19:13.532: INFO: Checking APIGroup: policy
May 11 18:19:13.597: INFO: PreferredVersion.GroupVersion: policy/v1beta1
May 11 18:19:13.597: INFO: Versions found [{policy/v1beta1 v1beta1}]
May 11 18:19:13.597: INFO: policy/v1beta1 matches policy/v1beta1
May 11 18:19:13.597: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 11 18:19:13.662: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 11 18:19:13.662: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.662: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 11 18:19:13.662: INFO: Checking APIGroup: storage.k8s.io
May 11 18:19:13.727: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 11 18:19:13.727: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.727: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 11 18:19:13.727: INFO: Checking APIGroup: admissionregistration.k8s.io
May 11 18:19:13.792: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 11 18:19:13.792: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.792: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 11 18:19:13.792: INFO: Checking APIGroup: apiextensions.k8s.io
May 11 18:19:13.858: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 11 18:19:13.858: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.858: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 11 18:19:13.858: INFO: Checking APIGroup: scheduling.k8s.io
May 11 18:19:13.923: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 11 18:19:13.923: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.923: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 11 18:19:13.923: INFO: Checking APIGroup: coordination.k8s.io
May 11 18:19:13.988: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 11 18:19:13.988: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 11 18:19:13.988: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 11 18:19:13.988: INFO: Checking APIGroup: node.k8s.io
May 11 18:19:14.053: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 11 18:19:14.053: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May 11 18:19:14.053: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 11 18:19:14.053: INFO: Checking APIGroup: discovery.k8s.io
May 11 18:19:14.118: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
May 11 18:19:14.118: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
May 11 18:19:14.118: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
May 11 18:19:14.118: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 11 18:19:14.183: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
May 11 18:19:14.183: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1} {flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
May 11 18:19:14.183: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
May 11 18:19:14.183: INFO: Checking APIGroup: apps.openshift.io
May 11 18:19:14.248: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
May 11 18:19:14.248: INFO: Versions found [{apps.openshift.io/v1 v1}]
May 11 18:19:14.248: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
May 11 18:19:14.248: INFO: Checking APIGroup: authorization.openshift.io
May 11 18:19:14.313: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
May 11 18:19:14.313: INFO: Versions found [{authorization.openshift.io/v1 v1}]
May 11 18:19:14.313: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
May 11 18:19:14.313: INFO: Checking APIGroup: build.openshift.io
May 11 18:19:14.379: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
May 11 18:19:14.379: INFO: Versions found [{build.openshift.io/v1 v1}]
May 11 18:19:14.379: INFO: build.openshift.io/v1 matches build.openshift.io/v1
May 11 18:19:14.379: INFO: Checking APIGroup: image.openshift.io
May 11 18:19:14.444: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
May 11 18:19:14.444: INFO: Versions found [{image.openshift.io/v1 v1}]
May 11 18:19:14.444: INFO: image.openshift.io/v1 matches image.openshift.io/v1
May 11 18:19:14.444: INFO: Checking APIGroup: oauth.openshift.io
May 11 18:19:14.509: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
May 11 18:19:14.509: INFO: Versions found [{oauth.openshift.io/v1 v1}]
May 11 18:19:14.509: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
May 11 18:19:14.509: INFO: Checking APIGroup: project.openshift.io
May 11 18:19:14.574: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
May 11 18:19:14.574: INFO: Versions found [{project.openshift.io/v1 v1}]
May 11 18:19:14.574: INFO: project.openshift.io/v1 matches project.openshift.io/v1
May 11 18:19:14.574: INFO: Checking APIGroup: quota.openshift.io
May 11 18:19:14.639: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
May 11 18:19:14.639: INFO: Versions found [{quota.openshift.io/v1 v1}]
May 11 18:19:14.639: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
May 11 18:19:14.639: INFO: Checking APIGroup: route.openshift.io
May 11 18:19:14.704: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
May 11 18:19:14.704: INFO: Versions found [{route.openshift.io/v1 v1}]
May 11 18:19:14.704: INFO: route.openshift.io/v1 matches route.openshift.io/v1
May 11 18:19:14.704: INFO: Checking APIGroup: security.openshift.io
May 11 18:19:14.770: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
May 11 18:19:14.770: INFO: Versions found [{security.openshift.io/v1 v1}]
May 11 18:19:14.770: INFO: security.openshift.io/v1 matches security.openshift.io/v1
May 11 18:19:14.770: INFO: Checking APIGroup: template.openshift.io
May 11 18:19:14.835: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
May 11 18:19:14.835: INFO: Versions found [{template.openshift.io/v1 v1}]
May 11 18:19:14.835: INFO: template.openshift.io/v1 matches template.openshift.io/v1
May 11 18:19:14.835: INFO: Checking APIGroup: user.openshift.io
May 11 18:19:14.900: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
May 11 18:19:14.900: INFO: Versions found [{user.openshift.io/v1 v1}]
May 11 18:19:14.900: INFO: user.openshift.io/v1 matches user.openshift.io/v1
May 11 18:19:14.900: INFO: Checking APIGroup: packages.operators.coreos.com
May 11 18:19:14.965: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
May 11 18:19:14.965: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
May 11 18:19:14.965: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
May 11 18:19:14.965: INFO: Checking APIGroup: config.openshift.io
May 11 18:19:15.030: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
May 11 18:19:15.031: INFO: Versions found [{config.openshift.io/v1 v1}]
May 11 18:19:15.031: INFO: config.openshift.io/v1 matches config.openshift.io/v1
May 11 18:19:15.031: INFO: Checking APIGroup: operator.openshift.io
May 11 18:19:15.095: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
May 11 18:19:15.095: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
May 11 18:19:15.095: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
May 11 18:19:15.095: INFO: Checking APIGroup: autoscaling.openshift.io
May 11 18:19:15.160: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
May 11 18:19:15.160: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
May 11 18:19:15.160: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
May 11 18:19:15.160: INFO: Checking APIGroup: cloudcredential.openshift.io
May 11 18:19:15.225: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
May 11 18:19:15.225: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
May 11 18:19:15.225: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
May 11 18:19:15.225: INFO: Checking APIGroup: console.openshift.io
May 11 18:19:15.290: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
May 11 18:19:15.290: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
May 11 18:19:15.290: INFO: console.openshift.io/v1 matches console.openshift.io/v1
May 11 18:19:15.290: INFO: Checking APIGroup: imageregistry.operator.openshift.io
May 11 18:19:15.355: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
May 11 18:19:15.355: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
May 11 18:19:15.355: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
May 11 18:19:15.355: INFO: Checking APIGroup: ingress.operator.openshift.io
May 11 18:19:15.420: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
May 11 18:19:15.421: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
May 11 18:19:15.421: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
May 11 18:19:15.421: INFO: Checking APIGroup: k8s.cni.cncf.io
May 11 18:19:15.485: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
May 11 18:19:15.485: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
May 11 18:19:15.485: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
May 11 18:19:15.485: INFO: Checking APIGroup: machineconfiguration.openshift.io
May 11 18:19:15.550: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
May 11 18:19:15.550: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
May 11 18:19:15.550: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
May 11 18:19:15.550: INFO: Checking APIGroup: monitoring.coreos.com
May 11 18:19:15.616: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
May 11 18:19:15.616: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
May 11 18:19:15.616: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
May 11 18:19:15.616: INFO: Checking APIGroup: network.openshift.io
May 11 18:19:15.681: INFO: PreferredVersion.GroupVersion: network.openshift.io/v1
May 11 18:19:15.681: INFO: Versions found [{network.openshift.io/v1 v1}]
May 11 18:19:15.681: INFO: network.openshift.io/v1 matches network.openshift.io/v1
May 11 18:19:15.681: INFO: Checking APIGroup: network.operator.openshift.io
May 11 18:19:15.746: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
May 11 18:19:15.746: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
May 11 18:19:15.746: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
May 11 18:19:15.746: INFO: Checking APIGroup: operators.coreos.com
May 11 18:19:15.811: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
May 11 18:19:15.811: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
May 11 18:19:15.811: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
May 11 18:19:15.811: INFO: Checking APIGroup: samples.operator.openshift.io
May 11 18:19:15.876: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
May 11 18:19:15.876: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
May 11 18:19:15.876: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
May 11 18:19:15.876: INFO: Checking APIGroup: security.internal.openshift.io
May 11 18:19:15.941: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
May 11 18:19:15.941: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
May 11 18:19:15.941: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
May 11 18:19:15.941: INFO: Checking APIGroup: snapshot.storage.k8s.io
May 11 18:19:16.006: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
May 11 18:19:16.006: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
May 11 18:19:16.006: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
May 11 18:19:16.006: INFO: Checking APIGroup: tuned.openshift.io
May 11 18:19:16.071: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
May 11 18:19:16.071: INFO: Versions found [{tuned.openshift.io/v1 v1}]
May 11 18:19:16.071: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
May 11 18:19:16.071: INFO: Checking APIGroup: controlplane.operator.openshift.io
May 11 18:19:16.136: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
May 11 18:19:16.137: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
May 11 18:19:16.137: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
May 11 18:19:16.137: INFO: Checking APIGroup: metal3.io
May 11 18:19:16.202: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
May 11 18:19:16.202: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
May 11 18:19:16.202: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
May 11 18:19:16.202: INFO: Checking APIGroup: migration.k8s.io
May 11 18:19:16.266: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
May 11 18:19:16.267: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
May 11 18:19:16.267: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
May 11 18:19:16.267: INFO: Checking APIGroup: whereabouts.cni.cncf.io
May 11 18:19:16.335: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
May 11 18:19:16.336: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
May 11 18:19:16.336: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
May 11 18:19:16.336: INFO: Checking APIGroup: helm.openshift.io
May 11 18:19:16.400: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
May 11 18:19:16.400: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
May 11 18:19:16.400: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
May 11 18:19:16.400: INFO: Checking APIGroup: machine.openshift.io
May 11 18:19:16.466: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1beta1
May 11 18:19:16.466: INFO: Versions found [{machine.openshift.io/v1beta1 v1beta1}]
May 11 18:19:16.466: INFO: machine.openshift.io/v1beta1 matches machine.openshift.io/v1beta1
May 11 18:19:16.466: INFO: Checking APIGroup: metrics.k8s.io
May 11 18:19:16.531: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May 11 18:19:16.531: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May 11 18:19:16.531: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:19:16.531: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "discovery-3255" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":-1,"completed":77,"skipped":1255,"failed":0}
May 11 18:19:16.737: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 11 18:19:07.477: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 11 18:19:11.614: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 11 18:19:13.679: INFO: Creating deployment "test-rollover-deployment"
May 11 18:19:13.812: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 11 18:19:13.881: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 11 18:19:14.014: INFO: Ensure that both replica sets have 1 created replica
May 11 18:19:14.148: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 11 18:19:14.282: INFO: Updating deployment test-rollover-deployment
May 11 18:19:14.282: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 11 18:19:14.352: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 11 18:19:14.482: INFO: Make sure deployment "test-rollover-deployment" is complete
May 11 18:19:14.613: INFO: all replica sets need to contain the pod-template-hash label
May 11 18:19:14.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353954, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:19:16.747: INFO: all replica sets need to contain the pod-template-hash label
May 11 18:19:16.747: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353954, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:19:18.749: INFO: all replica sets need to contain the pod-template-hash label
May 11 18:19:18.749: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353957, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:19:20.743: INFO: all replica sets need to contain the pod-template-hash label
May 11 18:19:20.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353957, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:19:22.744: INFO: all replica sets need to contain the pod-template-hash label
May 11 18:19:22.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353957, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:19:24.748: INFO: all replica sets need to contain the pod-template-hash label
May 11 18:19:24.748: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353957, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:19:26.747: INFO: all replica sets need to contain the pod-template-hash label
May 11 18:19:26.747: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353957, loc:(*time.Location)(0x7bb6980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756353953, loc:(*time.Location)(0x7bb6980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 18:19:28.743: INFO: 
May 11 18:19:28.743: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 11 18:19:28.946: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2582 /apis/apps/v1/namespaces/deployment-2582/deployments/test-rollover-deployment 03d24c61-1ebb-47aa-acbf-180f0dba0294 80376 2 2021-05-11 18:19:13 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-11 18:19:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-11 18:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009fd8a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-11 18:19:13 +0000 UTC,LastTransitionTime:2021-05-11 18:19:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-05-11 18:19:27 +0000 UTC,LastTransitionTime:2021-05-11 18:19:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 11 18:19:29.011: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-2582 /apis/apps/v1/namespaces/deployment-2582/replicasets/test-rollover-deployment-668db69979 c47b7a44-35ba-4d15-9550-918cfbe519c4 80363 2 2021-05-11 18:19:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 03d24c61-1ebb-47aa-acbf-180f0dba0294 0xc001b9efd7 0xc001b9efd8}] []  [{kube-controller-manager Update apps/v1 2021-05-11 18:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03d24c61-1ebb-47aa-acbf-180f0dba0294\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b9f088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 11 18:19:29.011: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 11 18:19:29.011: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2582 /apis/apps/v1/namespaces/deployment-2582/replicasets/test-rollover-controller 6088a05e-0215-43c1-abf3-a571d877b75b 80375 2 2021-05-11 18:19:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 03d24c61-1ebb-47aa-acbf-180f0dba0294 0xc001b9ee97 0xc001b9ee98}] []  [{e2e.test Update apps/v1 2021-05-11 18:19:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-11 18:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03d24c61-1ebb-47aa-acbf-180f0dba0294\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001b9ef68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 11 18:19:29.011: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-2582 /apis/apps/v1/namespaces/deployment-2582/replicasets/test-rollover-deployment-78bc8b888c b712af27-8b7b-416e-8545-5eeb01b5e070 80198 2 2021-05-11 18:19:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 03d24c61-1ebb-47aa-acbf-180f0dba0294 0xc001b9f297 0xc001b9f298}] []  [{kube-controller-manager Update apps/v1 2021-05-11 18:19:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03d24c61-1ebb-47aa-acbf-180f0dba0294\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b9f328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 11 18:19:29.077: INFO: Pod "test-rollover-deployment-668db69979-b6cm8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-b6cm8 test-rollover-deployment-668db69979- deployment-2582 /api/v1/namespaces/deployment-2582/pods/test-rollover-deployment-668db69979-b6cm8 76e1bff1-9012-482e-85c7-65df0b75ef51 80232 0 2021-05-11 18:19:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.172"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.172"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 c47b7a44-35ba-4d15-9550-918cfbe519c4 0xc009f92ef7 0xc009f92ef8}] []  [{kube-controller-manager Update v1 2021-05-11 18:19:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c47b7a44-35ba-4d15-9550-918cfbe519c4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-05-11 18:19:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-05-11 18:19:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-j5gsz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-j5gsz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-j5gsz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-198-156.us-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c37,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6w9t7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-11 18:19:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.198.156,PodIP:10.128.2.172,StartTime:2021-05-11 18:19:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-11 18:19:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://09f583903be21ffae9ceca93be693fa3f052b57ff5d8fc55e46b5e137ce77ed9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:19:29.077: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-2582" for this suite.


• [SLOW TEST:22.322 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-4124fe93-fa79-482d-a968-502b6ac5ba10 in namespace container-probe-7421
May 11 18:18:54.361: INFO: Started pod liveness-4124fe93-fa79-482d-a968-502b6ac5ba10 in namespace container-probe-7421
STEP: checking the pod's current state and verifying that restartCount is present
May 11 18:18:54.431: INFO: Initial restart count of pod liveness-4124fe93-fa79-482d-a968-502b6ac5ba10 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 11 18:22:56.221: INFO: Waiting up to 7m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-7421" for this suite.


• [SLOW TEST:246.654 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":-1,"completed":94,"skipped":1819,"failed":0}
May 11 18:22:56.482: INFO: Running AfterSuite actions on all nodes


{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":-1,"completed":53,"skipped":1077,"failed":0}
May 11 18:19:29.339: INFO: Running AfterSuite actions on all nodes
May 11 18:22:56.550: INFO: Running AfterSuite actions on node 1
May 11 18:22:56.550: INFO: Dumping logs locally to: /logs/artifacts
May 11 18:22:56.551: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory


Ran 291 of 5670 Specs in 1573.521 seconds
SUCCESS! -- 291 Passed | 0 Failed | 0 Pending | 5379 Skipped


Ginkgo ran 1 suite in 26m13.657954491s
Test Suite Passed
