I0521 05:36:18.629996      20 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-149702678
I0521 05:36:18.630023      20 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0521 05:36:18.630119      20 e2e.go:129] Starting e2e run "470995b3-d148-4053-a105-0c0200a6d845" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1621575376 - Will randomize all specs
Will run 311 of 5667 specs

May 21 05:36:18.643: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 05:36:18.645: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0521 05:36:18.648311      20 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
May 21 05:36:18.680: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 21 05:36:18.702: INFO: 4 / 4 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 21 05:36:18.702: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
May 21 05:36:18.702: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 21 05:36:18.707: INFO: e2e test version: v1.20.6
May 21 05:36:18.708: INFO: kube-apiserver version: v1.20.6+vmware.1
May 21 05:36:18.708: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 05:36:18.712: INFO: Cluster IP family: ipv4
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:36:18.712: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
May 21 05:36:18.745: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
May 21 05:36:18.763: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
May 21 05:36:18.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-7953 create -f -'
May 21 05:36:19.948: INFO: stderr: ""
May 21 05:36:19.948: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 21 05:36:19.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-7953 diff -f -'
May 21 05:36:20.274: INFO: rc: 1
May 21 05:36:20.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-7953 delete -f -'
May 21 05:36:20.347: INFO: stderr: ""
May 21 05:36:20.347: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:36:20.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7953" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":1,"skipped":12,"failed":0}

------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:36:20.360: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
May 21 05:36:20.395: INFO: created test-event-1
May 21 05:36:20.397: INFO: created test-event-2
May 21 05:36:20.399: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 21 05:36:20.401: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 21 05:36:20.409: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:36:20.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8510" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":2,"skipped":12,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:36:20.416: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-w8qq
STEP: Creating a pod to test atomic-volume-subpath
May 21 05:36:20.457: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-w8qq" in namespace "subpath-5890" to be "Succeeded or Failed"
May 21 05:36:20.459: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199474ms
May 21 05:36:22.465: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007742598s
May 21 05:36:24.470: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01360177s
May 21 05:36:26.475: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018114727s
May 21 05:36:28.479: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Running", Reason="", readiness=true. Elapsed: 8.02222787s
May 21 05:36:30.486: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Running", Reason="", readiness=true. Elapsed: 10.028874376s
May 21 05:36:32.490: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Running", Reason="", readiness=true. Elapsed: 12.033322267s
May 21 05:36:34.495: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Running", Reason="", readiness=true. Elapsed: 14.038166162s
May 21 05:36:36.500: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Running", Reason="", readiness=true. Elapsed: 16.043090827s
May 21 05:36:38.506: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Running", Reason="", readiness=true. Elapsed: 18.048875531s
May 21 05:36:40.509: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Running", Reason="", readiness=true. Elapsed: 20.052641685s
May 21 05:36:42.515: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Running", Reason="", readiness=true. Elapsed: 22.058154996s
May 21 05:36:44.520: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Running", Reason="", readiness=true. Elapsed: 24.063365513s
May 21 05:36:46.525: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Running", Reason="", readiness=true. Elapsed: 26.068264532s
May 21 05:36:48.529: INFO: Pod "pod-subpath-test-secret-w8qq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.071931984s
STEP: Saw pod success
May 21 05:36:48.529: INFO: Pod "pod-subpath-test-secret-w8qq" satisfied condition "Succeeded or Failed"
May 21 05:36:48.531: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-subpath-test-secret-w8qq container test-container-subpath-secret-w8qq: <nil>
STEP: delete the pod
May 21 05:36:48.568: INFO: Waiting for pod pod-subpath-test-secret-w8qq to disappear
May 21 05:36:48.577: INFO: Pod pod-subpath-test-secret-w8qq no longer exists
STEP: Deleting pod pod-subpath-test-secret-w8qq
May 21 05:36:48.577: INFO: Deleting pod "pod-subpath-test-secret-w8qq" in namespace "subpath-5890"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:36:48.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5890" for this suite.

• [SLOW TEST:28.168 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":3,"skipped":34,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:36:48.585: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-400
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 05:36:48.614: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 05:36:48.667: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 05:36:50.681: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 05:36:52.682: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 05:36:54.681: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 05:36:56.680: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 05:36:58.681: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 05:37:00.681: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 05:37:02.681: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 05:37:04.682: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 05:37:04.685: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 05:37:06.690: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 05:37:08.691: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 05:37:10.691: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 05:37:12.691: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 05:37:14.690: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 21 05:37:14.694: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 21 05:37:16.723: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 21 05:37:16.723: INFO: Going to poll 10.200.88.12 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 21 05:37:16.725: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.88.12 8081 | grep -v '^\s*$'] Namespace:pod-network-test-400 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:37:16.725: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 05:37:17.819: INFO: Found all 1 expected endpoints: [netserver-0]
May 21 05:37:17.819: INFO: Going to poll 10.200.66.10 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 21 05:37:17.822: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.66.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-400 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:37:17.822: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 05:37:18.927: INFO: Found all 1 expected endpoints: [netserver-1]
May 21 05:37:18.928: INFO: Going to poll 10.200.34.7 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 21 05:37:18.930: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.34.7 8081 | grep -v '^\s*$'] Namespace:pod-network-test-400 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:37:18.930: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 05:37:20.031: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:37:20.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-400" for this suite.

• [SLOW TEST:31.455 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":4,"skipped":38,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:37:20.041: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 21 05:37:20.084: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3630  0edbd43a-6cd4-46e8-9184-5313a9ddd0d7 7561 0 2021-05-21 05:37:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 05:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 05:37:20.085: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3630  0edbd43a-6cd4-46e8-9184-5313a9ddd0d7 7562 0 2021-05-21 05:37:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 05:37:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 05:37:20.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3630  0edbd43a-6cd4-46e8-9184-5313a9ddd0d7 7563 0 2021-05-21 05:37:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 05:37:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 21 05:37:30.114: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3630  0edbd43a-6cd4-46e8-9184-5313a9ddd0d7 7619 0 2021-05-21 05:37:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 05:37:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 05:37:30.114: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3630  0edbd43a-6cd4-46e8-9184-5313a9ddd0d7 7620 0 2021-05-21 05:37:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 05:37:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 05:37:30.114: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3630  0edbd43a-6cd4-46e8-9184-5313a9ddd0d7 7621 0 2021-05-21 05:37:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 05:37:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:37:30.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3630" for this suite.

• [SLOW TEST:10.081 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":5,"skipped":58,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:37:30.123: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 05:37:30.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc343e05-3d2a-438e-a707-a12aa1ad94fb" in namespace "projected-4441" to be "Succeeded or Failed"
May 21 05:37:30.161: INFO: Pod "downwardapi-volume-fc343e05-3d2a-438e-a707-a12aa1ad94fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926847ms
May 21 05:37:32.164: INFO: Pod "downwardapi-volume-fc343e05-3d2a-438e-a707-a12aa1ad94fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0052315s
STEP: Saw pod success
May 21 05:37:32.164: INFO: Pod "downwardapi-volume-fc343e05-3d2a-438e-a707-a12aa1ad94fb" satisfied condition "Succeeded or Failed"
May 21 05:37:32.166: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-fc343e05-3d2a-438e-a707-a12aa1ad94fb container client-container: <nil>
STEP: delete the pod
May 21 05:37:32.180: INFO: Waiting for pod downwardapi-volume-fc343e05-3d2a-438e-a707-a12aa1ad94fb to disappear
May 21 05:37:32.186: INFO: Pod downwardapi-volume-fc343e05-3d2a-438e-a707-a12aa1ad94fb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:37:32.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4441" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":73,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:37:32.192: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 21 05:37:32.219: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 05:37:32.231: INFO: Waiting for terminating namespaces to be deleted...
May 21 05:37:32.233: INFO: 
Logging pods the apiserver thinks is on node 16c19df1-13dd-4e26-99a1-a192204dc796 before test
May 21 05:37:32.241: INFO: coredns-6d5447c5f4-sdg2p from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.241: INFO: 	Container coredns ready: true, restart count 0
May 21 05:37:32.241: INFO: metrics-server-7d476fdfbd-f88wj from kube-system started at 2021-05-21 04:30:11 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.241: INFO: 	Container metrics-server ready: true, restart count 0
May 21 05:37:32.241: INFO: cert-generator-cbc5efd76cef99ea15cff1408e85404dd343c0a5-zdnb8 from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.241: INFO: 	Container cert-generator ready: false, restart count 0
May 21 05:37:32.241: INFO: fluent-bit-rkcnw from pks-system started at 2021-05-21 04:30:22 +0000 UTC (2 container statuses recorded)
May 21 05:37:32.241: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 05:37:32.241: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 05:37:32.241: INFO: node-exporter-gbfjt from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.241: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 05:37:32.241: INFO: sink-controller-5bf464ff96-8nj6l from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.241: INFO: 	Container sink-controller ready: true, restart count 0
May 21 05:37:32.241: INFO: telegraf-hwv6v from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.241: INFO: 	Container telegraf ready: true, restart count 0
May 21 05:37:32.241: INFO: wavefront-collector-5795f87754-rfj4n from pks-system started at 2021-05-21 04:32:41 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.241: INFO: 	Container wavefront-collector ready: true, restart count 0
May 21 05:37:32.241: INFO: sonobuoy from sonobuoy started at 2021-05-21 05:36:00 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.241: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 05:37:32.241: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-b9nwr from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 05:37:32.241: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 05:37:32.241: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 05:37:32.241: INFO: 
Logging pods the apiserver thinks is on node 53f495cf-0382-4128-b4b3-0979b69a209b before test
May 21 05:37:32.248: INFO: coredns-6d5447c5f4-fv5hd from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.248: INFO: 	Container coredns ready: true, restart count 0
May 21 05:37:32.248: INFO: fluent-bit-kr585 from pks-system started at 2021-05-21 04:30:23 +0000 UTC (2 container statuses recorded)
May 21 05:37:32.248: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 05:37:32.248: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 05:37:32.248: INFO: metric-controller-5754b4489d-c246d from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.248: INFO: 	Container metric-controller ready: true, restart count 0
May 21 05:37:32.248: INFO: node-exporter-fmcv8 from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.248: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 05:37:32.248: INFO: observability-manager-7f5c97895-xsj2f from pks-system started at 2021-05-21 04:30:14 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.248: INFO: 	Container observability-manager ready: true, restart count 0
May 21 05:37:32.248: INFO: telegraf-hlmgc from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.248: INFO: 	Container telegraf ready: true, restart count 0
May 21 05:37:32.248: INFO: telemetry-agent-8f56d9865-l6ntw from pks-system started at 2021-05-21 04:35:59 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.248: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
May 21 05:37:32.248: INFO: validator-54f6bbd67b-snnrv from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.248: INFO: 	Container validator ready: true, restart count 0
May 21 05:37:32.248: INFO: sonobuoy-e2e-job-a4c84655dab54066 from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 05:37:32.248: INFO: 	Container e2e ready: true, restart count 0
May 21 05:37:32.248: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 05:37:32.248: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-g25r8 from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 05:37:32.248: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 05:37:32.248: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 05:37:32.248: INFO: 
Logging pods the apiserver thinks is on node 973ef262-7e5e-4fd5-8bf3-6139f838e19b before test
May 21 05:37:32.256: INFO: coredns-6d5447c5f4-7dcmd from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.256: INFO: 	Container coredns ready: true, restart count 0
May 21 05:37:32.256: INFO: event-controller-55dd5b5777-4mn7d from pks-system started at 2021-05-21 04:30:17 +0000 UTC (2 container statuses recorded)
May 21 05:37:32.256: INFO: 	Container event-controller ready: true, restart count 0
May 21 05:37:32.256: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 05:37:32.256: INFO: fluent-bit-jjrdh from pks-system started at 2021-05-21 04:30:31 +0000 UTC (2 container statuses recorded)
May 21 05:37:32.256: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 05:37:32.256: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 05:37:32.257: INFO: node-exporter-6k9jm from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.257: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 05:37:32.257: INFO: telegraf-jqzbt from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.257: INFO: 	Container telegraf ready: true, restart count 0
May 21 05:37:32.257: INFO: wavefront-proxy-6446ff4bd7-lw9wt from pks-system started at 2021-05-21 04:32:41 +0000 UTC (1 container statuses recorded)
May 21 05:37:32.257: INFO: 	Container wavefront-proxy ready: true, restart count 0
May 21 05:37:32.257: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-ppgpw from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 05:37:32.257: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 05:37:32.257: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cedaee3b-0aa6-4511-8217-a4e91b13f2af 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-cedaee3b-0aa6-4511-8217-a4e91b13f2af off the node 16c19df1-13dd-4e26-99a1-a192204dc796
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cedaee3b-0aa6-4511-8217-a4e91b13f2af
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:37:38.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7029" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.148 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":7,"skipped":84,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:37:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-ac92d00b-f68a-4113-aec6-7a50ef3fd8a3
STEP: Creating a pod to test consume secrets
May 21 05:37:38.387: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f35489f-f97e-4a56-adaf-16e5318cfbb4" in namespace "projected-3545" to be "Succeeded or Failed"
May 21 05:37:38.391: INFO: Pod "pod-projected-secrets-5f35489f-f97e-4a56-adaf-16e5318cfbb4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.752776ms
May 21 05:37:40.394: INFO: Pod "pod-projected-secrets-5f35489f-f97e-4a56-adaf-16e5318cfbb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007165143s
STEP: Saw pod success
May 21 05:37:40.394: INFO: Pod "pod-projected-secrets-5f35489f-f97e-4a56-adaf-16e5318cfbb4" satisfied condition "Succeeded or Failed"
May 21 05:37:40.396: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-secrets-5f35489f-f97e-4a56-adaf-16e5318cfbb4 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 05:37:40.409: INFO: Waiting for pod pod-projected-secrets-5f35489f-f97e-4a56-adaf-16e5318cfbb4 to disappear
May 21 05:37:40.415: INFO: Pod pod-projected-secrets-5f35489f-f97e-4a56-adaf-16e5318cfbb4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:37:40.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3545" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":85,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:37:40.423: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 21 05:37:40.452: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 05:37:43.389: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:37:54.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7362" for this suite.

• [SLOW TEST:14.553 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":9,"skipped":109,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:37:54.978: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:37:55.011: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b0302fea-8fbb-4070-8e34-15e63fec420a" in namespace "security-context-test-2466" to be "Succeeded or Failed"
May 21 05:37:55.015: INFO: Pod "busybox-privileged-false-b0302fea-8fbb-4070-8e34-15e63fec420a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316778ms
May 21 05:37:57.018: INFO: Pod "busybox-privileged-false-b0302fea-8fbb-4070-8e34-15e63fec420a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006062393s
May 21 05:37:59.021: INFO: Pod "busybox-privileged-false-b0302fea-8fbb-4070-8e34-15e63fec420a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008976395s
May 21 05:37:59.021: INFO: Pod "busybox-privileged-false-b0302fea-8fbb-4070-8e34-15e63fec420a" satisfied condition "Succeeded or Failed"
May 21 05:37:59.026: INFO: Got logs for pod "busybox-privileged-false-b0302fea-8fbb-4070-8e34-15e63fec420a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:37:59.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2466" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":10,"skipped":116,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:37:59.033: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:37:59.070: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 21 05:38:04.073: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 05:38:04.073: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 21 05:38:06.077: INFO: Creating deployment "test-rollover-deployment"
May 21 05:38:06.093: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 21 05:38:08.099: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 21 05:38:08.102: INFO: Ensure that both replica sets have 1 created replica
May 21 05:38:08.107: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 21 05:38:08.112: INFO: Updating deployment test-rollover-deployment
May 21 05:38:08.113: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 21 05:38:10.117: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 21 05:38:10.121: INFO: Make sure deployment "test-rollover-deployment" is complete
May 21 05:38:10.124: INFO: all replica sets need to contain the pod-template-hash label
May 21 05:38:10.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172289, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 05:38:12.130: INFO: all replica sets need to contain the pod-template-hash label
May 21 05:38:12.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172289, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 05:38:14.129: INFO: all replica sets need to contain the pod-template-hash label
May 21 05:38:14.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172289, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 05:38:16.130: INFO: all replica sets need to contain the pod-template-hash label
May 21 05:38:16.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172289, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 05:38:18.129: INFO: all replica sets need to contain the pod-template-hash label
May 21 05:38:18.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172289, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172286, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 05:38:20.129: INFO: 
May 21 05:38:20.130: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 05:38:20.135: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1847  2d375092-6b9c-4505-8dc3-7971b2492d72 7941 2 2021-05-21 05:38:06 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 05:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 05:38:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0069fa278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-21 05:38:06 +0000 UTC,LastTransitionTime:2021-05-21 05:38:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-05-21 05:38:19 +0000 UTC,LastTransitionTime:2021-05-21 05:38:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 21 05:38:20.137: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-1847  cbbc6361-a195-4a58-90e5-cdf238eac042 7930 2 2021-05-21 05:38:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 2d375092-6b9c-4505-8dc3-7971b2492d72 0xc0069fa6f7 0xc0069fa6f8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 05:38:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d375092-6b9c-4505-8dc3-7971b2492d72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0069fa798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 05:38:20.137: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 21 05:38:20.138: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1847  5f1d8cd9-e306-4aa3-b0e6-1bf8c4283cd0 7939 2 2021-05-21 05:37:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 2d375092-6b9c-4505-8dc3-7971b2492d72 0xc0069fa5af 0xc0069fa5c0}] []  [{e2e.test Update apps/v1 2021-05-21 05:37:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 05:38:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d375092-6b9c-4505-8dc3-7971b2492d72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0069fa698 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 05:38:20.138: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-1847  9c0c198d-a439-42cc-a57d-50ee39370d84 7904 2 2021-05-21 05:38:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 2d375092-6b9c-4505-8dc3-7971b2492d72 0xc0069fa7f7 0xc0069fa7f8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 05:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d375092-6b9c-4505-8dc3-7971b2492d72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0069fa888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 05:38:20.140: INFO: Pod "test-rollover-deployment-668db69979-dx46m" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-dx46m test-rollover-deployment-668db69979- deployment-1847  a63cf21d-43d4-4927-8254-5c122dd05c84 7913 0 2021-05-21 05:38:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 cbbc6361-a195-4a58-90e5-cdf238eac042 0xc0069fae57 0xc0069fae58}] []  [{kube-controller-manager Update v1 2021-05-21 05:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbbc6361-a195-4a58-90e5-cdf238eac042\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 05:38:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.88.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ztgcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ztgcd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ztgcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 05:38:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 05:38:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 05:38:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 05:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.88.21,StartTime:2021-05-21 05:38:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 05:38:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://620eb321744ffac30c54415c423240473adf730e31fd851cdacd2f7048c5256e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.88.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:38:20.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1847" for this suite.

• [SLOW TEST:21.113 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":11,"skipped":180,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:38:20.147: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 21 05:38:22.706: INFO: Successfully updated pod "annotationupdateb5174d70-a3d0-42af-8c56-f22cf16818c2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:38:26.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2558" for this suite.

• [SLOW TEST:6.591 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":12,"skipped":181,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:38:26.739: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:38:32.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5642" for this suite.

• [SLOW TEST:5.455 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":13,"skipped":188,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:38:32.195: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-gw4p
STEP: Creating a pod to test atomic-volume-subpath
May 21 05:38:32.247: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-gw4p" in namespace "subpath-4689" to be "Succeeded or Failed"
May 21 05:38:32.251: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277832ms
May 21 05:38:34.254: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Running", Reason="", readiness=true. Elapsed: 2.007364735s
May 21 05:38:36.258: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Running", Reason="", readiness=true. Elapsed: 4.010872164s
May 21 05:38:38.261: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Running", Reason="", readiness=true. Elapsed: 6.014107199s
May 21 05:38:40.271: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Running", Reason="", readiness=true. Elapsed: 8.024147221s
May 21 05:38:42.274: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Running", Reason="", readiness=true. Elapsed: 10.027186411s
May 21 05:38:44.277: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Running", Reason="", readiness=true. Elapsed: 12.030120817s
May 21 05:38:46.280: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Running", Reason="", readiness=true. Elapsed: 14.033413995s
May 21 05:38:48.284: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Running", Reason="", readiness=true. Elapsed: 16.036945149s
May 21 05:38:50.287: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Running", Reason="", readiness=true. Elapsed: 18.04004212s
May 21 05:38:52.290: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Running", Reason="", readiness=true. Elapsed: 20.043312756s
May 21 05:38:54.293: INFO: Pod "pod-subpath-test-projected-gw4p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.046050089s
STEP: Saw pod success
May 21 05:38:54.293: INFO: Pod "pod-subpath-test-projected-gw4p" satisfied condition "Succeeded or Failed"
May 21 05:38:54.295: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-subpath-test-projected-gw4p container test-container-subpath-projected-gw4p: <nil>
STEP: delete the pod
May 21 05:38:54.307: INFO: Waiting for pod pod-subpath-test-projected-gw4p to disappear
May 21 05:38:54.314: INFO: Pod pod-subpath-test-projected-gw4p no longer exists
STEP: Deleting pod pod-subpath-test-projected-gw4p
May 21 05:38:54.314: INFO: Deleting pod "pod-subpath-test-projected-gw4p" in namespace "subpath-4689"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:38:54.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4689" for this suite.

• [SLOW TEST:22.127 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":14,"skipped":194,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:38:54.323: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-d5e61aeb-b7e3-4c63-bed9-7ecf0faa2172 in namespace container-probe-3991
May 21 05:38:56.363: INFO: Started pod busybox-d5e61aeb-b7e3-4c63-bed9-7ecf0faa2172 in namespace container-probe-3991
STEP: checking the pod's current state and verifying that restartCount is present
May 21 05:38:56.365: INFO: Initial restart count of pod busybox-d5e61aeb-b7e3-4c63-bed9-7ecf0faa2172 is 0
May 21 05:39:42.433: INFO: Restart count of pod container-probe-3991/busybox-d5e61aeb-b7e3-4c63-bed9-7ecf0faa2172 is now 1 (46.067578315s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:39:42.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3991" for this suite.

• [SLOW TEST:48.130 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":15,"skipped":211,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:39:42.453: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
May 21 05:39:42.492: INFO: Waiting up to 5m0s for pod "pod-ce7ade48-e279-4746-b70e-072672ce8b34" in namespace "emptydir-6569" to be "Succeeded or Failed"
May 21 05:39:42.501: INFO: Pod "pod-ce7ade48-e279-4746-b70e-072672ce8b34": Phase="Pending", Reason="", readiness=false. Elapsed: 9.548831ms
May 21 05:39:44.504: INFO: Pod "pod-ce7ade48-e279-4746-b70e-072672ce8b34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012037948s
May 21 05:39:46.507: INFO: Pod "pod-ce7ade48-e279-4746-b70e-072672ce8b34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014882789s
STEP: Saw pod success
May 21 05:39:46.507: INFO: Pod "pod-ce7ade48-e279-4746-b70e-072672ce8b34" satisfied condition "Succeeded or Failed"
May 21 05:39:46.509: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-ce7ade48-e279-4746-b70e-072672ce8b34 container test-container: <nil>
STEP: delete the pod
May 21 05:39:46.525: INFO: Waiting for pod pod-ce7ade48-e279-4746-b70e-072672ce8b34 to disappear
May 21 05:39:46.530: INFO: Pod pod-ce7ade48-e279-4746-b70e-072672ce8b34 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:39:46.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6569" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":230,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:39:46.536: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:39:46.588: INFO: Create a RollingUpdate DaemonSet
May 21 05:39:46.591: INFO: Check that daemon pods launch on every node of the cluster
May 21 05:39:46.602: INFO: Number of nodes with available pods: 0
May 21 05:39:46.602: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:39:47.619: INFO: Number of nodes with available pods: 0
May 21 05:39:47.619: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:39:48.612: INFO: Number of nodes with available pods: 0
May 21 05:39:48.612: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:39:49.608: INFO: Number of nodes with available pods: 1
May 21 05:39:49.608: INFO: Node 53f495cf-0382-4128-b4b3-0979b69a209b is running more than one daemon pod
May 21 05:39:50.613: INFO: Number of nodes with available pods: 1
May 21 05:39:50.613: INFO: Node 53f495cf-0382-4128-b4b3-0979b69a209b is running more than one daemon pod
May 21 05:39:51.608: INFO: Number of nodes with available pods: 1
May 21 05:39:51.608: INFO: Node 53f495cf-0382-4128-b4b3-0979b69a209b is running more than one daemon pod
May 21 05:39:52.608: INFO: Number of nodes with available pods: 1
May 21 05:39:52.608: INFO: Node 53f495cf-0382-4128-b4b3-0979b69a209b is running more than one daemon pod
May 21 05:39:53.609: INFO: Number of nodes with available pods: 1
May 21 05:39:53.609: INFO: Node 53f495cf-0382-4128-b4b3-0979b69a209b is running more than one daemon pod
May 21 05:39:54.608: INFO: Number of nodes with available pods: 3
May 21 05:39:54.609: INFO: Number of running nodes: 3, number of available pods: 3
May 21 05:39:54.609: INFO: Update the DaemonSet to trigger a rollout
May 21 05:39:54.617: INFO: Updating DaemonSet daemon-set
May 21 05:40:01.638: INFO: Roll back the DaemonSet before rollout is complete
May 21 05:40:01.646: INFO: Updating DaemonSet daemon-set
May 21 05:40:01.646: INFO: Make sure DaemonSet rollback is complete
May 21 05:40:01.657: INFO: Wrong image for pod: daemon-set-jrzkb. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 05:40:01.657: INFO: Pod daemon-set-jrzkb is not available
May 21 05:40:02.665: INFO: Wrong image for pod: daemon-set-jrzkb. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 05:40:02.665: INFO: Pod daemon-set-jrzkb is not available
May 21 05:40:03.665: INFO: Wrong image for pod: daemon-set-jrzkb. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 05:40:03.665: INFO: Pod daemon-set-jrzkb is not available
May 21 05:40:04.665: INFO: Pod daemon-set-fxw76 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2559, will wait for the garbage collector to delete the pods
May 21 05:40:04.728: INFO: Deleting DaemonSet.extensions daemon-set took: 4.798778ms
May 21 05:40:05.329: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.131481ms
May 21 05:41:40.932: INFO: Number of nodes with available pods: 0
May 21 05:41:40.932: INFO: Number of running nodes: 0, number of available pods: 0
May 21 05:41:40.936: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8596"},"items":null}

May 21 05:41:40.937: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8596"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:41:40.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2559" for this suite.

• [SLOW TEST:114.415 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":17,"skipped":236,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:41:40.952: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:41:43.006: INFO: Waiting up to 5m0s for pod "client-envvars-7c4876df-8dd1-4371-8480-a800daef4acd" in namespace "pods-5359" to be "Succeeded or Failed"
May 21 05:41:43.016: INFO: Pod "client-envvars-7c4876df-8dd1-4371-8480-a800daef4acd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.214598ms
May 21 05:41:45.018: INFO: Pod "client-envvars-7c4876df-8dd1-4371-8480-a800daef4acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012440576s
STEP: Saw pod success
May 21 05:41:45.018: INFO: Pod "client-envvars-7c4876df-8dd1-4371-8480-a800daef4acd" satisfied condition "Succeeded or Failed"
May 21 05:41:45.020: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod client-envvars-7c4876df-8dd1-4371-8480-a800daef4acd container env3cont: <nil>
STEP: delete the pod
May 21 05:41:45.048: INFO: Waiting for pod client-envvars-7c4876df-8dd1-4371-8480-a800daef4acd to disappear
May 21 05:41:45.050: INFO: Pod client-envvars-7c4876df-8dd1-4371-8480-a800daef4acd no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:41:45.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5359" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":18,"skipped":244,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:41:45.057: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:42:01.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1175" for this suite.

• [SLOW TEST:16.116 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":19,"skipped":265,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:42:01.174: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:42:01.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5103" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":20,"skipped":268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:42:01.235: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
May 21 05:42:01.271: INFO: Waiting up to 5m0s for pod "var-expansion-1d06dee5-9977-49f8-93d2-fa85f15bbe01" in namespace "var-expansion-5588" to be "Succeeded or Failed"
May 21 05:42:01.283: INFO: Pod "var-expansion-1d06dee5-9977-49f8-93d2-fa85f15bbe01": Phase="Pending", Reason="", readiness=false. Elapsed: 12.258807ms
May 21 05:42:03.287: INFO: Pod "var-expansion-1d06dee5-9977-49f8-93d2-fa85f15bbe01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015892554s
May 21 05:42:05.290: INFO: Pod "var-expansion-1d06dee5-9977-49f8-93d2-fa85f15bbe01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019299548s
STEP: Saw pod success
May 21 05:42:05.290: INFO: Pod "var-expansion-1d06dee5-9977-49f8-93d2-fa85f15bbe01" satisfied condition "Succeeded or Failed"
May 21 05:42:05.292: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod var-expansion-1d06dee5-9977-49f8-93d2-fa85f15bbe01 container dapi-container: <nil>
STEP: delete the pod
May 21 05:42:05.307: INFO: Waiting for pod var-expansion-1d06dee5-9977-49f8-93d2-fa85f15bbe01 to disappear
May 21 05:42:05.313: INFO: Pod var-expansion-1d06dee5-9977-49f8-93d2-fa85f15bbe01 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:42:05.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5588" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":21,"skipped":351,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:42:05.319: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 21 05:42:05.892: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 05:42:08.907: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:42:08.910: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:42:10.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9840" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":22,"skipped":355,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:42:10.094: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 05:42:10.499: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 05:42:12.506: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172530, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172530, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172530, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172530, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 05:42:15.519: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:42:15.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8914" for this suite.
STEP: Destroying namespace "webhook-8914-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.470 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":23,"skipped":361,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:42:15.566: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-b1ab89d5-a757-41dd-9173-195a8e253306
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:42:19.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2113" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":24,"skipped":362,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:42:19.648: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-8913da95-6c6b-4cc2-b28a-1d3e8d7f0cec in namespace container-probe-2847
May 21 05:42:21.693: INFO: Started pod test-webserver-8913da95-6c6b-4cc2-b28a-1d3e8d7f0cec in namespace container-probe-2847
STEP: checking the pod's current state and verifying that restartCount is present
May 21 05:42:21.695: INFO: Initial restart count of pod test-webserver-8913da95-6c6b-4cc2-b28a-1d3e8d7f0cec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:46:22.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2847" for this suite.

• [SLOW TEST:242.697 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:46:22.346: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 21 05:46:25.407: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:46:26.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2338" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":26,"skipped":394,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:46:26.425: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:46:28.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9897" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:46:28.486: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-38789998-345d-4aec-9a47-1acf38c2c8ed
STEP: Creating a pod to test consume configMaps
May 21 05:46:28.525: INFO: Waiting up to 5m0s for pod "pod-configmaps-b4880491-981a-47b1-978a-d743d5422875" in namespace "configmap-8804" to be "Succeeded or Failed"
May 21 05:46:28.533: INFO: Pod "pod-configmaps-b4880491-981a-47b1-978a-d743d5422875": Phase="Pending", Reason="", readiness=false. Elapsed: 7.674293ms
May 21 05:46:30.536: INFO: Pod "pod-configmaps-b4880491-981a-47b1-978a-d743d5422875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010820558s
STEP: Saw pod success
May 21 05:46:30.536: INFO: Pod "pod-configmaps-b4880491-981a-47b1-978a-d743d5422875" satisfied condition "Succeeded or Failed"
May 21 05:46:30.538: INFO: Trying to get logs from node 53f495cf-0382-4128-b4b3-0979b69a209b pod pod-configmaps-b4880491-981a-47b1-978a-d743d5422875 container configmap-volume-test: <nil>
STEP: delete the pod
May 21 05:46:30.572: INFO: Waiting for pod pod-configmaps-b4880491-981a-47b1-978a-d743d5422875 to disappear
May 21 05:46:30.578: INFO: Pod pod-configmaps-b4880491-981a-47b1-978a-d743d5422875 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:46:30.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8804" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":28,"skipped":428,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:46:30.592: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 21 05:46:30.634: INFO: PodSpec: initContainers in spec.initContainers
May 21 05:47:17.866: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-33b3680d-89ab-47ad-876c-31e427c663bc", GenerateName:"", Namespace:"init-container-6929", SelfLink:"", UID:"ed78319a-38c8-486f-9d49-c002a1e35475", ResourceVersion:"9596", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63757172790, loc:(*time.Location)(0x7975ee0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"634047996"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004084d00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004084d20)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004084d40), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004084d60)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ncrxc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00286a300), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ncrxc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ncrxc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ncrxc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000160278), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"53f495cf-0382-4128-b4b3-0979b69a209b", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00355b880), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0001602f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000160310)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000160318), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00016031c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc006a5c260), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172790, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172790, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172790, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172790, loc:(*time.Location)(0x7975ee0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"30.0.0.11", PodIP:"10.200.66.13", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.200.66.13"}}, StartTime:(*v1.Time)(0xc004084d80), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc004084dc0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00355b960)}, Ready:false, RestartCount:3, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"docker-pullable://mirror.gcr.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://638ce17834561786bb986f1390416cedda80c90bdad99af21baf788599b744fb", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004084de0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004084da0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0001603ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:47:17.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6929" for this suite.

• [SLOW TEST:47.282 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":29,"skipped":446,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:47:17.873: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-cb21cc8e-024f-4c13-9616-5c8a374fec98
STEP: Creating a pod to test consume secrets
May 21 05:47:17.915: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0a7dd9a-c319-418c-8aa1-26861049cd0e" in namespace "projected-5645" to be "Succeeded or Failed"
May 21 05:47:17.926: INFO: Pod "pod-projected-secrets-b0a7dd9a-c319-418c-8aa1-26861049cd0e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.223512ms
May 21 05:47:19.929: INFO: Pod "pod-projected-secrets-b0a7dd9a-c319-418c-8aa1-26861049cd0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014618792s
STEP: Saw pod success
May 21 05:47:19.930: INFO: Pod "pod-projected-secrets-b0a7dd9a-c319-418c-8aa1-26861049cd0e" satisfied condition "Succeeded or Failed"
May 21 05:47:19.932: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-secrets-b0a7dd9a-c319-418c-8aa1-26861049cd0e container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 05:47:19.946: INFO: Waiting for pod pod-projected-secrets-b0a7dd9a-c319-418c-8aa1-26861049cd0e to disappear
May 21 05:47:19.952: INFO: Pod pod-projected-secrets-b0a7dd9a-c319-418c-8aa1-26861049cd0e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:47:19.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5645" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":463,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:47:19.958: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 21 05:47:19.988: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7571  71ccd41f-844a-4bab-b071-00d77332a1b9 9624 0 2021-05-21 05:47:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 05:47:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 05:47:19.988: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7571  71ccd41f-844a-4bab-b071-00d77332a1b9 9624 0 2021-05-21 05:47:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 05:47:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 21 05:47:29.995: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7571  71ccd41f-844a-4bab-b071-00d77332a1b9 9663 0 2021-05-21 05:47:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 05:47:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 05:47:29.995: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7571  71ccd41f-844a-4bab-b071-00d77332a1b9 9663 0 2021-05-21 05:47:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 05:47:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 21 05:47:40.004: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7571  71ccd41f-844a-4bab-b071-00d77332a1b9 9680 0 2021-05-21 05:47:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 05:47:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 05:47:40.004: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7571  71ccd41f-844a-4bab-b071-00d77332a1b9 9680 0 2021-05-21 05:47:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 05:47:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 21 05:47:50.010: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7571  71ccd41f-844a-4bab-b071-00d77332a1b9 9697 0 2021-05-21 05:47:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 05:47:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 05:47:50.010: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7571  71ccd41f-844a-4bab-b071-00d77332a1b9 9697 0 2021-05-21 05:47:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 05:47:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 21 05:48:00.020: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7571  249e6f46-e8db-4c72-8f5c-2417a4917eb2 9712 0 2021-05-21 05:48:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 05:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 05:48:00.020: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7571  249e6f46-e8db-4c72-8f5c-2417a4917eb2 9712 0 2021-05-21 05:48:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 05:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 21 05:48:10.025: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7571  249e6f46-e8db-4c72-8f5c-2417a4917eb2 9728 0 2021-05-21 05:48:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 05:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 05:48:10.025: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7571  249e6f46-e8db-4c72-8f5c-2417a4917eb2 9728 0 2021-05-21 05:48:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 05:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:48:20.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7571" for this suite.

• [SLOW TEST:60.076 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":31,"skipped":465,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:48:20.034: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:48:20.072: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 21 05:48:20.081: INFO: Number of nodes with available pods: 0
May 21 05:48:20.081: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 21 05:48:20.102: INFO: Number of nodes with available pods: 0
May 21 05:48:20.102: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:21.105: INFO: Number of nodes with available pods: 0
May 21 05:48:21.105: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:22.105: INFO: Number of nodes with available pods: 0
May 21 05:48:22.105: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:23.105: INFO: Number of nodes with available pods: 1
May 21 05:48:23.105: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 21 05:48:23.119: INFO: Number of nodes with available pods: 1
May 21 05:48:23.119: INFO: Number of running nodes: 0, number of available pods: 1
May 21 05:48:24.123: INFO: Number of nodes with available pods: 0
May 21 05:48:24.123: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 21 05:48:24.145: INFO: Number of nodes with available pods: 0
May 21 05:48:24.145: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:25.148: INFO: Number of nodes with available pods: 0
May 21 05:48:25.148: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:26.149: INFO: Number of nodes with available pods: 0
May 21 05:48:26.149: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:27.149: INFO: Number of nodes with available pods: 0
May 21 05:48:27.149: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:28.149: INFO: Number of nodes with available pods: 0
May 21 05:48:28.149: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:29.149: INFO: Number of nodes with available pods: 0
May 21 05:48:29.149: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:30.149: INFO: Number of nodes with available pods: 0
May 21 05:48:30.149: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:31.148: INFO: Number of nodes with available pods: 0
May 21 05:48:31.148: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:32.149: INFO: Number of nodes with available pods: 0
May 21 05:48:32.149: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:33.149: INFO: Number of nodes with available pods: 0
May 21 05:48:33.149: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:34.149: INFO: Number of nodes with available pods: 0
May 21 05:48:34.149: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 05:48:35.149: INFO: Number of nodes with available pods: 1
May 21 05:48:35.149: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2481, will wait for the garbage collector to delete the pods
May 21 05:48:35.209: INFO: Deleting DaemonSet.extensions daemon-set took: 4.94339ms
May 21 05:48:35.809: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.154033ms
May 21 05:48:42.412: INFO: Number of nodes with available pods: 0
May 21 05:48:42.412: INFO: Number of running nodes: 0, number of available pods: 0
May 21 05:48:42.413: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9835"},"items":null}

May 21 05:48:42.415: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9835"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:48:42.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2481" for this suite.

• [SLOW TEST:22.405 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":32,"skipped":486,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:48:42.439: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 05:48:42.813: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 05:48:44.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172922, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172922, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172922, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757172922, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 05:48:47.834: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 21 05:48:49.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=webhook-2759 attach --namespace=webhook-2759 to-be-attached-pod -i -c=container1'
May 21 05:48:51.011: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:48:51.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2759" for this suite.
STEP: Destroying namespace "webhook-2759-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.614 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":33,"skipped":494,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:48:51.053: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3033
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 21 05:48:51.112: INFO: Found 0 stateful pods, waiting for 3
May 21 05:49:01.115: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 05:49:01.115: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 05:49:01.115: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 21 05:49:01.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-3033 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 05:49:01.305: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 05:49:01.305: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 05:49:01.305: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
May 21 05:49:11.333: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 21 05:49:21.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-3033 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 05:49:21.520: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 05:49:21.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 05:49:21.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 05:49:31.540: INFO: Waiting for StatefulSet statefulset-3033/ss2 to complete update
May 21 05:49:31.540: INFO: Waiting for Pod statefulset-3033/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 05:49:31.540: INFO: Waiting for Pod statefulset-3033/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 05:49:41.546: INFO: Waiting for StatefulSet statefulset-3033/ss2 to complete update
May 21 05:49:41.546: INFO: Waiting for Pod statefulset-3033/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 05:49:41.546: INFO: Waiting for Pod statefulset-3033/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 05:49:51.546: INFO: Waiting for StatefulSet statefulset-3033/ss2 to complete update
May 21 05:49:51.546: INFO: Waiting for Pod statefulset-3033/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 05:50:01.552: INFO: Waiting for StatefulSet statefulset-3033/ss2 to complete update
May 21 05:50:01.552: INFO: Waiting for Pod statefulset-3033/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
STEP: Rolling back to a previous revision
May 21 05:50:11.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-3033 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 05:50:11.736: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 05:50:11.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 05:50:11.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 05:50:21.765: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 21 05:50:31.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-3033 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 05:50:31.966: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 05:50:31.966: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 05:50:31.966: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 05:50:41.979: INFO: Waiting for StatefulSet statefulset-3033/ss2 to complete update
May 21 05:50:41.979: INFO: Waiting for Pod statefulset-3033/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
May 21 05:50:41.979: INFO: Waiting for Pod statefulset-3033/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
May 21 05:50:41.979: INFO: Waiting for Pod statefulset-3033/ss2-2 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
May 21 05:50:51.984: INFO: Waiting for StatefulSet statefulset-3033/ss2 to complete update
May 21 05:50:51.984: INFO: Waiting for Pod statefulset-3033/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
May 21 05:50:51.984: INFO: Waiting for Pod statefulset-3033/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
May 21 05:51:01.986: INFO: Waiting for StatefulSet statefulset-3033/ss2 to complete update
May 21 05:51:01.986: INFO: Waiting for Pod statefulset-3033/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 05:51:11.985: INFO: Deleting all statefulset in ns statefulset-3033
May 21 05:51:11.987: INFO: Scaling statefulset ss2 to 0
May 21 05:51:52.010: INFO: Waiting for statefulset status.replicas updated to 0
May 21 05:51:52.012: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:51:52.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3033" for this suite.

• [SLOW TEST:180.980 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":34,"skipped":500,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:51:52.034: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-4171068a-ea55-4fd9-8de8-cbd59dea9a00
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:51:52.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4671" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":35,"skipped":501,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:51:52.070: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-b60f543f-51df-41c0-a4eb-3d085efa347a
STEP: Creating configMap with name cm-test-opt-upd-69edd4cb-e1bf-4ef8-b715-1e870afc204d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b60f543f-51df-41c0-a4eb-3d085efa347a
STEP: Updating configmap cm-test-opt-upd-69edd4cb-e1bf-4ef8-b715-1e870afc204d
STEP: Creating configMap with name cm-test-opt-create-605b6756-b28c-4300-aec4-d7aae8861ef0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:53:06.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5235" for this suite.

• [SLOW TEST:74.362 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:53:06.432: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-427.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-427.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 05:53:16.508: INFO: DNS probes using dns-427/dns-test-d1c39868-ee54-40cd-83e5-d66244c5f20b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:53:16.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-427" for this suite.

• [SLOW TEST:10.096 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":37,"skipped":549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:53:16.530: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-ab2e3bd1-48f2-4573-ab9c-d03b49a5e156 in namespace container-probe-6386
May 21 05:53:18.571: INFO: Started pod liveness-ab2e3bd1-48f2-4573-ab9c-d03b49a5e156 in namespace container-probe-6386
STEP: checking the pod's current state and verifying that restartCount is present
May 21 05:53:18.573: INFO: Initial restart count of pod liveness-ab2e3bd1-48f2-4573-ab9c-d03b49a5e156 is 0
May 21 05:53:30.598: INFO: Restart count of pod container-probe-6386/liveness-ab2e3bd1-48f2-4573-ab9c-d03b49a5e156 is now 1 (12.025140615s elapsed)
May 21 05:53:50.638: INFO: Restart count of pod container-probe-6386/liveness-ab2e3bd1-48f2-4573-ab9c-d03b49a5e156 is now 2 (32.065310428s elapsed)
May 21 05:54:10.695: INFO: Restart count of pod container-probe-6386/liveness-ab2e3bd1-48f2-4573-ab9c-d03b49a5e156 is now 3 (52.121914159s elapsed)
May 21 05:54:30.732: INFO: Restart count of pod container-probe-6386/liveness-ab2e3bd1-48f2-4573-ab9c-d03b49a5e156 is now 4 (1m12.158776547s elapsed)
May 21 05:55:34.834: INFO: Restart count of pod container-probe-6386/liveness-ab2e3bd1-48f2-4573-ab9c-d03b49a5e156 is now 5 (2m16.260775609s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:55:34.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6386" for this suite.

• [SLOW TEST:138.322 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":38,"skipped":583,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:55:34.853: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 21 05:55:34.899: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:55:34.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-783" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":39,"skipped":598,"failed":0}

------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:55:34.935: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-eefd63a4-cbf7-4a73-8066-f03354a438e8
STEP: Creating a pod to test consume secrets
May 21 05:55:34.969: INFO: Waiting up to 5m0s for pod "pod-secrets-86a87b6f-e8e0-4009-a358-028eafd0659e" in namespace "secrets-1027" to be "Succeeded or Failed"
May 21 05:55:34.973: INFO: Pod "pod-secrets-86a87b6f-e8e0-4009-a358-028eafd0659e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.965339ms
May 21 05:55:36.976: INFO: Pod "pod-secrets-86a87b6f-e8e0-4009-a358-028eafd0659e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007220374s
STEP: Saw pod success
May 21 05:55:36.977: INFO: Pod "pod-secrets-86a87b6f-e8e0-4009-a358-028eafd0659e" satisfied condition "Succeeded or Failed"
May 21 05:55:36.979: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-secrets-86a87b6f-e8e0-4009-a358-028eafd0659e container secret-env-test: <nil>
STEP: delete the pod
May 21 05:55:36.999: INFO: Waiting for pod pod-secrets-86a87b6f-e8e0-4009-a358-028eafd0659e to disappear
May 21 05:55:37.005: INFO: Pod pod-secrets-86a87b6f-e8e0-4009-a358-028eafd0659e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:55:37.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1027" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":598,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:55:37.011: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 21 05:55:43.582: INFO: Successfully updated pod "pod-update-13d042b7-2648-4e46-b529-056fbd9c03e8"
STEP: verifying the updated pod is in kubernetes
May 21 05:55:43.590: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:55:43.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3303" for this suite.

• [SLOW TEST:6.585 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":41,"skipped":609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:55:43.596: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9050
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9050
STEP: creating replication controller externalsvc in namespace services-9050
I0521 05:55:43.661383      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9050, replica count: 2
I0521 05:55:46.711945      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 21 05:55:46.729: INFO: Creating new exec pod
May 21 05:55:48.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-9050 exec execpodzszv7 -- /bin/sh -x -c nslookup clusterip-service.services-9050.svc.cluster.local'
May 21 05:55:48.936: INFO: stderr: "+ nslookup clusterip-service.services-9050.svc.cluster.local\n"
May 21 05:55:48.936: INFO: stdout: "Server:\t\t10.100.200.2\nAddress:\t10.100.200.2#53\n\nclusterip-service.services-9050.svc.cluster.local\tcanonical name = externalsvc.services-9050.svc.cluster.local.\nName:\texternalsvc.services-9050.svc.cluster.local\nAddress: 10.100.200.175\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9050, will wait for the garbage collector to delete the pods
May 21 05:55:48.994: INFO: Deleting ReplicationController externalsvc took: 5.172785ms
May 21 05:55:49.594: INFO: Terminating ReplicationController externalsvc pods took: 600.137662ms
May 21 05:56:03.708: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:03.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9050" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:20.129 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":42,"skipped":643,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:03.726: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 21 05:56:08.278: INFO: Successfully updated pod "adopt-release-lnlfk"
STEP: Checking that the Job readopts the Pod
May 21 05:56:08.278: INFO: Waiting up to 15m0s for pod "adopt-release-lnlfk" in namespace "job-2916" to be "adopted"
May 21 05:56:08.288: INFO: Pod "adopt-release-lnlfk": Phase="Running", Reason="", readiness=true. Elapsed: 9.893542ms
May 21 05:56:10.292: INFO: Pod "adopt-release-lnlfk": Phase="Running", Reason="", readiness=true. Elapsed: 2.013911579s
May 21 05:56:10.292: INFO: Pod "adopt-release-lnlfk" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 21 05:56:10.803: INFO: Successfully updated pod "adopt-release-lnlfk"
STEP: Checking that the Job releases the Pod
May 21 05:56:10.803: INFO: Waiting up to 15m0s for pod "adopt-release-lnlfk" in namespace "job-2916" to be "released"
May 21 05:56:10.810: INFO: Pod "adopt-release-lnlfk": Phase="Running", Reason="", readiness=true. Elapsed: 6.867764ms
May 21 05:56:12.816: INFO: Pod "adopt-release-lnlfk": Phase="Running", Reason="", readiness=true. Elapsed: 2.012601358s
May 21 05:56:12.816: INFO: Pod "adopt-release-lnlfk" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:12.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2916" for this suite.

• [SLOW TEST:9.097 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":43,"skipped":651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:12.824: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
May 21 05:56:12.852: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
May 21 05:56:13.468: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 21 05:56:15.511: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 05:56:17.515: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 05:56:19.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 05:56:21.515: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 05:56:23.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173373, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 05:56:28.328: INFO: Waited 2.808511673s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:29.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-267" for this suite.

• [SLOW TEST:16.347 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":44,"skipped":674,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:29.171: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-7198f5d6-4516-4994-b367-8a33de402e97
STEP: Creating a pod to test consume secrets
May 21 05:56:29.267: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc6b6589-96a9-4664-9f45-61c9e0c34769" in namespace "projected-6688" to be "Succeeded or Failed"
May 21 05:56:29.287: INFO: Pod "pod-projected-secrets-bc6b6589-96a9-4664-9f45-61c9e0c34769": Phase="Pending", Reason="", readiness=false. Elapsed: 20.228699ms
May 21 05:56:31.290: INFO: Pod "pod-projected-secrets-bc6b6589-96a9-4664-9f45-61c9e0c34769": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023496721s
STEP: Saw pod success
May 21 05:56:31.290: INFO: Pod "pod-projected-secrets-bc6b6589-96a9-4664-9f45-61c9e0c34769" satisfied condition "Succeeded or Failed"
May 21 05:56:31.292: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-secrets-bc6b6589-96a9-4664-9f45-61c9e0c34769 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 05:56:31.305: INFO: Waiting for pod pod-projected-secrets-bc6b6589-96a9-4664-9f45-61c9e0c34769 to disappear
May 21 05:56:31.311: INFO: Pod pod-projected-secrets-bc6b6589-96a9-4664-9f45-61c9e0c34769 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:31.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6688" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":674,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:31.317: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:56:31.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-662 version'
May 21 05:56:31.418: INFO: stderr: ""
May 21 05:56:31.418: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:28:42Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6+vmware.1\", GitCommit:\"088f01db2ffab397a290be443902918b59ee032c\", GitTreeState:\"clean\", BuildDate:\"2021-04-17T01:01:00Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:31.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-662" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":46,"skipped":690,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:31.426: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:56:31.461: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-470d71f5-7fa3-472e-96e5-be8da3be54f9" in namespace "security-context-test-5035" to be "Succeeded or Failed"
May 21 05:56:31.464: INFO: Pod "alpine-nnp-false-470d71f5-7fa3-472e-96e5-be8da3be54f9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.162839ms
May 21 05:56:33.467: INFO: Pod "alpine-nnp-false-470d71f5-7fa3-472e-96e5-be8da3be54f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005758555s
May 21 05:56:35.471: INFO: Pod "alpine-nnp-false-470d71f5-7fa3-472e-96e5-be8da3be54f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009693513s
May 21 05:56:35.471: INFO: Pod "alpine-nnp-false-470d71f5-7fa3-472e-96e5-be8da3be54f9" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:35.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5035" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":47,"skipped":702,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:35.489: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
May 21 05:56:35.519: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 21 05:56:35.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 create -f -'
May 21 05:56:35.816: INFO: stderr: ""
May 21 05:56:35.816: INFO: stdout: "service/agnhost-replica created\n"
May 21 05:56:35.816: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 21 05:56:35.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 create -f -'
May 21 05:56:36.068: INFO: stderr: ""
May 21 05:56:36.068: INFO: stdout: "service/agnhost-primary created\n"
May 21 05:56:36.068: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 21 05:56:36.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 create -f -'
May 21 05:56:36.258: INFO: stderr: ""
May 21 05:56:36.258: INFO: stdout: "service/frontend created\n"
May 21 05:56:36.258: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 21 05:56:36.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 create -f -'
May 21 05:56:36.461: INFO: stderr: ""
May 21 05:56:36.461: INFO: stdout: "deployment.apps/frontend created\n"
May 21 05:56:36.461: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 21 05:56:36.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 create -f -'
May 21 05:56:36.677: INFO: stderr: ""
May 21 05:56:36.677: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 21 05:56:36.677: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 21 05:56:36.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 create -f -'
May 21 05:56:36.868: INFO: stderr: ""
May 21 05:56:36.868: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 21 05:56:36.868: INFO: Waiting for all frontend pods to be Running.
May 21 05:56:41.918: INFO: Waiting for frontend to serve content.
May 21 05:56:41.926: INFO: Trying to add a new entry to the guestbook.
May 21 05:56:41.933: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 21 05:56:41.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 delete --grace-period=0 --force -f -'
May 21 05:56:42.019: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 05:56:42.019: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 21 05:56:42.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 delete --grace-period=0 --force -f -'
May 21 05:56:42.094: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 05:56:42.094: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 21 05:56:42.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 delete --grace-period=0 --force -f -'
May 21 05:56:42.178: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 05:56:42.178: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 21 05:56:42.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 delete --grace-period=0 --force -f -'
May 21 05:56:42.243: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 05:56:42.243: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 21 05:56:42.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 delete --grace-period=0 --force -f -'
May 21 05:56:42.309: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 05:56:42.309: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 21 05:56:42.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3420 delete --grace-period=0 --force -f -'
May 21 05:56:42.374: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 05:56:42.374: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:42.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3420" for this suite.

• [SLOW TEST:6.890 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":48,"skipped":703,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:42.380: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 21 05:56:42.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6106 create -f -'
May 21 05:56:42.619: INFO: stderr: ""
May 21 05:56:42.620: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 05:56:43.624: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 05:56:43.624: INFO: Found 0 / 1
May 21 05:56:44.624: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 05:56:44.624: INFO: Found 1 / 1
May 21 05:56:44.624: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 21 05:56:44.626: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 05:56:44.626: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 05:56:44.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6106 patch pod agnhost-primary-rfdpc -p {"metadata":{"annotations":{"x":"y"}}}'
May 21 05:56:44.704: INFO: stderr: ""
May 21 05:56:44.704: INFO: stdout: "pod/agnhost-primary-rfdpc patched\n"
STEP: checking annotations
May 21 05:56:44.706: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 05:56:44.706: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:44.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6106" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":49,"skipped":744,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:44.712: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 21 05:56:44.751: INFO: Waiting up to 5m0s for pod "pod-2f97a3b4-79c1-4359-9321-43062cc44657" in namespace "emptydir-6561" to be "Succeeded or Failed"
May 21 05:56:44.761: INFO: Pod "pod-2f97a3b4-79c1-4359-9321-43062cc44657": Phase="Pending", Reason="", readiness=false. Elapsed: 9.825939ms
May 21 05:56:46.766: INFO: Pod "pod-2f97a3b4-79c1-4359-9321-43062cc44657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014987004s
May 21 05:56:48.771: INFO: Pod "pod-2f97a3b4-79c1-4359-9321-43062cc44657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019513048s
STEP: Saw pod success
May 21 05:56:48.771: INFO: Pod "pod-2f97a3b4-79c1-4359-9321-43062cc44657" satisfied condition "Succeeded or Failed"
May 21 05:56:48.773: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-2f97a3b4-79c1-4359-9321-43062cc44657 container test-container: <nil>
STEP: delete the pod
May 21 05:56:48.790: INFO: Waiting for pod pod-2f97a3b4-79c1-4359-9321-43062cc44657 to disappear
May 21 05:56:48.793: INFO: Pod pod-2f97a3b4-79c1-4359-9321-43062cc44657 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:48.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6561" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":50,"skipped":764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:48.799: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
May 21 05:56:48.842: INFO: created test-podtemplate-1
May 21 05:56:48.844: INFO: created test-podtemplate-2
May 21 05:56:48.856: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 21 05:56:48.866: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 21 05:56:48.891: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:48.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8457" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":51,"skipped":822,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:48.900: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 21 05:56:48.937: INFO: Waiting up to 5m0s for pod "pod-5cf40128-54df-40e5-8c46-5e315826bc58" in namespace "emptydir-3334" to be "Succeeded or Failed"
May 21 05:56:48.946: INFO: Pod "pod-5cf40128-54df-40e5-8c46-5e315826bc58": Phase="Pending", Reason="", readiness=false. Elapsed: 8.40022ms
May 21 05:56:50.949: INFO: Pod "pod-5cf40128-54df-40e5-8c46-5e315826bc58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011847104s
May 21 05:56:52.962: INFO: Pod "pod-5cf40128-54df-40e5-8c46-5e315826bc58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024381536s
STEP: Saw pod success
May 21 05:56:52.962: INFO: Pod "pod-5cf40128-54df-40e5-8c46-5e315826bc58" satisfied condition "Succeeded or Failed"
May 21 05:56:52.964: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-5cf40128-54df-40e5-8c46-5e315826bc58 container test-container: <nil>
STEP: delete the pod
May 21 05:56:52.980: INFO: Waiting for pod pod-5cf40128-54df-40e5-8c46-5e315826bc58 to disappear
May 21 05:56:52.986: INFO: Pod pod-5cf40128-54df-40e5-8c46-5e315826bc58 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:56:52.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3334" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":52,"skipped":825,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:56:52.993: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3071.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3071.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3071.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3071.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3071.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3071.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 05:57:03.069: INFO: DNS probes using dns-3071/dns-test-d254abb0-6bd2-45aa-8d1a-2a0d50b3e0fc succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:57:03.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3071" for this suite.

• [SLOW TEST:10.140 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":53,"skipped":837,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:57:03.134: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 05:57:03.170: INFO: Waiting up to 5m0s for pod "downwardapi-volume-340626e5-8f40-4b90-b05b-3e108e9b2560" in namespace "projected-1635" to be "Succeeded or Failed"
May 21 05:57:03.180: INFO: Pod "downwardapi-volume-340626e5-8f40-4b90-b05b-3e108e9b2560": Phase="Pending", Reason="", readiness=false. Elapsed: 10.530936ms
May 21 05:57:05.183: INFO: Pod "downwardapi-volume-340626e5-8f40-4b90-b05b-3e108e9b2560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013587509s
May 21 05:57:07.187: INFO: Pod "downwardapi-volume-340626e5-8f40-4b90-b05b-3e108e9b2560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016791302s
STEP: Saw pod success
May 21 05:57:07.187: INFO: Pod "downwardapi-volume-340626e5-8f40-4b90-b05b-3e108e9b2560" satisfied condition "Succeeded or Failed"
May 21 05:57:07.189: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-340626e5-8f40-4b90-b05b-3e108e9b2560 container client-container: <nil>
STEP: delete the pod
May 21 05:57:07.204: INFO: Waiting for pod downwardapi-volume-340626e5-8f40-4b90-b05b-3e108e9b2560 to disappear
May 21 05:57:07.209: INFO: Pod downwardapi-volume-340626e5-8f40-4b90-b05b-3e108e9b2560 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:57:07.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1635" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:57:07.217: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:57:07.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6782" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":55,"skipped":896,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:57:07.255: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 21 05:57:07.277: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 05:57:07.287: INFO: Waiting for terminating namespaces to be deleted...
May 21 05:57:07.289: INFO: 
Logging pods the apiserver thinks is on node 16c19df1-13dd-4e26-99a1-a192204dc796 before test
May 21 05:57:07.301: INFO: coredns-6d5447c5f4-sdg2p from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.301: INFO: 	Container coredns ready: true, restart count 0
May 21 05:57:07.301: INFO: metrics-server-7d476fdfbd-f88wj from kube-system started at 2021-05-21 04:30:11 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.301: INFO: 	Container metrics-server ready: true, restart count 0
May 21 05:57:07.301: INFO: cert-generator-cbc5efd76cef99ea15cff1408e85404dd343c0a5-zdnb8 from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.301: INFO: 	Container cert-generator ready: false, restart count 0
May 21 05:57:07.301: INFO: fluent-bit-rkcnw from pks-system started at 2021-05-21 04:30:22 +0000 UTC (2 container statuses recorded)
May 21 05:57:07.301: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 05:57:07.301: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 05:57:07.301: INFO: node-exporter-gbfjt from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.301: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 05:57:07.301: INFO: sink-controller-5bf464ff96-8nj6l from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.301: INFO: 	Container sink-controller ready: true, restart count 0
May 21 05:57:07.301: INFO: telegraf-hwv6v from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.301: INFO: 	Container telegraf ready: true, restart count 0
May 21 05:57:07.301: INFO: wavefront-collector-5795f87754-rfj4n from pks-system started at 2021-05-21 04:32:41 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.302: INFO: 	Container wavefront-collector ready: true, restart count 0
May 21 05:57:07.302: INFO: sonobuoy from sonobuoy started at 2021-05-21 05:36:00 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.302: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 05:57:07.302: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-b9nwr from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 05:57:07.302: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 05:57:07.302: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 05:57:07.302: INFO: 
Logging pods the apiserver thinks is on node 53f495cf-0382-4128-b4b3-0979b69a209b before test
May 21 05:57:07.310: INFO: coredns-6d5447c5f4-fv5hd from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.310: INFO: 	Container coredns ready: true, restart count 0
May 21 05:57:07.310: INFO: fluent-bit-kr585 from pks-system started at 2021-05-21 04:30:23 +0000 UTC (2 container statuses recorded)
May 21 05:57:07.310: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 05:57:07.310: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 05:57:07.310: INFO: metric-controller-5754b4489d-c246d from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.310: INFO: 	Container metric-controller ready: true, restart count 0
May 21 05:57:07.310: INFO: node-exporter-fmcv8 from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.310: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 05:57:07.310: INFO: observability-manager-7f5c97895-xsj2f from pks-system started at 2021-05-21 04:30:14 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.310: INFO: 	Container observability-manager ready: true, restart count 0
May 21 05:57:07.310: INFO: telegraf-hlmgc from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.310: INFO: 	Container telegraf ready: true, restart count 0
May 21 05:57:07.310: INFO: telemetry-agent-8f56d9865-l6ntw from pks-system started at 2021-05-21 04:35:59 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.310: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
May 21 05:57:07.310: INFO: validator-54f6bbd67b-snnrv from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.310: INFO: 	Container validator ready: true, restart count 0
May 21 05:57:07.310: INFO: sonobuoy-e2e-job-a4c84655dab54066 from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 05:57:07.310: INFO: 	Container e2e ready: true, restart count 0
May 21 05:57:07.310: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 05:57:07.310: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-g25r8 from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 05:57:07.310: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 05:57:07.310: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 05:57:07.310: INFO: 
Logging pods the apiserver thinks is on node 973ef262-7e5e-4fd5-8bf3-6139f838e19b before test
May 21 05:57:07.318: INFO: coredns-6d5447c5f4-7dcmd from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.318: INFO: 	Container coredns ready: true, restart count 0
May 21 05:57:07.318: INFO: event-controller-55dd5b5777-4mn7d from pks-system started at 2021-05-21 04:30:17 +0000 UTC (2 container statuses recorded)
May 21 05:57:07.318: INFO: 	Container event-controller ready: true, restart count 0
May 21 05:57:07.318: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 05:57:07.318: INFO: fluent-bit-jjrdh from pks-system started at 2021-05-21 04:30:31 +0000 UTC (2 container statuses recorded)
May 21 05:57:07.318: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 05:57:07.318: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 05:57:07.318: INFO: node-exporter-6k9jm from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.318: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 05:57:07.318: INFO: telegraf-jqzbt from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.318: INFO: 	Container telegraf ready: true, restart count 0
May 21 05:57:07.318: INFO: wavefront-proxy-6446ff4bd7-lw9wt from pks-system started at 2021-05-21 04:32:41 +0000 UTC (1 container statuses recorded)
May 21 05:57:07.318: INFO: 	Container wavefront-proxy ready: true, restart count 0
May 21 05:57:07.318: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-ppgpw from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 05:57:07.318: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 05:57:07.318: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3e9de641-804d-4824-a4b1-af048746b744 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 30.0.0.12 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 30.0.0.12 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 21 05:57:19.428: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 30.0.0.12 http://127.0.0.1:54321/hostname] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:19.428: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.0.0.12, port: 54321
May 21 05:57:19.520: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://30.0.0.12:54321/hostname] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:19.520: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.0.0.12, port: 54321 UDP
May 21 05:57:19.618: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 30.0.0.12 54321] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:19.618: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 21 05:57:24.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 30.0.0.12 http://127.0.0.1:54321/hostname] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:24.707: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.0.0.12, port: 54321
May 21 05:57:24.799: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://30.0.0.12:54321/hostname] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:24.799: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.0.0.12, port: 54321 UDP
May 21 05:57:24.898: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 30.0.0.12 54321] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:24.898: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 21 05:57:29.989: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 30.0.0.12 http://127.0.0.1:54321/hostname] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:29.989: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.0.0.12, port: 54321
May 21 05:57:30.080: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://30.0.0.12:54321/hostname] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:30.080: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.0.0.12, port: 54321 UDP
May 21 05:57:30.167: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 30.0.0.12 54321] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:30.167: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 21 05:57:35.263: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 30.0.0.12 http://127.0.0.1:54321/hostname] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:35.263: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.0.0.12, port: 54321
May 21 05:57:35.379: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://30.0.0.12:54321/hostname] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:35.379: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.0.0.12, port: 54321 UDP
May 21 05:57:35.465: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 30.0.0.12 54321] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:35.465: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 21 05:57:40.552: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 30.0.0.12 http://127.0.0.1:54321/hostname] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:40.552: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.0.0.12, port: 54321
May 21 05:57:40.635: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://30.0.0.12:54321/hostname] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:40.635: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.0.0.12, port: 54321 UDP
May 21 05:57:40.724: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 30.0.0.12 54321] Namespace:sched-pred-1147 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 05:57:40.724: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: removing the label kubernetes.io/e2e-3e9de641-804d-4824-a4b1-af048746b744 off the node 16c19df1-13dd-4e26-99a1-a192204dc796
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3e9de641-804d-4824-a4b1-af048746b744
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:57:45.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1147" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:38.590 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":56,"skipped":909,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:57:45.847: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:57:45.884: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 05:57:48.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-8176 --namespace=crd-publish-openapi-8176 create -f -'
May 21 05:57:50.171: INFO: stderr: ""
May 21 05:57:50.171: INFO: stdout: "e2e-test-crd-publish-openapi-6467-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 21 05:57:50.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-8176 --namespace=crd-publish-openapi-8176 delete e2e-test-crd-publish-openapi-6467-crds test-cr'
May 21 05:57:50.239: INFO: stderr: ""
May 21 05:57:50.239: INFO: stdout: "e2e-test-crd-publish-openapi-6467-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 21 05:57:50.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-8176 --namespace=crd-publish-openapi-8176 apply -f -'
May 21 05:57:50.433: INFO: stderr: ""
May 21 05:57:50.433: INFO: stdout: "e2e-test-crd-publish-openapi-6467-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 21 05:57:50.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-8176 --namespace=crd-publish-openapi-8176 delete e2e-test-crd-publish-openapi-6467-crds test-cr'
May 21 05:57:50.502: INFO: stderr: ""
May 21 05:57:50.502: INFO: stdout: "e2e-test-crd-publish-openapi-6467-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 21 05:57:50.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-8176 explain e2e-test-crd-publish-openapi-6467-crds'
May 21 05:57:50.683: INFO: stderr: ""
May 21 05:57:50.683: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6467-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:57:53.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8176" for this suite.

• [SLOW TEST:7.731 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":57,"skipped":924,"failed":0}
SS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:57:53.578: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 21 05:57:53.614: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 21 05:57:53.619: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 21 05:57:53.619: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 21 05:57:53.628: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 21 05:57:53.628: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 21 05:57:53.648: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 21 05:57:53.648: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 21 05:58:00.685: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:00.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9833" for this suite.

• [SLOW TEST:7.136 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":58,"skipped":926,"failed":0}
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:00.715: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 05:58:00.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4466 run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 21 05:58:00.825: INFO: stderr: ""
May 21 05:58:00.825: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 21 05:58:05.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4466 get pod e2e-test-httpd-pod -o json'
May 21 05:58:05.940: INFO: stderr: ""
May 21 05:58:05.940: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-05-21T05:58:00Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-21T05:58:00Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.200.66.20\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-21T05:58:02Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4466\",\n        \"resourceVersion\": \"12337\",\n        \"uid\": \"1e71f2f9-6606-416a-92df-cac323232e23\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-j28qm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"53f495cf-0382-4128-b4b3-0979b69a209b\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-j28qm\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-j28qm\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T05:58:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T05:58:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T05:58:02Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T05:58:00Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2823774b01a6f5373abd6459410e760f26ae7333d977f2c28ab2d01582d26a82\",\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-21T05:58:01Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"30.0.0.11\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.200.66.20\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.200.66.20\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-21T05:58:00Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 21 05:58:05.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4466 replace -f -'
May 21 05:58:06.227: INFO: stderr: ""
May 21 05:58:06.227: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
May 21 05:58:06.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4466 delete pods e2e-test-httpd-pod'
May 21 05:58:07.711: INFO: stderr: ""
May 21 05:58:07.711: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:07.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4466" for this suite.

• [SLOW TEST:7.003 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":59,"skipped":926,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:07.718: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-fb71be5e-cdde-4171-9a4c-36dcba589117
STEP: Creating a pod to test consume secrets
May 21 05:58:07.760: INFO: Waiting up to 5m0s for pod "pod-secrets-b6e06774-0bb6-4ccd-b05c-9a45d83b50cf" in namespace "secrets-5684" to be "Succeeded or Failed"
May 21 05:58:07.763: INFO: Pod "pod-secrets-b6e06774-0bb6-4ccd-b05c-9a45d83b50cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.236986ms
May 21 05:58:09.766: INFO: Pod "pod-secrets-b6e06774-0bb6-4ccd-b05c-9a45d83b50cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005708474s
STEP: Saw pod success
May 21 05:58:09.766: INFO: Pod "pod-secrets-b6e06774-0bb6-4ccd-b05c-9a45d83b50cf" satisfied condition "Succeeded or Failed"
May 21 05:58:09.768: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-secrets-b6e06774-0bb6-4ccd-b05c-9a45d83b50cf container secret-volume-test: <nil>
STEP: delete the pod
May 21 05:58:09.787: INFO: Waiting for pod pod-secrets-b6e06774-0bb6-4ccd-b05c-9a45d83b50cf to disappear
May 21 05:58:09.794: INFO: Pod pod-secrets-b6e06774-0bb6-4ccd-b05c-9a45d83b50cf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:09.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5684" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":930,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:09.799: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 05:58:10.336: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 05:58:13.349: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:58:13.352: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8621-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:14.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4489" for this suite.
STEP: Destroying namespace "webhook-4489-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":61,"skipped":942,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:14.573: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 05:58:14.608: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc80bcae-8841-4b04-bbf5-93a2051aadd8" in namespace "projected-5249" to be "Succeeded or Failed"
May 21 05:58:14.612: INFO: Pod "downwardapi-volume-bc80bcae-8841-4b04-bbf5-93a2051aadd8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.46472ms
May 21 05:58:16.615: INFO: Pod "downwardapi-volume-bc80bcae-8841-4b04-bbf5-93a2051aadd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007135313s
May 21 05:58:18.619: INFO: Pod "downwardapi-volume-bc80bcae-8841-4b04-bbf5-93a2051aadd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010705159s
STEP: Saw pod success
May 21 05:58:18.619: INFO: Pod "downwardapi-volume-bc80bcae-8841-4b04-bbf5-93a2051aadd8" satisfied condition "Succeeded or Failed"
May 21 05:58:18.621: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-bc80bcae-8841-4b04-bbf5-93a2051aadd8 container client-container: <nil>
STEP: delete the pod
May 21 05:58:18.637: INFO: Waiting for pod downwardapi-volume-bc80bcae-8841-4b04-bbf5-93a2051aadd8 to disappear
May 21 05:58:18.639: INFO: Pod downwardapi-volume-bc80bcae-8841-4b04-bbf5-93a2051aadd8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:18.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5249" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":62,"skipped":942,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:18.646: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
May 21 05:58:18.682: INFO: Waiting up to 5m0s for pod "client-containers-81f71028-0ccb-4758-90fa-39c031a9cecd" in namespace "containers-975" to be "Succeeded or Failed"
May 21 05:58:18.683: INFO: Pod "client-containers-81f71028-0ccb-4758-90fa-39c031a9cecd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.624172ms
May 21 05:58:20.687: INFO: Pod "client-containers-81f71028-0ccb-4758-90fa-39c031a9cecd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005124558s
May 21 05:58:22.691: INFO: Pod "client-containers-81f71028-0ccb-4758-90fa-39c031a9cecd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009497086s
STEP: Saw pod success
May 21 05:58:22.691: INFO: Pod "client-containers-81f71028-0ccb-4758-90fa-39c031a9cecd" satisfied condition "Succeeded or Failed"
May 21 05:58:22.693: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod client-containers-81f71028-0ccb-4758-90fa-39c031a9cecd container agnhost-container: <nil>
STEP: delete the pod
May 21 05:58:22.708: INFO: Waiting for pod client-containers-81f71028-0ccb-4758-90fa-39c031a9cecd to disappear
May 21 05:58:22.716: INFO: Pod client-containers-81f71028-0ccb-4758-90fa-39c031a9cecd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:22.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-975" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":956,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:22.723: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:58:22.763: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 21 05:58:27.766: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 05:58:27.766: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 05:58:27.792: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8692  e375b492-a0e1-4ef0-ad5a-6ff501fbc01d 12591 1 2021-05-21 05:58:27 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-05-21 05:58:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003370108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 21 05:58:27.812: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-8692  7f9d6be9-0a9b-4237-b540-88839dd5d6c1 12593 1 2021-05-21 05:58:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e375b492-a0e1-4ef0-ad5a-6ff501fbc01d 0xc003370677 0xc003370678}] []  [{kube-controller-manager Update apps/v1 2021-05-21 05:58:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e375b492-a0e1-4ef0-ad5a-6ff501fbc01d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003370708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 05:58:27.812: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 21 05:58:27.812: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8692  3cb02226-8041-43bb-bf58-22203297d0e3 12592 1 2021-05-21 05:58:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment e375b492-a0e1-4ef0-ad5a-6ff501fbc01d 0xc003370567 0xc003370568}] []  [{e2e.test Update apps/v1 2021-05-21 05:58:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 05:58:27 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"e375b492-a0e1-4ef0-ad5a-6ff501fbc01d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003370608 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 05:58:27.835: INFO: Pod "test-cleanup-controller-b4lr6" is available:
&Pod{ObjectMeta:{test-cleanup-controller-b4lr6 test-cleanup-controller- deployment-8692  f552683c-db15-44a9-9050-04cc6fa6a271 12584 0 2021-05-21 05:58:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 3cb02226-8041-43bb-bf58-22203297d0e3 0xc002e405f7 0xc002e405f8}] []  [{kube-controller-manager Update v1 2021-05-21 05:58:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3cb02226-8041-43bb-bf58-22203297d0e3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 05:58:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.88.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7m8tw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7m8tw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7m8tw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 05:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 05:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 05:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 05:58:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.88.73,StartTime:2021-05-21 05:58:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 05:58:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a0ef909aff2c8f19ab5021ebf61dd843afce3adc75feafde94b05bf18f0251ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.88.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 05:58:27.835: INFO: Pod "test-cleanup-deployment-685c4f8568-fxg4d" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-fxg4d test-cleanup-deployment-685c4f8568- deployment-8692  121ba132-80bc-49f8-9705-62748ead809c 12597 0 2021-05-21 05:58:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 7f9d6be9-0a9b-4237-b540-88839dd5d6c1 0xc002e407a7 0xc002e407a8}] []  [{kube-controller-manager Update v1 2021-05-21 05:58:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f9d6be9-0a9b-4237-b540-88839dd5d6c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7m8tw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7m8tw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7m8tw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 05:58:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:27.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8692" for this suite.

• [SLOW TEST:5.144 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":64,"skipped":965,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:27.869: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:27.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5983" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":1023,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:27.945: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
May 21 05:58:27.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-965 api-versions'
May 21 05:58:28.061: INFO: stderr: ""
May 21 05:58:28.061: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npksapi.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:28.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-965" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":66,"skipped":1036,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:28.068: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 21 05:58:28.871: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 21 05:58:30.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173508, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173508, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173508, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173508, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 05:58:33.887: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:58:33.891: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:35.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-473" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.135 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":67,"skipped":1039,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:35.203: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 05:58:35.840: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 05:58:37.856: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173515, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173515, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173515, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173515, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 05:58:40.866: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:40.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2620" for this suite.
STEP: Destroying namespace "webhook-2620-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.828 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":68,"skipped":1046,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:41.032: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 05:58:41.616: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 05:58:43.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173521, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173521, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173521, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757173521, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 05:58:46.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:56.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3420" for this suite.
STEP: Destroying namespace "webhook-3420-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:15.740 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":69,"skipped":1053,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:56.772: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-5dcb7e79-2030-4cbd-931d-c1db2a5ccc95
STEP: Creating a pod to test consume configMaps
May 21 05:58:56.810: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0c5be244-5919-4e08-b0d3-6c78705ee8b4" in namespace "projected-1432" to be "Succeeded or Failed"
May 21 05:58:56.832: INFO: Pod "pod-projected-configmaps-0c5be244-5919-4e08-b0d3-6c78705ee8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.189968ms
May 21 05:58:58.835: INFO: Pod "pod-projected-configmaps-0c5be244-5919-4e08-b0d3-6c78705ee8b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024643541s
STEP: Saw pod success
May 21 05:58:58.835: INFO: Pod "pod-projected-configmaps-0c5be244-5919-4e08-b0d3-6c78705ee8b4" satisfied condition "Succeeded or Failed"
May 21 05:58:58.837: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-configmaps-0c5be244-5919-4e08-b0d3-6c78705ee8b4 container agnhost-container: <nil>
STEP: delete the pod
May 21 05:58:58.850: INFO: Waiting for pod pod-projected-configmaps-0c5be244-5919-4e08-b0d3-6c78705ee8b4 to disappear
May 21 05:58:58.856: INFO: Pod pod-projected-configmaps-0c5be244-5919-4e08-b0d3-6c78705ee8b4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:58:58.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1432" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:58:58.864: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 05:58:59.530: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 05:59:02.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:59:02.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1632" for this suite.
STEP: Destroying namespace "webhook-1632-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":71,"skipped":1089,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:59:02.649: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-rg82
STEP: Creating a pod to test atomic-volume-subpath
May 21 05:59:02.699: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rg82" in namespace "subpath-6826" to be "Succeeded or Failed"
May 21 05:59:02.709: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Pending", Reason="", readiness=false. Elapsed: 10.786562ms
May 21 05:59:04.712: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013556601s
May 21 05:59:06.715: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Running", Reason="", readiness=true. Elapsed: 4.016676773s
May 21 05:59:08.719: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Running", Reason="", readiness=true. Elapsed: 6.020202891s
May 21 05:59:10.722: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Running", Reason="", readiness=true. Elapsed: 8.023656422s
May 21 05:59:12.725: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Running", Reason="", readiness=true. Elapsed: 10.026841497s
May 21 05:59:14.729: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Running", Reason="", readiness=true. Elapsed: 12.029987247s
May 21 05:59:16.732: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Running", Reason="", readiness=true. Elapsed: 14.033747281s
May 21 05:59:18.736: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Running", Reason="", readiness=true. Elapsed: 16.037404484s
May 21 05:59:20.739: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Running", Reason="", readiness=true. Elapsed: 18.040537313s
May 21 05:59:22.742: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Running", Reason="", readiness=true. Elapsed: 20.043529486s
May 21 05:59:24.745: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Running", Reason="", readiness=true. Elapsed: 22.046496667s
May 21 05:59:26.748: INFO: Pod "pod-subpath-test-configmap-rg82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.049757326s
STEP: Saw pod success
May 21 05:59:26.748: INFO: Pod "pod-subpath-test-configmap-rg82" satisfied condition "Succeeded or Failed"
May 21 05:59:26.750: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-subpath-test-configmap-rg82 container test-container-subpath-configmap-rg82: <nil>
STEP: delete the pod
May 21 05:59:26.766: INFO: Waiting for pod pod-subpath-test-configmap-rg82 to disappear
May 21 05:59:26.770: INFO: Pod pod-subpath-test-configmap-rg82 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rg82
May 21 05:59:26.770: INFO: Deleting pod "pod-subpath-test-configmap-rg82" in namespace "subpath-6826"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:59:26.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6826" for this suite.

• [SLOW TEST:24.136 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":72,"skipped":1095,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:59:26.786: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:59:26.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4461 create -f -'
May 21 05:59:27.077: INFO: stderr: ""
May 21 05:59:27.077: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 21 05:59:27.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4461 create -f -'
May 21 05:59:27.267: INFO: stderr: ""
May 21 05:59:27.267: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 05:59:28.274: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 05:59:28.275: INFO: Found 0 / 1
May 21 05:59:29.272: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 05:59:29.272: INFO: Found 1 / 1
May 21 05:59:29.272: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 21 05:59:29.273: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 05:59:29.274: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 05:59:29.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4461 describe pod agnhost-primary-rcz8r'
May 21 05:59:29.359: INFO: stderr: ""
May 21 05:59:29.359: INFO: stdout: "Name:         agnhost-primary-rcz8r\nNamespace:    kubectl-4461\nPriority:     0\nNode:         16c19df1-13dd-4e26-99a1-a192204dc796/30.0.0.12\nStart Time:   Fri, 21 May 2021 05:59:27 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.200.88.81\nIPs:\n  IP:           10.200.88.81\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://05a32be760fd059999be319fa7b5bc46b36f1da3d7add1356c0b869a4ca23ebe\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 21 May 2021 05:59:28 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-plxjc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-plxjc:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-plxjc\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4461/agnhost-primary-rcz8r to 16c19df1-13dd-4e26-99a1-a192204dc796\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May 21 05:59:29.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4461 describe rc agnhost-primary'
May 21 05:59:29.449: INFO: stderr: ""
May 21 05:59:29.449: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4461\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-rcz8r\n"
May 21 05:59:29.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4461 describe service agnhost-primary'
May 21 05:59:29.520: INFO: stderr: ""
May 21 05:59:29.520: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4461\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.100.200.42\nIPs:               10.100.200.42\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.200.88.81:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 21 05:59:29.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4461 describe node 16c19df1-13dd-4e26-99a1-a192204dc796'
May 21 05:59:29.656: INFO: stderr: ""
May 21 05:59:29.656: INFO: stdout: "Name:               16c19df1-13dd-4e26-99a1-a192204dc796\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    bosh.id=aadf90da-5745-4643-bf87-cb50884aba05\n                    bosh.zone=shepherd-333865-210520-200241-az\n                    failure-domain.beta.kubernetes.io/zone=shepherd-333865-210520-200241-az\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=30.0.0.12\n                    kubernetes.io/os=linux\n                    pks-system/cluster.name=a90c9c06-8b30-4e5a-9d58-9106e086ad47-internal\n                    pks-system/cluster.uuid=service-instance_2d28dae7-79dc-459c-8933-3909e6244ebc\n                    spec.ip=30.0.0.12\n                    topology.kubernetes.io/zone=shepherd-333865-210520-200241-az\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 21 May 2021 04:27:32 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  16c19df1-13dd-4e26-99a1-a192204dc796\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 21 May 2021 05:59:29 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 21 May 2021 05:57:09 +0000   Fri, 21 May 2021 04:27:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 21 May 2021 05:57:09 +0000   Fri, 21 May 2021 04:27:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 21 May 2021 05:57:09 +0000   Fri, 21 May 2021 04:27:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 21 May 2021 05:57:09 +0000   Fri, 21 May 2021 04:27:42 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  30.0.0.12\n  InternalIP:  30.0.0.12\n  Hostname:    30.0.0.12\nCapacity:\n  cpu:                2\n  ephemeral-storage:  32894832Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4039716Ki\n  pods:               110\nAllocatable:\n  cpu:                1500m\n  ephemeral-storage:  29242135298\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2479140Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 f5a50980187e49ecac2bd2b08401ba13\n  System UUID:                4220D98C-F7C3-42EC-0282-74063A02A0A5\n  Boot ID:                    1cac706e-0e3d-4886-9894-1a4f4f197743\n  Kernel Version:             4.15.0-142-generic\n  OS Image:                   Ubuntu 16.04.7 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.14\n  Kubelet Version:            v1.20.6+vmware.1\n  Kube-Proxy Version:         v1.20.6+vmware.1\nProviderID:                   vsphere://4220d98c-f7c3-42ec-0282-74063a02a0a5\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-6d5447c5f4-sdg2p                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (7%)     89m\n  kube-system                 metrics-server-7d476fdfbd-f88wj                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  kubectl-4461                agnhost-primary-rcz8r                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  pks-system                  fluent-bit-rkcnw                                           0 (0%)        0 (0%)      100Mi (4%)       100Mi (4%)     89m\n  pks-system                  node-exporter-gbfjt                                        10m (0%)      0 (0%)      50Mi (2%)        150Mi (6%)     89m\n  pks-system                  sink-controller-5bf464ff96-8nj6l                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  pks-system                  telegraf-hwv6v                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  pks-system                  wavefront-collector-5795f87754-rfj4n                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-b9nwr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                110m (7%)   0 (0%)\n  memory             220Mi (9%)  420Mi (17%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
May 21 05:59:29.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-4461 describe namespace kubectl-4461'
May 21 05:59:29.727: INFO: stderr: ""
May 21 05:59:29.727: INFO: stdout: "Name:         kubectl-4461\nLabels:       e2e-framework=kubectl\n              e2e-run=470995b3-d148-4053-a105-0c0200a6d845\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:59:29.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4461" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":73,"skipped":1102,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:59:29.733: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 05:59:29.759: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:59:35.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7090" for this suite.

• [SLOW TEST:5.785 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":74,"skipped":1114,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:59:35.518: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 21 05:59:35.567: INFO: observed Pod pod-test in namespace pods-9174 in phase Pending conditions []
May 21 05:59:35.567: INFO: observed Pod pod-test in namespace pods-9174 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 05:59:35 +0000 UTC  }]
May 21 05:59:35.589: INFO: observed Pod pod-test in namespace pods-9174 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 05:59:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 05:59:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 05:59:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 05:59:35 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 21 05:59:37.412: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 21 05:59:37.450: INFO: observed event type ADDED
May 21 05:59:37.450: INFO: observed event type MODIFIED
May 21 05:59:37.450: INFO: observed event type MODIFIED
May 21 05:59:37.450: INFO: observed event type MODIFIED
May 21 05:59:37.451: INFO: observed event type MODIFIED
May 21 05:59:37.451: INFO: observed event type MODIFIED
May 21 05:59:37.451: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:59:37.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9174" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":75,"skipped":1122,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:59:37.461: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-4788/configmap-test-6e75a44d-88a3-4c1c-bf58-41f4b787677f
STEP: Creating a pod to test consume configMaps
May 21 05:59:37.496: INFO: Waiting up to 5m0s for pod "pod-configmaps-ae59944a-9ff8-4565-a136-abb502549cb3" in namespace "configmap-4788" to be "Succeeded or Failed"
May 21 05:59:37.499: INFO: Pod "pod-configmaps-ae59944a-9ff8-4565-a136-abb502549cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.629198ms
May 21 05:59:39.503: INFO: Pod "pod-configmaps-ae59944a-9ff8-4565-a136-abb502549cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006933024s
May 21 05:59:41.508: INFO: Pod "pod-configmaps-ae59944a-9ff8-4565-a136-abb502549cb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011774446s
STEP: Saw pod success
May 21 05:59:41.508: INFO: Pod "pod-configmaps-ae59944a-9ff8-4565-a136-abb502549cb3" satisfied condition "Succeeded or Failed"
May 21 05:59:41.510: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-configmaps-ae59944a-9ff8-4565-a136-abb502549cb3 container env-test: <nil>
STEP: delete the pod
May 21 05:59:41.524: INFO: Waiting for pod pod-configmaps-ae59944a-9ff8-4565-a136-abb502549cb3 to disappear
May 21 05:59:41.540: INFO: Pod pod-configmaps-ae59944a-9ff8-4565-a136-abb502549cb3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:59:41.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4788" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:59:41.549: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-7f446cbf-75a1-495e-8fe4-5ca65d466b9e
STEP: Creating a pod to test consume configMaps
May 21 05:59:41.587: INFO: Waiting up to 5m0s for pod "pod-configmaps-4f2ffe47-e8a3-4ee6-a60d-6fd87b07edf5" in namespace "configmap-433" to be "Succeeded or Failed"
May 21 05:59:41.589: INFO: Pod "pod-configmaps-4f2ffe47-e8a3-4ee6-a60d-6fd87b07edf5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.924053ms
May 21 05:59:43.595: INFO: Pod "pod-configmaps-4f2ffe47-e8a3-4ee6-a60d-6fd87b07edf5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007467975s
May 21 05:59:45.599: INFO: Pod "pod-configmaps-4f2ffe47-e8a3-4ee6-a60d-6fd87b07edf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012431157s
STEP: Saw pod success
May 21 05:59:45.600: INFO: Pod "pod-configmaps-4f2ffe47-e8a3-4ee6-a60d-6fd87b07edf5" satisfied condition "Succeeded or Failed"
May 21 05:59:45.602: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-configmaps-4f2ffe47-e8a3-4ee6-a60d-6fd87b07edf5 container agnhost-container: <nil>
STEP: delete the pod
May 21 05:59:45.617: INFO: Waiting for pod pod-configmaps-4f2ffe47-e8a3-4ee6-a60d-6fd87b07edf5 to disappear
May 21 05:59:45.622: INFO: Pod pod-configmaps-4f2ffe47-e8a3-4ee6-a60d-6fd87b07edf5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:59:45.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-433" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:59:45.630: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 05:59:45.668: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d379952-c718-4db3-9168-2532f817f8f1" in namespace "projected-8812" to be "Succeeded or Failed"
May 21 05:59:45.679: INFO: Pod "downwardapi-volume-1d379952-c718-4db3-9168-2532f817f8f1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.747382ms
May 21 05:59:47.682: INFO: Pod "downwardapi-volume-1d379952-c718-4db3-9168-2532f817f8f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013708006s
May 21 05:59:49.690: INFO: Pod "downwardapi-volume-1d379952-c718-4db3-9168-2532f817f8f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021554642s
STEP: Saw pod success
May 21 05:59:49.690: INFO: Pod "downwardapi-volume-1d379952-c718-4db3-9168-2532f817f8f1" satisfied condition "Succeeded or Failed"
May 21 05:59:49.691: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-1d379952-c718-4db3-9168-2532f817f8f1 container client-container: <nil>
STEP: delete the pod
May 21 05:59:49.706: INFO: Waiting for pod downwardapi-volume-1d379952-c718-4db3-9168-2532f817f8f1 to disappear
May 21 05:59:49.723: INFO: Pod downwardapi-volume-1d379952-c718-4db3-9168-2532f817f8f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:59:49.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8812" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":78,"skipped":1210,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:59:49.729: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 05:59:50.391: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 05:59:53.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 05:59:53.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5766" for this suite.
STEP: Destroying namespace "webhook-5766-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":79,"skipped":1211,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 05:59:53.677: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 21 06:00:01.753: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 06:00:01.758: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 06:00:03.758: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 06:00:03.761: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 06:00:05.758: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 06:00:05.761: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 06:00:07.758: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 06:00:07.761: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 06:00:09.758: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 06:00:09.760: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 06:00:11.758: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 06:00:11.763: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 06:00:13.758: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 06:00:13.764: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:00:13.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-729" for this suite.

• [SLOW TEST:20.095 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":80,"skipped":1218,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:00:13.773: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-34c8a4fb-e6e3-45f5-91ad-6906500b3a0d
STEP: Creating a pod to test consume configMaps
May 21 06:00:13.816: INFO: Waiting up to 5m0s for pod "pod-configmaps-f06a1dff-870d-448f-a77f-4b01e4d834e8" in namespace "configmap-5011" to be "Succeeded or Failed"
May 21 06:00:13.825: INFO: Pod "pod-configmaps-f06a1dff-870d-448f-a77f-4b01e4d834e8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.579976ms
May 21 06:00:15.828: INFO: Pod "pod-configmaps-f06a1dff-870d-448f-a77f-4b01e4d834e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011353938s
STEP: Saw pod success
May 21 06:00:15.828: INFO: Pod "pod-configmaps-f06a1dff-870d-448f-a77f-4b01e4d834e8" satisfied condition "Succeeded or Failed"
May 21 06:00:15.829: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-configmaps-f06a1dff-870d-448f-a77f-4b01e4d834e8 container agnhost-container: <nil>
STEP: delete the pod
May 21 06:00:15.844: INFO: Waiting for pod pod-configmaps-f06a1dff-870d-448f-a77f-4b01e4d834e8 to disappear
May 21 06:00:15.848: INFO: Pod pod-configmaps-f06a1dff-870d-448f-a77f-4b01e4d834e8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:00:15.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5011" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":81,"skipped":1219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:00:15.862: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 21 06:00:15.902: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 06:01:15.928: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 21 06:01:15.945: INFO: Created pod: pod0-sched-preemption-low-priority
May 21 06:01:15.981: INFO: Created pod: pod1-sched-preemption-medium-priority
May 21 06:01:16.001: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:01:34.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-183" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:78.212 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":82,"skipped":1270,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:01:34.075: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-0acdad6e-58ad-4cee-b78e-8e4294ebaccd
STEP: Creating configMap with name cm-test-opt-upd-d0de5fb7-02d4-4d7d-91cd-1ddc1d401bc5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0acdad6e-58ad-4cee-b78e-8e4294ebaccd
STEP: Updating configmap cm-test-opt-upd-d0de5fb7-02d4-4d7d-91cd-1ddc1d401bc5
STEP: Creating configMap with name cm-test-opt-create-0e802b60-91d3-457a-bcb0-e6dd33f6aeb1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:01:38.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5315" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":83,"skipped":1287,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:01:38.198: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:01:38.238: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 21 06:01:38.258: INFO: Number of nodes with available pods: 0
May 21 06:01:38.258: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 06:01:39.264: INFO: Number of nodes with available pods: 0
May 21 06:01:39.264: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 06:01:40.263: INFO: Number of nodes with available pods: 3
May 21 06:01:40.263: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 21 06:01:40.284: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:40.284: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:40.284: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:41.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:41.291: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:41.291: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:42.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:42.291: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:42.291: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:43.295: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:43.295: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:43.295: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:43.295: INFO: Pod daemon-set-j8bs4 is not available
May 21 06:01:44.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:44.291: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:44.291: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:44.291: INFO: Pod daemon-set-j8bs4 is not available
May 21 06:01:45.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:45.291: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:45.291: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:45.291: INFO: Pod daemon-set-j8bs4 is not available
May 21 06:01:46.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:46.291: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:46.291: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:46.291: INFO: Pod daemon-set-j8bs4 is not available
May 21 06:01:47.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:47.291: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:47.291: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:47.291: INFO: Pod daemon-set-j8bs4 is not available
May 21 06:01:48.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:48.291: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:48.291: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:48.291: INFO: Pod daemon-set-j8bs4 is not available
May 21 06:01:49.292: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:49.292: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:49.292: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:49.292: INFO: Pod daemon-set-j8bs4 is not available
May 21 06:01:50.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:50.291: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:50.291: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:50.291: INFO: Pod daemon-set-j8bs4 is not available
May 21 06:01:51.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:51.291: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:51.291: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:51.291: INFO: Pod daemon-set-j8bs4 is not available
May 21 06:01:52.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:52.291: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:52.291: INFO: Wrong image for pod: daemon-set-j8bs4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:52.291: INFO: Pod daemon-set-j8bs4 is not available
May 21 06:01:53.290: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:53.290: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:53.290: INFO: Pod daemon-set-hxs27 is not available
May 21 06:01:54.290: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:54.290: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:55.290: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:55.290: INFO: Wrong image for pod: daemon-set-f7655. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:55.290: INFO: Pod daemon-set-f7655 is not available
May 21 06:01:56.297: INFO: Pod daemon-set-96t6l is not available
May 21 06:01:56.297: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:57.293: INFO: Pod daemon-set-96t6l is not available
May 21 06:01:57.293: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:58.291: INFO: Pod daemon-set-96t6l is not available
May 21 06:01:58.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:59.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:01:59.291: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:00.290: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:00.290: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:01.290: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:01.290: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:02.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:02.291: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:03.290: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:03.290: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:04.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:04.291: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:05.293: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:05.293: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:06.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:06.291: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:07.292: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:07.292: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:08.291: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:08.291: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:09.292: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:09.292: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:10.290: INFO: Wrong image for pod: daemon-set-cfjws. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 06:02:10.290: INFO: Pod daemon-set-cfjws is not available
May 21 06:02:11.290: INFO: Pod daemon-set-fk4wj is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 21 06:02:11.298: INFO: Number of nodes with available pods: 2
May 21 06:02:11.298: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:02:12.304: INFO: Number of nodes with available pods: 2
May 21 06:02:12.304: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:02:13.304: INFO: Number of nodes with available pods: 3
May 21 06:02:13.304: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7403, will wait for the garbage collector to delete the pods
May 21 06:02:13.368: INFO: Deleting DaemonSet.extensions daemon-set took: 3.461403ms
May 21 06:02:13.969: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.131871ms
May 21 06:02:23.671: INFO: Number of nodes with available pods: 0
May 21 06:02:23.671: INFO: Number of running nodes: 0, number of available pods: 0
May 21 06:02:23.673: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14042"},"items":null}

May 21 06:02:23.674: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14042"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:02:23.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7403" for this suite.

• [SLOW TEST:45.489 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":84,"skipped":1288,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:02:23.688: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 21 06:02:27.752: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 06:02:27.754: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 06:02:29.754: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 06:02:29.758: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 06:02:31.754: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 06:02:31.758: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 06:02:33.754: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 06:02:33.758: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 06:02:35.754: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 06:02:35.758: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 06:02:37.754: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 06:02:37.758: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 06:02:39.754: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 06:02:39.758: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 06:02:41.754: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 06:02:41.758: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 06:02:43.754: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 06:02:43.758: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:02:43.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5726" for this suite.

• [SLOW TEST:20.089 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:02:43.777: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-a59f6839-6652-4304-bf3d-fe2bec83663a
STEP: Creating a pod to test consume configMaps
May 21 06:02:43.824: INFO: Waiting up to 5m0s for pod "pod-configmaps-f9c077e4-0b47-4227-bdeb-448a3e8f6269" in namespace "configmap-4924" to be "Succeeded or Failed"
May 21 06:02:43.825: INFO: Pod "pod-configmaps-f9c077e4-0b47-4227-bdeb-448a3e8f6269": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573308ms
May 21 06:02:45.829: INFO: Pod "pod-configmaps-f9c077e4-0b47-4227-bdeb-448a3e8f6269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004987047s
STEP: Saw pod success
May 21 06:02:45.829: INFO: Pod "pod-configmaps-f9c077e4-0b47-4227-bdeb-448a3e8f6269" satisfied condition "Succeeded or Failed"
May 21 06:02:45.831: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-configmaps-f9c077e4-0b47-4227-bdeb-448a3e8f6269 container agnhost-container: <nil>
STEP: delete the pod
May 21 06:02:45.842: INFO: Waiting for pod pod-configmaps-f9c077e4-0b47-4227-bdeb-448a3e8f6269 to disappear
May 21 06:02:45.849: INFO: Pod pod-configmaps-f9c077e4-0b47-4227-bdeb-448a3e8f6269 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:02:45.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4924" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":86,"skipped":1377,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:02:45.856: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 21 06:02:46.189: INFO: Pod name wrapped-volume-race-0b399274-d53e-43b8-9e58-b5a7b6e65168: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0b399274-d53e-43b8-9e58-b5a7b6e65168 in namespace emptydir-wrapper-5602, will wait for the garbage collector to delete the pods
May 21 06:03:02.288: INFO: Deleting ReplicationController wrapped-volume-race-0b399274-d53e-43b8-9e58-b5a7b6e65168 took: 7.297151ms
May 21 06:03:02.988: INFO: Terminating ReplicationController wrapped-volume-race-0b399274-d53e-43b8-9e58-b5a7b6e65168 pods took: 700.183618ms
STEP: Creating RC which spawns configmap-volume pods
May 21 06:03:13.812: INFO: Pod name wrapped-volume-race-279e9943-d86d-44c6-99c6-c7ca49fe28b7: Found 0 pods out of 5
May 21 06:03:18.819: INFO: Pod name wrapped-volume-race-279e9943-d86d-44c6-99c6-c7ca49fe28b7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-279e9943-d86d-44c6-99c6-c7ca49fe28b7 in namespace emptydir-wrapper-5602, will wait for the garbage collector to delete the pods
May 21 06:03:28.893: INFO: Deleting ReplicationController wrapped-volume-race-279e9943-d86d-44c6-99c6-c7ca49fe28b7 took: 5.248384ms
May 21 06:03:29.493: INFO: Terminating ReplicationController wrapped-volume-race-279e9943-d86d-44c6-99c6-c7ca49fe28b7 pods took: 600.164609ms
STEP: Creating RC which spawns configmap-volume pods
May 21 06:03:41.013: INFO: Pod name wrapped-volume-race-c8ac1d3a-e15e-4dfe-b5eb-c76136cd7c9e: Found 0 pods out of 5
May 21 06:03:46.021: INFO: Pod name wrapped-volume-race-c8ac1d3a-e15e-4dfe-b5eb-c76136cd7c9e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c8ac1d3a-e15e-4dfe-b5eb-c76136cd7c9e in namespace emptydir-wrapper-5602, will wait for the garbage collector to delete the pods
May 21 06:03:56.096: INFO: Deleting ReplicationController wrapped-volume-race-c8ac1d3a-e15e-4dfe-b5eb-c76136cd7c9e took: 5.914975ms
May 21 06:03:56.796: INFO: Terminating ReplicationController wrapped-volume-race-c8ac1d3a-e15e-4dfe-b5eb-c76136cd7c9e pods took: 700.38328ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:04:03.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5602" for this suite.

• [SLOW TEST:78.040 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":87,"skipped":1380,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:04:03.897: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:04:03.927: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 21 06:04:05.952: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:04:06.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1590" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":88,"skipped":1382,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:04:06.963: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:04:06.992: INFO: Creating deployment "webserver-deployment"
May 21 06:04:06.999: INFO: Waiting for observed generation 1
May 21 06:04:09.009: INFO: Waiting for all required pods to come up
May 21 06:04:09.015: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 21 06:04:11.036: INFO: Waiting for deployment "webserver-deployment" to complete
May 21 06:04:11.039: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 21 06:04:11.046: INFO: Updating deployment webserver-deployment
May 21 06:04:11.046: INFO: Waiting for observed generation 2
May 21 06:04:13.058: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 21 06:04:13.060: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 21 06:04:13.061: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 21 06:04:13.076: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 21 06:04:13.076: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 21 06:04:13.078: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 21 06:04:13.080: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 21 06:04:13.080: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 21 06:04:13.086: INFO: Updating deployment webserver-deployment
May 21 06:04:13.086: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 21 06:04:13.090: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 21 06:04:13.098: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 06:04:13.131: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2162  08acf04c-eb84-4a5b-99bf-c29519cb2e4d 15431 3 2021-05-21 06:04:06 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 06:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0068ca0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-21 06:04:11 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-21 06:04:13 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 21 06:04:13.167: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-2162  e0d474bc-3305-4a57-b402-f219b8b6f8fe 15418 3 2021-05-21 06:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 08acf04c-eb84-4a5b-99bf-c29519cb2e4d 0xc0046834d7 0xc0046834d8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08acf04c-eb84-4a5b-99bf-c29519cb2e4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004683558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 06:04:13.167: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 21 06:04:13.168: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-867f44f6fb  deployment-2162  82815475-2606-4383-84c3-64ebfad83e18 15415 3 2021-05-21 06:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 08acf04c-eb84-4a5b-99bf-c29519cb2e4d 0xc0046835b7 0xc0046835b8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 06:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08acf04c-eb84-4a5b-99bf-c29519cb2e4d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 867f44f6fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004683628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 21 06:04:13.250: INFO: Pod "webserver-deployment-795d758f88-6hb4x" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6hb4x webserver-deployment-795d758f88- deployment-2162  77722bd9-2c1d-4981-af33-8e7084c2ce11 15340 0 2021-05-21 06:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0d474bc-3305-4a57-b402-f219b8b6f8fe 0xc004683ab7 0xc004683ab8}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0d474bc-3305-4a57-b402-f219b8b6f8fe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:973ef262-7e5e-4fd5-8bf3-6139f838e19b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.10,PodIP:,StartTime:2021-05-21 06:04:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.250: INFO: Pod "webserver-deployment-795d758f88-bcjj6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bcjj6 webserver-deployment-795d758f88- deployment-2162  9625905d-2563-4c8f-98cd-d6705ffdd20d 15445 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0d474bc-3305-4a57-b402-f219b8b6f8fe 0xc004683c50 0xc004683c51}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0d474bc-3305-4a57-b402-f219b8b6f8fe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.250: INFO: Pod "webserver-deployment-795d758f88-fbh4p" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fbh4p webserver-deployment-795d758f88- deployment-2162  3670eab5-de39-4177-9a73-08402d71d6a6 15371 0 2021-05-21 06:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0d474bc-3305-4a57-b402-f219b8b6f8fe 0xc004683d67 0xc004683d68}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0d474bc-3305-4a57-b402-f219b8b6f8fe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:53f495cf-0382-4128-b4b3-0979b69a209b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:,StartTime:2021-05-21 06:04:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.251: INFO: Pod "webserver-deployment-795d758f88-m5mw9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m5mw9 webserver-deployment-795d758f88- deployment-2162  cf6709aa-6c86-4345-818b-2fe17ed89ed8 15345 0 2021-05-21 06:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0d474bc-3305-4a57-b402-f219b8b6f8fe 0xc004683f00 0xc004683f01}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0d474bc-3305-4a57-b402-f219b8b6f8fe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:53f495cf-0382-4128-b4b3-0979b69a209b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:,StartTime:2021-05-21 06:04:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.251: INFO: Pod "webserver-deployment-795d758f88-rtwcv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rtwcv webserver-deployment-795d758f88- deployment-2162  3c6dec19-cc34-4ec2-86fe-7a449bb82935 15344 0 2021-05-21 06:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0d474bc-3305-4a57-b402-f219b8b6f8fe 0xc0039b6090 0xc0039b6091}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0d474bc-3305-4a57-b402-f219b8b6f8fe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:,StartTime:2021-05-21 06:04:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.251: INFO: Pod "webserver-deployment-795d758f88-sqj2b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sqj2b webserver-deployment-795d758f88- deployment-2162  6e66abfd-f5d0-4bda-8ac4-0dd0190c3599 15367 0 2021-05-21 06:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0d474bc-3305-4a57-b402-f219b8b6f8fe 0xc0039b6220 0xc0039b6221}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0d474bc-3305-4a57-b402-f219b8b6f8fe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:973ef262-7e5e-4fd5-8bf3-6139f838e19b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.10,PodIP:,StartTime:2021-05-21 06:04:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.251: INFO: Pod "webserver-deployment-867f44f6fb-52zdx" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-52zdx webserver-deployment-867f44f6fb- deployment-2162  1fcee5e8-a948-4bd8-93a1-e19f934a47c1 15437 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b63b0 0xc0039b63b1}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.251: INFO: Pod "webserver-deployment-867f44f6fb-5cwh7" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-5cwh7 webserver-deployment-867f44f6fb- deployment-2162  5bd1a78f-3813-41a8-9801-56209b9188ea 15426 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b64b7 0xc0039b64b8}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.251: INFO: Pod "webserver-deployment-867f44f6fb-6r64c" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-6r64c webserver-deployment-867f44f6fb- deployment-2162  3dacc498-18b8-4a86-bcf9-08f71772f26d 15438 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b65d0 0xc0039b65d1}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.252: INFO: Pod "webserver-deployment-867f44f6fb-76fjg" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-76fjg webserver-deployment-867f44f6fb- deployment-2162  05f0f567-a6a3-484a-9fb2-862e9b484d9d 15441 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b66d7 0xc0039b66d8}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:53f495cf-0382-4128-b4b3-0979b69a209b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.252: INFO: Pod "webserver-deployment-867f44f6fb-9kfsj" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-9kfsj webserver-deployment-867f44f6fb- deployment-2162  e7405aa2-1196-4e69-860e-424cfe4524e8 15433 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b67f0 0xc0039b67f1}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:973ef262-7e5e-4fd5-8bf3-6139f838e19b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.10,PodIP:,StartTime:2021-05-21 06:04:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.252: INFO: Pod "webserver-deployment-867f44f6fb-cftfq" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-cftfq webserver-deployment-867f44f6fb- deployment-2162  6dfba7ca-2759-4f93-bb8b-7c5380fe7143 15440 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b6960 0xc0039b6961}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.252: INFO: Pod "webserver-deployment-867f44f6fb-cktv4" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-cktv4 webserver-deployment-867f44f6fb- deployment-2162  504c1bee-e0ac-455e-a6cc-db068e19abdb 15079 0 2021-05-21 06:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b6a67 0xc0039b6a68}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.88.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.88.98,StartTime:2021-05-21 06:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 06:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c0847776589319f70382c49638b95656331370febc3943378e69ba3ac09fb437,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.88.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.253: INFO: Pod "webserver-deployment-867f44f6fb-d656z" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-d656z webserver-deployment-867f44f6fb- deployment-2162  2f24d982-4d73-4a48-a074-a8149f153349 15097 0 2021-05-21 06:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b6c00 0xc0039b6c01}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.34.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:973ef262-7e5e-4fd5-8bf3-6139f838e19b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.10,PodIP:10.200.34.26,StartTime:2021-05-21 06:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 06:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://faaad621581c5ae88cb2b8d0a5e3489294254708fc3327d3c25459ace27a801f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.34.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.253: INFO: Pod "webserver-deployment-867f44f6fb-g2fqr" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-g2fqr webserver-deployment-867f44f6fb- deployment-2162  68a60e58-2630-43ae-bfb6-4ee42988304b 15090 0 2021-05-21 06:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b6d90 0xc0039b6d91}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.66.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:53f495cf-0382-4128-b4b3-0979b69a209b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:10.200.66.36,StartTime:2021-05-21 06:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 06:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://82da4483627714140c598e6b4d91b6526648179ac99a3f9f0e221e3b0bfdff97,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.66.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.253: INFO: Pod "webserver-deployment-867f44f6fb-gfjq7" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-gfjq7 webserver-deployment-867f44f6fb- deployment-2162  3aa2deb7-6c8d-4554-964c-2efe52e99d8e 15221 0 2021-05-21 06:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b6f20 0xc0039b6f21}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.66.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:53f495cf-0382-4128-b4b3-0979b69a209b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:10.200.66.37,StartTime:2021-05-21 06:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 06:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://80a9029982baf70cae9eee3858b9dba236668361d817e11f0b9acd7a1589cece,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.66.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.253: INFO: Pod "webserver-deployment-867f44f6fb-ghwpw" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-ghwpw webserver-deployment-867f44f6fb- deployment-2162  4c5acda1-c2ca-477d-b7ee-0c7a7a0405f1 15435 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b70b0 0xc0039b70b1}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.253: INFO: Pod "webserver-deployment-867f44f6fb-l757g" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-l757g webserver-deployment-867f44f6fb- deployment-2162  bd6192be-b549-4528-a314-37cf9c526d1b 15439 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b71b7 0xc0039b71b8}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.253: INFO: Pod "webserver-deployment-867f44f6fb-ldskz" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-ldskz webserver-deployment-867f44f6fb- deployment-2162  37c34e8b-a011-4334-b35d-1791132ab927 15430 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b72b7 0xc0039b72b8}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:973ef262-7e5e-4fd5-8bf3-6139f838e19b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.253: INFO: Pod "webserver-deployment-867f44f6fb-mpk5g" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-mpk5g webserver-deployment-867f44f6fb- deployment-2162  bb642352-0852-4270-ab97-0d52c15966ab 15429 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b73d0 0xc0039b73d1}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:53f495cf-0382-4128-b4b3-0979b69a209b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.253: INFO: Pod "webserver-deployment-867f44f6fb-nbckr" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-nbckr webserver-deployment-867f44f6fb- deployment-2162  7aa2c051-87ac-4501-9c37-a1f5f3aaf1ca 15443 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b74f0 0xc0039b74f1}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.254: INFO: Pod "webserver-deployment-867f44f6fb-nxblh" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-nxblh webserver-deployment-867f44f6fb- deployment-2162  1ed841de-bdc2-4741-8862-550209e125c2 15210 0 2021-05-21 06:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b7610 0xc0039b7611}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.88.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.88.99,StartTime:2021-05-21 06:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 06:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a7a96be0afbac61d4392ebac6bb595fcbfe245d3003690503a38adaa6b1511c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.88.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.254: INFO: Pod "webserver-deployment-867f44f6fb-q7sq4" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-q7sq4 webserver-deployment-867f44f6fb- deployment-2162  e1320eae-14eb-42ed-8acf-3a230a1ec6fc 15205 0 2021-05-21 06:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b77a0 0xc0039b77a1}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.88.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.88.100,StartTime:2021-05-21 06:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 06:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ac6d7a7f4a11ec135fb0145115434bb1fe63f786ef95ed0bd5918580d9cce751,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.88.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.254: INFO: Pod "webserver-deployment-867f44f6fb-tpwqj" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-tpwqj webserver-deployment-867f44f6fb- deployment-2162  d276b7fc-a694-4751-b561-7167b41aa2a3 15252 0 2021-05-21 06:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b7930 0xc0039b7931}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.34.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:973ef262-7e5e-4fd5-8bf3-6139f838e19b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.10,PodIP:10.200.34.28,StartTime:2021-05-21 06:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 06:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2e0ab17411cea0e91ce9b5c41b45e1122ad87642b0375d3fe64ee2513736c477,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.34.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.255: INFO: Pod "webserver-deployment-867f44f6fb-vwdg4" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-vwdg4 webserver-deployment-867f44f6fb- deployment-2162  7149fedf-ac4f-4ed2-9d07-1ac03c66ee80 15225 0 2021-05-21 06:04:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b7ac0 0xc0039b7ac1}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:04:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.66.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:53f495cf-0382-4128-b4b3-0979b69a209b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:10.200.66.38,StartTime:2021-05-21 06:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 06:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://eb4a2b48a1f511a8d69f96b704588a8a3b0a811ed5db2fe77ad262570967729e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.66.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:04:13.255: INFO: Pod "webserver-deployment-867f44f6fb-x49nw" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-x49nw webserver-deployment-867f44f6fb- deployment-2162  604f494b-47b0-47ef-8b02-fe5e00eb7360 15436 0 2021-05-21 06:04:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 82815475-2606-4383-84c3-64ebfad83e18 0xc0039b7c50 0xc0039b7c51}] []  [{kube-controller-manager Update v1 2021-05-21 06:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82815475-2606-4383-84c3-64ebfad83e18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vn2fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vn2fx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vn2fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:973ef262-7e5e-4fd5-8bf3-6139f838e19b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:04:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:04:13.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2162" for this suite.

• [SLOW TEST:6.413 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":89,"skipped":1388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:04:13.376: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 21 06:04:13.516: INFO: Waiting up to 5m0s for pod "downward-api-7100ccb3-aa80-471a-8235-7a2cd32f9dac" in namespace "downward-api-2113" to be "Succeeded or Failed"
May 21 06:04:13.525: INFO: Pod "downward-api-7100ccb3-aa80-471a-8235-7a2cd32f9dac": Phase="Pending", Reason="", readiness=false. Elapsed: 9.575985ms
May 21 06:04:15.529: INFO: Pod "downward-api-7100ccb3-aa80-471a-8235-7a2cd32f9dac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013148073s
May 21 06:04:17.532: INFO: Pod "downward-api-7100ccb3-aa80-471a-8235-7a2cd32f9dac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016606752s
STEP: Saw pod success
May 21 06:04:17.532: INFO: Pod "downward-api-7100ccb3-aa80-471a-8235-7a2cd32f9dac" satisfied condition "Succeeded or Failed"
May 21 06:04:17.534: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downward-api-7100ccb3-aa80-471a-8235-7a2cd32f9dac container dapi-container: <nil>
STEP: delete the pod
May 21 06:04:17.945: INFO: Waiting for pod downward-api-7100ccb3-aa80-471a-8235-7a2cd32f9dac to disappear
May 21 06:04:17.952: INFO: Pod downward-api-7100ccb3-aa80-471a-8235-7a2cd32f9dac no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:04:17.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2113" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1439,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:04:17.957: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-2tnv
STEP: Creating a pod to test atomic-volume-subpath
May 21 06:04:18.014: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2tnv" in namespace "subpath-4521" to be "Succeeded or Failed"
May 21 06:04:18.025: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Pending", Reason="", readiness=false. Elapsed: 11.000508ms
May 21 06:04:20.028: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Running", Reason="", readiness=true. Elapsed: 2.01404912s
May 21 06:04:22.031: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Running", Reason="", readiness=true. Elapsed: 4.017326916s
May 21 06:04:24.034: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Running", Reason="", readiness=true. Elapsed: 6.020843811s
May 21 06:04:26.038: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Running", Reason="", readiness=true. Elapsed: 8.0242827s
May 21 06:04:28.041: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Running", Reason="", readiness=true. Elapsed: 10.027391429s
May 21 06:04:30.044: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Running", Reason="", readiness=true. Elapsed: 12.030440018s
May 21 06:04:32.048: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Running", Reason="", readiness=true. Elapsed: 14.033858578s
May 21 06:04:34.050: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Running", Reason="", readiness=true. Elapsed: 16.03678635s
May 21 06:04:36.053: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Running", Reason="", readiness=true. Elapsed: 18.039763577s
May 21 06:04:38.056: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Running", Reason="", readiness=true. Elapsed: 20.042792558s
May 21 06:04:40.059: INFO: Pod "pod-subpath-test-downwardapi-2tnv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045673737s
STEP: Saw pod success
May 21 06:04:40.059: INFO: Pod "pod-subpath-test-downwardapi-2tnv" satisfied condition "Succeeded or Failed"
May 21 06:04:40.061: INFO: Trying to get logs from node 973ef262-7e5e-4fd5-8bf3-6139f838e19b pod pod-subpath-test-downwardapi-2tnv container test-container-subpath-downwardapi-2tnv: <nil>
STEP: delete the pod
May 21 06:04:40.081: INFO: Waiting for pod pod-subpath-test-downwardapi-2tnv to disappear
May 21 06:04:40.089: INFO: Pod pod-subpath-test-downwardapi-2tnv no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2tnv
May 21 06:04:40.089: INFO: Deleting pod "pod-subpath-test-downwardapi-2tnv" in namespace "subpath-4521"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:04:40.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4521" for this suite.

• [SLOW TEST:22.138 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":91,"skipped":1444,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:04:40.097: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 06:04:40.148: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 21 06:04:40.150: INFO: starting watch
STEP: patching
STEP: updating
May 21 06:04:40.169: INFO: waiting for watch events with expected annotations
May 21 06:04:40.169: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:04:40.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5063" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":92,"skipped":1453,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:04:40.214: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 21 06:04:40.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 create -f -'
May 21 06:04:40.496: INFO: stderr: ""
May 21 06:04:40.497: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 06:04:40.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 06:04:40.579: INFO: stderr: ""
May 21 06:04:40.579: INFO: stdout: "update-demo-nautilus-65rqg update-demo-nautilus-gkljz "
May 21 06:04:40.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get pods update-demo-nautilus-65rqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 06:04:40.659: INFO: stderr: ""
May 21 06:04:40.659: INFO: stdout: ""
May 21 06:04:40.659: INFO: update-demo-nautilus-65rqg is created but not running
May 21 06:04:45.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 06:04:45.729: INFO: stderr: ""
May 21 06:04:45.729: INFO: stdout: "update-demo-nautilus-65rqg update-demo-nautilus-gkljz "
May 21 06:04:45.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get pods update-demo-nautilus-65rqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 06:04:45.793: INFO: stderr: ""
May 21 06:04:45.793: INFO: stdout: "true"
May 21 06:04:45.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get pods update-demo-nautilus-65rqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 06:04:45.865: INFO: stderr: ""
May 21 06:04:45.865: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 06:04:45.865: INFO: validating pod update-demo-nautilus-65rqg
May 21 06:04:45.868: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 06:04:45.869: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 06:04:45.869: INFO: update-demo-nautilus-65rqg is verified up and running
May 21 06:04:45.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get pods update-demo-nautilus-gkljz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 06:04:45.937: INFO: stderr: ""
May 21 06:04:45.937: INFO: stdout: "true"
May 21 06:04:45.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get pods update-demo-nautilus-gkljz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 06:04:46.010: INFO: stderr: ""
May 21 06:04:46.010: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 06:04:46.010: INFO: validating pod update-demo-nautilus-gkljz
May 21 06:04:46.013: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 06:04:46.013: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 06:04:46.014: INFO: update-demo-nautilus-gkljz is verified up and running
STEP: using delete to clean up resources
May 21 06:04:46.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 delete --grace-period=0 --force -f -'
May 21 06:04:46.082: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 06:04:46.082: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 21 06:04:46.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get rc,svc -l name=update-demo --no-headers'
May 21 06:04:46.151: INFO: stderr: "No resources found in kubectl-3403 namespace.\n"
May 21 06:04:46.151: INFO: stdout: ""
May 21 06:04:46.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 06:04:46.218: INFO: stderr: ""
May 21 06:04:46.218: INFO: stdout: "update-demo-nautilus-65rqg\nupdate-demo-nautilus-gkljz\n"
May 21 06:04:46.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get rc,svc -l name=update-demo --no-headers'
May 21 06:04:46.827: INFO: stderr: "No resources found in kubectl-3403 namespace.\n"
May 21 06:04:46.827: INFO: stdout: ""
May 21 06:04:46.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3403 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 06:04:46.930: INFO: stderr: ""
May 21 06:04:46.930: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:04:46.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3403" for this suite.

• [SLOW TEST:6.725 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":93,"skipped":1464,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:04:46.940: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
May 21 06:04:49.494: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2353 pod-service-account-d70b9176-8828-483e-abdc-b885ab071396 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 21 06:04:49.680: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2353 pod-service-account-d70b9176-8828-483e-abdc-b885ab071396 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 21 06:04:49.836: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2353 pod-service-account-d70b9176-8828-483e-abdc-b885ab071396 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:04:49.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2353" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":94,"skipped":1472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:04:49.992: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 21 06:04:50.028: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 06:05:50.053: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:05:50.055: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:05:50.109: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 21 06:05:50.111: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:05:50.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6218" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:05:50.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5725" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.209 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":95,"skipped":1500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:05:50.203: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 06:05:52.254: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:05:52.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-66" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":96,"skipped":1562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:05:52.276: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:05:52.342: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"059ea25e-1b79-4234-8dd7-3a69ef8aa2a3", Controller:(*bool)(0xc0044fea96), BlockOwnerDeletion:(*bool)(0xc0044fea97)}}
May 21 06:05:52.352: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9fe6c460-40de-4901-a1f1-a72465b12c5c", Controller:(*bool)(0xc0044fec66), BlockOwnerDeletion:(*bool)(0xc0044fec67)}}
May 21 06:05:52.361: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c55904db-4d99-4fc4-b8c4-d4e2e699b116", Controller:(*bool)(0xc00409c74e), BlockOwnerDeletion:(*bool)(0xc00409c74f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:05:57.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6194" for this suite.

• [SLOW TEST:5.124 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":97,"skipped":1618,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:05:57.400: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8718
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8718
I0521 06:05:57.479282      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8718, replica count: 2
May 21 06:06:00.529: INFO: Creating new exec pod
I0521 06:06:00.529524      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:06:03.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-8718 exec execpods6vtl -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 21 06:06:03.757: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 21 06:06:03.757: INFO: stdout: ""
May 21 06:06:03.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-8718 exec execpods6vtl -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.69 80'
May 21 06:06:03.925: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.69 80\nConnection to 10.100.200.69 80 port [tcp/http] succeeded!\n"
May 21 06:06:03.925: INFO: stdout: ""
May 21 06:06:03.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-8718 exec execpods6vtl -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30577'
May 21 06:06:04.096: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30577\nConnection to 30.0.0.11 30577 port [tcp/30577] succeeded!\n"
May 21 06:06:04.096: INFO: stdout: ""
May 21 06:06:04.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-8718 exec execpods6vtl -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 30577'
May 21 06:06:04.282: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 30577\nConnection to 30.0.0.10 30577 port [tcp/30577] succeeded!\n"
May 21 06:06:04.282: INFO: stdout: ""
May 21 06:06:04.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-8718 exec execpods6vtl -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30577'
May 21 06:06:04.453: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30577\nConnection to 30.0.0.11 30577 port [tcp/30577] succeeded!\n"
May 21 06:06:04.453: INFO: stdout: ""
May 21 06:06:04.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-8718 exec execpods6vtl -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 30577'
May 21 06:06:04.617: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 30577\nConnection to 30.0.0.10 30577 port [tcp/30577] succeeded!\n"
May 21 06:06:04.617: INFO: stdout: ""
May 21 06:06:04.617: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:06:04.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8718" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.239 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":98,"skipped":1630,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:06:04.639: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:06:04.689: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed1f9226-f754-4a9c-8fd3-4207cc9852ea" in namespace "downward-api-106" to be "Succeeded or Failed"
May 21 06:06:04.693: INFO: Pod "downwardapi-volume-ed1f9226-f754-4a9c-8fd3-4207cc9852ea": Phase="Pending", Reason="", readiness=false. Elapsed: 3.889063ms
May 21 06:06:06.698: INFO: Pod "downwardapi-volume-ed1f9226-f754-4a9c-8fd3-4207cc9852ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008053042s
STEP: Saw pod success
May 21 06:06:06.698: INFO: Pod "downwardapi-volume-ed1f9226-f754-4a9c-8fd3-4207cc9852ea" satisfied condition "Succeeded or Failed"
May 21 06:06:06.700: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-ed1f9226-f754-4a9c-8fd3-4207cc9852ea container client-container: <nil>
STEP: delete the pod
May 21 06:06:06.720: INFO: Waiting for pod downwardapi-volume-ed1f9226-f754-4a9c-8fd3-4207cc9852ea to disappear
May 21 06:06:06.726: INFO: Pod downwardapi-volume-ed1f9226-f754-4a9c-8fd3-4207cc9852ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:06:06.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-106" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":99,"skipped":1635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:06:06.734: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:06:06.769: INFO: Creating ReplicaSet my-hostname-basic-8a76a4df-12f3-499a-afc0-01216af8a318
May 21 06:06:06.778: INFO: Pod name my-hostname-basic-8a76a4df-12f3-499a-afc0-01216af8a318: Found 0 pods out of 1
May 21 06:06:11.783: INFO: Pod name my-hostname-basic-8a76a4df-12f3-499a-afc0-01216af8a318: Found 1 pods out of 1
May 21 06:06:11.783: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8a76a4df-12f3-499a-afc0-01216af8a318" is running
May 21 06:06:11.785: INFO: Pod "my-hostname-basic-8a76a4df-12f3-499a-afc0-01216af8a318-rpq5g" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 06:06:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 06:06:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 06:06:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 06:06:06 +0000 UTC Reason: Message:}])
May 21 06:06:11.786: INFO: Trying to dial the pod
May 21 06:06:16.795: INFO: Controller my-hostname-basic-8a76a4df-12f3-499a-afc0-01216af8a318: Got expected result from replica 1 [my-hostname-basic-8a76a4df-12f3-499a-afc0-01216af8a318-rpq5g]: "my-hostname-basic-8a76a4df-12f3-499a-afc0-01216af8a318-rpq5g", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:06:16.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4077" for this suite.

• [SLOW TEST:10.068 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":100,"skipped":1659,"failed":0}
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:06:16.803: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 21 06:06:19.358: INFO: Successfully updated pod "annotationupdate748f058d-dee3-48f6-a675-5d8d1d80ef69"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:06:23.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6581" for this suite.

• [SLOW TEST:6.584 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":101,"skipped":1659,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:06:23.388: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-fd2e9e22-a2c6-45e4-af8e-a6807fecf483
STEP: Creating a pod to test consume secrets
May 21 06:06:23.426: INFO: Waiting up to 5m0s for pod "pod-secrets-e1636d48-126c-418d-859c-8a958571eefa" in namespace "secrets-2989" to be "Succeeded or Failed"
May 21 06:06:23.428: INFO: Pod "pod-secrets-e1636d48-126c-418d-859c-8a958571eefa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087324ms
May 21 06:06:25.432: INFO: Pod "pod-secrets-e1636d48-126c-418d-859c-8a958571eefa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005460336s
STEP: Saw pod success
May 21 06:06:25.432: INFO: Pod "pod-secrets-e1636d48-126c-418d-859c-8a958571eefa" satisfied condition "Succeeded or Failed"
May 21 06:06:25.434: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-secrets-e1636d48-126c-418d-859c-8a958571eefa container secret-volume-test: <nil>
STEP: delete the pod
May 21 06:06:25.447: INFO: Waiting for pod pod-secrets-e1636d48-126c-418d-859c-8a958571eefa to disappear
May 21 06:06:25.452: INFO: Pod pod-secrets-e1636d48-126c-418d-859c-8a958571eefa no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:06:25.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2989" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":1672,"failed":0}
S
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:06:25.467: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:06:25.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1978" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":103,"skipped":1673,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:06:25.529: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
May 21 06:06:25.565: INFO: Waiting up to 5m0s for pod "pod-9e9df454-67fa-4c0a-ab8a-4e754091ef0a" in namespace "emptydir-8757" to be "Succeeded or Failed"
May 21 06:06:25.578: INFO: Pod "pod-9e9df454-67fa-4c0a-ab8a-4e754091ef0a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.816116ms
May 21 06:06:27.581: INFO: Pod "pod-9e9df454-67fa-4c0a-ab8a-4e754091ef0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016434072s
STEP: Saw pod success
May 21 06:06:27.581: INFO: Pod "pod-9e9df454-67fa-4c0a-ab8a-4e754091ef0a" satisfied condition "Succeeded or Failed"
May 21 06:06:27.583: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-9e9df454-67fa-4c0a-ab8a-4e754091ef0a container test-container: <nil>
STEP: delete the pod
May 21 06:06:27.596: INFO: Waiting for pod pod-9e9df454-67fa-4c0a-ab8a-4e754091ef0a to disappear
May 21 06:06:27.603: INFO: Pod pod-9e9df454-67fa-4c0a-ab8a-4e754091ef0a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:06:27.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8757" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":104,"skipped":1674,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:06:27.616: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:06:27.652: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec7827b1-7684-4472-aa8b-8718727c6218" in namespace "projected-3843" to be "Succeeded or Failed"
May 21 06:06:27.658: INFO: Pod "downwardapi-volume-ec7827b1-7684-4472-aa8b-8718727c6218": Phase="Pending", Reason="", readiness=false. Elapsed: 5.751579ms
May 21 06:06:29.661: INFO: Pod "downwardapi-volume-ec7827b1-7684-4472-aa8b-8718727c6218": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009386017s
STEP: Saw pod success
May 21 06:06:29.661: INFO: Pod "downwardapi-volume-ec7827b1-7684-4472-aa8b-8718727c6218" satisfied condition "Succeeded or Failed"
May 21 06:06:29.663: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-ec7827b1-7684-4472-aa8b-8718727c6218 container client-container: <nil>
STEP: delete the pod
May 21 06:06:29.679: INFO: Waiting for pod downwardapi-volume-ec7827b1-7684-4472-aa8b-8718727c6218 to disappear
May 21 06:06:29.683: INFO: Pod downwardapi-volume-ec7827b1-7684-4472-aa8b-8718727c6218 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:06:29.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3843" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":105,"skipped":1679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:06:29.693: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 21 06:06:29.732: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 06:07:29.754: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:07:29.756: INFO: Starting informer...
STEP: Starting pods...
May 21 06:07:29.788: INFO: Pod1 is running on 16c19df1-13dd-4e26-99a1-a192204dc796. Tainting Node
May 21 06:07:34.007: INFO: Pod2 is running on 16c19df1-13dd-4e26-99a1-a192204dc796. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 21 06:07:42.848: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 21 06:08:12.341: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:08:12.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6896" for this suite.

• [SLOW TEST:102.675 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":106,"skipped":1731,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:08:12.369: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-60e85c15-c626-4f31-ba6e-26ec33534598
STEP: Creating a pod to test consume secrets
May 21 06:08:12.469: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-959d6fc9-1d7c-4bd2-8040-0d84511339e6" in namespace "projected-2325" to be "Succeeded or Failed"
May 21 06:08:12.471: INFO: Pod "pod-projected-secrets-959d6fc9-1d7c-4bd2-8040-0d84511339e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107942ms
May 21 06:08:14.475: INFO: Pod "pod-projected-secrets-959d6fc9-1d7c-4bd2-8040-0d84511339e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006027585s
May 21 06:08:16.479: INFO: Pod "pod-projected-secrets-959d6fc9-1d7c-4bd2-8040-0d84511339e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010040743s
STEP: Saw pod success
May 21 06:08:16.479: INFO: Pod "pod-projected-secrets-959d6fc9-1d7c-4bd2-8040-0d84511339e6" satisfied condition "Succeeded or Failed"
May 21 06:08:16.481: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-secrets-959d6fc9-1d7c-4bd2-8040-0d84511339e6 container secret-volume-test: <nil>
STEP: delete the pod
May 21 06:08:16.500: INFO: Waiting for pod pod-projected-secrets-959d6fc9-1d7c-4bd2-8040-0d84511339e6 to disappear
May 21 06:08:16.506: INFO: Pod pod-projected-secrets-959d6fc9-1d7c-4bd2-8040-0d84511339e6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:08:16.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2325" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":107,"skipped":1740,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:08:16.512: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-59995edd-dc82-4976-8711-3bb28963b7f6
STEP: Creating a pod to test consume configMaps
May 21 06:08:16.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-20b9e689-b020-4dcc-990b-f5477e61cd62" in namespace "configmap-9260" to be "Succeeded or Failed"
May 21 06:08:16.562: INFO: Pod "pod-configmaps-20b9e689-b020-4dcc-990b-f5477e61cd62": Phase="Pending", Reason="", readiness=false. Elapsed: 12.325368ms
May 21 06:08:18.565: INFO: Pod "pod-configmaps-20b9e689-b020-4dcc-990b-f5477e61cd62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016033617s
May 21 06:08:20.569: INFO: Pod "pod-configmaps-20b9e689-b020-4dcc-990b-f5477e61cd62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019538026s
STEP: Saw pod success
May 21 06:08:20.569: INFO: Pod "pod-configmaps-20b9e689-b020-4dcc-990b-f5477e61cd62" satisfied condition "Succeeded or Failed"
May 21 06:08:20.571: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-configmaps-20b9e689-b020-4dcc-990b-f5477e61cd62 container agnhost-container: <nil>
STEP: delete the pod
May 21 06:08:20.585: INFO: Waiting for pod pod-configmaps-20b9e689-b020-4dcc-990b-f5477e61cd62 to disappear
May 21 06:08:20.592: INFO: Pod pod-configmaps-20b9e689-b020-4dcc-990b-f5477e61cd62 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:08:20.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9260" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":108,"skipped":1746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:08:20.599: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 21 06:08:20.639: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 06:09:20.657: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 21 06:09:20.674: INFO: Created pod: pod0-sched-preemption-low-priority
May 21 06:09:20.698: INFO: Created pod: pod1-sched-preemption-medium-priority
May 21 06:09:20.717: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:09:34.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2468" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:74.217 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":109,"skipped":1772,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:09:34.817: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:09:34.854: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d47b4ab-18b4-4534-8c93-f3786480fa42" in namespace "downward-api-2366" to be "Succeeded or Failed"
May 21 06:09:34.857: INFO: Pod "downwardapi-volume-8d47b4ab-18b4-4534-8c93-f3786480fa42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.475683ms
May 21 06:09:36.861: INFO: Pod "downwardapi-volume-8d47b4ab-18b4-4534-8c93-f3786480fa42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006694436s
STEP: Saw pod success
May 21 06:09:36.861: INFO: Pod "downwardapi-volume-8d47b4ab-18b4-4534-8c93-f3786480fa42" satisfied condition "Succeeded or Failed"
May 21 06:09:36.863: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-8d47b4ab-18b4-4534-8c93-f3786480fa42 container client-container: <nil>
STEP: delete the pod
May 21 06:09:36.879: INFO: Waiting for pod downwardapi-volume-8d47b4ab-18b4-4534-8c93-f3786480fa42 to disappear
May 21 06:09:36.885: INFO: Pod downwardapi-volume-8d47b4ab-18b4-4534-8c93-f3786480fa42 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:09:36.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2366" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":1778,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:09:36.891: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 21 06:09:40.973: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 06:09:40.975: INFO: Pod pod-with-poststart-http-hook still exists
May 21 06:09:42.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 06:09:42.980: INFO: Pod pod-with-poststart-http-hook still exists
May 21 06:09:44.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 06:09:44.978: INFO: Pod pod-with-poststart-http-hook still exists
May 21 06:09:46.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 06:09:46.980: INFO: Pod pod-with-poststart-http-hook still exists
May 21 06:09:48.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 06:09:48.980: INFO: Pod pod-with-poststart-http-hook still exists
May 21 06:09:50.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 06:09:50.979: INFO: Pod pod-with-poststart-http-hook still exists
May 21 06:09:52.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 06:09:52.979: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:09:52.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7937" for this suite.

• [SLOW TEST:16.094 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":111,"skipped":1793,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:09:52.986: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 21 06:09:53.008: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:10:07.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6034" for this suite.

• [SLOW TEST:14.433 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":112,"skipped":1854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:10:07.421: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:10:09.467: INFO: Deleting pod "var-expansion-bdb7a62a-d825-4f46-9516-955306ac5915" in namespace "var-expansion-7838"
May 21 06:10:09.474: INFO: Wait up to 5m0s for pod "var-expansion-bdb7a62a-d825-4f46-9516-955306ac5915" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:10:11.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7838" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":113,"skipped":1885,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:10:11.489: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8298.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8298.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8298.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8298.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 06:10:13.561: INFO: DNS probes using dns-test-e68ec8c9-f50d-4265-bee7-4c7ad7cc203f succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8298.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8298.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8298.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8298.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 06:10:15.612: INFO: DNS probes using dns-test-813efa09-45ce-48e8-b8e2-d8451d37e781 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8298.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8298.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8298.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8298.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 06:10:17.662: INFO: DNS probes using dns-test-79cfe868-8b09-4fc5-b752-9b758f912f29 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:10:17.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8298" for this suite.

• [SLOW TEST:6.197 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":114,"skipped":1886,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:10:17.686: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2833
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2833
STEP: creating replication controller externalsvc in namespace services-2833
I0521 06:10:17.763193      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-2833, replica count: 2
I0521 06:10:20.813450      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 21 06:10:20.837: INFO: Creating new exec pod
May 21 06:10:22.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-2833 exec execpodgxgxr -- /bin/sh -x -c nslookup nodeport-service.services-2833.svc.cluster.local'
May 21 06:10:24.066: INFO: stderr: "+ nslookup nodeport-service.services-2833.svc.cluster.local\n"
May 21 06:10:24.066: INFO: stdout: "Server:\t\t10.100.200.2\nAddress:\t10.100.200.2#53\n\nnodeport-service.services-2833.svc.cluster.local\tcanonical name = externalsvc.services-2833.svc.cluster.local.\nName:\texternalsvc.services-2833.svc.cluster.local\nAddress: 10.100.200.50\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2833, will wait for the garbage collector to delete the pods
May 21 06:10:24.125: INFO: Deleting ReplicationController externalsvc took: 6.444408ms
May 21 06:10:24.725: INFO: Terminating ReplicationController externalsvc pods took: 600.211479ms
May 21 06:10:32.442: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:10:32.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2833" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:14.786 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":115,"skipped":1927,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:10:32.475: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 06:10:33.006: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 06:10:36.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:10:36.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1529" for this suite.
STEP: Destroying namespace "webhook-1529-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":116,"skipped":1930,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:10:36.128: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:10:42.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7558" for this suite.
STEP: Destroying namespace "nsdeletetest-7010" for this suite.
May 21 06:10:42.291: INFO: Namespace nsdeletetest-7010 was already deleted
STEP: Destroying namespace "nsdeletetest-9338" for this suite.

• [SLOW TEST:6.167 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":117,"skipped":1934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:10:42.298: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 21 06:10:46.858: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e292ebb8-21af-4f22-ba2e-5ec2e7dffd24"
May 21 06:10:46.859: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e292ebb8-21af-4f22-ba2e-5ec2e7dffd24" in namespace "pods-3558" to be "terminated due to deadline exceeded"
May 21 06:10:46.860: INFO: Pod "pod-update-activedeadlineseconds-e292ebb8-21af-4f22-ba2e-5ec2e7dffd24": Phase="Running", Reason="", readiness=true. Elapsed: 1.763937ms
May 21 06:10:48.864: INFO: Pod "pod-update-activedeadlineseconds-e292ebb8-21af-4f22-ba2e-5ec2e7dffd24": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.00512102s
May 21 06:10:48.864: INFO: Pod "pod-update-activedeadlineseconds-e292ebb8-21af-4f22-ba2e-5ec2e7dffd24" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:10:48.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3558" for this suite.

• [SLOW TEST:6.573 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":1968,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:10:48.871: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 21 06:10:48.907: INFO: Waiting up to 5m0s for pod "pod-72cc131c-bafa-42c0-81ac-4373e6911d60" in namespace "emptydir-7227" to be "Succeeded or Failed"
May 21 06:10:48.912: INFO: Pod "pod-72cc131c-bafa-42c0-81ac-4373e6911d60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.951072ms
May 21 06:10:50.915: INFO: Pod "pod-72cc131c-bafa-42c0-81ac-4373e6911d60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007979399s
STEP: Saw pod success
May 21 06:10:50.915: INFO: Pod "pod-72cc131c-bafa-42c0-81ac-4373e6911d60" satisfied condition "Succeeded or Failed"
May 21 06:10:50.917: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-72cc131c-bafa-42c0-81ac-4373e6911d60 container test-container: <nil>
STEP: delete the pod
May 21 06:10:50.930: INFO: Waiting for pod pod-72cc131c-bafa-42c0-81ac-4373e6911d60 to disappear
May 21 06:10:50.936: INFO: Pod pod-72cc131c-bafa-42c0-81ac-4373e6911d60 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:10:50.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7227" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":1969,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:10:50.942: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-14f4eaad-fc16-46d1-90c4-b88113186331
STEP: Creating a pod to test consume configMaps
May 21 06:10:50.983: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c42b763d-454e-4944-b189-4eeafb062ed9" in namespace "projected-7902" to be "Succeeded or Failed"
May 21 06:10:50.989: INFO: Pod "pod-projected-configmaps-c42b763d-454e-4944-b189-4eeafb062ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.774737ms
May 21 06:10:52.992: INFO: Pod "pod-projected-configmaps-c42b763d-454e-4944-b189-4eeafb062ed9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008968633s
STEP: Saw pod success
May 21 06:10:52.992: INFO: Pod "pod-projected-configmaps-c42b763d-454e-4944-b189-4eeafb062ed9" satisfied condition "Succeeded or Failed"
May 21 06:10:52.994: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-configmaps-c42b763d-454e-4944-b189-4eeafb062ed9 container agnhost-container: <nil>
STEP: delete the pod
May 21 06:10:53.006: INFO: Waiting for pod pod-projected-configmaps-c42b763d-454e-4944-b189-4eeafb062ed9 to disappear
May 21 06:10:53.013: INFO: Pod pod-projected-configmaps-c42b763d-454e-4944-b189-4eeafb062ed9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:10:53.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7902" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":1973,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:10:53.021: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:10:53.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3255" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":121,"skipped":2006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:10:53.099: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-6367
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 06:10:53.142: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 06:10:53.186: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 06:10:55.189: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:10:57.191: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:10:59.190: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:11:01.190: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:11:03.190: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:11:05.190: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:11:07.191: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:11:09.191: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:11:11.189: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:11:13.190: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 06:11:13.193: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 21 06:11:13.196: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 21 06:11:15.224: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 21 06:11:15.224: INFO: Going to poll 10.200.88.146 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 21 06:11:15.225: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.88.146:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6367 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:11:15.225: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:11:15.319: INFO: Found all 1 expected endpoints: [netserver-0]
May 21 06:11:15.319: INFO: Going to poll 10.200.66.52 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 21 06:11:15.322: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.66.52:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6367 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:11:15.322: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:11:15.417: INFO: Found all 1 expected endpoints: [netserver-1]
May 21 06:11:15.417: INFO: Going to poll 10.200.34.45 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 21 06:11:15.420: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.34.45:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6367 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:11:15.420: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:11:15.519: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:11:15.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6367" for this suite.

• [SLOW TEST:22.428 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2058,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:11:15.527: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 06:11:16.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 06:11:18.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757174276, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757174276, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757174276, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757174276, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 06:11:21.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 21 06:11:21.100: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:11:21.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4928" for this suite.
STEP: Destroying namespace "webhook-4928-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.631 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":123,"skipped":2074,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:11:21.160: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0521 06:11:31.309132      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 06:11:31.309238      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 06:11:31.309289      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 06:11:31.309: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 21 06:11:31.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fsnl" in namespace "gc-3786"
May 21 06:11:31.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-99lns" in namespace "gc-3786"
May 21 06:11:31.347: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7v8l" in namespace "gc-3786"
May 21 06:11:31.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvg6q" in namespace "gc-3786"
May 21 06:11:31.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-l9v7x" in namespace "gc-3786"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:11:31.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3786" for this suite.

• [SLOW TEST:10.258 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":124,"skipped":2086,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:11:31.419: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 21 06:11:33.479: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9443 PodName:var-expansion-ccad742c-55fc-44f9-870f-188f959a97fb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:11:33.479: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: test for file in mounted path
May 21 06:11:33.567: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9443 PodName:var-expansion-ccad742c-55fc-44f9-870f-188f959a97fb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:11:33.567: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: updating the annotation value
May 21 06:11:34.157: INFO: Successfully updated pod "var-expansion-ccad742c-55fc-44f9-870f-188f959a97fb"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 21 06:11:34.160: INFO: Deleting pod "var-expansion-ccad742c-55fc-44f9-870f-188f959a97fb" in namespace "var-expansion-9443"
May 21 06:11:34.163: INFO: Wait up to 5m0s for pod "var-expansion-ccad742c-55fc-44f9-870f-188f959a97fb" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:14.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9443" for this suite.

• [SLOW TEST:42.760 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":125,"skipped":2114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:14.180: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:14.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8254" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":126,"skipped":2137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:14.272: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
May 21 06:12:14.300: INFO: Major version: 1
STEP: Confirm minor version
May 21 06:12:14.300: INFO: cleanMinorVersion: 20
May 21 06:12:14.300: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:14.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6487" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":127,"skipped":2173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:14.309: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:14.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9134" for this suite.
STEP: Destroying namespace "nspatchtest-3235cd2a-ffb5-4f6b-b74c-af75747bfa08-4108" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":128,"skipped":2214,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:14.388: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:14.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4491" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":129,"skipped":2255,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:14.432: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:14.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8428" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":130,"skipped":2281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:14.471: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 06:12:14.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6743 run e2e-test-httpd-pod --restart=Never --image=mirror.gcr.io/library/httpd:2.4.38-alpine'
May 21 06:12:14.567: INFO: stderr: ""
May 21 06:12:14.567: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
May 21 06:12:14.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6743 delete pods e2e-test-httpd-pod'
May 21 06:12:22.347: INFO: stderr: ""
May 21 06:12:22.347: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:22.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6743" for this suite.

• [SLOW TEST:7.887 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":131,"skipped":2310,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:22.358: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:12:22.418: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 06:12:25.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-7877 --namespace=crd-publish-openapi-7877 create -f -'
May 21 06:12:26.664: INFO: stderr: ""
May 21 06:12:26.664: INFO: stdout: "e2e-test-crd-publish-openapi-4453-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 21 06:12:26.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-7877 --namespace=crd-publish-openapi-7877 delete e2e-test-crd-publish-openapi-4453-crds test-cr'
May 21 06:12:26.738: INFO: stderr: ""
May 21 06:12:26.738: INFO: stdout: "e2e-test-crd-publish-openapi-4453-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 21 06:12:26.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-7877 --namespace=crd-publish-openapi-7877 apply -f -'
May 21 06:12:26.922: INFO: stderr: ""
May 21 06:12:26.922: INFO: stdout: "e2e-test-crd-publish-openapi-4453-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 21 06:12:26.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-7877 --namespace=crd-publish-openapi-7877 delete e2e-test-crd-publish-openapi-4453-crds test-cr'
May 21 06:12:26.990: INFO: stderr: ""
May 21 06:12:26.990: INFO: stdout: "e2e-test-crd-publish-openapi-4453-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 21 06:12:26.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-7877 explain e2e-test-crd-publish-openapi-4453-crds'
May 21 06:12:27.180: INFO: stderr: ""
May 21 06:12:27.180: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4453-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:30.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7877" for this suite.

• [SLOW TEST:7.743 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":132,"skipped":2318,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:30.104: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:12:30.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-497f814f-cd45-41a3-9b96-429b3d22071f" in namespace "downward-api-8578" to be "Succeeded or Failed"
May 21 06:12:30.140: INFO: Pod "downwardapi-volume-497f814f-cd45-41a3-9b96-429b3d22071f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.213877ms
May 21 06:12:32.143: INFO: Pod "downwardapi-volume-497f814f-cd45-41a3-9b96-429b3d22071f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005367881s
STEP: Saw pod success
May 21 06:12:32.143: INFO: Pod "downwardapi-volume-497f814f-cd45-41a3-9b96-429b3d22071f" satisfied condition "Succeeded or Failed"
May 21 06:12:32.145: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-497f814f-cd45-41a3-9b96-429b3d22071f container client-container: <nil>
STEP: delete the pod
May 21 06:12:32.164: INFO: Waiting for pod downwardapi-volume-497f814f-cd45-41a3-9b96-429b3d22071f to disappear
May 21 06:12:32.171: INFO: Pod downwardapi-volume-497f814f-cd45-41a3-9b96-429b3d22071f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:32.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8578" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":133,"skipped":2330,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:32.178: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 21 06:12:34.754: INFO: Successfully updated pod "labelsupdatea9814850-d513-4c38-8cf5-d5ecf1b61258"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:36.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2557" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2343,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:36.790: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 21 06:12:36.818: INFO: namespace kubectl-3383
May 21 06:12:36.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3383 create -f -'
May 21 06:12:37.066: INFO: stderr: ""
May 21 06:12:37.066: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 06:12:38.069: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 06:12:38.069: INFO: Found 0 / 1
May 21 06:12:39.069: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 06:12:39.069: INFO: Found 0 / 1
May 21 06:12:40.069: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 06:12:40.069: INFO: Found 1 / 1
May 21 06:12:40.069: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 21 06:12:40.071: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 06:12:40.071: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 06:12:40.071: INFO: wait on agnhost-primary startup in kubectl-3383 
May 21 06:12:40.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3383 logs agnhost-primary-crxvs agnhost-primary'
May 21 06:12:40.148: INFO: stderr: ""
May 21 06:12:40.148: INFO: stdout: "Paused\n"
STEP: exposing RC
May 21 06:12:40.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3383 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 21 06:12:40.239: INFO: stderr: ""
May 21 06:12:40.239: INFO: stdout: "service/rm2 exposed\n"
May 21 06:12:40.244: INFO: Service rm2 in namespace kubectl-3383 found.
STEP: exposing service
May 21 06:12:42.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-3383 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 21 06:12:42.329: INFO: stderr: ""
May 21 06:12:42.329: INFO: stdout: "service/rm3 exposed\n"
May 21 06:12:42.336: INFO: Service rm3 in namespace kubectl-3383 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:12:44.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3383" for this suite.

• [SLOW TEST:7.556 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":135,"skipped":2357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:12:44.346: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6007
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-6007
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6007
May 21 06:12:44.396: INFO: Found 0 stateful pods, waiting for 1
May 21 06:12:54.401: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 21 06:12:54.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-6007 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 06:12:54.570: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 06:12:54.570: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 06:12:54.570: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 06:12:54.574: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 21 06:13:04.577: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 06:13:04.577: INFO: Waiting for statefulset status.replicas updated to 0
May 21 06:13:04.602: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May 21 06:13:04.602: INFO: ss-0  16c19df1-13dd-4e26-99a1-a192204dc796  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  }]
May 21 06:13:04.602: INFO: ss-1                                        Pending         []
May 21 06:13:04.602: INFO: 
May 21 06:13:04.602: INFO: StatefulSet ss has not reached scale 3, at 2
May 21 06:13:05.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984353919s
May 21 06:13:06.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97954565s
May 21 06:13:07.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975694244s
May 21 06:13:08.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970534304s
May 21 06:13:09.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966697033s
May 21 06:13:10.629: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961488337s
May 21 06:13:11.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957718551s
May 21 06:13:12.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.94629219s
May 21 06:13:13.648: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.357636ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6007
May 21 06:13:14.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-6007 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 06:13:14.820: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 06:13:14.820: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 06:13:14.820: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 06:13:14.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-6007 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 06:13:14.976: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 21 06:13:14.976: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 06:13:14.976: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 06:13:14.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-6007 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 06:13:15.134: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 21 06:13:15.134: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 06:13:15.134: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 06:13:15.137: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 21 06:13:25.140: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 06:13:25.141: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 06:13:25.141: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 21 06:13:25.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-6007 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 06:13:25.301: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 06:13:25.301: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 06:13:25.301: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 06:13:25.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-6007 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 06:13:25.474: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 06:13:25.474: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 06:13:25.474: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 06:13:25.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-6007 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 06:13:25.640: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 06:13:25.640: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 06:13:25.640: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 06:13:25.640: INFO: Waiting for statefulset status.replicas updated to 0
May 21 06:13:25.643: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 21 06:13:35.649: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 06:13:35.649: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 21 06:13:35.649: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 21 06:13:35.663: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May 21 06:13:35.663: INFO: ss-0  16c19df1-13dd-4e26-99a1-a192204dc796  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  }]
May 21 06:13:35.663: INFO: ss-1  16c19df1-13dd-4e26-99a1-a192204dc796  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:35.663: INFO: ss-2  16c19df1-13dd-4e26-99a1-a192204dc796  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:35.663: INFO: 
May 21 06:13:35.663: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 06:13:36.668: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May 21 06:13:36.668: INFO: ss-0  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  }]
May 21 06:13:36.668: INFO: ss-1  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:36.668: INFO: ss-2  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:36.668: INFO: 
May 21 06:13:36.668: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 06:13:37.672: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May 21 06:13:37.672: INFO: ss-0  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  }]
May 21 06:13:37.672: INFO: ss-1  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:37.672: INFO: ss-2  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:37.672: INFO: 
May 21 06:13:37.672: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 06:13:38.678: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May 21 06:13:38.678: INFO: ss-0  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  }]
May 21 06:13:38.678: INFO: ss-1  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:38.678: INFO: ss-2  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:38.678: INFO: 
May 21 06:13:38.678: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 06:13:39.683: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May 21 06:13:39.683: INFO: ss-0  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  }]
May 21 06:13:39.683: INFO: ss-1  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:39.683: INFO: ss-2  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:39.683: INFO: 
May 21 06:13:39.683: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 06:13:40.687: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May 21 06:13:40.688: INFO: ss-0  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  }]
May 21 06:13:40.688: INFO: ss-1  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:40.688: INFO: ss-2  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:40.688: INFO: 
May 21 06:13:40.688: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 06:13:41.691: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May 21 06:13:41.691: INFO: ss-0  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:12:44 +0000 UTC  }]
May 21 06:13:41.691: INFO: ss-1  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:41.691: INFO: ss-2  16c19df1-13dd-4e26-99a1-a192204dc796  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 06:13:04 +0000 UTC  }]
May 21 06:13:41.691: INFO: 
May 21 06:13:41.691: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 06:13:42.693: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.96814764s
May 21 06:13:43.697: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.966399863s
May 21 06:13:44.702: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.124405ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6007
May 21 06:13:45.705: INFO: Scaling statefulset ss to 0
May 21 06:13:45.712: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 06:13:45.714: INFO: Deleting all statefulset in ns statefulset-6007
May 21 06:13:45.715: INFO: Scaling statefulset ss to 0
May 21 06:13:45.721: INFO: Waiting for statefulset status.replicas updated to 0
May 21 06:13:45.722: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:13:45.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6007" for this suite.

• [SLOW TEST:61.395 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":136,"skipped":2379,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:13:45.744: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:13:45.784: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ef0f35f-d3d7-493a-8660-7ec55b02f580" in namespace "downward-api-4545" to be "Succeeded or Failed"
May 21 06:13:45.794: INFO: Pod "downwardapi-volume-0ef0f35f-d3d7-493a-8660-7ec55b02f580": Phase="Pending", Reason="", readiness=false. Elapsed: 10.424169ms
May 21 06:13:47.800: INFO: Pod "downwardapi-volume-0ef0f35f-d3d7-493a-8660-7ec55b02f580": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015773961s
STEP: Saw pod success
May 21 06:13:47.800: INFO: Pod "downwardapi-volume-0ef0f35f-d3d7-493a-8660-7ec55b02f580" satisfied condition "Succeeded or Failed"
May 21 06:13:47.802: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-0ef0f35f-d3d7-493a-8660-7ec55b02f580 container client-container: <nil>
STEP: delete the pod
May 21 06:13:47.816: INFO: Waiting for pod downwardapi-volume-0ef0f35f-d3d7-493a-8660-7ec55b02f580 to disappear
May 21 06:13:47.822: INFO: Pod downwardapi-volume-0ef0f35f-d3d7-493a-8660-7ec55b02f580 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:13:47.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4545" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2392,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:13:47.828: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-04f715a7-c147-441b-b059-e546df5ef124
STEP: Creating a pod to test consume configMaps
May 21 06:13:47.865: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-20c179dc-8d26-433a-9a71-da5fe9f4a3e6" in namespace "projected-63" to be "Succeeded or Failed"
May 21 06:13:47.874: INFO: Pod "pod-projected-configmaps-20c179dc-8d26-433a-9a71-da5fe9f4a3e6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.312734ms
May 21 06:13:49.877: INFO: Pod "pod-projected-configmaps-20c179dc-8d26-433a-9a71-da5fe9f4a3e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012053966s
STEP: Saw pod success
May 21 06:13:49.878: INFO: Pod "pod-projected-configmaps-20c179dc-8d26-433a-9a71-da5fe9f4a3e6" satisfied condition "Succeeded or Failed"
May 21 06:13:49.879: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-configmaps-20c179dc-8d26-433a-9a71-da5fe9f4a3e6 container agnhost-container: <nil>
STEP: delete the pod
May 21 06:13:49.892: INFO: Waiting for pod pod-projected-configmaps-20c179dc-8d26-433a-9a71-da5fe9f4a3e6 to disappear
May 21 06:13:49.899: INFO: Pod pod-projected-configmaps-20c179dc-8d26-433a-9a71-da5fe9f4a3e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:13:49.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-63" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":138,"skipped":2394,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:13:49.905: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:13:51.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5430" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":139,"skipped":2394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:13:51.963: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:13:51.997: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0181ed24-75de-419f-beec-bfefc6edd20a" in namespace "security-context-test-8574" to be "Succeeded or Failed"
May 21 06:13:51.999: INFO: Pod "busybox-readonly-false-0181ed24-75de-419f-beec-bfefc6edd20a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.496621ms
May 21 06:13:54.004: INFO: Pod "busybox-readonly-false-0181ed24-75de-419f-beec-bfefc6edd20a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006828215s
May 21 06:13:54.004: INFO: Pod "busybox-readonly-false-0181ed24-75de-419f-beec-bfefc6edd20a" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:13:54.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8574" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":140,"skipped":2439,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:13:54.012: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:13:54.038: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:13:54.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8572" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":141,"skipped":2471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:13:54.585: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7111.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7111.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7111.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7111.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7111.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7111.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7111.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7111.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7111.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7111.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 78.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.78_udp@PTR;check="$$(dig +tcp +noall +answer +search 78.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.78_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7111.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7111.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7111.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7111.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7111.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7111.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7111.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7111.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7111.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7111.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7111.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 78.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.78_udp@PTR;check="$$(dig +tcp +noall +answer +search 78.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.78_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 06:13:56.673: INFO: Unable to read wheezy_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:13:56.675: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:13:56.677: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:13:56.679: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:13:56.693: INFO: Unable to read jessie_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:13:56.695: INFO: Unable to read jessie_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:13:56.697: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:13:56.699: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:13:56.724: INFO: Lookups using dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8 failed for: [wheezy_udp@dns-test-service.dns-7111.svc.cluster.local wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local jessie_udp@dns-test-service.dns-7111.svc.cluster.local jessie_tcp@dns-test-service.dns-7111.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7111.svc.cluster.local]

May 21 06:14:01.728: INFO: Unable to read wheezy_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:01.732: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:01.751: INFO: Unable to read jessie_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:01.753: INFO: Unable to read jessie_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:01.769: INFO: Lookups using dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8 failed for: [wheezy_udp@dns-test-service.dns-7111.svc.cluster.local wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local jessie_udp@dns-test-service.dns-7111.svc.cluster.local jessie_tcp@dns-test-service.dns-7111.svc.cluster.local]

May 21 06:14:06.728: INFO: Unable to read wheezy_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:06.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:06.748: INFO: Unable to read jessie_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:06.751: INFO: Unable to read jessie_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:06.766: INFO: Lookups using dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8 failed for: [wheezy_udp@dns-test-service.dns-7111.svc.cluster.local wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local jessie_udp@dns-test-service.dns-7111.svc.cluster.local jessie_tcp@dns-test-service.dns-7111.svc.cluster.local]

May 21 06:14:11.728: INFO: Unable to read wheezy_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:11.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:11.748: INFO: Unable to read jessie_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:11.750: INFO: Unable to read jessie_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:11.765: INFO: Lookups using dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8 failed for: [wheezy_udp@dns-test-service.dns-7111.svc.cluster.local wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local jessie_udp@dns-test-service.dns-7111.svc.cluster.local jessie_tcp@dns-test-service.dns-7111.svc.cluster.local]

May 21 06:14:16.728: INFO: Unable to read wheezy_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:16.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:16.748: INFO: Unable to read jessie_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:16.750: INFO: Unable to read jessie_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:16.765: INFO: Lookups using dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8 failed for: [wheezy_udp@dns-test-service.dns-7111.svc.cluster.local wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local jessie_udp@dns-test-service.dns-7111.svc.cluster.local jessie_tcp@dns-test-service.dns-7111.svc.cluster.local]

May 21 06:14:21.728: INFO: Unable to read wheezy_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:21.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:21.749: INFO: Unable to read jessie_udp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:21.751: INFO: Unable to read jessie_tcp@dns-test-service.dns-7111.svc.cluster.local from pod dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8: the server could not find the requested resource (get pods dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8)
May 21 06:14:21.768: INFO: Lookups using dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8 failed for: [wheezy_udp@dns-test-service.dns-7111.svc.cluster.local wheezy_tcp@dns-test-service.dns-7111.svc.cluster.local jessie_udp@dns-test-service.dns-7111.svc.cluster.local jessie_tcp@dns-test-service.dns-7111.svc.cluster.local]

May 21 06:14:26.766: INFO: DNS probes using dns-7111/dns-test-2db4f17e-4e2b-4e49-af77-3bbd62f63fc8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:14:26.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7111" for this suite.

• [SLOW TEST:32.292 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":142,"skipped":2494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:14:26.877: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 21 06:14:26.905: INFO: Waiting up to 5m0s for pod "pod-fd3313d5-7cca-4c28-bbb7-4d4ecae9e2e2" in namespace "emptydir-2898" to be "Succeeded or Failed"
May 21 06:14:26.910: INFO: Pod "pod-fd3313d5-7cca-4c28-bbb7-4d4ecae9e2e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.096553ms
May 21 06:14:28.914: INFO: Pod "pod-fd3313d5-7cca-4c28-bbb7-4d4ecae9e2e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008217923s
STEP: Saw pod success
May 21 06:14:28.914: INFO: Pod "pod-fd3313d5-7cca-4c28-bbb7-4d4ecae9e2e2" satisfied condition "Succeeded or Failed"
May 21 06:14:28.916: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-fd3313d5-7cca-4c28-bbb7-4d4ecae9e2e2 container test-container: <nil>
STEP: delete the pod
May 21 06:14:28.928: INFO: Waiting for pod pod-fd3313d5-7cca-4c28-bbb7-4d4ecae9e2e2 to disappear
May 21 06:14:28.935: INFO: Pod pod-fd3313d5-7cca-4c28-bbb7-4d4ecae9e2e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:14:28.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2898" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":143,"skipped":2516,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:14:28.941: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
May 21 06:14:28.986: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-1127 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:14:29.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1127" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":144,"skipped":2517,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:14:29.057: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:14:29.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b966c01-b157-4436-b79c-a798eaf8de76" in namespace "downward-api-5838" to be "Succeeded or Failed"
May 21 06:14:29.092: INFO: Pod "downwardapi-volume-1b966c01-b157-4436-b79c-a798eaf8de76": Phase="Pending", Reason="", readiness=false. Elapsed: 1.678136ms
May 21 06:14:31.096: INFO: Pod "downwardapi-volume-1b966c01-b157-4436-b79c-a798eaf8de76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005911828s
STEP: Saw pod success
May 21 06:14:31.096: INFO: Pod "downwardapi-volume-1b966c01-b157-4436-b79c-a798eaf8de76" satisfied condition "Succeeded or Failed"
May 21 06:14:31.098: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-1b966c01-b157-4436-b79c-a798eaf8de76 container client-container: <nil>
STEP: delete the pod
May 21 06:14:31.119: INFO: Waiting for pod downwardapi-volume-1b966c01-b157-4436-b79c-a798eaf8de76 to disappear
May 21 06:14:31.126: INFO: Pod downwardapi-volume-1b966c01-b157-4436-b79c-a798eaf8de76 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:14:31.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5838" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2531,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:14:31.132: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 21 06:14:31.175: INFO: Pod name pod-release: Found 0 pods out of 1
May 21 06:14:36.179: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:14:37.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8048" for this suite.

• [SLOW TEST:6.073 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":146,"skipped":2539,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:14:37.205: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:14:37.243: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1ae0ccc-9770-423e-86e3-db506674283d" in namespace "downward-api-1033" to be "Succeeded or Failed"
May 21 06:14:37.245: INFO: Pod "downwardapi-volume-e1ae0ccc-9770-423e-86e3-db506674283d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093455ms
May 21 06:14:39.250: INFO: Pod "downwardapi-volume-e1ae0ccc-9770-423e-86e3-db506674283d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007022724s
STEP: Saw pod success
May 21 06:14:39.250: INFO: Pod "downwardapi-volume-e1ae0ccc-9770-423e-86e3-db506674283d" satisfied condition "Succeeded or Failed"
May 21 06:14:39.252: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-e1ae0ccc-9770-423e-86e3-db506674283d container client-container: <nil>
STEP: delete the pod
May 21 06:14:39.277: INFO: Waiting for pod downwardapi-volume-e1ae0ccc-9770-423e-86e3-db506674283d to disappear
May 21 06:14:39.284: INFO: Pod downwardapi-volume-e1ae0ccc-9770-423e-86e3-db506674283d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:14:39.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1033" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:14:39.292: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:14:39.316: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Creating first CR 
May 21 06:14:39.950: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T06:14:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T06:14:39Z]] name:name1 resourceVersion:19507 uid:a2b3757d-b7db-4128-b6ec-a6fe291ac5f6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 21 06:14:49.955: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T06:14:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T06:14:49Z]] name:name2 resourceVersion:19553 uid:74048231-4664-4157-8de5-652bddde3b3a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 21 06:14:59.961: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T06:14:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T06:14:59Z]] name:name1 resourceVersion:19568 uid:a2b3757d-b7db-4128-b6ec-a6fe291ac5f6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 21 06:15:09.966: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T06:14:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T06:15:09Z]] name:name2 resourceVersion:19583 uid:74048231-4664-4157-8de5-652bddde3b3a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 21 06:15:19.973: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T06:14:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T06:14:59Z]] name:name1 resourceVersion:19598 uid:a2b3757d-b7db-4128-b6ec-a6fe291ac5f6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 21 06:15:29.979: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T06:14:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T06:15:09Z]] name:name2 resourceVersion:19613 uid:74048231-4664-4157-8de5-652bddde3b3a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:15:40.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1947" for this suite.

• [SLOW TEST:61.205 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":148,"skipped":2578,"failed":0}
SS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:15:40.497: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:15:40.866: INFO: Checking APIGroup: apiregistration.k8s.io
May 21 06:15:40.867: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 21 06:15:40.867: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.867: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 21 06:15:40.867: INFO: Checking APIGroup: apps
May 21 06:15:40.868: INFO: PreferredVersion.GroupVersion: apps/v1
May 21 06:15:40.868: INFO: Versions found [{apps/v1 v1}]
May 21 06:15:40.868: INFO: apps/v1 matches apps/v1
May 21 06:15:40.868: INFO: Checking APIGroup: events.k8s.io
May 21 06:15:40.869: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 21 06:15:40.869: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.869: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 21 06:15:40.869: INFO: Checking APIGroup: authentication.k8s.io
May 21 06:15:40.870: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 21 06:15:40.870: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.870: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 21 06:15:40.870: INFO: Checking APIGroup: authorization.k8s.io
May 21 06:15:40.870: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 21 06:15:40.870: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.871: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 21 06:15:40.871: INFO: Checking APIGroup: autoscaling
May 21 06:15:40.871: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 21 06:15:40.871: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 21 06:15:40.871: INFO: autoscaling/v1 matches autoscaling/v1
May 21 06:15:40.871: INFO: Checking APIGroup: batch
May 21 06:15:40.872: INFO: PreferredVersion.GroupVersion: batch/v1
May 21 06:15:40.872: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 21 06:15:40.872: INFO: batch/v1 matches batch/v1
May 21 06:15:40.872: INFO: Checking APIGroup: certificates.k8s.io
May 21 06:15:40.873: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 21 06:15:40.873: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.873: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 21 06:15:40.873: INFO: Checking APIGroup: networking.k8s.io
May 21 06:15:40.873: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 21 06:15:40.873: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.873: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 21 06:15:40.874: INFO: Checking APIGroup: extensions
May 21 06:15:40.874: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 21 06:15:40.874: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 21 06:15:40.874: INFO: extensions/v1beta1 matches extensions/v1beta1
May 21 06:15:40.874: INFO: Checking APIGroup: policy
May 21 06:15:40.875: INFO: PreferredVersion.GroupVersion: policy/v1beta1
May 21 06:15:40.875: INFO: Versions found [{policy/v1beta1 v1beta1}]
May 21 06:15:40.875: INFO: policy/v1beta1 matches policy/v1beta1
May 21 06:15:40.875: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 21 06:15:40.876: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 21 06:15:40.876: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.876: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 21 06:15:40.876: INFO: Checking APIGroup: storage.k8s.io
May 21 06:15:40.876: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 21 06:15:40.876: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.876: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 21 06:15:40.876: INFO: Checking APIGroup: admissionregistration.k8s.io
May 21 06:15:40.877: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 21 06:15:40.877: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.877: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 21 06:15:40.877: INFO: Checking APIGroup: apiextensions.k8s.io
May 21 06:15:40.878: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 21 06:15:40.878: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.878: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 21 06:15:40.878: INFO: Checking APIGroup: scheduling.k8s.io
May 21 06:15:40.879: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 21 06:15:40.879: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.879: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 21 06:15:40.879: INFO: Checking APIGroup: coordination.k8s.io
May 21 06:15:40.879: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 21 06:15:40.879: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.879: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 21 06:15:40.879: INFO: Checking APIGroup: node.k8s.io
May 21 06:15:40.880: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 21 06:15:40.880: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.880: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 21 06:15:40.880: INFO: Checking APIGroup: discovery.k8s.io
May 21 06:15:40.881: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
May 21 06:15:40.881: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.881: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
May 21 06:15:40.881: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 21 06:15:40.882: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
May 21 06:15:40.882: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.882: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
May 21 06:15:40.882: INFO: Checking APIGroup: pksapi.io
May 21 06:15:40.882: INFO: PreferredVersion.GroupVersion: pksapi.io/v1beta1
May 21 06:15:40.883: INFO: Versions found [{pksapi.io/v1beta1 v1beta1}]
May 21 06:15:40.883: INFO: pksapi.io/v1beta1 matches pksapi.io/v1beta1
May 21 06:15:40.883: INFO: Checking APIGroup: metrics.k8s.io
May 21 06:15:40.883: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May 21 06:15:40.883: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May 21 06:15:40.883: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:15:40.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7374" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":149,"skipped":2580,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:15:40.892: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
May 21 06:15:40.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-5776 create -f -'
May 21 06:15:41.215: INFO: stderr: ""
May 21 06:15:41.215: INFO: stdout: "pod/pause created\n"
May 21 06:15:41.215: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 21 06:15:41.215: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5776" to be "running and ready"
May 21 06:15:41.220: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.99146ms
May 21 06:15:43.223: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007811976s
May 21 06:15:43.223: INFO: Pod "pause" satisfied condition "running and ready"
May 21 06:15:43.223: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
May 21 06:15:43.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-5776 label pods pause testing-label=testing-label-value'
May 21 06:15:43.294: INFO: stderr: ""
May 21 06:15:43.294: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 21 06:15:43.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-5776 get pod pause -L testing-label'
May 21 06:15:43.357: INFO: stderr: ""
May 21 06:15:43.357: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 21 06:15:43.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-5776 label pods pause testing-label-'
May 21 06:15:43.423: INFO: stderr: ""
May 21 06:15:43.423: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 21 06:15:43.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-5776 get pod pause -L testing-label'
May 21 06:15:43.489: INFO: stderr: ""
May 21 06:15:43.489: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
May 21 06:15:43.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-5776 delete --grace-period=0 --force -f -'
May 21 06:15:43.568: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 06:15:43.568: INFO: stdout: "pod \"pause\" force deleted\n"
May 21 06:15:43.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-5776 get rc,svc -l name=pause --no-headers'
May 21 06:15:43.650: INFO: stderr: "No resources found in kubectl-5776 namespace.\n"
May 21 06:15:43.650: INFO: stdout: ""
May 21 06:15:43.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-5776 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 06:15:43.717: INFO: stderr: ""
May 21 06:15:43.717: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:15:43.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5776" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":150,"skipped":2591,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:15:43.724: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 21 06:15:46.283: INFO: Successfully updated pod "labelsupdate40aac006-eb84-4f65-bfa1-00d8fe99781c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:15:48.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1614" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":151,"skipped":2603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:15:48.312: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-81a188ac-fa78-4b84-93d3-ec98e462052a
STEP: Creating a pod to test consume secrets
May 21 06:15:48.343: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-904b76da-ad49-4ad5-8baa-b9d817323d7d" in namespace "projected-9357" to be "Succeeded or Failed"
May 21 06:15:48.350: INFO: Pod "pod-projected-secrets-904b76da-ad49-4ad5-8baa-b9d817323d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.657237ms
May 21 06:15:50.354: INFO: Pod "pod-projected-secrets-904b76da-ad49-4ad5-8baa-b9d817323d7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01079759s
STEP: Saw pod success
May 21 06:15:50.354: INFO: Pod "pod-projected-secrets-904b76da-ad49-4ad5-8baa-b9d817323d7d" satisfied condition "Succeeded or Failed"
May 21 06:15:50.356: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-secrets-904b76da-ad49-4ad5-8baa-b9d817323d7d container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 06:15:50.369: INFO: Waiting for pod pod-projected-secrets-904b76da-ad49-4ad5-8baa-b9d817323d7d to disappear
May 21 06:15:50.377: INFO: Pod pod-projected-secrets-904b76da-ad49-4ad5-8baa-b9d817323d7d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:15:50.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9357" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2626,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:15:50.384: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9669
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9669
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9669
May 21 06:15:50.437: INFO: Found 0 stateful pods, waiting for 1
May 21 06:16:00.440: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 21 06:16:00.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-9669 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 06:16:00.626: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 06:16:00.626: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 06:16:00.626: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 06:16:00.629: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 21 06:16:10.632: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 06:16:10.632: INFO: Waiting for statefulset status.replicas updated to 0
May 21 06:16:10.649: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999821s
May 21 06:16:11.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991164502s
May 21 06:16:12.655: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988073077s
May 21 06:16:13.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985119567s
May 21 06:16:14.661: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981853677s
May 21 06:16:15.664: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978587293s
May 21 06:16:16.667: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.97561837s
May 21 06:16:17.670: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972483558s
May 21 06:16:18.673: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.9696553s
May 21 06:16:19.677: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.406308ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9669
May 21 06:16:20.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-9669 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 06:16:20.847: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 06:16:20.847: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 06:16:20.847: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 06:16:20.850: INFO: Found 1 stateful pods, waiting for 3
May 21 06:16:30.856: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 06:16:30.856: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 06:16:30.856: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 21 06:16:30.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-9669 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 06:16:31.016: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 06:16:31.016: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 06:16:31.016: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 06:16:31.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-9669 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 06:16:31.183: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 06:16:31.183: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 06:16:31.183: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 06:16:31.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-9669 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 06:16:31.342: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 06:16:31.342: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 06:16:31.342: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 06:16:31.342: INFO: Waiting for statefulset status.replicas updated to 0
May 21 06:16:31.344: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 21 06:16:41.351: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 06:16:41.351: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 21 06:16:41.351: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 21 06:16:41.359: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999801s
May 21 06:16:42.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996858726s
May 21 06:16:43.372: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991276113s
May 21 06:16:44.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984455182s
May 21 06:16:45.382: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97947111s
May 21 06:16:46.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974644604s
May 21 06:16:47.392: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969235096s
May 21 06:16:48.398: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964232919s
May 21 06:16:49.404: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.958560159s
May 21 06:16:50.409: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.00647ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9669
May 21 06:16:51.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-9669 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 06:16:51.568: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 06:16:51.568: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 06:16:51.568: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 06:16:51.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-9669 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 06:16:51.737: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 06:16:51.737: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 06:16:51.737: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 06:16:51.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=statefulset-9669 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 06:16:51.900: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 06:16:51.900: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 06:16:51.900: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 06:16:51.900: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 06:17:11.910: INFO: Deleting all statefulset in ns statefulset-9669
May 21 06:17:11.912: INFO: Scaling statefulset ss to 0
May 21 06:17:11.918: INFO: Waiting for statefulset status.replicas updated to 0
May 21 06:17:11.919: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:11.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9669" for this suite.

• [SLOW TEST:81.561 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":153,"skipped":2635,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:11.946: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:17:11.973: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:14.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7031" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2642,"failed":0}

------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:14.085: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:17:14.114: INFO: Waiting up to 5m0s for pod "busybox-user-65534-7bbab6d6-156c-486f-91c0-0985442f0f32" in namespace "security-context-test-3684" to be "Succeeded or Failed"
May 21 06:17:14.125: INFO: Pod "busybox-user-65534-7bbab6d6-156c-486f-91c0-0985442f0f32": Phase="Pending", Reason="", readiness=false. Elapsed: 11.176125ms
May 21 06:17:16.128: INFO: Pod "busybox-user-65534-7bbab6d6-156c-486f-91c0-0985442f0f32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014298397s
May 21 06:17:16.128: INFO: Pod "busybox-user-65534-7bbab6d6-156c-486f-91c0-0985442f0f32" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:16.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3684" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2642,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:16.136: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 06:17:16.565: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 06:17:18.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757174636, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757174636, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757174636, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757174636, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 06:17:21.583: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:17:21.587: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8770-crds.webhook.example.com via the AdmissionRegistration API
May 21 06:17:22.111: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:22.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2857" for this suite.
STEP: Destroying namespace "webhook-2857-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.704 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":156,"skipped":2657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:22.840: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:22.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4969" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":157,"skipped":2682,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:22.907: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-0c27fa37-5afc-492b-b5fd-86ecec34d6db
STEP: Creating a pod to test consume configMaps
May 21 06:17:22.947: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-83b44af3-773b-42c0-bafb-085b454c5486" in namespace "projected-4149" to be "Succeeded or Failed"
May 21 06:17:22.955: INFO: Pod "pod-projected-configmaps-83b44af3-773b-42c0-bafb-085b454c5486": Phase="Pending", Reason="", readiness=false. Elapsed: 7.850183ms
May 21 06:17:24.957: INFO: Pod "pod-projected-configmaps-83b44af3-773b-42c0-bafb-085b454c5486": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010249923s
STEP: Saw pod success
May 21 06:17:24.957: INFO: Pod "pod-projected-configmaps-83b44af3-773b-42c0-bafb-085b454c5486" satisfied condition "Succeeded or Failed"
May 21 06:17:24.959: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-configmaps-83b44af3-773b-42c0-bafb-085b454c5486 container agnhost-container: <nil>
STEP: delete the pod
May 21 06:17:24.977: INFO: Waiting for pod pod-projected-configmaps-83b44af3-773b-42c0-bafb-085b454c5486 to disappear
May 21 06:17:24.984: INFO: Pod pod-projected-configmaps-83b44af3-773b-42c0-bafb-085b454c5486 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:24.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4149" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":2685,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:24.991: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 06:17:25.698: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 06:17:28.713: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:28.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6211" for this suite.
STEP: Destroying namespace "webhook-6211-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":159,"skipped":2696,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:28.813: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-c3ee19fa-ce33-4542-b69b-a8077d5ce87a
STEP: Creating a pod to test consume configMaps
May 21 06:17:28.854: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e0880ec-54c4-4fa2-9890-aad6dd43ebed" in namespace "configmap-7232" to be "Succeeded or Failed"
May 21 06:17:28.860: INFO: Pod "pod-configmaps-8e0880ec-54c4-4fa2-9890-aad6dd43ebed": Phase="Pending", Reason="", readiness=false. Elapsed: 5.891585ms
May 21 06:17:30.863: INFO: Pod "pod-configmaps-8e0880ec-54c4-4fa2-9890-aad6dd43ebed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00845271s
May 21 06:17:32.866: INFO: Pod "pod-configmaps-8e0880ec-54c4-4fa2-9890-aad6dd43ebed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01141679s
STEP: Saw pod success
May 21 06:17:32.866: INFO: Pod "pod-configmaps-8e0880ec-54c4-4fa2-9890-aad6dd43ebed" satisfied condition "Succeeded or Failed"
May 21 06:17:32.868: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-configmaps-8e0880ec-54c4-4fa2-9890-aad6dd43ebed container agnhost-container: <nil>
STEP: delete the pod
May 21 06:17:32.882: INFO: Waiting for pod pod-configmaps-8e0880ec-54c4-4fa2-9890-aad6dd43ebed to disappear
May 21 06:17:32.890: INFO: Pod pod-configmaps-8e0880ec-54c4-4fa2-9890-aad6dd43ebed no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:32.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7232" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":2697,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:32.901: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-cb6e73c4-e46e-4cd0-aa65-65d8574675dd
STEP: Creating a pod to test consume configMaps
May 21 06:17:32.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64d47342-108e-4907-b24a-837e1a61224c" in namespace "projected-8795" to be "Succeeded or Failed"
May 21 06:17:32.953: INFO: Pod "pod-projected-configmaps-64d47342-108e-4907-b24a-837e1a61224c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.807756ms
May 21 06:17:34.956: INFO: Pod "pod-projected-configmaps-64d47342-108e-4907-b24a-837e1a61224c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010042533s
May 21 06:17:36.959: INFO: Pod "pod-projected-configmaps-64d47342-108e-4907-b24a-837e1a61224c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013460597s
STEP: Saw pod success
May 21 06:17:36.959: INFO: Pod "pod-projected-configmaps-64d47342-108e-4907-b24a-837e1a61224c" satisfied condition "Succeeded or Failed"
May 21 06:17:36.961: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-configmaps-64d47342-108e-4907-b24a-837e1a61224c container agnhost-container: <nil>
STEP: delete the pod
May 21 06:17:36.975: INFO: Waiting for pod pod-projected-configmaps-64d47342-108e-4907-b24a-837e1a61224c to disappear
May 21 06:17:36.981: INFO: Pod pod-projected-configmaps-64d47342-108e-4907-b24a-837e1a61224c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:36.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8795" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":2707,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:36.989: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 21 06:17:37.025: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 21 06:17:37.037: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:37.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-72" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":162,"skipped":2738,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:37.061: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7764
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7764
I0521 06:17:37.123409      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7764, replica count: 2
I0521 06:17:40.173666      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:17:40.173: INFO: Creating new exec pod
May 21 06:17:45.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-7764 exec execpodpzmsn -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 21 06:17:45.336: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 21 06:17:45.336: INFO: stdout: ""
May 21 06:17:45.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-7764 exec execpodpzmsn -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.138 80'
May 21 06:17:45.492: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.138 80\nConnection to 10.100.200.138 80 port [tcp/http] succeeded!\n"
May 21 06:17:45.492: INFO: stdout: ""
May 21 06:17:45.492: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:17:45.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7764" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:8.458 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":163,"skipped":2744,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:17:45.519: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-ca1e935c-edaa-445d-b3b6-7ca48f782ed9 in namespace container-probe-3988
May 21 06:17:47.566: INFO: Started pod liveness-ca1e935c-edaa-445d-b3b6-7ca48f782ed9 in namespace container-probe-3988
STEP: checking the pod's current state and verifying that restartCount is present
May 21 06:17:47.568: INFO: Initial restart count of pod liveness-ca1e935c-edaa-445d-b3b6-7ca48f782ed9 is 0
May 21 06:18:05.618: INFO: Restart count of pod container-probe-3988/liveness-ca1e935c-edaa-445d-b3b6-7ca48f782ed9 is now 1 (18.050072119s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:18:05.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3988" for this suite.

• [SLOW TEST:20.125 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":164,"skipped":2752,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:18:05.645: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6038
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6038
STEP: Creating statefulset with conflicting port in namespace statefulset-6038
STEP: Waiting until pod test-pod will start running in namespace statefulset-6038
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6038
May 21 06:18:09.716: INFO: Observed stateful pod in namespace: statefulset-6038, name: ss-0, uid: 14142305-92e3-4ded-8b2b-ed4809cfeaf7, status phase: Failed. Waiting for statefulset controller to delete.
May 21 06:18:09.719: INFO: Observed stateful pod in namespace: statefulset-6038, name: ss-0, uid: 14142305-92e3-4ded-8b2b-ed4809cfeaf7, status phase: Failed. Waiting for statefulset controller to delete.
May 21 06:18:09.729: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6038
STEP: Removing pod with conflicting port in namespace statefulset-6038
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6038 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 06:18:13.771: INFO: Deleting all statefulset in ns statefulset-6038
May 21 06:18:13.773: INFO: Scaling statefulset ss to 0
May 21 06:18:23.786: INFO: Waiting for statefulset status.replicas updated to 0
May 21 06:18:23.788: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:18:23.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6038" for this suite.

• [SLOW TEST:18.161 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":165,"skipped":2764,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:18:23.807: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-1674
May 21 06:18:25.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 21 06:18:26.019: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 21 06:18:26.019: INFO: stdout: "iptables"
May 21 06:18:26.019: INFO: proxyMode: iptables
May 21 06:18:26.027: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 06:18:26.034: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1674
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1674
I0521 06:18:26.052426      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1674, replica count: 3
I0521 06:18:29.102712      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:18:29.109: INFO: Creating new exec pod
May 21 06:18:32.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
May 21 06:18:32.287: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 21 06:18:32.287: INFO: stdout: ""
May 21 06:18:32.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.110 80'
May 21 06:18:32.440: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.110 80\nConnection to 10.100.200.110 80 port [tcp/http] succeeded!\n"
May 21 06:18:32.440: INFO: stdout: ""
May 21 06:18:32.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 30503'
May 21 06:18:32.616: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 30503\nConnection to 30.0.0.12 30503 port [tcp/30503] succeeded!\n"
May 21 06:18:32.616: INFO: stdout: ""
May 21 06:18:32.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30503'
May 21 06:18:32.777: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30503\nConnection to 30.0.0.11 30503 port [tcp/30503] succeeded!\n"
May 21 06:18:32.777: INFO: stdout: ""
May 21 06:18:32.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 30503'
May 21 06:18:32.940: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 30503\nConnection to 30.0.0.12 30503 port [tcp/30503] succeeded!\n"
May 21 06:18:32.940: INFO: stdout: ""
May 21 06:18:32.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30503'
May 21 06:18:33.093: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30503\nConnection to 30.0.0.11 30503 port [tcp/30503] succeeded!\n"
May 21 06:18:33.093: INFO: stdout: ""
May 21 06:18:33.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.0.0.12:30503/ ; done'
May 21 06:18:33.335: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n"
May 21 06:18:33.335: INFO: stdout: "\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c\naffinity-nodeport-timeout-pvx6c"
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Received response from host: affinity-nodeport-timeout-pvx6c
May 21 06:18:33.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://30.0.0.12:30503/'
May 21 06:18:33.487: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n"
May 21 06:18:33.487: INFO: stdout: "affinity-nodeport-timeout-pvx6c"
May 21 06:18:53.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://30.0.0.12:30503/'
May 21 06:18:53.656: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n"
May 21 06:18:53.656: INFO: stdout: "affinity-nodeport-timeout-pvx6c"
May 21 06:19:13.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://30.0.0.12:30503/'
May 21 06:19:13.812: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n"
May 21 06:19:13.812: INFO: stdout: "affinity-nodeport-timeout-pvx6c"
May 21 06:19:33.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-1674 exec execpod-affinityt9g47 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://30.0.0.12:30503/'
May 21 06:19:33.998: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://30.0.0.12:30503/\n"
May 21 06:19:33.998: INFO: stdout: "affinity-nodeport-timeout-kknbt"
May 21 06:19:33.998: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1674, will wait for the garbage collector to delete the pods
May 21 06:19:34.067: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 3.78755ms
May 21 06:19:34.667: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.372236ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:19:38.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1674" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:75.082 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":166,"skipped":2772,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:19:38.891: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
May 21 06:19:39.442: INFO: created pod pod-service-account-defaultsa
May 21 06:19:39.442: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 21 06:19:39.447: INFO: created pod pod-service-account-mountsa
May 21 06:19:39.447: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 21 06:19:39.468: INFO: created pod pod-service-account-nomountsa
May 21 06:19:39.468: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 21 06:19:39.479: INFO: created pod pod-service-account-defaultsa-mountspec
May 21 06:19:39.479: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 21 06:19:39.493: INFO: created pod pod-service-account-mountsa-mountspec
May 21 06:19:39.493: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 21 06:19:39.502: INFO: created pod pod-service-account-nomountsa-mountspec
May 21 06:19:39.502: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 21 06:19:39.512: INFO: created pod pod-service-account-defaultsa-nomountspec
May 21 06:19:39.512: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 21 06:19:39.523: INFO: created pod pod-service-account-mountsa-nomountspec
May 21 06:19:39.524: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 21 06:19:39.533: INFO: created pod pod-service-account-nomountsa-nomountspec
May 21 06:19:39.533: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:19:39.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7162" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":167,"skipped":2780,"failed":0}
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:19:39.565: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:19:39.614: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3017
I0521 06:19:39.624739      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3017, replica count: 1
I0521 06:19:40.674983      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 06:19:41.675098      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:19:41.782: INFO: Created: latency-svc-s9bj2
May 21 06:19:41.794: INFO: Got endpoints: latency-svc-s9bj2 [19.015076ms]
May 21 06:19:41.809: INFO: Created: latency-svc-z595n
May 21 06:19:41.812: INFO: Got endpoints: latency-svc-z595n [18.323153ms]
May 21 06:19:41.816: INFO: Created: latency-svc-fb77k
May 21 06:19:41.821: INFO: Got endpoints: latency-svc-fb77k [26.585884ms]
May 21 06:19:41.834: INFO: Created: latency-svc-nzxld
May 21 06:19:41.834: INFO: Got endpoints: latency-svc-nzxld [39.058854ms]
May 21 06:19:41.836: INFO: Created: latency-svc-vxdq6
May 21 06:19:41.846: INFO: Got endpoints: latency-svc-vxdq6 [51.560238ms]
May 21 06:19:41.849: INFO: Created: latency-svc-pj76z
May 21 06:19:41.859: INFO: Got endpoints: latency-svc-pj76z [63.610328ms]
May 21 06:19:41.859: INFO: Created: latency-svc-7clcj
May 21 06:19:41.868: INFO: Got endpoints: latency-svc-7clcj [72.910342ms]
May 21 06:19:41.882: INFO: Created: latency-svc-24zxn
May 21 06:19:41.883: INFO: Created: latency-svc-qvhwq
May 21 06:19:41.884: INFO: Got endpoints: latency-svc-24zxn [89.094805ms]
May 21 06:19:41.893: INFO: Got endpoints: latency-svc-qvhwq [97.197111ms]
May 21 06:19:41.907: INFO: Created: latency-svc-xpt2c
May 21 06:19:41.915: INFO: Created: latency-svc-xgw9x
May 21 06:19:41.916: INFO: Got endpoints: latency-svc-xpt2c [119.871506ms]
May 21 06:19:41.926: INFO: Created: latency-svc-j8wkc
May 21 06:19:41.926: INFO: Got endpoints: latency-svc-xgw9x [130.544743ms]
May 21 06:19:41.937: INFO: Created: latency-svc-rwjz2
May 21 06:19:41.940: INFO: Got endpoints: latency-svc-rwjz2 [143.670344ms]
May 21 06:19:41.940: INFO: Got endpoints: latency-svc-j8wkc [144.036253ms]
May 21 06:19:41.948: INFO: Created: latency-svc-z2hx7
May 21 06:19:41.957: INFO: Got endpoints: latency-svc-z2hx7 [160.325811ms]
May 21 06:19:41.959: INFO: Created: latency-svc-mqffh
May 21 06:19:41.967: INFO: Got endpoints: latency-svc-mqffh [170.757859ms]
May 21 06:19:41.970: INFO: Created: latency-svc-sxrbt
May 21 06:19:41.978: INFO: Got endpoints: latency-svc-sxrbt [181.706714ms]
May 21 06:19:41.982: INFO: Created: latency-svc-l7n5p
May 21 06:19:41.990: INFO: Got endpoints: latency-svc-l7n5p [177.824072ms]
May 21 06:19:42.001: INFO: Created: latency-svc-c8825
May 21 06:19:42.003: INFO: Got endpoints: latency-svc-c8825 [182.138634ms]
May 21 06:19:42.017: INFO: Created: latency-svc-lk67h
May 21 06:19:42.020: INFO: Got endpoints: latency-svc-lk67h [185.939697ms]
May 21 06:19:42.024: INFO: Created: latency-svc-q99qv
May 21 06:19:42.040: INFO: Got endpoints: latency-svc-q99qv [193.662079ms]
May 21 06:19:42.047: INFO: Created: latency-svc-z46z2
May 21 06:19:42.053: INFO: Got endpoints: latency-svc-z46z2 [194.283669ms]
May 21 06:19:42.059: INFO: Created: latency-svc-hjtsq
May 21 06:19:42.061: INFO: Got endpoints: latency-svc-hjtsq [193.125268ms]
May 21 06:19:42.071: INFO: Created: latency-svc-cd6vg
May 21 06:19:42.083: INFO: Got endpoints: latency-svc-cd6vg [198.621335ms]
May 21 06:19:42.089: INFO: Created: latency-svc-bmxq4
May 21 06:19:42.095: INFO: Got endpoints: latency-svc-bmxq4 [202.589483ms]
May 21 06:19:42.100: INFO: Created: latency-svc-4nbqh
May 21 06:19:42.105: INFO: Got endpoints: latency-svc-4nbqh [189.648532ms]
May 21 06:19:42.116: INFO: Created: latency-svc-4xmrf
May 21 06:19:42.120: INFO: Created: latency-svc-trbpm
May 21 06:19:42.121: INFO: Got endpoints: latency-svc-4xmrf [194.995657ms]
May 21 06:19:42.131: INFO: Got endpoints: latency-svc-trbpm [191.536378ms]
May 21 06:19:42.132: INFO: Created: latency-svc-cwxt7
May 21 06:19:42.140: INFO: Got endpoints: latency-svc-cwxt7 [200.273539ms]
May 21 06:19:42.141: INFO: Created: latency-svc-8kbw2
May 21 06:19:42.154: INFO: Created: latency-svc-dxpmn
May 21 06:19:42.157: INFO: Got endpoints: latency-svc-8kbw2 [200.632337ms]
May 21 06:19:42.163: INFO: Got endpoints: latency-svc-dxpmn [195.32826ms]
May 21 06:19:42.174: INFO: Created: latency-svc-nzwqr
May 21 06:19:42.177: INFO: Got endpoints: latency-svc-nzwqr [199.188668ms]
May 21 06:19:42.193: INFO: Created: latency-svc-w4tjp
May 21 06:19:42.204: INFO: Created: latency-svc-wdknm
May 21 06:19:42.204: INFO: Got endpoints: latency-svc-w4tjp [213.361777ms]
May 21 06:19:42.215: INFO: Got endpoints: latency-svc-wdknm [211.783546ms]
May 21 06:19:42.215: INFO: Created: latency-svc-wj6fj
May 21 06:19:42.226: INFO: Created: latency-svc-w2kzp
May 21 06:19:42.229: INFO: Got endpoints: latency-svc-wj6fj [208.913308ms]
May 21 06:19:42.241: INFO: Got endpoints: latency-svc-w2kzp [36.989291ms]
May 21 06:19:42.242: INFO: Created: latency-svc-gccxc
May 21 06:19:42.249: INFO: Got endpoints: latency-svc-gccxc [208.373458ms]
May 21 06:19:42.251: INFO: Created: latency-svc-k4gq7
May 21 06:19:42.260: INFO: Got endpoints: latency-svc-k4gq7 [206.779018ms]
May 21 06:19:42.261: INFO: Created: latency-svc-9jgkv
May 21 06:19:42.274: INFO: Got endpoints: latency-svc-9jgkv [212.572666ms]
May 21 06:19:42.274: INFO: Created: latency-svc-njcgx
May 21 06:19:42.274: INFO: Got endpoints: latency-svc-njcgx [190.836637ms]
May 21 06:19:42.284: INFO: Created: latency-svc-h6gqh
May 21 06:19:42.293: INFO: Got endpoints: latency-svc-h6gqh [197.573557ms]
May 21 06:19:42.294: INFO: Created: latency-svc-8cpxr
May 21 06:19:42.304: INFO: Created: latency-svc-vjwbm
May 21 06:19:42.316: INFO: Created: latency-svc-57msk
May 21 06:19:42.321: INFO: Created: latency-svc-mftbz
May 21 06:19:42.323: INFO: Created: latency-svc-xv68x
May 21 06:19:42.333: INFO: Created: latency-svc-c7gck
May 21 06:19:42.336: INFO: Created: latency-svc-7zhxb
May 21 06:19:42.338: INFO: Got endpoints: latency-svc-8cpxr [232.420921ms]
May 21 06:19:42.346: INFO: Created: latency-svc-jhtdg
May 21 06:19:42.357: INFO: Created: latency-svc-47w6f
May 21 06:19:42.358: INFO: Created: latency-svc-xpc6d
May 21 06:19:42.360: INFO: Created: latency-svc-mw2dh
May 21 06:19:42.371: INFO: Created: latency-svc-fx7qs
May 21 06:19:42.374: INFO: Created: latency-svc-6g8l2
May 21 06:19:42.376: INFO: Created: latency-svc-jkvwz
May 21 06:19:42.388: INFO: Got endpoints: latency-svc-vjwbm [265.872063ms]
May 21 06:19:42.388: INFO: Created: latency-svc-m4bbv
May 21 06:19:42.397: INFO: Created: latency-svc-mrg22
May 21 06:19:42.401: INFO: Created: latency-svc-8kjhf
May 21 06:19:42.436: INFO: Got endpoints: latency-svc-57msk [305.053654ms]
May 21 06:19:42.444: INFO: Created: latency-svc-vkrcd
May 21 06:19:42.486: INFO: Got endpoints: latency-svc-mftbz [346.08671ms]
May 21 06:19:42.496: INFO: Created: latency-svc-2gmvd
May 21 06:19:42.536: INFO: Got endpoints: latency-svc-xv68x [378.691239ms]
May 21 06:19:42.542: INFO: Created: latency-svc-6nn48
May 21 06:19:42.586: INFO: Got endpoints: latency-svc-c7gck [423.066486ms]
May 21 06:19:42.598: INFO: Created: latency-svc-djmtn
May 21 06:19:42.637: INFO: Got endpoints: latency-svc-7zhxb [459.957624ms]
May 21 06:19:42.649: INFO: Created: latency-svc-5hkqf
May 21 06:19:42.686: INFO: Got endpoints: latency-svc-jhtdg [470.75957ms]
May 21 06:19:42.698: INFO: Created: latency-svc-w2cd5
May 21 06:19:42.736: INFO: Got endpoints: latency-svc-47w6f [506.566797ms]
May 21 06:19:42.742: INFO: Created: latency-svc-2kkn8
May 21 06:19:42.787: INFO: Got endpoints: latency-svc-xpc6d [546.048825ms]
May 21 06:19:42.795: INFO: Created: latency-svc-rmxs6
May 21 06:19:42.837: INFO: Got endpoints: latency-svc-mw2dh [588.345135ms]
May 21 06:19:42.851: INFO: Created: latency-svc-54882
May 21 06:19:42.886: INFO: Got endpoints: latency-svc-fx7qs [626.407401ms]
May 21 06:19:42.897: INFO: Created: latency-svc-mgh9t
May 21 06:19:42.936: INFO: Got endpoints: latency-svc-6g8l2 [662.435151ms]
May 21 06:19:42.948: INFO: Created: latency-svc-krnzm
May 21 06:19:42.985: INFO: Got endpoints: latency-svc-jkvwz [711.381522ms]
May 21 06:19:42.998: INFO: Created: latency-svc-pjrzb
May 21 06:19:43.038: INFO: Got endpoints: latency-svc-m4bbv [744.769136ms]
May 21 06:19:43.048: INFO: Created: latency-svc-vhz5h
May 21 06:19:43.085: INFO: Got endpoints: latency-svc-mrg22 [747.68823ms]
May 21 06:19:43.098: INFO: Created: latency-svc-4qkmd
May 21 06:19:43.137: INFO: Got endpoints: latency-svc-8kjhf [749.513315ms]
May 21 06:19:43.148: INFO: Created: latency-svc-7786x
May 21 06:19:43.185: INFO: Got endpoints: latency-svc-vkrcd [748.983622ms]
May 21 06:19:43.191: INFO: Created: latency-svc-4pgs7
May 21 06:19:43.235: INFO: Got endpoints: latency-svc-2gmvd [748.50312ms]
May 21 06:19:43.243: INFO: Created: latency-svc-n4ksf
May 21 06:19:43.286: INFO: Got endpoints: latency-svc-6nn48 [750.456752ms]
May 21 06:19:43.296: INFO: Created: latency-svc-w959v
May 21 06:19:43.335: INFO: Got endpoints: latency-svc-djmtn [749.601249ms]
May 21 06:19:43.344: INFO: Created: latency-svc-q4cl4
May 21 06:19:43.387: INFO: Got endpoints: latency-svc-5hkqf [749.268008ms]
May 21 06:19:43.399: INFO: Created: latency-svc-p4vpp
May 21 06:19:43.439: INFO: Got endpoints: latency-svc-w2cd5 [753.407007ms]
May 21 06:19:43.447: INFO: Created: latency-svc-9prxg
May 21 06:19:43.485: INFO: Got endpoints: latency-svc-2kkn8 [749.149874ms]
May 21 06:19:43.490: INFO: Created: latency-svc-vnct4
May 21 06:19:43.536: INFO: Got endpoints: latency-svc-rmxs6 [748.4463ms]
May 21 06:19:43.548: INFO: Created: latency-svc-snhkd
May 21 06:19:43.592: INFO: Got endpoints: latency-svc-54882 [754.482162ms]
May 21 06:19:43.599: INFO: Created: latency-svc-mn4z4
May 21 06:19:43.635: INFO: Got endpoints: latency-svc-mgh9t [748.705667ms]
May 21 06:19:43.648: INFO: Created: latency-svc-wgbg8
May 21 06:19:43.685: INFO: Got endpoints: latency-svc-krnzm [748.56922ms]
May 21 06:19:43.689: INFO: Created: latency-svc-wlbmc
May 21 06:19:43.736: INFO: Got endpoints: latency-svc-pjrzb [750.258231ms]
May 21 06:19:43.744: INFO: Created: latency-svc-pxkns
May 21 06:19:43.788: INFO: Got endpoints: latency-svc-vhz5h [750.022507ms]
May 21 06:19:43.796: INFO: Created: latency-svc-mqs5t
May 21 06:19:43.837: INFO: Got endpoints: latency-svc-4qkmd [752.057393ms]
May 21 06:19:43.848: INFO: Created: latency-svc-mdp2v
May 21 06:19:43.885: INFO: Got endpoints: latency-svc-7786x [747.735707ms]
May 21 06:19:43.903: INFO: Created: latency-svc-hmqgh
May 21 06:19:43.935: INFO: Got endpoints: latency-svc-4pgs7 [749.751207ms]
May 21 06:19:43.948: INFO: Created: latency-svc-cjg2s
May 21 06:19:44.027: INFO: Got endpoints: latency-svc-n4ksf [791.869927ms]
May 21 06:19:44.040: INFO: Got endpoints: latency-svc-w959v [753.204877ms]
May 21 06:19:44.049: INFO: Created: latency-svc-v97kx
May 21 06:19:44.058: INFO: Created: latency-svc-d9f4v
May 21 06:19:44.086: INFO: Got endpoints: latency-svc-q4cl4 [750.573397ms]
May 21 06:19:44.093: INFO: Created: latency-svc-n65x2
May 21 06:19:44.136: INFO: Got endpoints: latency-svc-p4vpp [748.819929ms]
May 21 06:19:44.151: INFO: Created: latency-svc-p5b5r
May 21 06:19:44.186: INFO: Got endpoints: latency-svc-9prxg [746.568896ms]
May 21 06:19:44.193: INFO: Created: latency-svc-z7vb7
May 21 06:19:44.236: INFO: Got endpoints: latency-svc-vnct4 [750.935334ms]
May 21 06:19:44.248: INFO: Created: latency-svc-zqqnn
May 21 06:19:44.288: INFO: Got endpoints: latency-svc-snhkd [752.400315ms]
May 21 06:19:44.297: INFO: Created: latency-svc-2m89k
May 21 06:19:44.337: INFO: Got endpoints: latency-svc-mn4z4 [745.151136ms]
May 21 06:19:44.348: INFO: Created: latency-svc-lll4n
May 21 06:19:44.386: INFO: Got endpoints: latency-svc-wgbg8 [750.799146ms]
May 21 06:19:44.399: INFO: Created: latency-svc-7nczx
May 21 06:19:44.435: INFO: Got endpoints: latency-svc-wlbmc [750.236832ms]
May 21 06:19:44.448: INFO: Created: latency-svc-mmwzv
May 21 06:19:44.485: INFO: Got endpoints: latency-svc-pxkns [749.002829ms]
May 21 06:19:44.498: INFO: Created: latency-svc-bfzjj
May 21 06:19:44.535: INFO: Got endpoints: latency-svc-mqs5t [747.470564ms]
May 21 06:19:44.549: INFO: Created: latency-svc-grjpq
May 21 06:19:44.585: INFO: Got endpoints: latency-svc-mdp2v [747.411842ms]
May 21 06:19:44.602: INFO: Created: latency-svc-rztfk
May 21 06:19:44.644: INFO: Got endpoints: latency-svc-hmqgh [759.175774ms]
May 21 06:19:44.663: INFO: Created: latency-svc-hnpgp
May 21 06:19:44.692: INFO: Got endpoints: latency-svc-cjg2s [757.106221ms]
May 21 06:19:44.707: INFO: Created: latency-svc-t496s
May 21 06:19:44.738: INFO: Got endpoints: latency-svc-v97kx [710.94042ms]
May 21 06:19:44.751: INFO: Created: latency-svc-bgshp
May 21 06:19:44.786: INFO: Got endpoints: latency-svc-d9f4v [746.007781ms]
May 21 06:19:44.794: INFO: Created: latency-svc-f7ftj
May 21 06:19:44.840: INFO: Got endpoints: latency-svc-n65x2 [753.679748ms]
May 21 06:19:44.858: INFO: Created: latency-svc-xgjsh
May 21 06:19:44.886: INFO: Got endpoints: latency-svc-p5b5r [750.65676ms]
May 21 06:19:44.897: INFO: Created: latency-svc-xxpr5
May 21 06:19:44.936: INFO: Got endpoints: latency-svc-z7vb7 [750.319028ms]
May 21 06:19:44.947: INFO: Created: latency-svc-4lsjm
May 21 06:19:44.986: INFO: Got endpoints: latency-svc-zqqnn [749.582272ms]
May 21 06:19:44.992: INFO: Created: latency-svc-242n5
May 21 06:19:45.036: INFO: Got endpoints: latency-svc-2m89k [747.335594ms]
May 21 06:19:45.055: INFO: Created: latency-svc-sfgvf
May 21 06:19:45.086: INFO: Got endpoints: latency-svc-lll4n [749.066798ms]
May 21 06:19:45.097: INFO: Created: latency-svc-cjr9j
May 21 06:19:45.146: INFO: Got endpoints: latency-svc-7nczx [760.089939ms]
May 21 06:19:45.158: INFO: Created: latency-svc-rg77r
May 21 06:19:45.188: INFO: Got endpoints: latency-svc-mmwzv [753.070522ms]
May 21 06:19:45.199: INFO: Created: latency-svc-rs6mj
May 21 06:19:45.242: INFO: Got endpoints: latency-svc-bfzjj [756.552753ms]
May 21 06:19:45.252: INFO: Created: latency-svc-jbr7z
May 21 06:19:45.285: INFO: Got endpoints: latency-svc-grjpq [749.579998ms]
May 21 06:19:45.298: INFO: Created: latency-svc-nffsz
May 21 06:19:45.336: INFO: Got endpoints: latency-svc-rztfk [750.892076ms]
May 21 06:19:45.349: INFO: Created: latency-svc-lcd6h
May 21 06:19:45.389: INFO: Got endpoints: latency-svc-hnpgp [744.37984ms]
May 21 06:19:45.398: INFO: Created: latency-svc-6rgqb
May 21 06:19:45.437: INFO: Got endpoints: latency-svc-t496s [743.519775ms]
May 21 06:19:45.442: INFO: Created: latency-svc-jqqmg
May 21 06:19:45.485: INFO: Got endpoints: latency-svc-bgshp [746.701758ms]
May 21 06:19:45.492: INFO: Created: latency-svc-7ppfh
May 21 06:19:45.535: INFO: Got endpoints: latency-svc-f7ftj [749.036894ms]
May 21 06:19:45.543: INFO: Created: latency-svc-5gm8c
May 21 06:19:45.585: INFO: Got endpoints: latency-svc-xgjsh [745.091792ms]
May 21 06:19:45.597: INFO: Created: latency-svc-xs6tr
May 21 06:19:45.635: INFO: Got endpoints: latency-svc-xxpr5 [748.721842ms]
May 21 06:19:45.646: INFO: Created: latency-svc-8m2d8
May 21 06:19:45.685: INFO: Got endpoints: latency-svc-4lsjm [748.760732ms]
May 21 06:19:45.699: INFO: Created: latency-svc-h6wbm
May 21 06:19:45.735: INFO: Got endpoints: latency-svc-242n5 [749.167641ms]
May 21 06:19:45.748: INFO: Created: latency-svc-l9r7v
May 21 06:19:45.787: INFO: Got endpoints: latency-svc-sfgvf [750.897988ms]
May 21 06:19:45.798: INFO: Created: latency-svc-6wrhf
May 21 06:19:45.836: INFO: Got endpoints: latency-svc-cjr9j [750.28251ms]
May 21 06:19:45.844: INFO: Created: latency-svc-dnrfd
May 21 06:19:45.885: INFO: Got endpoints: latency-svc-rg77r [738.920958ms]
May 21 06:19:45.892: INFO: Created: latency-svc-2bkpz
May 21 06:19:45.938: INFO: Got endpoints: latency-svc-rs6mj [749.517004ms]
May 21 06:19:45.944: INFO: Created: latency-svc-5t9vh
May 21 06:19:45.986: INFO: Got endpoints: latency-svc-jbr7z [743.52822ms]
May 21 06:19:45.998: INFO: Created: latency-svc-bscnt
May 21 06:19:46.040: INFO: Got endpoints: latency-svc-nffsz [754.661585ms]
May 21 06:19:46.048: INFO: Created: latency-svc-cszrv
May 21 06:19:46.089: INFO: Got endpoints: latency-svc-lcd6h [753.010073ms]
May 21 06:19:46.098: INFO: Created: latency-svc-sst6x
May 21 06:19:46.135: INFO: Got endpoints: latency-svc-6rgqb [746.664672ms]
May 21 06:19:46.147: INFO: Created: latency-svc-gz2ht
May 21 06:19:46.185: INFO: Got endpoints: latency-svc-jqqmg [748.075495ms]
May 21 06:19:46.198: INFO: Created: latency-svc-9z7lc
May 21 06:19:46.235: INFO: Got endpoints: latency-svc-7ppfh [750.407218ms]
May 21 06:19:46.247: INFO: Created: latency-svc-txrtm
May 21 06:19:46.286: INFO: Got endpoints: latency-svc-5gm8c [750.321379ms]
May 21 06:19:46.290: INFO: Created: latency-svc-g45wz
May 21 06:19:46.339: INFO: Got endpoints: latency-svc-xs6tr [753.713613ms]
May 21 06:19:46.344: INFO: Created: latency-svc-mdfn4
May 21 06:19:46.386: INFO: Got endpoints: latency-svc-8m2d8 [750.340101ms]
May 21 06:19:46.398: INFO: Created: latency-svc-wkq6l
May 21 06:19:46.436: INFO: Got endpoints: latency-svc-h6wbm [750.899035ms]
May 21 06:19:46.447: INFO: Created: latency-svc-r5c6r
May 21 06:19:46.493: INFO: Got endpoints: latency-svc-l9r7v [757.969801ms]
May 21 06:19:46.501: INFO: Created: latency-svc-2bv7l
May 21 06:19:46.536: INFO: Got endpoints: latency-svc-6wrhf [749.591367ms]
May 21 06:19:46.542: INFO: Created: latency-svc-5htjd
May 21 06:19:46.586: INFO: Got endpoints: latency-svc-dnrfd [749.046506ms]
May 21 06:19:46.597: INFO: Created: latency-svc-2qdvr
May 21 06:19:46.636: INFO: Got endpoints: latency-svc-2bkpz [750.170142ms]
May 21 06:19:46.647: INFO: Created: latency-svc-rf279
May 21 06:19:46.685: INFO: Got endpoints: latency-svc-5t9vh [747.275478ms]
May 21 06:19:46.698: INFO: Created: latency-svc-cfrwk
May 21 06:19:46.736: INFO: Got endpoints: latency-svc-bscnt [749.850649ms]
May 21 06:19:46.748: INFO: Created: latency-svc-9mvj2
May 21 06:19:46.791: INFO: Got endpoints: latency-svc-cszrv [751.240781ms]
May 21 06:19:46.797: INFO: Created: latency-svc-gcwj9
May 21 06:19:46.836: INFO: Got endpoints: latency-svc-sst6x [747.333396ms]
May 21 06:19:46.849: INFO: Created: latency-svc-vw78w
May 21 06:19:46.885: INFO: Got endpoints: latency-svc-gz2ht [749.967663ms]
May 21 06:19:46.891: INFO: Created: latency-svc-tk4gr
May 21 06:19:46.936: INFO: Got endpoints: latency-svc-9z7lc [750.38293ms]
May 21 06:19:46.942: INFO: Created: latency-svc-k5mv5
May 21 06:19:46.987: INFO: Got endpoints: latency-svc-txrtm [751.613226ms]
May 21 06:19:46.998: INFO: Created: latency-svc-khgvj
May 21 06:19:47.036: INFO: Got endpoints: latency-svc-g45wz [749.959191ms]
May 21 06:19:47.041: INFO: Created: latency-svc-jhrj6
May 21 06:19:47.085: INFO: Got endpoints: latency-svc-mdfn4 [746.698602ms]
May 21 06:19:47.099: INFO: Created: latency-svc-f685z
May 21 06:19:47.136: INFO: Got endpoints: latency-svc-wkq6l [750.658135ms]
May 21 06:19:47.148: INFO: Created: latency-svc-p6mh6
May 21 06:19:47.186: INFO: Got endpoints: latency-svc-r5c6r [749.734154ms]
May 21 06:19:47.198: INFO: Created: latency-svc-zkk5q
May 21 06:19:47.235: INFO: Got endpoints: latency-svc-2bv7l [742.303259ms]
May 21 06:19:47.241: INFO: Created: latency-svc-lp6lf
May 21 06:19:47.285: INFO: Got endpoints: latency-svc-5htjd [748.931467ms]
May 21 06:19:47.292: INFO: Created: latency-svc-qt6sd
May 21 06:19:47.336: INFO: Got endpoints: latency-svc-2qdvr [750.89353ms]
May 21 06:19:47.343: INFO: Created: latency-svc-x6fb5
May 21 06:19:47.386: INFO: Got endpoints: latency-svc-rf279 [750.231284ms]
May 21 06:19:47.397: INFO: Created: latency-svc-hfjsb
May 21 06:19:47.438: INFO: Got endpoints: latency-svc-cfrwk [753.034163ms]
May 21 06:19:47.454: INFO: Created: latency-svc-pqlxd
May 21 06:19:47.485: INFO: Got endpoints: latency-svc-9mvj2 [749.38769ms]
May 21 06:19:47.498: INFO: Created: latency-svc-tbz92
May 21 06:19:47.535: INFO: Got endpoints: latency-svc-gcwj9 [744.095ms]
May 21 06:19:47.548: INFO: Created: latency-svc-sw6gz
May 21 06:19:47.586: INFO: Got endpoints: latency-svc-vw78w [749.230759ms]
May 21 06:19:47.598: INFO: Created: latency-svc-v2ksh
May 21 06:19:47.635: INFO: Got endpoints: latency-svc-tk4gr [749.063103ms]
May 21 06:19:47.648: INFO: Created: latency-svc-htbl6
May 21 06:19:47.688: INFO: Got endpoints: latency-svc-k5mv5 [751.674091ms]
May 21 06:19:47.692: INFO: Created: latency-svc-m6fg9
May 21 06:19:47.735: INFO: Got endpoints: latency-svc-khgvj [748.037396ms]
May 21 06:19:47.742: INFO: Created: latency-svc-g2zjz
May 21 06:19:47.786: INFO: Got endpoints: latency-svc-jhrj6 [750.515832ms]
May 21 06:19:47.798: INFO: Created: latency-svc-wg6jc
May 21 06:19:47.835: INFO: Got endpoints: latency-svc-f685z [749.61907ms]
May 21 06:19:47.850: INFO: Created: latency-svc-8zlpg
May 21 06:19:47.885: INFO: Got endpoints: latency-svc-p6mh6 [749.158335ms]
May 21 06:19:47.903: INFO: Created: latency-svc-xz66m
May 21 06:19:47.935: INFO: Got endpoints: latency-svc-zkk5q [749.183488ms]
May 21 06:19:47.949: INFO: Created: latency-svc-vvbwg
May 21 06:19:47.986: INFO: Got endpoints: latency-svc-lp6lf [750.039882ms]
May 21 06:19:47.997: INFO: Created: latency-svc-mlrdw
May 21 06:19:48.039: INFO: Got endpoints: latency-svc-qt6sd [753.79975ms]
May 21 06:19:48.047: INFO: Created: latency-svc-svbsp
May 21 06:19:48.085: INFO: Got endpoints: latency-svc-x6fb5 [748.343413ms]
May 21 06:19:48.091: INFO: Created: latency-svc-fwcrl
May 21 06:19:48.137: INFO: Got endpoints: latency-svc-hfjsb [750.959115ms]
May 21 06:19:48.144: INFO: Created: latency-svc-ztb6p
May 21 06:19:48.185: INFO: Got endpoints: latency-svc-pqlxd [746.613947ms]
May 21 06:19:48.199: INFO: Created: latency-svc-x5cns
May 21 06:19:48.240: INFO: Got endpoints: latency-svc-tbz92 [754.384567ms]
May 21 06:19:48.251: INFO: Created: latency-svc-zmvg5
May 21 06:19:48.285: INFO: Got endpoints: latency-svc-sw6gz [750.083593ms]
May 21 06:19:48.298: INFO: Created: latency-svc-wwwl7
May 21 06:19:48.338: INFO: Got endpoints: latency-svc-v2ksh [752.582969ms]
May 21 06:19:48.349: INFO: Created: latency-svc-cgjzk
May 21 06:19:48.385: INFO: Got endpoints: latency-svc-htbl6 [749.850111ms]
May 21 06:19:48.400: INFO: Created: latency-svc-jvmxf
May 21 06:19:48.435: INFO: Got endpoints: latency-svc-m6fg9 [747.754026ms]
May 21 06:19:48.448: INFO: Created: latency-svc-6lt5m
May 21 06:19:48.488: INFO: Got endpoints: latency-svc-g2zjz [752.588271ms]
May 21 06:19:48.497: INFO: Created: latency-svc-qpddg
May 21 06:19:48.537: INFO: Got endpoints: latency-svc-wg6jc [751.261501ms]
May 21 06:19:48.548: INFO: Created: latency-svc-nnptq
May 21 06:19:48.585: INFO: Got endpoints: latency-svc-8zlpg [750.086545ms]
May 21 06:19:48.599: INFO: Created: latency-svc-2s7c2
May 21 06:19:48.636: INFO: Got endpoints: latency-svc-xz66m [750.31345ms]
May 21 06:19:48.647: INFO: Created: latency-svc-vmld5
May 21 06:19:48.687: INFO: Got endpoints: latency-svc-vvbwg [751.646374ms]
May 21 06:19:48.698: INFO: Created: latency-svc-kzcmr
May 21 06:19:48.736: INFO: Got endpoints: latency-svc-mlrdw [750.233841ms]
May 21 06:19:48.743: INFO: Created: latency-svc-7q6cz
May 21 06:19:48.785: INFO: Got endpoints: latency-svc-svbsp [746.238896ms]
May 21 06:19:48.792: INFO: Created: latency-svc-dr8fc
May 21 06:19:48.837: INFO: Got endpoints: latency-svc-fwcrl [751.794398ms]
May 21 06:19:48.848: INFO: Created: latency-svc-xwgvr
May 21 06:19:48.893: INFO: Got endpoints: latency-svc-ztb6p [756.470896ms]
May 21 06:19:48.899: INFO: Created: latency-svc-zv25b
May 21 06:19:48.937: INFO: Got endpoints: latency-svc-x5cns [751.15017ms]
May 21 06:19:48.947: INFO: Created: latency-svc-h7cj9
May 21 06:19:48.985: INFO: Got endpoints: latency-svc-zmvg5 [745.22121ms]
May 21 06:19:49.000: INFO: Created: latency-svc-f8rp6
May 21 06:19:49.036: INFO: Got endpoints: latency-svc-wwwl7 [750.179464ms]
May 21 06:19:49.048: INFO: Created: latency-svc-7vnms
May 21 06:19:49.085: INFO: Got endpoints: latency-svc-cgjzk [746.824671ms]
May 21 06:19:49.097: INFO: Created: latency-svc-9gnfd
May 21 06:19:49.136: INFO: Got endpoints: latency-svc-jvmxf [750.955528ms]
May 21 06:19:49.142: INFO: Created: latency-svc-f9t65
May 21 06:19:49.186: INFO: Got endpoints: latency-svc-6lt5m [750.248111ms]
May 21 06:19:49.193: INFO: Created: latency-svc-flj6p
May 21 06:19:49.237: INFO: Got endpoints: latency-svc-qpddg [748.695237ms]
May 21 06:19:49.243: INFO: Created: latency-svc-b9xpg
May 21 06:19:49.287: INFO: Got endpoints: latency-svc-nnptq [749.879387ms]
May 21 06:19:49.297: INFO: Created: latency-svc-88vbf
May 21 06:19:49.335: INFO: Got endpoints: latency-svc-2s7c2 [749.954885ms]
May 21 06:19:49.347: INFO: Created: latency-svc-2sqch
May 21 06:19:49.385: INFO: Got endpoints: latency-svc-vmld5 [749.531672ms]
May 21 06:19:49.397: INFO: Created: latency-svc-74zcx
May 21 06:19:49.436: INFO: Got endpoints: latency-svc-kzcmr [748.87311ms]
May 21 06:19:49.447: INFO: Created: latency-svc-dwwr9
May 21 06:19:49.490: INFO: Got endpoints: latency-svc-7q6cz [753.566492ms]
May 21 06:19:49.498: INFO: Created: latency-svc-42pht
May 21 06:19:49.539: INFO: Got endpoints: latency-svc-dr8fc [753.471161ms]
May 21 06:19:49.549: INFO: Created: latency-svc-vddzp
May 21 06:19:49.585: INFO: Got endpoints: latency-svc-xwgvr [748.432128ms]
May 21 06:19:49.593: INFO: Created: latency-svc-nbg8z
May 21 06:19:49.635: INFO: Got endpoints: latency-svc-zv25b [741.706855ms]
May 21 06:19:49.685: INFO: Got endpoints: latency-svc-h7cj9 [748.74735ms]
May 21 06:19:49.735: INFO: Got endpoints: latency-svc-f8rp6 [749.865991ms]
May 21 06:19:49.786: INFO: Got endpoints: latency-svc-7vnms [749.970695ms]
May 21 06:19:49.837: INFO: Got endpoints: latency-svc-9gnfd [751.360221ms]
May 21 06:19:49.886: INFO: Got endpoints: latency-svc-f9t65 [750.224413ms]
May 21 06:19:49.936: INFO: Got endpoints: latency-svc-flj6p [750.660707ms]
May 21 06:19:49.987: INFO: Got endpoints: latency-svc-b9xpg [749.73626ms]
May 21 06:19:50.037: INFO: Got endpoints: latency-svc-88vbf [749.696239ms]
May 21 06:19:50.086: INFO: Got endpoints: latency-svc-2sqch [749.314156ms]
May 21 06:19:50.135: INFO: Got endpoints: latency-svc-74zcx [749.565299ms]
May 21 06:19:50.188: INFO: Got endpoints: latency-svc-dwwr9 [752.368668ms]
May 21 06:19:50.250: INFO: Got endpoints: latency-svc-42pht [759.939619ms]
May 21 06:19:50.286: INFO: Got endpoints: latency-svc-vddzp [746.722929ms]
May 21 06:19:50.338: INFO: Got endpoints: latency-svc-nbg8z [752.671784ms]
May 21 06:19:50.338: INFO: Latencies: [18.323153ms 26.585884ms 36.989291ms 39.058854ms 51.560238ms 63.610328ms 72.910342ms 89.094805ms 97.197111ms 119.871506ms 130.544743ms 143.670344ms 144.036253ms 160.325811ms 170.757859ms 177.824072ms 181.706714ms 182.138634ms 185.939697ms 189.648532ms 190.836637ms 191.536378ms 193.125268ms 193.662079ms 194.283669ms 194.995657ms 195.32826ms 197.573557ms 198.621335ms 199.188668ms 200.273539ms 200.632337ms 202.589483ms 206.779018ms 208.373458ms 208.913308ms 211.783546ms 212.572666ms 213.361777ms 232.420921ms 265.872063ms 305.053654ms 346.08671ms 378.691239ms 423.066486ms 459.957624ms 470.75957ms 506.566797ms 546.048825ms 588.345135ms 626.407401ms 662.435151ms 710.94042ms 711.381522ms 738.920958ms 741.706855ms 742.303259ms 743.519775ms 743.52822ms 744.095ms 744.37984ms 744.769136ms 745.091792ms 745.151136ms 745.22121ms 746.007781ms 746.238896ms 746.568896ms 746.613947ms 746.664672ms 746.698602ms 746.701758ms 746.722929ms 746.824671ms 747.275478ms 747.333396ms 747.335594ms 747.411842ms 747.470564ms 747.68823ms 747.735707ms 747.754026ms 748.037396ms 748.075495ms 748.343413ms 748.432128ms 748.4463ms 748.50312ms 748.56922ms 748.695237ms 748.705667ms 748.721842ms 748.74735ms 748.760732ms 748.819929ms 748.87311ms 748.931467ms 748.983622ms 749.002829ms 749.036894ms 749.046506ms 749.063103ms 749.066798ms 749.149874ms 749.158335ms 749.167641ms 749.183488ms 749.230759ms 749.268008ms 749.314156ms 749.38769ms 749.513315ms 749.517004ms 749.531672ms 749.565299ms 749.579998ms 749.582272ms 749.591367ms 749.601249ms 749.61907ms 749.696239ms 749.734154ms 749.73626ms 749.751207ms 749.850111ms 749.850649ms 749.865991ms 749.879387ms 749.954885ms 749.959191ms 749.967663ms 749.970695ms 750.022507ms 750.039882ms 750.083593ms 750.086545ms 750.170142ms 750.179464ms 750.224413ms 750.231284ms 750.233841ms 750.236832ms 750.248111ms 750.258231ms 750.28251ms 750.31345ms 750.319028ms 750.321379ms 750.340101ms 750.38293ms 750.407218ms 750.456752ms 750.515832ms 750.573397ms 750.65676ms 750.658135ms 750.660707ms 750.799146ms 750.892076ms 750.89353ms 750.897988ms 750.899035ms 750.935334ms 750.955528ms 750.959115ms 751.15017ms 751.240781ms 751.261501ms 751.360221ms 751.613226ms 751.646374ms 751.674091ms 751.794398ms 752.057393ms 752.368668ms 752.400315ms 752.582969ms 752.588271ms 752.671784ms 753.010073ms 753.034163ms 753.070522ms 753.204877ms 753.407007ms 753.471161ms 753.566492ms 753.679748ms 753.713613ms 753.79975ms 754.384567ms 754.482162ms 754.661585ms 756.470896ms 756.552753ms 757.106221ms 757.969801ms 759.175774ms 759.939619ms 760.089939ms 791.869927ms]
May 21 06:19:50.339: INFO: 50 %ile: 749.046506ms
May 21 06:19:50.339: INFO: 90 %ile: 753.034163ms
May 21 06:19:50.339: INFO: 99 %ile: 760.089939ms
May 21 06:19:50.339: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:19:50.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3017" for this suite.

• [SLOW TEST:10.782 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":168,"skipped":2784,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:19:50.348: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:20:07.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8733" for this suite.

• [SLOW TEST:17.065 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":169,"skipped":2806,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:20:07.413: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-4359
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 06:20:07.437: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 06:20:07.482: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 06:20:09.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:20:11.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:20:13.486: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:20:15.486: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:20:17.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:20:19.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:20:21.486: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:20:23.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:20:25.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:20:27.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:20:29.485: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 06:20:29.489: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 21 06:20:29.492: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 21 06:20:31.520: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 21 06:20:31.521: INFO: Breadth first check of 10.200.88.202 on host 30.0.0.12...
May 21 06:20:31.524: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.88.203:9080/dial?request=hostname&protocol=udp&host=10.200.88.202&port=8081&tries=1'] Namespace:pod-network-test-4359 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:20:31.524: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:20:31.620: INFO: Waiting for responses: map[]
May 21 06:20:31.620: INFO: reached 10.200.88.202 after 0/1 tries
May 21 06:20:31.620: INFO: Breadth first check of 10.200.66.58 on host 30.0.0.11...
May 21 06:20:31.623: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.88.203:9080/dial?request=hostname&protocol=udp&host=10.200.66.58&port=8081&tries=1'] Namespace:pod-network-test-4359 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:20:31.623: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:20:31.722: INFO: Waiting for responses: map[]
May 21 06:20:31.722: INFO: reached 10.200.66.58 after 0/1 tries
May 21 06:20:31.722: INFO: Breadth first check of 10.200.34.51 on host 30.0.0.10...
May 21 06:20:31.724: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.88.203:9080/dial?request=hostname&protocol=udp&host=10.200.34.51&port=8081&tries=1'] Namespace:pod-network-test-4359 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:20:31.725: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:20:31.801: INFO: Waiting for responses: map[]
May 21 06:20:31.801: INFO: reached 10.200.34.51 after 0/1 tries
May 21 06:20:31.801: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:20:31.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4359" for this suite.

• [SLOW TEST:24.399 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":170,"skipped":2808,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:20:31.814: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 21 06:20:31.845: INFO: Waiting up to 5m0s for pod "pod-be66400b-0ff7-4b52-aaad-b7c4c9a62026" in namespace "emptydir-8177" to be "Succeeded or Failed"
May 21 06:20:31.849: INFO: Pod "pod-be66400b-0ff7-4b52-aaad-b7c4c9a62026": Phase="Pending", Reason="", readiness=false. Elapsed: 4.373571ms
May 21 06:20:33.853: INFO: Pod "pod-be66400b-0ff7-4b52-aaad-b7c4c9a62026": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007920853s
STEP: Saw pod success
May 21 06:20:33.853: INFO: Pod "pod-be66400b-0ff7-4b52-aaad-b7c4c9a62026" satisfied condition "Succeeded or Failed"
May 21 06:20:33.855: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-be66400b-0ff7-4b52-aaad-b7c4c9a62026 container test-container: <nil>
STEP: delete the pod
May 21 06:20:33.880: INFO: Waiting for pod pod-be66400b-0ff7-4b52-aaad-b7c4c9a62026 to disappear
May 21 06:20:33.898: INFO: Pod pod-be66400b-0ff7-4b52-aaad-b7c4c9a62026 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:20:33.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8177" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":2826,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:20:33.907: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:20:33.941: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08d025ad-89c3-4890-9fa8-9aa4ecd1920b" in namespace "projected-1256" to be "Succeeded or Failed"
May 21 06:20:33.944: INFO: Pod "downwardapi-volume-08d025ad-89c3-4890-9fa8-9aa4ecd1920b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.40699ms
May 21 06:20:35.948: INFO: Pod "downwardapi-volume-08d025ad-89c3-4890-9fa8-9aa4ecd1920b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006506484s
STEP: Saw pod success
May 21 06:20:35.948: INFO: Pod "downwardapi-volume-08d025ad-89c3-4890-9fa8-9aa4ecd1920b" satisfied condition "Succeeded or Failed"
May 21 06:20:35.950: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-08d025ad-89c3-4890-9fa8-9aa4ecd1920b container client-container: <nil>
STEP: delete the pod
May 21 06:20:35.961: INFO: Waiting for pod downwardapi-volume-08d025ad-89c3-4890-9fa8-9aa4ecd1920b to disappear
May 21 06:20:35.968: INFO: Pod downwardapi-volume-08d025ad-89c3-4890-9fa8-9aa4ecd1920b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:20:35.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1256" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":2869,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:20:35.974: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 06:20:36.523: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 06:20:39.539: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:20:39.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8166" for this suite.
STEP: Destroying namespace "webhook-8166-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":173,"skipped":2875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:20:39.632: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0521 06:20:45.700684      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 06:20:45.700777      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 06:20:45.700794      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 06:20:45.700: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:20:45.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5094" for this suite.

• [SLOW TEST:6.073 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":174,"skipped":2939,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:20:45.706: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 21 06:20:45.729: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:21:01.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3711" for this suite.

• [SLOW TEST:15.886 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":175,"skipped":2944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:21:01.592: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 21 06:21:01.635: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 06:22:01.653: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:22:01.655: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 21 06:22:03.711: INFO: found a healthy node: 16c19df1-13dd-4e26-99a1-a192204dc796
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:22:15.798: INFO: pods created so far: [1 1 1]
May 21 06:22:15.798: INFO: length of pods created so far: 3
May 21 06:22:27.806: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:22:34.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8785" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:22:34.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-969" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:93.277 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":176,"skipped":2997,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:22:34.870: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
May 21 06:22:34.903: INFO: Waiting up to 5m0s for pod "var-expansion-df88a89c-f4af-42b3-af05-3e081767f7f8" in namespace "var-expansion-8810" to be "Succeeded or Failed"
May 21 06:22:34.908: INFO: Pod "var-expansion-df88a89c-f4af-42b3-af05-3e081767f7f8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.090898ms
May 21 06:22:36.911: INFO: Pod "var-expansion-df88a89c-f4af-42b3-af05-3e081767f7f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008090376s
STEP: Saw pod success
May 21 06:22:36.911: INFO: Pod "var-expansion-df88a89c-f4af-42b3-af05-3e081767f7f8" satisfied condition "Succeeded or Failed"
May 21 06:22:36.914: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod var-expansion-df88a89c-f4af-42b3-af05-3e081767f7f8 container dapi-container: <nil>
STEP: delete the pod
May 21 06:22:36.933: INFO: Waiting for pod var-expansion-df88a89c-f4af-42b3-af05-3e081767f7f8 to disappear
May 21 06:22:36.941: INFO: Pod var-expansion-df88a89c-f4af-42b3-af05-3e081767f7f8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:22:36.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8810" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":177,"skipped":3001,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:22:36.947: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:22:48.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3537" for this suite.

• [SLOW TEST:11.068 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":178,"skipped":3008,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:22:48.016: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
May 21 06:22:48.051: INFO: Waiting up to 5m0s for pod "client-containers-78143737-4463-453a-bdb4-2ab5c3d2d7dc" in namespace "containers-4885" to be "Succeeded or Failed"
May 21 06:22:48.061: INFO: Pod "client-containers-78143737-4463-453a-bdb4-2ab5c3d2d7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.36108ms
May 21 06:22:50.064: INFO: Pod "client-containers-78143737-4463-453a-bdb4-2ab5c3d2d7dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.013298144s
May 21 06:22:52.067: INFO: Pod "client-containers-78143737-4463-453a-bdb4-2ab5c3d2d7dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016748925s
STEP: Saw pod success
May 21 06:22:52.068: INFO: Pod "client-containers-78143737-4463-453a-bdb4-2ab5c3d2d7dc" satisfied condition "Succeeded or Failed"
May 21 06:22:52.069: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod client-containers-78143737-4463-453a-bdb4-2ab5c3d2d7dc container agnhost-container: <nil>
STEP: delete the pod
May 21 06:22:52.086: INFO: Waiting for pod client-containers-78143737-4463-453a-bdb4-2ab5c3d2d7dc to disappear
May 21 06:22:52.089: INFO: Pod client-containers-78143737-4463-453a-bdb4-2ab5c3d2d7dc no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:22:52.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4885" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":3040,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:22:52.096: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-718511e1-dc5f-44b1-8b95-fe7dbeb9b234
STEP: Creating a pod to test consume secrets
May 21 06:22:52.136: INFO: Waiting up to 5m0s for pod "pod-secrets-53aeef73-1d38-48d6-9f78-8a11cedf5121" in namespace "secrets-1264" to be "Succeeded or Failed"
May 21 06:22:52.148: INFO: Pod "pod-secrets-53aeef73-1d38-48d6-9f78-8a11cedf5121": Phase="Pending", Reason="", readiness=false. Elapsed: 12.303399ms
May 21 06:22:54.152: INFO: Pod "pod-secrets-53aeef73-1d38-48d6-9f78-8a11cedf5121": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016170366s
May 21 06:22:56.155: INFO: Pod "pod-secrets-53aeef73-1d38-48d6-9f78-8a11cedf5121": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019479053s
STEP: Saw pod success
May 21 06:22:56.155: INFO: Pod "pod-secrets-53aeef73-1d38-48d6-9f78-8a11cedf5121" satisfied condition "Succeeded or Failed"
May 21 06:22:56.157: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-secrets-53aeef73-1d38-48d6-9f78-8a11cedf5121 container secret-volume-test: <nil>
STEP: delete the pod
May 21 06:22:56.190: INFO: Waiting for pod pod-secrets-53aeef73-1d38-48d6-9f78-8a11cedf5121 to disappear
May 21 06:22:56.197: INFO: Pod pod-secrets-53aeef73-1d38-48d6-9f78-8a11cedf5121 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:22:56.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1264" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":180,"skipped":3058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:22:56.204: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 21 06:22:56.223: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 06:22:56.234: INFO: Waiting for terminating namespaces to be deleted...
May 21 06:22:56.236: INFO: 
Logging pods the apiserver thinks is on node 16c19df1-13dd-4e26-99a1-a192204dc796 before test
May 21 06:22:56.243: INFO: fluent-bit-mv9hz from pks-system started at 2021-05-21 06:08:12 +0000 UTC (2 container statuses recorded)
May 21 06:22:56.243: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 06:22:56.243: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:22:56.243: INFO: node-exporter-fz6jz from pks-system started at 2021-05-21 06:08:12 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.243: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 06:22:56.243: INFO: telegraf-nqkt5 from pks-system started at 2021-05-21 06:08:12 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.243: INFO: 	Container telegraf ready: true, restart count 0
May 21 06:22:56.243: INFO: sonobuoy from sonobuoy started at 2021-05-21 05:36:00 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.243: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 06:22:56.243: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-b9nwr from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:22:56.243: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 06:22:56.243: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 06:22:56.243: INFO: 
Logging pods the apiserver thinks is on node 53f495cf-0382-4128-b4b3-0979b69a209b before test
May 21 06:22:56.251: INFO: coredns-6d5447c5f4-fv5hd from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.251: INFO: 	Container coredns ready: true, restart count 0
May 21 06:22:56.251: INFO: coredns-6d5447c5f4-jms82 from kube-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.251: INFO: 	Container coredns ready: true, restart count 0
May 21 06:22:56.251: INFO: fluent-bit-vsrb2 from pks-system started at 2021-05-21 06:07:53 +0000 UTC (2 container statuses recorded)
May 21 06:22:56.251: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 06:22:56.251: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:22:56.251: INFO: metric-controller-5754b4489d-c246d from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.251: INFO: 	Container metric-controller ready: true, restart count 0
May 21 06:22:56.251: INFO: node-exporter-fmcv8 from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.251: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 06:22:56.251: INFO: observability-manager-7f5c97895-xsj2f from pks-system started at 2021-05-21 04:30:14 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.252: INFO: 	Container observability-manager ready: true, restart count 0
May 21 06:22:56.252: INFO: telegraf-hlmgc from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.252: INFO: 	Container telegraf ready: true, restart count 0
May 21 06:22:56.252: INFO: telemetry-agent-8f56d9865-l6ntw from pks-system started at 2021-05-21 04:35:59 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.252: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
May 21 06:22:56.252: INFO: validator-54f6bbd67b-snnrv from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.252: INFO: 	Container validator ready: true, restart count 0
May 21 06:22:56.252: INFO: sonobuoy-e2e-job-a4c84655dab54066 from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:22:56.252: INFO: 	Container e2e ready: true, restart count 0
May 21 06:22:56.252: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 06:22:56.252: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-g25r8 from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:22:56.252: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 06:22:56.252: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 06:22:56.252: INFO: 
Logging pods the apiserver thinks is on node 973ef262-7e5e-4fd5-8bf3-6139f838e19b before test
May 21 06:22:56.259: INFO: coredns-6d5447c5f4-7dcmd from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.259: INFO: 	Container coredns ready: true, restart count 0
May 21 06:22:56.259: INFO: metrics-server-7d476fdfbd-phhm6 from kube-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.259: INFO: 	Container metrics-server ready: true, restart count 0
May 21 06:22:56.259: INFO: event-controller-55dd5b5777-4mn7d from pks-system started at 2021-05-21 04:30:17 +0000 UTC (2 container statuses recorded)
May 21 06:22:56.259: INFO: 	Container event-controller ready: true, restart count 0
May 21 06:22:56.259: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:22:56.259: INFO: fluent-bit-hnqwh from pks-system started at 2021-05-21 06:07:50 +0000 UTC (2 container statuses recorded)
May 21 06:22:56.259: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 06:22:56.259: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:22:56.259: INFO: node-exporter-6k9jm from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.259: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 06:22:56.259: INFO: sink-controller-5bf464ff96-4nbts from pks-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.259: INFO: 	Container sink-controller ready: true, restart count 0
May 21 06:22:56.259: INFO: telegraf-jqzbt from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.259: INFO: 	Container telegraf ready: true, restart count 0
May 21 06:22:56.259: INFO: wavefront-collector-5795f87754-596hz from pks-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.259: INFO: 	Container wavefront-collector ready: true, restart count 0
May 21 06:22:56.259: INFO: wavefront-proxy-6446ff4bd7-lw9wt from pks-system started at 2021-05-21 04:32:41 +0000 UTC (1 container statuses recorded)
May 21 06:22:56.259: INFO: 	Container wavefront-proxy ready: true, restart count 0
May 21 06:22:56.259: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-ppgpw from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:22:56.259: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 06:22:56.259: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.168100c2844f6ef1], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.168100c28498f931], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:22:57.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8554" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":181,"skipped":3092,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:22:57.285: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5635
STEP: creating service affinity-clusterip-transition in namespace services-5635
STEP: creating replication controller affinity-clusterip-transition in namespace services-5635
I0521 06:22:57.326450      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-5635, replica count: 3
I0521 06:23:00.376733      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:23:00.382: INFO: Creating new exec pod
May 21 06:23:03.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-5635 exec execpod-affinitynhw9n -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
May 21 06:23:04.603: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 21 06:23:04.603: INFO: stdout: ""
May 21 06:23:04.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-5635 exec execpod-affinitynhw9n -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.129 80'
May 21 06:23:04.768: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.129 80\nConnection to 10.100.200.129 80 port [tcp/http] succeeded!\n"
May 21 06:23:04.768: INFO: stdout: ""
May 21 06:23:04.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-5635 exec execpod-affinitynhw9n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.200.129:80/ ; done'
May 21 06:23:05.048: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n"
May 21 06:23:05.048: INFO: stdout: "\naffinity-clusterip-transition-cp69s\naffinity-clusterip-transition-5kdhh\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-5kdhh\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cp69s\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-5kdhh\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-5kdhh\naffinity-clusterip-transition-cp69s\naffinity-clusterip-transition-cp69s\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-5kdhh"
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cp69s
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-5kdhh
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-5kdhh
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cp69s
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-5kdhh
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-5kdhh
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cp69s
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cp69s
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.048: INFO: Received response from host: affinity-clusterip-transition-5kdhh
May 21 06:23:05.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-5635 exec execpod-affinitynhw9n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.200.129:80/ ; done'
May 21 06:23:05.324: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.129:80/\n"
May 21 06:23:05.324: INFO: stdout: "\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd\naffinity-clusterip-transition-cg9zd"
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.324: INFO: Received response from host: affinity-clusterip-transition-cg9zd
May 21 06:23:05.325: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5635, will wait for the garbage collector to delete the pods
May 21 06:23:05.396: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.749678ms
May 21 06:23:05.996: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 600.123166ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:23:12.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5635" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.140 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":182,"skipped":3105,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:23:12.424: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 21 06:23:12.462: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 21 06:23:24.029: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:23:26.959: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:23:38.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6588" for this suite.

• [SLOW TEST:26.224 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":183,"skipped":3113,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:23:38.651: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1496.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1496.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1496.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1496.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1496.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1496.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 06:23:42.714: INFO: DNS probes using dns-1496/dns-test-d7875823-a6f2-465f-8da5-189af9de6d5c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:23:42.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1496" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":184,"skipped":3133,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:23:42.739: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
May 21 06:23:42.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6393 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 21 06:23:42.842: INFO: stderr: ""
May 21 06:23:42.842: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
May 21 06:23:42.842: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 21 06:23:42.842: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6393" to be "running and ready, or succeeded"
May 21 06:23:42.850: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.643748ms
May 21 06:23:44.853: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.010340805s
May 21 06:23:44.853: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 21 06:23:44.853: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 21 06:23:44.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6393 logs logs-generator logs-generator'
May 21 06:23:44.936: INFO: stderr: ""
May 21 06:23:44.937: INFO: stdout: "I0521 06:23:43.921206       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/b56 539\nI0521 06:23:44.121298       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/rfs 461\nI0521 06:23:44.321302       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/q4dh 326\nI0521 06:23:44.521272       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/x75n 438\nI0521 06:23:44.721592       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/2cq6 382\nI0521 06:23:44.921303       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/c86s 444\n"
STEP: limiting log lines
May 21 06:23:44.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6393 logs logs-generator logs-generator --tail=1'
May 21 06:23:45.008: INFO: stderr: ""
May 21 06:23:45.008: INFO: stdout: "I0521 06:23:44.921303       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/c86s 444\n"
May 21 06:23:45.008: INFO: got output "I0521 06:23:44.921303       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/c86s 444\n"
STEP: limiting log bytes
May 21 06:23:45.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6393 logs logs-generator logs-generator --limit-bytes=1'
May 21 06:23:45.077: INFO: stderr: ""
May 21 06:23:45.077: INFO: stdout: "I"
May 21 06:23:45.077: INFO: got output "I"
STEP: exposing timestamps
May 21 06:23:45.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6393 logs logs-generator logs-generator --tail=1 --timestamps'
May 21 06:23:45.153: INFO: stderr: ""
May 21 06:23:45.153: INFO: stdout: "2021-05-21T06:23:45.121382777Z I0521 06:23:45.121287       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/rbv 586\n"
May 21 06:23:45.153: INFO: got output "2021-05-21T06:23:45.121382777Z I0521 06:23:45.121287       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/rbv 586\n"
STEP: restricting to a time range
May 21 06:23:47.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6393 logs logs-generator logs-generator --since=1s'
May 21 06:23:47.732: INFO: stderr: ""
May 21 06:23:47.732: INFO: stdout: "I0521 06:23:46.921288       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/fqsb 586\nI0521 06:23:47.121295       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/pl7h 376\nI0521 06:23:47.321314       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/s7z 395\nI0521 06:23:47.521297       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/7bx 570\nI0521 06:23:47.721302       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/n6l 366\n"
May 21 06:23:47.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6393 logs logs-generator logs-generator --since=24h'
May 21 06:23:47.809: INFO: stderr: ""
May 21 06:23:47.809: INFO: stdout: "I0521 06:23:43.921206       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/b56 539\nI0521 06:23:44.121298       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/rfs 461\nI0521 06:23:44.321302       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/q4dh 326\nI0521 06:23:44.521272       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/x75n 438\nI0521 06:23:44.721592       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/2cq6 382\nI0521 06:23:44.921303       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/c86s 444\nI0521 06:23:45.121287       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/rbv 586\nI0521 06:23:45.321300       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/k5h 253\nI0521 06:23:45.521298       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/pmw 450\nI0521 06:23:45.721293       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/h74 468\nI0521 06:23:45.921290       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/fjs 446\nI0521 06:23:46.121295       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/r29 515\nI0521 06:23:46.321296       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/89k 489\nI0521 06:23:46.521294       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/vs2 556\nI0521 06:23:46.721286       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/wqw6 371\nI0521 06:23:46.921288       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/fqsb 586\nI0521 06:23:47.121295       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/pl7h 376\nI0521 06:23:47.321314       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/s7z 395\nI0521 06:23:47.521297       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/7bx 570\nI0521 06:23:47.721302       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/n6l 366\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
May 21 06:23:47.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6393 delete pod logs-generator'
May 21 06:23:52.345: INFO: stderr: ""
May 21 06:23:52.345: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:23:52.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6393" for this suite.

• [SLOW TEST:9.615 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":185,"skipped":3151,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:23:52.354: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
May 21 06:23:54.409: INFO: Pod pod-hostip-025c66df-709c-4888-8c31-d560f0f0ed0e has hostIP: 30.0.0.12
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:23:54.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2039" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":186,"skipped":3156,"failed":0}
SSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:23:54.416: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:23:54.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8357" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":187,"skipped":3163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:23:54.467: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:23:54.487: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 06:23:57.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-6539 --namespace=crd-publish-openapi-6539 create -f -'
May 21 06:23:58.734: INFO: stderr: ""
May 21 06:23:58.734: INFO: stdout: "e2e-test-crd-publish-openapi-1492-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 21 06:23:58.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-6539 --namespace=crd-publish-openapi-6539 delete e2e-test-crd-publish-openapi-1492-crds test-cr'
May 21 06:23:58.811: INFO: stderr: ""
May 21 06:23:58.811: INFO: stdout: "e2e-test-crd-publish-openapi-1492-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 21 06:23:58.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-6539 --namespace=crd-publish-openapi-6539 apply -f -'
May 21 06:23:58.999: INFO: stderr: ""
May 21 06:23:58.999: INFO: stdout: "e2e-test-crd-publish-openapi-1492-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 21 06:23:58.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-6539 --namespace=crd-publish-openapi-6539 delete e2e-test-crd-publish-openapi-1492-crds test-cr'
May 21 06:23:59.069: INFO: stderr: ""
May 21 06:23:59.069: INFO: stdout: "e2e-test-crd-publish-openapi-1492-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 21 06:23:59.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-6539 explain e2e-test-crd-publish-openapi-1492-crds'
May 21 06:23:59.261: INFO: stderr: ""
May 21 06:23:59.261: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1492-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:24:02.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6539" for this suite.

• [SLOW TEST:7.746 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":188,"skipped":3209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:24:02.218: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:24:02.247: INFO: Creating deployment "test-recreate-deployment"
May 21 06:24:02.253: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 21 06:24:02.268: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 21 06:24:04.273: INFO: Waiting deployment "test-recreate-deployment" to complete
May 21 06:24:04.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757175042, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757175042, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757175042, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757175042, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 06:24:06.279: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 21 06:24:06.286: INFO: Updating deployment test-recreate-deployment
May 21 06:24:06.286: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 06:24:06.402: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-441  b113a658-a1cc-41cc-864c-a686d9dac166 24476 2 2021-05-21 06:24:02 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 06:24:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 06:24:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eb1968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-21 06:24:06 +0000 UTC,LastTransitionTime:2021-05-21 06:24:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f756bcb56" is progressing.,LastUpdateTime:2021-05-21 06:24:06 +0000 UTC,LastTransitionTime:2021-05-21 06:24:02 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 21 06:24:06.406: INFO: New ReplicaSet "test-recreate-deployment-f756bcb56" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f756bcb56  deployment-441  f4ca3b34-f976-482d-a3ad-e7ceee81abe4 24472 1 2021-05-21 06:24:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b113a658-a1cc-41cc-864c-a686d9dac166 0xc003f86000 0xc003f86001}] []  [{kube-controller-manager Update apps/v1 2021-05-21 06:24:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b113a658-a1cc-41cc-864c-a686d9dac166\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f756bcb56,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f860a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 06:24:06.406: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 21 06:24:06.406: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-441  d1cd3995-1392-4aff-8427-22470f0c3780 24463 2 2021-05-21 06:24:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b113a658-a1cc-41cc-864c-a686d9dac166 0xc003eb1eb7 0xc003eb1eb8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 06:24:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b113a658-a1cc-41cc-864c-a686d9dac166\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eb1f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 06:24:06.408: INFO: Pod "test-recreate-deployment-f756bcb56-6r652" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f756bcb56-6r652 test-recreate-deployment-f756bcb56- deployment-441  4703b7df-fd99-4be7-9fb0-ab94f668a164 24473 0 2021-05-21 06:24:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f756bcb56 f4ca3b34-f976-482d-a3ad-e7ceee81abe4 0xc003f865c0 0xc003f865c1}] []  [{kube-controller-manager Update v1 2021-05-21 06:24:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ca3b34-f976-482d-a3ad-e7ceee81abe4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:24:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-thk9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-thk9b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-thk9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:24:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:24:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:24:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:24:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:,StartTime:2021-05-21 06:24:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:24:06.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-441" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":189,"skipped":3264,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:24:06.413: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:24:08.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4269" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:24:08.477: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8191
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 06:24:08.501: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 06:24:08.560: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 06:24:10.563: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:24:12.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:24:14.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:24:16.565: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:24:18.562: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:24:20.563: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:24:22.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:24:24.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:24:26.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 06:24:28.563: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 06:24:28.573: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 21 06:24:28.581: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 21 06:24:32.607: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 21 06:24:32.607: INFO: Breadth first check of 10.200.88.230 on host 30.0.0.12...
May 21 06:24:32.609: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.88.231:9080/dial?request=hostname&protocol=http&host=10.200.88.230&port=8080&tries=1'] Namespace:pod-network-test-8191 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:24:32.609: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:24:32.706: INFO: Waiting for responses: map[]
May 21 06:24:32.706: INFO: reached 10.200.88.230 after 0/1 tries
May 21 06:24:32.706: INFO: Breadth first check of 10.200.66.62 on host 30.0.0.11...
May 21 06:24:32.709: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.88.231:9080/dial?request=hostname&protocol=http&host=10.200.66.62&port=8080&tries=1'] Namespace:pod-network-test-8191 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:24:32.709: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:24:32.799: INFO: Waiting for responses: map[]
May 21 06:24:32.799: INFO: reached 10.200.66.62 after 0/1 tries
May 21 06:24:32.799: INFO: Breadth first check of 10.200.34.55 on host 30.0.0.10...
May 21 06:24:32.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.88.231:9080/dial?request=hostname&protocol=http&host=10.200.34.55&port=8080&tries=1'] Namespace:pod-network-test-8191 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:24:32.802: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:24:32.887: INFO: Waiting for responses: map[]
May 21 06:24:32.887: INFO: reached 10.200.34.55 after 0/1 tries
May 21 06:24:32.887: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:24:32.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8191" for this suite.

• [SLOW TEST:24.417 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3310,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:24:32.894: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:24:32.921: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 21 06:24:35.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 --namespace=crd-publish-openapi-3312 create -f -'
May 21 06:24:37.150: INFO: stderr: ""
May 21 06:24:37.150: INFO: stdout: "e2e-test-crd-publish-openapi-2585-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 21 06:24:37.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 --namespace=crd-publish-openapi-3312 delete e2e-test-crd-publish-openapi-2585-crds test-foo'
May 21 06:24:37.221: INFO: stderr: ""
May 21 06:24:37.221: INFO: stdout: "e2e-test-crd-publish-openapi-2585-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 21 06:24:37.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 --namespace=crd-publish-openapi-3312 apply -f -'
May 21 06:24:37.417: INFO: stderr: ""
May 21 06:24:37.417: INFO: stdout: "e2e-test-crd-publish-openapi-2585-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 21 06:24:37.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 --namespace=crd-publish-openapi-3312 delete e2e-test-crd-publish-openapi-2585-crds test-foo'
May 21 06:24:37.490: INFO: stderr: ""
May 21 06:24:37.490: INFO: stdout: "e2e-test-crd-publish-openapi-2585-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 21 06:24:37.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 --namespace=crd-publish-openapi-3312 create -f -'
May 21 06:24:37.673: INFO: rc: 1
May 21 06:24:37.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 --namespace=crd-publish-openapi-3312 apply -f -'
May 21 06:24:37.847: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 21 06:24:37.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 --namespace=crd-publish-openapi-3312 create -f -'
May 21 06:24:38.038: INFO: rc: 1
May 21 06:24:38.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 --namespace=crd-publish-openapi-3312 apply -f -'
May 21 06:24:38.277: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 21 06:24:38.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 explain e2e-test-crd-publish-openapi-2585-crds'
May 21 06:24:38.460: INFO: stderr: ""
May 21 06:24:38.460: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2585-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 21 06:24:38.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 explain e2e-test-crd-publish-openapi-2585-crds.metadata'
May 21 06:24:38.683: INFO: stderr: ""
May 21 06:24:38.683: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2585-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 21 06:24:38.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 explain e2e-test-crd-publish-openapi-2585-crds.spec'
May 21 06:24:38.875: INFO: stderr: ""
May 21 06:24:38.875: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2585-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 21 06:24:38.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 explain e2e-test-crd-publish-openapi-2585-crds.spec.bars'
May 21 06:24:39.055: INFO: stderr: ""
May 21 06:24:39.055: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2585-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 21 06:24:39.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=crd-publish-openapi-3312 explain e2e-test-crd-publish-openapi-2585-crds.spec.bars2'
May 21 06:24:39.233: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:24:42.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3312" for this suite.

• [SLOW TEST:9.298 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":192,"skipped":3314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:24:42.196: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
May 21 06:24:42.229: INFO: created test-pod-1
May 21 06:24:42.238: INFO: created test-pod-2
May 21 06:24:42.251: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:24:42.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2303" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":193,"skipped":3376,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:24:42.329: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 06:24:44.384: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:24:44.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8754" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":194,"skipped":3378,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:24:44.404: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:24:48.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1032" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3444,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:24:48.458: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 06:24:48.936: INFO: starting watch
STEP: patching
STEP: updating
May 21 06:24:48.943: INFO: waiting for watch events with expected annotations
May 21 06:24:48.943: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:24:49.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6005" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":196,"skipped":3457,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:24:49.008: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 06:24:49.049: INFO: starting watch
STEP: patching
STEP: updating
May 21 06:24:49.055: INFO: waiting for watch events with expected annotations
May 21 06:24:49.055: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:24:49.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2223" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":197,"skipped":3478,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:24:49.075: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:25:10.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6779" for this suite.

• [SLOW TEST:21.223 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3488,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:25:10.299: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 06:25:10.685: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 06:25:12.693: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757175110, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757175110, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757175110, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757175110, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 06:25:15.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:25:15.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4466" for this suite.
STEP: Destroying namespace "webhook-4466-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.629 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":199,"skipped":3510,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:25:15.928: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0521 06:25:55.982956      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 06:25:55.983079      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 06:25:55.983121      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 06:25:55.983: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 21 06:25:55.983: INFO: Deleting pod "simpletest.rc-2pgjx" in namespace "gc-7085"
May 21 06:25:55.990: INFO: Deleting pod "simpletest.rc-66wdp" in namespace "gc-7085"
May 21 06:25:56.003: INFO: Deleting pod "simpletest.rc-69mrd" in namespace "gc-7085"
May 21 06:25:56.032: INFO: Deleting pod "simpletest.rc-9k9rj" in namespace "gc-7085"
May 21 06:25:56.053: INFO: Deleting pod "simpletest.rc-9q27g" in namespace "gc-7085"
May 21 06:25:56.081: INFO: Deleting pod "simpletest.rc-bdh9z" in namespace "gc-7085"
May 21 06:25:56.096: INFO: Deleting pod "simpletest.rc-cgfzt" in namespace "gc-7085"
May 21 06:25:56.117: INFO: Deleting pod "simpletest.rc-fx8td" in namespace "gc-7085"
May 21 06:25:56.130: INFO: Deleting pod "simpletest.rc-gzx62" in namespace "gc-7085"
May 21 06:25:56.159: INFO: Deleting pod "simpletest.rc-qb85q" in namespace "gc-7085"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:25:56.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7085" for this suite.

• [SLOW TEST:40.264 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":200,"skipped":3512,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:25:56.192: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
May 21 06:25:56.231: INFO: Waiting up to 5m0s for pod "var-expansion-49135236-99a2-41f3-9a5e-bb0c0d305520" in namespace "var-expansion-6356" to be "Succeeded or Failed"
May 21 06:25:56.235: INFO: Pod "var-expansion-49135236-99a2-41f3-9a5e-bb0c0d305520": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349953ms
May 21 06:25:58.240: INFO: Pod "var-expansion-49135236-99a2-41f3-9a5e-bb0c0d305520": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00848335s
STEP: Saw pod success
May 21 06:25:58.240: INFO: Pod "var-expansion-49135236-99a2-41f3-9a5e-bb0c0d305520" satisfied condition "Succeeded or Failed"
May 21 06:25:58.242: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod var-expansion-49135236-99a2-41f3-9a5e-bb0c0d305520 container dapi-container: <nil>
STEP: delete the pod
May 21 06:25:58.257: INFO: Waiting for pod var-expansion-49135236-99a2-41f3-9a5e-bb0c0d305520 to disappear
May 21 06:25:58.263: INFO: Pod var-expansion-49135236-99a2-41f3-9a5e-bb0c0d305520 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:25:58.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6356" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":201,"skipped":3513,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:25:58.270: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:06.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-819" for this suite.

• [SLOW TEST:8.052 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":202,"skipped":3518,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:06.322: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-3dc0a13e-b000-48a8-8b10-0cf1dfa1cb30
STEP: Creating a pod to test consume secrets
May 21 06:26:06.369: INFO: Waiting up to 5m0s for pod "pod-secrets-8c84e51f-bb82-4f29-b9e8-d7fab784132a" in namespace "secrets-4878" to be "Succeeded or Failed"
May 21 06:26:06.378: INFO: Pod "pod-secrets-8c84e51f-bb82-4f29-b9e8-d7fab784132a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.63235ms
May 21 06:26:08.382: INFO: Pod "pod-secrets-8c84e51f-bb82-4f29-b9e8-d7fab784132a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012207922s
STEP: Saw pod success
May 21 06:26:08.382: INFO: Pod "pod-secrets-8c84e51f-bb82-4f29-b9e8-d7fab784132a" satisfied condition "Succeeded or Failed"
May 21 06:26:08.383: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-secrets-8c84e51f-bb82-4f29-b9e8-d7fab784132a container secret-volume-test: <nil>
STEP: delete the pod
May 21 06:26:08.396: INFO: Waiting for pod pod-secrets-8c84e51f-bb82-4f29-b9e8-d7fab784132a to disappear
May 21 06:26:08.402: INFO: Pod pod-secrets-8c84e51f-bb82-4f29-b9e8-d7fab784132a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:08.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4878" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":203,"skipped":3528,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:08.409: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 21 06:26:08.438: INFO: Waiting up to 5m0s for pod "pod-1fd0e03e-4286-4602-9dfc-d208ba8b2f4c" in namespace "emptydir-2248" to be "Succeeded or Failed"
May 21 06:26:08.450: INFO: Pod "pod-1fd0e03e-4286-4602-9dfc-d208ba8b2f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.812637ms
May 21 06:26:10.452: INFO: Pod "pod-1fd0e03e-4286-4602-9dfc-d208ba8b2f4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014209899s
STEP: Saw pod success
May 21 06:26:10.453: INFO: Pod "pod-1fd0e03e-4286-4602-9dfc-d208ba8b2f4c" satisfied condition "Succeeded or Failed"
May 21 06:26:10.454: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-1fd0e03e-4286-4602-9dfc-d208ba8b2f4c container test-container: <nil>
STEP: delete the pod
May 21 06:26:10.467: INFO: Waiting for pod pod-1fd0e03e-4286-4602-9dfc-d208ba8b2f4c to disappear
May 21 06:26:10.473: INFO: Pod pod-1fd0e03e-4286-4602-9dfc-d208ba8b2f4c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:10.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2248" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":204,"skipped":3534,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:10.480: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:26:10.522: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dbc98d5b-ad95-4690-869c-1b316824bec4" in namespace "projected-332" to be "Succeeded or Failed"
May 21 06:26:10.529: INFO: Pod "downwardapi-volume-dbc98d5b-ad95-4690-869c-1b316824bec4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.520006ms
May 21 06:26:12.532: INFO: Pod "downwardapi-volume-dbc98d5b-ad95-4690-869c-1b316824bec4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010611697s
STEP: Saw pod success
May 21 06:26:12.532: INFO: Pod "downwardapi-volume-dbc98d5b-ad95-4690-869c-1b316824bec4" satisfied condition "Succeeded or Failed"
May 21 06:26:12.534: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-dbc98d5b-ad95-4690-869c-1b316824bec4 container client-container: <nil>
STEP: delete the pod
May 21 06:26:12.547: INFO: Waiting for pod downwardapi-volume-dbc98d5b-ad95-4690-869c-1b316824bec4 to disappear
May 21 06:26:12.552: INFO: Pod downwardapi-volume-dbc98d5b-ad95-4690-869c-1b316824bec4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:12.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-332" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":205,"skipped":3539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:12.559: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 06:26:14.605: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:14.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8477" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:14.629: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 21 06:26:18.688: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 06:26:18.690: INFO: Pod pod-with-prestop-http-hook still exists
May 21 06:26:20.690: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 06:26:20.693: INFO: Pod pod-with-prestop-http-hook still exists
May 21 06:26:22.690: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 06:26:22.693: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:22.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7205" for this suite.

• [SLOW TEST:8.077 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3606,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:22.707: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 21 06:26:22.728: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:26.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9948" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":208,"skipped":3612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:26.645: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 21 06:26:26.673: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:30.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1625" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":209,"skipped":3652,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:30.832: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 21 06:26:30.868: INFO: Waiting up to 5m0s for pod "downward-api-b3f25ac6-17d9-4b8e-92b8-b582a54998d8" in namespace "downward-api-8696" to be "Succeeded or Failed"
May 21 06:26:30.880: INFO: Pod "downward-api-b3f25ac6-17d9-4b8e-92b8-b582a54998d8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.936218ms
May 21 06:26:32.884: INFO: Pod "downward-api-b3f25ac6-17d9-4b8e-92b8-b582a54998d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015665845s
May 21 06:26:34.887: INFO: Pod "downward-api-b3f25ac6-17d9-4b8e-92b8-b582a54998d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01844371s
STEP: Saw pod success
May 21 06:26:34.887: INFO: Pod "downward-api-b3f25ac6-17d9-4b8e-92b8-b582a54998d8" satisfied condition "Succeeded or Failed"
May 21 06:26:34.889: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downward-api-b3f25ac6-17d9-4b8e-92b8-b582a54998d8 container dapi-container: <nil>
STEP: delete the pod
May 21 06:26:34.901: INFO: Waiting for pod downward-api-b3f25ac6-17d9-4b8e-92b8-b582a54998d8 to disappear
May 21 06:26:34.908: INFO: Pod downward-api-b3f25ac6-17d9-4b8e-92b8-b582a54998d8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:34.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8696" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3658,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:34.914: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 21 06:26:38.954: INFO: &Pod{ObjectMeta:{send-events-322b2bb9-238c-482f-b376-0f27b0452efb  events-4051  72c155b4-fe66-4f8c-acdb-8e3fc03d38d1 25776 0 2021-05-21 06:26:34 +0000 UTC <nil> <nil> map[name:foo time:937109058] map[] [] []  [{e2e.test Update v1 2021-05-21 06:26:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:26:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.88.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mjqwv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mjqwv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mjqwv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:26:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:26:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:26:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:26:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.88.3,StartTime:2021-05-21 06:26:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 06:26:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://458eb93732fcb00f4b5685fff7b284a6822b52339bc75fe626e0fa1a6c2d68b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.88.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 21 06:26:40.959: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 21 06:26:42.965: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:42.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4051" for this suite.

• [SLOW TEST:8.068 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":211,"skipped":3679,"failed":0}
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:42.982: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-931dfcc3-3b84-40d4-a079-b3ea5771ba2e
May 21 06:26:43.027: INFO: Pod name my-hostname-basic-931dfcc3-3b84-40d4-a079-b3ea5771ba2e: Found 1 pods out of 1
May 21 06:26:43.027: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-931dfcc3-3b84-40d4-a079-b3ea5771ba2e" are running
May 21 06:26:47.041: INFO: Pod "my-hostname-basic-931dfcc3-3b84-40d4-a079-b3ea5771ba2e-6lrxk" is running (conditions: [])
May 21 06:26:47.041: INFO: Trying to dial the pod
May 21 06:26:52.050: INFO: Controller my-hostname-basic-931dfcc3-3b84-40d4-a079-b3ea5771ba2e: Got expected result from replica 1 [my-hostname-basic-931dfcc3-3b84-40d4-a079-b3ea5771ba2e-6lrxk]: "my-hostname-basic-931dfcc3-3b84-40d4-a079-b3ea5771ba2e-6lrxk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:52.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9434" for this suite.

• [SLOW TEST:9.076 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":212,"skipped":3679,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:52.058: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-539dc6a9-b4dc-40cc-91e3-e538c5de4e95
STEP: Creating a pod to test consume secrets
May 21 06:26:52.103: INFO: Waiting up to 5m0s for pod "pod-secrets-ce3e90ea-d5b1-433d-9710-95185049d4ed" in namespace "secrets-2962" to be "Succeeded or Failed"
May 21 06:26:52.107: INFO: Pod "pod-secrets-ce3e90ea-d5b1-433d-9710-95185049d4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 3.373564ms
May 21 06:26:54.111: INFO: Pod "pod-secrets-ce3e90ea-d5b1-433d-9710-95185049d4ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007447487s
STEP: Saw pod success
May 21 06:26:54.111: INFO: Pod "pod-secrets-ce3e90ea-d5b1-433d-9710-95185049d4ed" satisfied condition "Succeeded or Failed"
May 21 06:26:54.113: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-secrets-ce3e90ea-d5b1-433d-9710-95185049d4ed container secret-volume-test: <nil>
STEP: delete the pod
May 21 06:26:54.128: INFO: Waiting for pod pod-secrets-ce3e90ea-d5b1-433d-9710-95185049d4ed to disappear
May 21 06:26:54.133: INFO: Pod pod-secrets-ce3e90ea-d5b1-433d-9710-95185049d4ed no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:54.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2962" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":213,"skipped":3682,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:54.140: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-e6ff59ba-d23a-44f9-9e78-107bc00a5cf7
STEP: Creating a pod to test consume configMaps
May 21 06:26:54.180: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f581c3cd-d326-4fd9-af5a-1936688a44b6" in namespace "projected-4739" to be "Succeeded or Failed"
May 21 06:26:54.182: INFO: Pod "pod-projected-configmaps-f581c3cd-d326-4fd9-af5a-1936688a44b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.525299ms
May 21 06:26:56.187: INFO: Pod "pod-projected-configmaps-f581c3cd-d326-4fd9-af5a-1936688a44b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007015325s
May 21 06:26:58.192: INFO: Pod "pod-projected-configmaps-f581c3cd-d326-4fd9-af5a-1936688a44b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011942234s
STEP: Saw pod success
May 21 06:26:58.192: INFO: Pod "pod-projected-configmaps-f581c3cd-d326-4fd9-af5a-1936688a44b6" satisfied condition "Succeeded or Failed"
May 21 06:26:58.194: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-configmaps-f581c3cd-d326-4fd9-af5a-1936688a44b6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 06:26:58.209: INFO: Waiting for pod pod-projected-configmaps-f581c3cd-d326-4fd9-af5a-1936688a44b6 to disappear
May 21 06:26:58.215: INFO: Pod pod-projected-configmaps-f581c3cd-d326-4fd9-af5a-1936688a44b6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:26:58.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4739" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":214,"skipped":3684,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:26:58.221: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-8814/secret-test-8e1c5210-29b8-416e-ab6b-9cb67efb9370
STEP: Creating a pod to test consume secrets
May 21 06:26:58.258: INFO: Waiting up to 5m0s for pod "pod-configmaps-6618cce2-d5e1-4d6a-a2b5-6f374b69c93c" in namespace "secrets-8814" to be "Succeeded or Failed"
May 21 06:26:58.268: INFO: Pod "pod-configmaps-6618cce2-d5e1-4d6a-a2b5-6f374b69c93c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.347362ms
May 21 06:27:00.271: INFO: Pod "pod-configmaps-6618cce2-d5e1-4d6a-a2b5-6f374b69c93c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013222302s
May 21 06:27:02.276: INFO: Pod "pod-configmaps-6618cce2-d5e1-4d6a-a2b5-6f374b69c93c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01798492s
STEP: Saw pod success
May 21 06:27:02.276: INFO: Pod "pod-configmaps-6618cce2-d5e1-4d6a-a2b5-6f374b69c93c" satisfied condition "Succeeded or Failed"
May 21 06:27:02.278: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-configmaps-6618cce2-d5e1-4d6a-a2b5-6f374b69c93c container env-test: <nil>
STEP: delete the pod
May 21 06:27:02.295: INFO: Waiting for pod pod-configmaps-6618cce2-d5e1-4d6a-a2b5-6f374b69c93c to disappear
May 21 06:27:02.299: INFO: Pod pod-configmaps-6618cce2-d5e1-4d6a-a2b5-6f374b69c93c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:27:02.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8814" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":215,"skipped":3686,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:27:02.305: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-9487
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9487 to expose endpoints map[]
May 21 06:27:02.355: INFO: successfully validated that service multi-endpoint-test in namespace services-9487 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9487
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9487 to expose endpoints map[pod1:[100]]
May 21 06:27:05.379: INFO: successfully validated that service multi-endpoint-test in namespace services-9487 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9487
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9487 to expose endpoints map[pod1:[100] pod2:[101]]
May 21 06:27:07.410: INFO: successfully validated that service multi-endpoint-test in namespace services-9487 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-9487
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9487 to expose endpoints map[pod2:[101]]
May 21 06:27:07.439: INFO: successfully validated that service multi-endpoint-test in namespace services-9487 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9487
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9487 to expose endpoints map[]
May 21 06:27:07.471: INFO: successfully validated that service multi-endpoint-test in namespace services-9487 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:27:07.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9487" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.205 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":216,"skipped":3698,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:27:07.510: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 21 06:27:07.538: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:27:10.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8630" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":217,"skipped":3700,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:27:10.699: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:27:14.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1820" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":218,"skipped":3717,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:27:14.778: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:27:30.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3666" for this suite.

• [SLOW TEST:16.127 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":219,"skipped":3722,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:27:30.905: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8461 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8461;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8461 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8461;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8461.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8461.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8461.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8461.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8461.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8461.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8461.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8461.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8461.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8461.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8461.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8461.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8461.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 13.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.13_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8461 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8461;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8461 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8461;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8461.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8461.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8461.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8461.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8461.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8461.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8461.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8461.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8461.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8461.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8461.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8461.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8461.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 13.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.13_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 06:27:34.991: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:34.995: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:34.997: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.001: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.004: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.007: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.010: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.012: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.026: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.028: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.030: INFO: Unable to read jessie_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.032: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.034: INFO: Unable to read jessie_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.038: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.043: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:35.057: INFO: Lookups using dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8461 wheezy_tcp@dns-test-service.dns-8461 wheezy_udp@dns-test-service.dns-8461.svc wheezy_tcp@dns-test-service.dns-8461.svc wheezy_udp@_http._tcp.dns-test-service.dns-8461.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8461 jessie_tcp@dns-test-service.dns-8461 jessie_udp@dns-test-service.dns-8461.svc jessie_tcp@dns-test-service.dns-8461.svc jessie_tcp@_http._tcp.dns-test-service.dns-8461.svc]

May 21 06:27:40.060: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.062: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.065: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.067: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.069: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.071: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.088: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.090: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.092: INFO: Unable to read jessie_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.094: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.095: INFO: Unable to read jessie_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:40.112: INFO: Lookups using dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8461 wheezy_tcp@dns-test-service.dns-8461 wheezy_udp@dns-test-service.dns-8461.svc wheezy_tcp@dns-test-service.dns-8461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8461 jessie_tcp@dns-test-service.dns-8461 jessie_udp@dns-test-service.dns-8461.svc jessie_tcp@dns-test-service.dns-8461.svc]

May 21 06:27:45.060: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.063: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.065: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.068: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.070: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.072: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.091: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.093: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.096: INFO: Unable to read jessie_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.100: INFO: Unable to read jessie_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.102: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:45.118: INFO: Lookups using dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8461 wheezy_tcp@dns-test-service.dns-8461 wheezy_udp@dns-test-service.dns-8461.svc wheezy_tcp@dns-test-service.dns-8461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8461 jessie_tcp@dns-test-service.dns-8461 jessie_udp@dns-test-service.dns-8461.svc jessie_tcp@dns-test-service.dns-8461.svc]

May 21 06:27:50.061: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.063: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.065: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.067: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.069: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.071: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.089: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.091: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.093: INFO: Unable to read jessie_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.095: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.097: INFO: Unable to read jessie_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.099: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:50.114: INFO: Lookups using dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8461 wheezy_tcp@dns-test-service.dns-8461 wheezy_udp@dns-test-service.dns-8461.svc wheezy_tcp@dns-test-service.dns-8461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8461 jessie_tcp@dns-test-service.dns-8461 jessie_udp@dns-test-service.dns-8461.svc jessie_tcp@dns-test-service.dns-8461.svc]

May 21 06:27:55.060: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.063: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.067: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.071: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.073: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.076: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.094: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.096: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.098: INFO: Unable to read jessie_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.100: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.101: INFO: Unable to read jessie_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.103: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:27:55.120: INFO: Lookups using dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8461 wheezy_tcp@dns-test-service.dns-8461 wheezy_udp@dns-test-service.dns-8461.svc wheezy_tcp@dns-test-service.dns-8461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8461 jessie_tcp@dns-test-service.dns-8461 jessie_udp@dns-test-service.dns-8461.svc jessie_tcp@dns-test-service.dns-8461.svc]

May 21 06:28:00.069: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.071: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.074: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.077: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.079: INFO: Unable to read wheezy_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.082: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.100: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.103: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.105: INFO: Unable to read jessie_udp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.107: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461 from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.109: INFO: Unable to read jessie_udp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.111: INFO: Unable to read jessie_tcp@dns-test-service.dns-8461.svc from pod dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0: the server could not find the requested resource (get pods dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0)
May 21 06:28:00.127: INFO: Lookups using dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8461 wheezy_tcp@dns-test-service.dns-8461 wheezy_udp@dns-test-service.dns-8461.svc wheezy_tcp@dns-test-service.dns-8461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8461 jessie_tcp@dns-test-service.dns-8461 jessie_udp@dns-test-service.dns-8461.svc jessie_tcp@dns-test-service.dns-8461.svc]

May 21 06:28:05.118: INFO: DNS probes using dns-8461/dns-test-5abebfc7-bf17-4f05-922f-07c4229512c0 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:28:05.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8461" for this suite.

• [SLOW TEST:34.298 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":220,"skipped":3731,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:28:05.204: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-8a5adfbd-c050-4b82-a957-12813a699e4c
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-8a5adfbd-c050-4b82-a957-12813a699e4c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:29:27.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-635" for this suite.

• [SLOW TEST:82.363 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:29:27.570: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7470, will wait for the garbage collector to delete the pods
May 21 06:29:31.665: INFO: Deleting Job.batch foo took: 4.405964ms
May 21 06:29:32.265: INFO: Terminating Job.batch foo pods took: 600.129863ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:30:12.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7470" for this suite.

• [SLOW TEST:44.806 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":222,"skipped":3796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:30:12.379: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:30:12.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3395" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":223,"skipped":3851,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:30:12.448: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9150.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9150.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9150.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9150.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9150.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9150.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9150.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9150.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9150.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9150.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 06:30:14.518: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local from pod dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8: the server could not find the requested resource (get pods dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8)
May 21 06:30:14.520: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local from pod dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8: the server could not find the requested resource (get pods dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8)
May 21 06:30:14.522: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9150.svc.cluster.local from pod dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8: the server could not find the requested resource (get pods dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8)
May 21 06:30:14.523: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9150.svc.cluster.local from pod dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8: the server could not find the requested resource (get pods dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8)
May 21 06:30:14.529: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local from pod dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8: the server could not find the requested resource (get pods dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8)
May 21 06:30:14.531: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local from pod dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8: the server could not find the requested resource (get pods dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8)
May 21 06:30:14.533: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9150.svc.cluster.local from pod dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8: the server could not find the requested resource (get pods dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8)
May 21 06:30:14.535: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9150.svc.cluster.local from pod dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8: the server could not find the requested resource (get pods dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8)
May 21 06:30:14.539: INFO: Lookups using dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9150.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9150.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9150.svc.cluster.local jessie_udp@dns-test-service-2.dns-9150.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9150.svc.cluster.local]

May 21 06:30:19.558: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9150.svc.cluster.local from pod dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8: the server could not find the requested resource (get pods dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8)
May 21 06:30:19.564: INFO: Lookups using dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8 failed for: [jessie_udp@dns-test-service-2.dns-9150.svc.cluster.local]

May 21 06:30:24.566: INFO: DNS probes using dns-9150/dns-test-6dcd443a-74c2-472f-9e12-20b26c819ce8 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:30:24.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9150" for this suite.

• [SLOW TEST:12.182 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":224,"skipped":3866,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:30:24.631: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:30:52.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2028" for this suite.

• [SLOW TEST:28.061 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":225,"skipped":3866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:30:52.694: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:30:52.728: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Pending, waiting for it to be Running (with Ready = true)
May 21 06:30:54.731: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Running (Ready = false)
May 21 06:30:56.731: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Running (Ready = false)
May 21 06:30:58.731: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Running (Ready = false)
May 21 06:31:00.731: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Running (Ready = false)
May 21 06:31:02.731: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Running (Ready = false)
May 21 06:31:04.731: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Running (Ready = false)
May 21 06:31:06.731: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Running (Ready = false)
May 21 06:31:08.731: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Running (Ready = false)
May 21 06:31:10.731: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Running (Ready = false)
May 21 06:31:12.731: INFO: The status of Pod test-webserver-a07a1e9c-13d6-4ac9-bce8-f30e3f8b9a4d is Running (Ready = true)
May 21 06:31:12.733: INFO: Container started at 2021-05-21 06:30:53 +0000 UTC, pod became ready at 2021-05-21 06:31:12 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:31:12.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1856" for this suite.

• [SLOW TEST:20.047 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":226,"skipped":3907,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:31:12.741: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-fbxn
STEP: Creating a pod to test atomic-volume-subpath
May 21 06:31:12.781: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fbxn" in namespace "subpath-8754" to be "Succeeded or Failed"
May 21 06:31:12.788: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Pending", Reason="", readiness=false. Elapsed: 7.178673ms
May 21 06:31:14.792: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Running", Reason="", readiness=true. Elapsed: 2.01028799s
May 21 06:31:16.795: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Running", Reason="", readiness=true. Elapsed: 4.013357426s
May 21 06:31:18.798: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Running", Reason="", readiness=true. Elapsed: 6.016433951s
May 21 06:31:20.801: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Running", Reason="", readiness=true. Elapsed: 8.019820041s
May 21 06:31:22.805: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Running", Reason="", readiness=true. Elapsed: 10.023349282s
May 21 06:31:24.808: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Running", Reason="", readiness=true. Elapsed: 12.026920395s
May 21 06:31:26.812: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Running", Reason="", readiness=true. Elapsed: 14.030480923s
May 21 06:31:28.815: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Running", Reason="", readiness=true. Elapsed: 16.033925904s
May 21 06:31:30.819: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Running", Reason="", readiness=true. Elapsed: 18.037583918s
May 21 06:31:32.822: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Running", Reason="", readiness=true. Elapsed: 20.040931055s
May 21 06:31:34.825: INFO: Pod "pod-subpath-test-configmap-fbxn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.044250519s
STEP: Saw pod success
May 21 06:31:34.825: INFO: Pod "pod-subpath-test-configmap-fbxn" satisfied condition "Succeeded or Failed"
May 21 06:31:34.828: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-subpath-test-configmap-fbxn container test-container-subpath-configmap-fbxn: <nil>
STEP: delete the pod
May 21 06:31:34.853: INFO: Waiting for pod pod-subpath-test-configmap-fbxn to disappear
May 21 06:31:34.860: INFO: Pod pod-subpath-test-configmap-fbxn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fbxn
May 21 06:31:34.860: INFO: Deleting pod "pod-subpath-test-configmap-fbxn" in namespace "subpath-8754"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:31:34.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8754" for this suite.

• [SLOW TEST:22.127 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":227,"skipped":3911,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:31:34.868: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:31:41.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2142" for this suite.

• [SLOW TEST:7.055 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":228,"skipped":3913,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:31:41.923: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:31:41.947: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:31:43.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7079" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":229,"skipped":3966,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:31:43.990: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:31:44.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9062" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":230,"skipped":3969,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:31:44.039: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:31:57.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9088" for this suite.

• [SLOW TEST:13.083 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":231,"skipped":3999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:31:57.122: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:32:08.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8861" for this suite.

• [SLOW TEST:11.068 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":232,"skipped":4036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:32:08.193: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:32:08.216: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 21 06:32:08.225: INFO: Pod name sample-pod: Found 0 pods out of 1
May 21 06:32:13.228: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 06:32:13.228: INFO: Creating deployment "test-rolling-update-deployment"
May 21 06:32:13.236: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 21 06:32:13.247: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 21 06:32:15.253: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 21 06:32:15.255: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 06:32:15.260: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9648  0dda7fd2-c541-4acb-90b1-ae3b748c82c9 27052 1 2021-05-21 06:32:13 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-21 06:32:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 06:32:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058621e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-21 06:32:13 +0000 UTC,LastTransitionTime:2021-05-21 06:32:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-05-21 06:32:15 +0000 UTC,LastTransitionTime:2021-05-21 06:32:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 21 06:32:15.270: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-9648  34123d7c-6d8f-4ad9-afc0-a8dd81a8b8e9 27041 1 2021-05-21 06:32:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 0dda7fd2-c541-4acb-90b1-ae3b748c82c9 0xc005065ff7 0xc005065ff8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 06:32:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dda7fd2-c541-4acb-90b1-ae3b748c82c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006934088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 06:32:15.270: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 21 06:32:15.270: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9648  3d871f90-cdab-4425-bd3f-f3a5e7809ee6 27051 2 2021-05-21 06:32:08 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 0dda7fd2-c541-4acb-90b1-ae3b748c82c9 0xc005065ee7 0xc005065ee8}] []  [{e2e.test Update apps/v1 2021-05-21 06:32:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 06:32:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dda7fd2-c541-4acb-90b1-ae3b748c82c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005065f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 06:32:15.274: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-8fq2k" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-8fq2k test-rolling-update-deployment-6b6bf9df46- deployment-9648  67c81cf7-92c3-4bd3-b2cd-d6b9fe569791 27040 0 2021-05-21 06:32:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 34123d7c-6d8f-4ad9-afc0-a8dd81a8b8e9 0xc0069344a7 0xc0069344a8}] []  [{kube-controller-manager Update v1 2021-05-21 06:32:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34123d7c-6d8f-4ad9-afc0-a8dd81a8b8e9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 06:32:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.88.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vz94n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vz94n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vz94n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:32:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:32:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:32:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 06:32:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.88.22,StartTime:2021-05-21 06:32:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 06:32:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://95c8ec0386cb199b36d5b990e5014f98988a1e80fbbbc5cba234813ad64d773d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.88.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:32:15.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9648" for this suite.

• [SLOW TEST:7.087 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":233,"skipped":4103,"failed":0}
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:32:15.281: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-7245
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7245
STEP: Deleting pre-stop pod
May 21 06:32:24.346: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:32:24.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7245" for this suite.

• [SLOW TEST:9.086 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":234,"skipped":4103,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:32:24.368: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:32:24.402: INFO: Waiting up to 5m0s for pod "downwardapi-volume-222d3dc0-cefa-4e21-b8a7-4dfd73dc41a1" in namespace "downward-api-4607" to be "Succeeded or Failed"
May 21 06:32:24.404: INFO: Pod "downwardapi-volume-222d3dc0-cefa-4e21-b8a7-4dfd73dc41a1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.912378ms
May 21 06:32:26.408: INFO: Pod "downwardapi-volume-222d3dc0-cefa-4e21-b8a7-4dfd73dc41a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005977905s
STEP: Saw pod success
May 21 06:32:26.408: INFO: Pod "downwardapi-volume-222d3dc0-cefa-4e21-b8a7-4dfd73dc41a1" satisfied condition "Succeeded or Failed"
May 21 06:32:26.411: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-222d3dc0-cefa-4e21-b8a7-4dfd73dc41a1 container client-container: <nil>
STEP: delete the pod
May 21 06:32:26.429: INFO: Waiting for pod downwardapi-volume-222d3dc0-cefa-4e21-b8a7-4dfd73dc41a1 to disappear
May 21 06:32:26.434: INFO: Pod downwardapi-volume-222d3dc0-cefa-4e21-b8a7-4dfd73dc41a1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:32:26.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4607" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":235,"skipped":4105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:32:26.443: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:32:26.480: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6645d7b-9d53-47ea-b195-7b3fde203d23" in namespace "downward-api-8404" to be "Succeeded or Failed"
May 21 06:32:26.493: INFO: Pod "downwardapi-volume-f6645d7b-9d53-47ea-b195-7b3fde203d23": Phase="Pending", Reason="", readiness=false. Elapsed: 12.485324ms
May 21 06:32:28.497: INFO: Pod "downwardapi-volume-f6645d7b-9d53-47ea-b195-7b3fde203d23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016721041s
STEP: Saw pod success
May 21 06:32:28.497: INFO: Pod "downwardapi-volume-f6645d7b-9d53-47ea-b195-7b3fde203d23" satisfied condition "Succeeded or Failed"
May 21 06:32:28.499: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-f6645d7b-9d53-47ea-b195-7b3fde203d23 container client-container: <nil>
STEP: delete the pod
May 21 06:32:28.513: INFO: Waiting for pod downwardapi-volume-f6645d7b-9d53-47ea-b195-7b3fde203d23 to disappear
May 21 06:32:28.520: INFO: Pod downwardapi-volume-f6645d7b-9d53-47ea-b195-7b3fde203d23 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:32:28.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8404" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":4150,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:32:28.526: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:32:28.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3d348b9-7b3d-481c-b603-9771bed74933" in namespace "projected-8343" to be "Succeeded or Failed"
May 21 06:32:28.565: INFO: Pod "downwardapi-volume-d3d348b9-7b3d-481c-b603-9771bed74933": Phase="Pending", Reason="", readiness=false. Elapsed: 1.87616ms
May 21 06:32:30.569: INFO: Pod "downwardapi-volume-d3d348b9-7b3d-481c-b603-9771bed74933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0060763s
STEP: Saw pod success
May 21 06:32:30.569: INFO: Pod "downwardapi-volume-d3d348b9-7b3d-481c-b603-9771bed74933" satisfied condition "Succeeded or Failed"
May 21 06:32:30.571: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-d3d348b9-7b3d-481c-b603-9771bed74933 container client-container: <nil>
STEP: delete the pod
May 21 06:32:30.589: INFO: Waiting for pod downwardapi-volume-d3d348b9-7b3d-481c-b603-9771bed74933 to disappear
May 21 06:32:30.595: INFO: Pod downwardapi-volume-d3d348b9-7b3d-481c-b603-9771bed74933 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:32:30.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8343" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":4158,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:32:30.601: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 21 06:32:30.627: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 06:32:30.637: INFO: Waiting for terminating namespaces to be deleted...
May 21 06:32:30.639: INFO: 
Logging pods the apiserver thinks is on node 16c19df1-13dd-4e26-99a1-a192204dc796 before test
May 21 06:32:30.647: INFO: fluent-bit-mv9hz from pks-system started at 2021-05-21 06:08:12 +0000 UTC (2 container statuses recorded)
May 21 06:32:30.647: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 06:32:30.647: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:32:30.647: INFO: node-exporter-fz6jz from pks-system started at 2021-05-21 06:08:12 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.647: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 06:32:30.647: INFO: telegraf-nqkt5 from pks-system started at 2021-05-21 06:08:12 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.648: INFO: 	Container telegraf ready: true, restart count 0
May 21 06:32:30.648: INFO: tester from prestop-7245 started at 2021-05-21 06:32:17 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.648: INFO: 	Container tester ready: true, restart count 0
May 21 06:32:30.648: INFO: sonobuoy from sonobuoy started at 2021-05-21 05:36:00 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.648: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 06:32:30.648: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-b9nwr from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:32:30.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 06:32:30.648: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 06:32:30.648: INFO: 
Logging pods the apiserver thinks is on node 53f495cf-0382-4128-b4b3-0979b69a209b before test
May 21 06:32:30.656: INFO: coredns-6d5447c5f4-fv5hd from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container coredns ready: true, restart count 0
May 21 06:32:30.657: INFO: coredns-6d5447c5f4-jms82 from kube-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container coredns ready: true, restart count 0
May 21 06:32:30.657: INFO: fluent-bit-vsrb2 from pks-system started at 2021-05-21 06:07:53 +0000 UTC (2 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 06:32:30.657: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:32:30.657: INFO: metric-controller-5754b4489d-c246d from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container metric-controller ready: true, restart count 0
May 21 06:32:30.657: INFO: node-exporter-fmcv8 from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 06:32:30.657: INFO: observability-manager-7f5c97895-xsj2f from pks-system started at 2021-05-21 04:30:14 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container observability-manager ready: true, restart count 0
May 21 06:32:30.657: INFO: telegraf-hlmgc from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container telegraf ready: true, restart count 0
May 21 06:32:30.657: INFO: telemetry-agent-8f56d9865-l6ntw from pks-system started at 2021-05-21 04:35:59 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
May 21 06:32:30.657: INFO: validator-54f6bbd67b-snnrv from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container validator ready: true, restart count 0
May 21 06:32:30.657: INFO: sonobuoy-e2e-job-a4c84655dab54066 from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container e2e ready: true, restart count 0
May 21 06:32:30.657: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 06:32:30.657: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-g25r8 from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:32:30.657: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 06:32:30.657: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 06:32:30.657: INFO: 
Logging pods the apiserver thinks is on node 973ef262-7e5e-4fd5-8bf3-6139f838e19b before test
May 21 06:32:30.666: INFO: coredns-6d5447c5f4-7dcmd from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.666: INFO: 	Container coredns ready: true, restart count 0
May 21 06:32:30.666: INFO: metrics-server-7d476fdfbd-phhm6 from kube-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.666: INFO: 	Container metrics-server ready: true, restart count 0
May 21 06:32:30.666: INFO: event-controller-55dd5b5777-4mn7d from pks-system started at 2021-05-21 04:30:17 +0000 UTC (2 container statuses recorded)
May 21 06:32:30.666: INFO: 	Container event-controller ready: true, restart count 0
May 21 06:32:30.666: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:32:30.666: INFO: fluent-bit-hnqwh from pks-system started at 2021-05-21 06:07:50 +0000 UTC (2 container statuses recorded)
May 21 06:32:30.666: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 06:32:30.666: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:32:30.666: INFO: node-exporter-6k9jm from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.666: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 06:32:30.666: INFO: sink-controller-5bf464ff96-4nbts from pks-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.666: INFO: 	Container sink-controller ready: true, restart count 0
May 21 06:32:30.666: INFO: telegraf-jqzbt from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.666: INFO: 	Container telegraf ready: true, restart count 0
May 21 06:32:30.666: INFO: wavefront-collector-5795f87754-596hz from pks-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.666: INFO: 	Container wavefront-collector ready: true, restart count 0
May 21 06:32:30.666: INFO: wavefront-proxy-6446ff4bd7-lw9wt from pks-system started at 2021-05-21 04:32:41 +0000 UTC (1 container statuses recorded)
May 21 06:32:30.666: INFO: 	Container wavefront-proxy ready: true, restart count 0
May 21 06:32:30.666: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-ppgpw from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:32:30.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 06:32:30.666: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-868ce9ea-3a2f-4d94-8f4c-d8ce08a4d50c 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 30.0.0.12 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-868ce9ea-3a2f-4d94-8f4c-d8ce08a4d50c off the node 16c19df1-13dd-4e26-99a1-a192204dc796
STEP: verifying the node doesn't have the label kubernetes.io/e2e-868ce9ea-3a2f-4d94-8f4c-d8ce08a4d50c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:37:34.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6313" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.147 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":238,"skipped":4184,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:37:34.749: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 21 06:37:34.787: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4006  da833b95-7452-47d9-b562-69d3857f1516 27800 0 2021-05-21 06:37:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 06:37:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 06:37:34.787: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4006  da833b95-7452-47d9-b562-69d3857f1516 27801 0 2021-05-21 06:37:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 06:37:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 21 06:37:34.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4006  da833b95-7452-47d9-b562-69d3857f1516 27802 0 2021-05-21 06:37:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 06:37:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 06:37:34.795: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4006  da833b95-7452-47d9-b562-69d3857f1516 27803 0 2021-05-21 06:37:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 06:37:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:37:34.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4006" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":239,"skipped":4185,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:37:34.805: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-f640bb1d-84cc-466d-91b0-2337c4fef6f4
STEP: Creating a pod to test consume configMaps
May 21 06:37:34.840: INFO: Waiting up to 5m0s for pod "pod-configmaps-5d495030-bb24-456f-a58c-388599c3caad" in namespace "configmap-7781" to be "Succeeded or Failed"
May 21 06:37:34.845: INFO: Pod "pod-configmaps-5d495030-bb24-456f-a58c-388599c3caad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.448241ms
May 21 06:37:36.850: INFO: Pod "pod-configmaps-5d495030-bb24-456f-a58c-388599c3caad": Phase="Running", Reason="", readiness=true. Elapsed: 2.009286886s
May 21 06:37:38.857: INFO: Pod "pod-configmaps-5d495030-bb24-456f-a58c-388599c3caad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016712223s
STEP: Saw pod success
May 21 06:37:38.857: INFO: Pod "pod-configmaps-5d495030-bb24-456f-a58c-388599c3caad" satisfied condition "Succeeded or Failed"
May 21 06:37:38.860: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-configmaps-5d495030-bb24-456f-a58c-388599c3caad container agnhost-container: <nil>
STEP: delete the pod
May 21 06:37:38.885: INFO: Waiting for pod pod-configmaps-5d495030-bb24-456f-a58c-388599c3caad to disappear
May 21 06:37:38.892: INFO: Pod pod-configmaps-5d495030-bb24-456f-a58c-388599c3caad no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:37:38.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7781" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":240,"skipped":4196,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:37:38.899: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4863
May 21 06:37:40.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-4863 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 21 06:37:42.118: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 21 06:37:42.118: INFO: stdout: "iptables"
May 21 06:37:42.118: INFO: proxyMode: iptables
May 21 06:37:42.127: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 06:37:42.134: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4863
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4863
I0521 06:37:42.145669      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4863, replica count: 3
I0521 06:37:45.196040      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:37:45.200: INFO: Creating new exec pod
May 21 06:37:48.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-4863 exec execpod-affinitytzsm2 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
May 21 06:37:48.370: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 21 06:37:48.370: INFO: stdout: ""
May 21 06:37:48.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-4863 exec execpod-affinitytzsm2 -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.212 80'
May 21 06:37:48.520: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.212 80\nConnection to 10.100.200.212 80 port [tcp/http] succeeded!\n"
May 21 06:37:48.520: INFO: stdout: ""
May 21 06:37:48.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-4863 exec execpod-affinitytzsm2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.200.212:80/ ; done'
May 21 06:37:48.757: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n"
May 21 06:37:48.757: INFO: stdout: "\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp\naffinity-clusterip-timeout-6fzzp"
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Received response from host: affinity-clusterip-timeout-6fzzp
May 21 06:37:48.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-4863 exec execpod-affinitytzsm2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.200.212:80/'
May 21 06:37:48.927: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n"
May 21 06:37:48.927: INFO: stdout: "affinity-clusterip-timeout-6fzzp"
May 21 06:38:08.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-4863 exec execpod-affinitytzsm2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.200.212:80/'
May 21 06:38:09.091: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n"
May 21 06:38:09.091: INFO: stdout: "affinity-clusterip-timeout-6fzzp"
May 21 06:38:29.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-4863 exec execpod-affinitytzsm2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.200.212:80/'
May 21 06:38:29.250: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.200.212:80/\n"
May 21 06:38:29.250: INFO: stdout: "affinity-clusterip-timeout-t4wwl"
May 21 06:38:29.250: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4863, will wait for the garbage collector to delete the pods
May 21 06:38:29.325: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.851262ms
May 21 06:38:29.925: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.128832ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:38:42.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4863" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:63.547 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":241,"skipped":4205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:38:42.447: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 21 06:38:42.492: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6725  e2f2bc7f-640f-4ab9-ab7a-a8bbea09d15b 28107 0 2021-05-21 06:38:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-21 06:38:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 06:38:42.492: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6725  e2f2bc7f-640f-4ab9-ab7a-a8bbea09d15b 28108 0 2021-05-21 06:38:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-21 06:38:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:38:42.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6725" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":242,"skipped":4234,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:38:42.498: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 21 06:38:42.534: INFO: Waiting up to 5m0s for pod "pod-8dc08f48-84a9-494a-92c3-1486a9da287c" in namespace "emptydir-6705" to be "Succeeded or Failed"
May 21 06:38:42.540: INFO: Pod "pod-8dc08f48-84a9-494a-92c3-1486a9da287c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.524172ms
May 21 06:38:44.544: INFO: Pod "pod-8dc08f48-84a9-494a-92c3-1486a9da287c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009122581s
STEP: Saw pod success
May 21 06:38:44.544: INFO: Pod "pod-8dc08f48-84a9-494a-92c3-1486a9da287c" satisfied condition "Succeeded or Failed"
May 21 06:38:44.546: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-8dc08f48-84a9-494a-92c3-1486a9da287c container test-container: <nil>
STEP: delete the pod
May 21 06:38:44.567: INFO: Waiting for pod pod-8dc08f48-84a9-494a-92c3-1486a9da287c to disappear
May 21 06:38:44.569: INFO: Pod pod-8dc08f48-84a9-494a-92c3-1486a9da287c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:38:44.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6705" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4237,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:38:44.576: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:38:44.606: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:38:45.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-601" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":244,"skipped":4239,"failed":0}

------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:38:45.747: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-121bb9d5-11bd-4762-a8d1-0d7a64329a60 in namespace container-probe-5686
May 21 06:38:47.788: INFO: Started pod busybox-121bb9d5-11bd-4762-a8d1-0d7a64329a60 in namespace container-probe-5686
STEP: checking the pod's current state and verifying that restartCount is present
May 21 06:38:47.790: INFO: Initial restart count of pod busybox-121bb9d5-11bd-4762-a8d1-0d7a64329a60 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:42:48.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5686" for this suite.

• [SLOW TEST:242.509 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4239,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:42:48.257: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:42:48.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1114" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":246,"skipped":4241,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:42:48.318: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7028
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 21 06:42:48.374: INFO: Found 0 stateful pods, waiting for 3
May 21 06:42:58.378: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 06:42:58.378: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 06:42:58.378: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
May 21 06:42:58.402: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 21 06:43:08.434: INFO: Updating stateful set ss2
May 21 06:43:08.445: INFO: Waiting for Pod statefulset-7028/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
STEP: Restoring Pods to the correct revision when they are deleted
May 21 06:43:18.520: INFO: Found 2 stateful pods, waiting for 3
May 21 06:43:28.524: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 06:43:28.524: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 06:43:28.524: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 21 06:43:28.546: INFO: Updating stateful set ss2
May 21 06:43:28.574: INFO: Waiting for Pod statefulset-7028/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 06:43:38.596: INFO: Updating stateful set ss2
May 21 06:43:38.609: INFO: Waiting for StatefulSet statefulset-7028/ss2 to complete update
May 21 06:43:38.609: INFO: Waiting for Pod statefulset-7028/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 06:43:48.615: INFO: Deleting all statefulset in ns statefulset-7028
May 21 06:43:48.616: INFO: Scaling statefulset ss2 to 0
May 21 06:44:18.633: INFO: Waiting for statefulset status.replicas updated to 0
May 21 06:44:18.635: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:44:18.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7028" for this suite.

• [SLOW TEST:90.344 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":247,"skipped":4243,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:44:18.664: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-9678/configmap-test-ee963a63-5210-4028-8b01-9562e7240d7c
STEP: Creating a pod to test consume configMaps
May 21 06:44:18.704: INFO: Waiting up to 5m0s for pod "pod-configmaps-232b6a75-6db0-4dae-a5a7-b7392f356187" in namespace "configmap-9678" to be "Succeeded or Failed"
May 21 06:44:18.711: INFO: Pod "pod-configmaps-232b6a75-6db0-4dae-a5a7-b7392f356187": Phase="Pending", Reason="", readiness=false. Elapsed: 7.430635ms
May 21 06:44:20.714: INFO: Pod "pod-configmaps-232b6a75-6db0-4dae-a5a7-b7392f356187": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010043681s
STEP: Saw pod success
May 21 06:44:20.714: INFO: Pod "pod-configmaps-232b6a75-6db0-4dae-a5a7-b7392f356187" satisfied condition "Succeeded or Failed"
May 21 06:44:20.716: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-configmaps-232b6a75-6db0-4dae-a5a7-b7392f356187 container env-test: <nil>
STEP: delete the pod
May 21 06:44:20.749: INFO: Waiting for pod pod-configmaps-232b6a75-6db0-4dae-a5a7-b7392f356187 to disappear
May 21 06:44:20.750: INFO: Pod pod-configmaps-232b6a75-6db0-4dae-a5a7-b7392f356187 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:44:20.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9678" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4254,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:44:20.758: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:44:33.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5623" for this suite.
STEP: Destroying namespace "nsdeletetest-4254" for this suite.
May 21 06:44:33.894: INFO: Namespace nsdeletetest-4254 was already deleted
STEP: Destroying namespace "nsdeletetest-9748" for this suite.

• [SLOW TEST:13.139 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":249,"skipped":4269,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:44:33.897: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 21 06:44:33.936: INFO: Waiting up to 5m0s for pod "pod-3e0b76f1-40a7-4e5d-93a5-a7550efe4dea" in namespace "emptydir-2108" to be "Succeeded or Failed"
May 21 06:44:33.943: INFO: Pod "pod-3e0b76f1-40a7-4e5d-93a5-a7550efe4dea": Phase="Pending", Reason="", readiness=false. Elapsed: 7.454143ms
May 21 06:44:35.947: INFO: Pod "pod-3e0b76f1-40a7-4e5d-93a5-a7550efe4dea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010659668s
STEP: Saw pod success
May 21 06:44:35.947: INFO: Pod "pod-3e0b76f1-40a7-4e5d-93a5-a7550efe4dea" satisfied condition "Succeeded or Failed"
May 21 06:44:35.949: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-3e0b76f1-40a7-4e5d-93a5-a7550efe4dea container test-container: <nil>
STEP: delete the pod
May 21 06:44:35.962: INFO: Waiting for pod pod-3e0b76f1-40a7-4e5d-93a5-a7550efe4dea to disappear
May 21 06:44:35.968: INFO: Pod pod-3e0b76f1-40a7-4e5d-93a5-a7550efe4dea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:44:35.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2108" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":250,"skipped":4280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:44:35.975: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
May 21 06:44:36.010: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-176 proxy --unix-socket=/tmp/kubectl-proxy-unix438004545/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:44:36.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-176" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":251,"skipped":4341,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:44:36.069: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 06:44:36.507: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 06:44:39.528: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:44:39.533: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:44:40.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2589" for this suite.
STEP: Destroying namespace "webhook-2589-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":252,"skipped":4348,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:44:40.774: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 21 06:44:40.836: INFO: Number of nodes with available pods: 0
May 21 06:44:40.836: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 06:44:41.842: INFO: Number of nodes with available pods: 0
May 21 06:44:41.842: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 06:44:42.842: INFO: Number of nodes with available pods: 3
May 21 06:44:42.842: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 21 06:44:42.857: INFO: Number of nodes with available pods: 2
May 21 06:44:42.857: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:44:43.863: INFO: Number of nodes with available pods: 2
May 21 06:44:43.863: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:44:44.863: INFO: Number of nodes with available pods: 2
May 21 06:44:44.863: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:44:45.863: INFO: Number of nodes with available pods: 2
May 21 06:44:45.863: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:44:46.863: INFO: Number of nodes with available pods: 2
May 21 06:44:46.863: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:44:47.867: INFO: Number of nodes with available pods: 2
May 21 06:44:47.867: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:44:48.863: INFO: Number of nodes with available pods: 2
May 21 06:44:48.863: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:44:49.863: INFO: Number of nodes with available pods: 2
May 21 06:44:49.863: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:44:50.879: INFO: Number of nodes with available pods: 2
May 21 06:44:50.879: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:44:51.863: INFO: Number of nodes with available pods: 2
May 21 06:44:51.863: INFO: Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b is running more than one daemon pod
May 21 06:44:52.862: INFO: Number of nodes with available pods: 3
May 21 06:44:52.862: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4808, will wait for the garbage collector to delete the pods
May 21 06:44:52.921: INFO: Deleting DaemonSet.extensions daemon-set took: 5.019851ms
May 21 06:44:53.521: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.161023ms
May 21 06:45:00.924: INFO: Number of nodes with available pods: 0
May 21 06:45:00.924: INFO: Number of running nodes: 0, number of available pods: 0
May 21 06:45:00.925: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29423"},"items":null}

May 21 06:45:00.927: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29423"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:45:00.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4808" for this suite.

• [SLOW TEST:20.166 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":253,"skipped":4352,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:45:00.940: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-89
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-89 to expose endpoints map[]
May 21 06:45:00.986: INFO: successfully validated that service endpoint-test2 in namespace services-89 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-89
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-89 to expose endpoints map[pod1:[80]]
May 21 06:45:03.014: INFO: successfully validated that service endpoint-test2 in namespace services-89 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-89
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-89 to expose endpoints map[pod1:[80] pod2:[80]]
May 21 06:45:05.044: INFO: successfully validated that service endpoint-test2 in namespace services-89 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-89
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-89 to expose endpoints map[pod2:[80]]
May 21 06:45:05.081: INFO: successfully validated that service endpoint-test2 in namespace services-89 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-89
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-89 to expose endpoints map[]
May 21 06:45:05.111: INFO: successfully validated that service endpoint-test2 in namespace services-89 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:45:05.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-89" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":254,"skipped":4372,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:45:05.132: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 06:45:07.176: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:45:07.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1187" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":255,"skipped":4380,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:45:07.200: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-6bd7d176-98df-4f84-b3f7-3abc03e02afd
STEP: Creating a pod to test consume secrets
May 21 06:45:07.265: INFO: Waiting up to 5m0s for pod "pod-secrets-14c03df9-bbdf-4ab8-839f-3f8a112b8048" in namespace "secrets-5954" to be "Succeeded or Failed"
May 21 06:45:07.269: INFO: Pod "pod-secrets-14c03df9-bbdf-4ab8-839f-3f8a112b8048": Phase="Pending", Reason="", readiness=false. Elapsed: 3.857686ms
May 21 06:45:09.273: INFO: Pod "pod-secrets-14c03df9-bbdf-4ab8-839f-3f8a112b8048": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007530965s
STEP: Saw pod success
May 21 06:45:09.273: INFO: Pod "pod-secrets-14c03df9-bbdf-4ab8-839f-3f8a112b8048" satisfied condition "Succeeded or Failed"
May 21 06:45:09.275: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-secrets-14c03df9-bbdf-4ab8-839f-3f8a112b8048 container secret-volume-test: <nil>
STEP: delete the pod
May 21 06:45:09.288: INFO: Waiting for pod pod-secrets-14c03df9-bbdf-4ab8-839f-3f8a112b8048 to disappear
May 21 06:45:09.296: INFO: Pod pod-secrets-14c03df9-bbdf-4ab8-839f-3f8a112b8048 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:45:09.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5954" for this suite.
STEP: Destroying namespace "secret-namespace-4930" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4382,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:45:09.306: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:45:12.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6750" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":257,"skipped":4408,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:45:12.372: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 06:45:12.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6494 run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 21 06:45:12.470: INFO: stderr: ""
May 21 06:45:12.470: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 21 06:45:12.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6494 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "mirror.gcr.io/library/busybox:1.29"}]}} --dry-run=server'
May 21 06:45:12.725: INFO: stderr: ""
May 21 06:45:12.725: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 06:45:12.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6494 delete pods e2e-test-httpd-pod'
May 21 06:45:15.282: INFO: stderr: ""
May 21 06:45:15.282: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:45:15.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6494" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":258,"skipped":4413,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:45:15.290: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 21 06:45:15.320: INFO: Waiting up to 5m0s for pod "pod-1a342e30-4f49-47a9-8df3-6a576c7b3a32" in namespace "emptydir-1482" to be "Succeeded or Failed"
May 21 06:45:15.332: INFO: Pod "pod-1a342e30-4f49-47a9-8df3-6a576c7b3a32": Phase="Pending", Reason="", readiness=false. Elapsed: 11.210671ms
May 21 06:45:17.336: INFO: Pod "pod-1a342e30-4f49-47a9-8df3-6a576c7b3a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015507736s
May 21 06:45:19.340: INFO: Pod "pod-1a342e30-4f49-47a9-8df3-6a576c7b3a32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020019746s
STEP: Saw pod success
May 21 06:45:19.340: INFO: Pod "pod-1a342e30-4f49-47a9-8df3-6a576c7b3a32" satisfied condition "Succeeded or Failed"
May 21 06:45:19.343: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-1a342e30-4f49-47a9-8df3-6a576c7b3a32 container test-container: <nil>
STEP: delete the pod
May 21 06:45:19.358: INFO: Waiting for pod pod-1a342e30-4f49-47a9-8df3-6a576c7b3a32 to disappear
May 21 06:45:19.364: INFO: Pod pod-1a342e30-4f49-47a9-8df3-6a576c7b3a32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:45:19.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1482" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":259,"skipped":4422,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:45:19.369: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-68bdf1f2-d1c9-4fae-ad89-f936a63fd3a8
STEP: Creating secret with name s-test-opt-upd-8449c6d4-1c39-4e8f-93de-3c69034b3e7d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-68bdf1f2-d1c9-4fae-ad89-f936a63fd3a8
STEP: Updating secret s-test-opt-upd-8449c6d4-1c39-4e8f-93de-3c69034b3e7d
STEP: Creating secret with name s-test-opt-create-3e8c2de1-6b34-4f35-bdae-f437c6a4f52b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:46:27.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2477" for this suite.

• [SLOW TEST:68.342 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:46:27.712: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 21 06:46:27.769: INFO: Number of nodes with available pods: 0
May 21 06:46:27.769: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 06:46:28.776: INFO: Number of nodes with available pods: 0
May 21 06:46:28.776: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 06:46:29.777: INFO: Number of nodes with available pods: 2
May 21 06:46:29.777: INFO: Node 16c19df1-13dd-4e26-99a1-a192204dc796 is running more than one daemon pod
May 21 06:46:30.776: INFO: Number of nodes with available pods: 3
May 21 06:46:30.776: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 21 06:46:30.816: INFO: Number of nodes with available pods: 2
May 21 06:46:30.816: INFO: Node 53f495cf-0382-4128-b4b3-0979b69a209b is running more than one daemon pod
May 21 06:46:31.826: INFO: Number of nodes with available pods: 2
May 21 06:46:31.826: INFO: Node 53f495cf-0382-4128-b4b3-0979b69a209b is running more than one daemon pod
May 21 06:46:32.843: INFO: Number of nodes with available pods: 3
May 21 06:46:32.843: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5281, will wait for the garbage collector to delete the pods
May 21 06:46:32.905: INFO: Deleting DaemonSet.extensions daemon-set took: 4.175061ms
May 21 06:46:33.505: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.382334ms
May 21 06:46:43.708: INFO: Number of nodes with available pods: 0
May 21 06:46:43.708: INFO: Number of running nodes: 0, number of available pods: 0
May 21 06:46:43.709: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29946"},"items":null}

May 21 06:46:43.711: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29946"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:46:43.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5281" for this suite.

• [SLOW TEST:16.013 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":261,"skipped":4453,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:46:43.725: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 21 06:46:43.763: INFO: Waiting up to 5m0s for pod "pod-f0d5d71f-d7c5-4bf8-a772-3fa24768fbd2" in namespace "emptydir-3091" to be "Succeeded or Failed"
May 21 06:46:43.773: INFO: Pod "pod-f0d5d71f-d7c5-4bf8-a772-3fa24768fbd2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.856249ms
May 21 06:46:45.777: INFO: Pod "pod-f0d5d71f-d7c5-4bf8-a772-3fa24768fbd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014563236s
STEP: Saw pod success
May 21 06:46:45.777: INFO: Pod "pod-f0d5d71f-d7c5-4bf8-a772-3fa24768fbd2" satisfied condition "Succeeded or Failed"
May 21 06:46:45.779: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-f0d5d71f-d7c5-4bf8-a772-3fa24768fbd2 container test-container: <nil>
STEP: delete the pod
May 21 06:46:45.799: INFO: Waiting for pod pod-f0d5d71f-d7c5-4bf8-a772-3fa24768fbd2 to disappear
May 21 06:46:45.804: INFO: Pod pod-f0d5d71f-d7c5-4bf8-a772-3fa24768fbd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:46:45.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3091" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":262,"skipped":4467,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:46:45.810: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 06:46:46.258: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 06:46:48.269: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757176406, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757176406, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757176406, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757176406, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 06:46:51.278: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:46:51.280: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4135-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:46:52.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2379" for this suite.
STEP: Destroying namespace "webhook-2379-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.711 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":263,"skipped":4476,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:46:52.523: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 21 06:46:56.576: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4848 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:46:56.576: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:46:56.663: INFO: Exec stderr: ""
May 21 06:46:56.663: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4848 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:46:56.663: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:46:56.742: INFO: Exec stderr: ""
May 21 06:46:56.742: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4848 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:46:56.742: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:46:56.821: INFO: Exec stderr: ""
May 21 06:46:56.822: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4848 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:46:56.822: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:46:56.901: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 21 06:46:56.901: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4848 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:46:56.901: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:46:56.989: INFO: Exec stderr: ""
May 21 06:46:56.989: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4848 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:46:56.989: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:46:57.077: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 21 06:46:57.078: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4848 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:46:57.078: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:46:57.159: INFO: Exec stderr: ""
May 21 06:46:57.159: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4848 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:46:57.159: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:46:57.238: INFO: Exec stderr: ""
May 21 06:46:57.238: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4848 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:46:57.238: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:46:57.317: INFO: Exec stderr: ""
May 21 06:46:57.317: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4848 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:46:57.317: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:46:57.397: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:46:57.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4848" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":264,"skipped":4494,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:46:57.406: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:46:57.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9728" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":265,"skipped":4510,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:46:57.485: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 21 06:46:57.535: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 06:47:57.555: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:47:57.557: INFO: Starting informer...
STEP: Starting pod...
May 21 06:47:57.775: INFO: Pod is running on 16c19df1-13dd-4e26-99a1-a192204dc796. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 21 06:47:57.795: INFO: Pod wasn't evicted. Proceeding
May 21 06:47:57.795: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 21 06:49:12.831: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:49:12.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-787" for this suite.

• [SLOW TEST:135.355 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":266,"skipped":4522,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:49:12.841: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:49:12.945: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2c74c0c-9e1c-4124-b388-26e07fa42094" in namespace "downward-api-1636" to be "Succeeded or Failed"
May 21 06:49:12.967: INFO: Pod "downwardapi-volume-a2c74c0c-9e1c-4124-b388-26e07fa42094": Phase="Pending", Reason="", readiness=false. Elapsed: 22.63017ms
May 21 06:49:14.971: INFO: Pod "downwardapi-volume-a2c74c0c-9e1c-4124-b388-26e07fa42094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026107379s
May 21 06:49:16.974: INFO: Pod "downwardapi-volume-a2c74c0c-9e1c-4124-b388-26e07fa42094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029395927s
STEP: Saw pod success
May 21 06:49:16.974: INFO: Pod "downwardapi-volume-a2c74c0c-9e1c-4124-b388-26e07fa42094" satisfied condition "Succeeded or Failed"
May 21 06:49:16.976: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-a2c74c0c-9e1c-4124-b388-26e07fa42094 container client-container: <nil>
STEP: delete the pod
May 21 06:49:17.020: INFO: Waiting for pod downwardapi-volume-a2c74c0c-9e1c-4124-b388-26e07fa42094 to disappear
May 21 06:49:17.037: INFO: Pod downwardapi-volume-a2c74c0c-9e1c-4124-b388-26e07fa42094 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:49:17.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1636" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4535,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:49:17.044: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 21 06:49:21.170: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3499 PodName:pod-sharedvolume-ffa24560-b696-4881-b49c-c516a29a00fa ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:49:21.170: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:49:21.253: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:49:21.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3499" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":268,"skipped":4555,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:49:21.261: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-dc096a9f-c02d-4ef8-9f97-89a398e483cc in namespace container-probe-9578
May 21 06:49:25.390: INFO: Started pod liveness-dc096a9f-c02d-4ef8-9f97-89a398e483cc in namespace container-probe-9578
STEP: checking the pod's current state and verifying that restartCount is present
May 21 06:49:25.392: INFO: Initial restart count of pod liveness-dc096a9f-c02d-4ef8-9f97-89a398e483cc is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:53:25.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9578" for this suite.

• [SLOW TEST:244.608 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4589,"failed":0}
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:53:25.870: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-5628
STEP: creating replication controller nodeport-test in namespace services-5628
I0521 06:53:25.921604      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-5628, replica count: 2
May 21 06:53:28.971: INFO: Creating new exec pod
I0521 06:53:28.971786      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:53:33.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-5628 exec execpodb7kh2 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 21 06:53:35.176: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 21 06:53:35.176: INFO: stdout: ""
May 21 06:53:35.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-5628 exec execpodb7kh2 -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.239 80'
May 21 06:53:35.325: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.239 80\nConnection to 10.100.200.239 80 port [tcp/http] succeeded!\n"
May 21 06:53:35.325: INFO: stdout: ""
May 21 06:53:35.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-5628 exec execpodb7kh2 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 31630'
May 21 06:53:35.484: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 31630\nConnection to 30.0.0.10 31630 port [tcp/31630] succeeded!\n"
May 21 06:53:35.484: INFO: stdout: ""
May 21 06:53:35.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-5628 exec execpodb7kh2 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 31630'
May 21 06:53:35.648: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 31630\nConnection to 30.0.0.12 31630 port [tcp/31630] succeeded!\n"
May 21 06:53:35.648: INFO: stdout: ""
May 21 06:53:35.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-5628 exec execpodb7kh2 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 31630'
May 21 06:53:35.800: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 31630\nConnection to 30.0.0.10 31630 port [tcp/31630] succeeded!\n"
May 21 06:53:35.800: INFO: stdout: ""
May 21 06:53:35.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-5628 exec execpodb7kh2 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 31630'
May 21 06:53:35.963: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 31630\nConnection to 30.0.0.12 31630 port [tcp/31630] succeeded!\n"
May 21 06:53:35.963: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:53:35.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5628" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:10.101 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":270,"skipped":4589,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:53:35.972: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:53:38.017: INFO: Deleting pod "var-expansion-69b9948f-2c49-4490-83b6-869a0a1dd378" in namespace "var-expansion-2768"
May 21 06:53:38.023: INFO: Wait up to 5m0s for pod "var-expansion-69b9948f-2c49-4490-83b6-869a0a1dd378" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:53:40.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2768" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":271,"skipped":4603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:53:40.036: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-93b21da2-3aff-4f6e-a865-6b3f62fab5de
STEP: Creating a pod to test consume configMaps
May 21 06:53:40.076: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f5f31595-da07-4609-adf6-cd200048146b" in namespace "projected-9505" to be "Succeeded or Failed"
May 21 06:53:40.088: INFO: Pod "pod-projected-configmaps-f5f31595-da07-4609-adf6-cd200048146b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.449645ms
May 21 06:53:42.090: INFO: Pod "pod-projected-configmaps-f5f31595-da07-4609-adf6-cd200048146b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013906113s
May 21 06:53:44.094: INFO: Pod "pod-projected-configmaps-f5f31595-da07-4609-adf6-cd200048146b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017794848s
STEP: Saw pod success
May 21 06:53:44.094: INFO: Pod "pod-projected-configmaps-f5f31595-da07-4609-adf6-cd200048146b" satisfied condition "Succeeded or Failed"
May 21 06:53:44.096: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-configmaps-f5f31595-da07-4609-adf6-cd200048146b container agnhost-container: <nil>
STEP: delete the pod
May 21 06:53:44.121: INFO: Waiting for pod pod-projected-configmaps-f5f31595-da07-4609-adf6-cd200048146b to disappear
May 21 06:53:44.128: INFO: Pod pod-projected-configmaps-f5f31595-da07-4609-adf6-cd200048146b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:53:44.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9505" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4633,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:53:44.135: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 06:53:44.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 06:53:46.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757176824, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757176824, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757176824, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757176824, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 06:53:49.639: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:54:01.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9612" for this suite.
STEP: Destroying namespace "webhook-9612-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.668 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":273,"skipped":4643,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:54:01.804: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 21 06:54:01.840: INFO: Waiting up to 5m0s for pod "downward-api-ee72aeec-0cea-4de3-a8b3-170212cc1b41" in namespace "downward-api-6387" to be "Succeeded or Failed"
May 21 06:54:01.842: INFO: Pod "downward-api-ee72aeec-0cea-4de3-a8b3-170212cc1b41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260765ms
May 21 06:54:03.847: INFO: Pod "downward-api-ee72aeec-0cea-4de3-a8b3-170212cc1b41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00660305s
STEP: Saw pod success
May 21 06:54:03.847: INFO: Pod "downward-api-ee72aeec-0cea-4de3-a8b3-170212cc1b41" satisfied condition "Succeeded or Failed"
May 21 06:54:03.849: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downward-api-ee72aeec-0cea-4de3-a8b3-170212cc1b41 container dapi-container: <nil>
STEP: delete the pod
May 21 06:54:03.862: INFO: Waiting for pod downward-api-ee72aeec-0cea-4de3-a8b3-170212cc1b41 to disappear
May 21 06:54:03.869: INFO: Pod downward-api-ee72aeec-0cea-4de3-a8b3-170212cc1b41 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:54:03.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6387" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4646,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:54:03.876: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 21 06:54:03.917: INFO: Waiting up to 5m0s for pod "pod-dfafdca0-ac0b-4c5e-94ab-df00ec654c4a" in namespace "emptydir-329" to be "Succeeded or Failed"
May 21 06:54:03.921: INFO: Pod "pod-dfafdca0-ac0b-4c5e-94ab-df00ec654c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.728771ms
May 21 06:54:05.925: INFO: Pod "pod-dfafdca0-ac0b-4c5e-94ab-df00ec654c4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007856729s
STEP: Saw pod success
May 21 06:54:05.925: INFO: Pod "pod-dfafdca0-ac0b-4c5e-94ab-df00ec654c4a" satisfied condition "Succeeded or Failed"
May 21 06:54:05.927: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-dfafdca0-ac0b-4c5e-94ab-df00ec654c4a container test-container: <nil>
STEP: delete the pod
May 21 06:54:05.941: INFO: Waiting for pod pod-dfafdca0-ac0b-4c5e-94ab-df00ec654c4a to disappear
May 21 06:54:05.948: INFO: Pod pod-dfafdca0-ac0b-4c5e-94ab-df00ec654c4a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:54:05.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-329" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4650,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:54:05.954: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-330c5d9d-608b-4c74-85b6-9993bb586fad
STEP: Creating a pod to test consume secrets
May 21 06:54:06.000: INFO: Waiting up to 5m0s for pod "pod-secrets-f553d788-e4e8-4827-a886-e4cdcaf33f86" in namespace "secrets-2170" to be "Succeeded or Failed"
May 21 06:54:06.002: INFO: Pod "pod-secrets-f553d788-e4e8-4827-a886-e4cdcaf33f86": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5386ms
May 21 06:54:08.005: INFO: Pod "pod-secrets-f553d788-e4e8-4827-a886-e4cdcaf33f86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004468402s
STEP: Saw pod success
May 21 06:54:08.005: INFO: Pod "pod-secrets-f553d788-e4e8-4827-a886-e4cdcaf33f86" satisfied condition "Succeeded or Failed"
May 21 06:54:08.007: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-secrets-f553d788-e4e8-4827-a886-e4cdcaf33f86 container secret-volume-test: <nil>
STEP: delete the pod
May 21 06:54:08.019: INFO: Waiting for pod pod-secrets-f553d788-e4e8-4827-a886-e4cdcaf33f86 to disappear
May 21 06:54:08.025: INFO: Pod pod-secrets-f553d788-e4e8-4827-a886-e4cdcaf33f86 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:54:08.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2170" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:54:08.032: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0521 06:54:18.089941      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 06:54:18.089960      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 06:54:18.089965      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 06:54:18.089: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:54:18.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8790" for this suite.

• [SLOW TEST:10.065 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":277,"skipped":4744,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:54:18.098: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 21 06:54:18.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f98d634b-f721-4d6f-aa1e-c588dc6e09ef" in namespace "projected-3100" to be "Succeeded or Failed"
May 21 06:54:18.143: INFO: Pod "downwardapi-volume-f98d634b-f721-4d6f-aa1e-c588dc6e09ef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.432234ms
May 21 06:54:20.147: INFO: Pod "downwardapi-volume-f98d634b-f721-4d6f-aa1e-c588dc6e09ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.009368844s
May 21 06:54:22.149: INFO: Pod "downwardapi-volume-f98d634b-f721-4d6f-aa1e-c588dc6e09ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012078714s
STEP: Saw pod success
May 21 06:54:22.150: INFO: Pod "downwardapi-volume-f98d634b-f721-4d6f-aa1e-c588dc6e09ef" satisfied condition "Succeeded or Failed"
May 21 06:54:22.151: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downwardapi-volume-f98d634b-f721-4d6f-aa1e-c588dc6e09ef container client-container: <nil>
STEP: delete the pod
May 21 06:54:22.162: INFO: Waiting for pod downwardapi-volume-f98d634b-f721-4d6f-aa1e-c588dc6e09ef to disappear
May 21 06:54:22.171: INFO: Pod downwardapi-volume-f98d634b-f721-4d6f-aa1e-c588dc6e09ef no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:54:22.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3100" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":278,"skipped":4757,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:54:22.177: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 21 06:54:22.198: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:54:25.122: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:54:34.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1988" for this suite.

• [SLOW TEST:12.632 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":279,"skipped":4770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:54:34.810: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:55:34.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9456" for this suite.

• [SLOW TEST:60.191 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":280,"skipped":4795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:55:35.002: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-xq295 in namespace proxy-308
I0521 06:55:35.047018      20 runners.go:190] Created replication controller with name: proxy-service-xq295, namespace: proxy-308, replica count: 1
I0521 06:55:36.097357      20 runners.go:190] proxy-service-xq295 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 06:55:37.097594      20 runners.go:190] proxy-service-xq295 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 06:55:38.097836      20 runners.go:190] proxy-service-xq295 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 06:55:39.098070      20 runners.go:190] proxy-service-xq295 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:55:39.102: INFO: setup took 4.073102807s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 21 06:55:39.113: INFO: (0) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 10.525374ms)
May 21 06:55:39.113: INFO: (0) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 10.962669ms)
May 21 06:55:39.114: INFO: (0) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 11.00643ms)
May 21 06:55:39.114: INFO: (0) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 11.279074ms)
May 21 06:55:39.116: INFO: (0) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 12.859391ms)
May 21 06:55:39.116: INFO: (0) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 13.350027ms)
May 21 06:55:39.116: INFO: (0) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 13.505353ms)
May 21 06:55:39.116: INFO: (0) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 13.430028ms)
May 21 06:55:39.116: INFO: (0) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 13.263805ms)
May 21 06:55:39.116: INFO: (0) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 13.292232ms)
May 21 06:55:39.116: INFO: (0) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 13.56383ms)
May 21 06:55:39.116: INFO: (0) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 13.675866ms)
May 21 06:55:39.116: INFO: (0) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 13.646731ms)
May 21 06:55:39.116: INFO: (0) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 13.619223ms)
May 21 06:55:39.117: INFO: (0) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 14.095285ms)
May 21 06:55:39.119: INFO: (0) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 16.087002ms)
May 21 06:55:39.122: INFO: (1) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 3.451574ms)
May 21 06:55:39.124: INFO: (1) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 4.689524ms)
May 21 06:55:39.124: INFO: (1) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 4.988335ms)
May 21 06:55:39.124: INFO: (1) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 5.42744ms)
May 21 06:55:39.126: INFO: (1) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 7.205565ms)
May 21 06:55:39.126: INFO: (1) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 7.347547ms)
May 21 06:55:39.126: INFO: (1) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 7.232428ms)
May 21 06:55:39.126: INFO: (1) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 7.321255ms)
May 21 06:55:39.127: INFO: (1) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 8.107069ms)
May 21 06:55:39.127: INFO: (1) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 8.022457ms)
May 21 06:55:39.127: INFO: (1) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 8.347522ms)
May 21 06:55:39.130: INFO: (1) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 10.57776ms)
May 21 06:55:39.131: INFO: (1) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 11.584121ms)
May 21 06:55:39.131: INFO: (1) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 11.450543ms)
May 21 06:55:39.131: INFO: (1) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 12.070116ms)
May 21 06:55:39.131: INFO: (1) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 12.154144ms)
May 21 06:55:39.136: INFO: (2) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 4.736877ms)
May 21 06:55:39.138: INFO: (2) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 6.229905ms)
May 21 06:55:39.138: INFO: (2) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 5.958213ms)
May 21 06:55:39.138: INFO: (2) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 6.61503ms)
May 21 06:55:39.140: INFO: (2) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 8.13717ms)
May 21 06:55:39.140: INFO: (2) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 8.60846ms)
May 21 06:55:39.144: INFO: (2) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 12.083117ms)
May 21 06:55:39.144: INFO: (2) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 12.118823ms)
May 21 06:55:39.144: INFO: (2) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 12.193294ms)
May 21 06:55:39.144: INFO: (2) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 12.371843ms)
May 21 06:55:39.144: INFO: (2) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 12.566778ms)
May 21 06:55:39.145: INFO: (2) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 12.305317ms)
May 21 06:55:39.145: INFO: (2) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 12.477184ms)
May 21 06:55:39.145: INFO: (2) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 12.897556ms)
May 21 06:55:39.145: INFO: (2) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 13.295111ms)
May 21 06:55:39.145: INFO: (2) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 12.71708ms)
May 21 06:55:39.151: INFO: (3) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 5.764921ms)
May 21 06:55:39.151: INFO: (3) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 5.890442ms)
May 21 06:55:39.151: INFO: (3) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 5.69722ms)
May 21 06:55:39.154: INFO: (3) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.342122ms)
May 21 06:55:39.155: INFO: (3) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 9.525226ms)
May 21 06:55:39.155: INFO: (3) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 9.581227ms)
May 21 06:55:39.155: INFO: (3) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.80496ms)
May 21 06:55:39.155: INFO: (3) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 9.859027ms)
May 21 06:55:39.155: INFO: (3) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 9.829622ms)
May 21 06:55:39.155: INFO: (3) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.978612ms)
May 21 06:55:39.156: INFO: (3) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 10.710016ms)
May 21 06:55:39.156: INFO: (3) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 10.612055ms)
May 21 06:55:39.156: INFO: (3) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 11.218455ms)
May 21 06:55:39.156: INFO: (3) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 11.255883ms)
May 21 06:55:39.156: INFO: (3) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 11.187539ms)
May 21 06:55:39.156: INFO: (3) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 11.488793ms)
May 21 06:55:39.159: INFO: (4) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 2.15696ms)
May 21 06:55:39.161: INFO: (4) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 4.066352ms)
May 21 06:55:39.161: INFO: (4) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 4.320136ms)
May 21 06:55:39.163: INFO: (4) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 5.835669ms)
May 21 06:55:39.164: INFO: (4) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 7.284368ms)
May 21 06:55:39.165: INFO: (4) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 8.425141ms)
May 21 06:55:39.166: INFO: (4) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 8.637414ms)
May 21 06:55:39.166: INFO: (4) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 9.018398ms)
May 21 06:55:39.166: INFO: (4) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 8.841068ms)
May 21 06:55:39.166: INFO: (4) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.059332ms)
May 21 06:55:39.166: INFO: (4) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 8.775378ms)
May 21 06:55:39.166: INFO: (4) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 9.177803ms)
May 21 06:55:39.166: INFO: (4) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 9.097576ms)
May 21 06:55:39.166: INFO: (4) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.658497ms)
May 21 06:55:39.166: INFO: (4) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 9.338544ms)
May 21 06:55:39.167: INFO: (4) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 10.05233ms)
May 21 06:55:39.176: INFO: (5) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.093099ms)
May 21 06:55:39.177: INFO: (5) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.242549ms)
May 21 06:55:39.177: INFO: (5) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 8.941163ms)
May 21 06:55:39.177: INFO: (5) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 9.547441ms)
May 21 06:55:39.178: INFO: (5) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 9.795025ms)
May 21 06:55:39.179: INFO: (5) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 10.899083ms)
May 21 06:55:39.179: INFO: (5) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 11.499495ms)
May 21 06:55:39.181: INFO: (5) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 13.711342ms)
May 21 06:55:39.181: INFO: (5) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 13.897179ms)
May 21 06:55:39.182: INFO: (5) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 14.372053ms)
May 21 06:55:39.182: INFO: (5) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 14.75187ms)
May 21 06:55:39.182: INFO: (5) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 14.577218ms)
May 21 06:55:39.182: INFO: (5) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 14.36507ms)
May 21 06:55:39.182: INFO: (5) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 14.327691ms)
May 21 06:55:39.182: INFO: (5) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 14.543797ms)
May 21 06:55:39.182: INFO: (5) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 14.583922ms)
May 21 06:55:39.189: INFO: (6) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 6.714308ms)
May 21 06:55:39.191: INFO: (6) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 8.453793ms)
May 21 06:55:39.191: INFO: (6) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 8.485596ms)
May 21 06:55:39.191: INFO: (6) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 8.620532ms)
May 21 06:55:39.191: INFO: (6) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 8.673166ms)
May 21 06:55:39.191: INFO: (6) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 8.785277ms)
May 21 06:55:39.192: INFO: (6) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 8.813828ms)
May 21 06:55:39.192: INFO: (6) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 8.94296ms)
May 21 06:55:39.192: INFO: (6) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 9.191746ms)
May 21 06:55:39.192: INFO: (6) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.2328ms)
May 21 06:55:39.192: INFO: (6) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 9.504053ms)
May 21 06:55:39.192: INFO: (6) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 9.939711ms)
May 21 06:55:39.193: INFO: (6) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 10.076038ms)
May 21 06:55:39.193: INFO: (6) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 10.097429ms)
May 21 06:55:39.193: INFO: (6) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 10.312266ms)
May 21 06:55:39.193: INFO: (6) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 10.605518ms)
May 21 06:55:39.198: INFO: (7) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 4.79469ms)
May 21 06:55:39.203: INFO: (7) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.210243ms)
May 21 06:55:39.203: INFO: (7) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 9.289354ms)
May 21 06:55:39.203: INFO: (7) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 9.429373ms)
May 21 06:55:39.203: INFO: (7) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.788489ms)
May 21 06:55:39.203: INFO: (7) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.70114ms)
May 21 06:55:39.209: INFO: (7) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 15.090411ms)
May 21 06:55:39.209: INFO: (7) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 15.410373ms)
May 21 06:55:39.210: INFO: (7) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 16.119691ms)
May 21 06:55:39.210: INFO: (7) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 16.730368ms)
May 21 06:55:39.214: INFO: (7) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 20.450639ms)
May 21 06:55:39.216: INFO: (7) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 22.110117ms)
May 21 06:55:39.216: INFO: (7) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 22.126018ms)
May 21 06:55:39.216: INFO: (7) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 22.269536ms)
May 21 06:55:39.216: INFO: (7) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 22.10229ms)
May 21 06:55:39.216: INFO: (7) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 22.071531ms)
May 21 06:55:39.220: INFO: (8) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 4.07146ms)
May 21 06:55:39.228: INFO: (8) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 10.873568ms)
May 21 06:55:39.228: INFO: (8) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 11.348201ms)
May 21 06:55:39.228: INFO: (8) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 11.952857ms)
May 21 06:55:39.228: INFO: (8) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 11.226819ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 14.388225ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 13.634504ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 13.952742ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 14.164046ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 14.290022ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 14.836877ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 14.581847ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 14.150981ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 14.38616ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 14.76874ms)
May 21 06:55:39.231: INFO: (8) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 14.707426ms)
May 21 06:55:39.241: INFO: (9) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.479475ms)
May 21 06:55:39.241: INFO: (9) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 9.965312ms)
May 21 06:55:39.241: INFO: (9) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.790363ms)
May 21 06:55:39.242: INFO: (9) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.815631ms)
May 21 06:55:39.242: INFO: (9) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 10.044346ms)
May 21 06:55:39.242: INFO: (9) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 10.234299ms)
May 21 06:55:39.242: INFO: (9) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 10.339523ms)
May 21 06:55:39.242: INFO: (9) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 10.566057ms)
May 21 06:55:39.242: INFO: (9) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 10.506083ms)
May 21 06:55:39.242: INFO: (9) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 10.781087ms)
May 21 06:55:39.242: INFO: (9) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 10.55231ms)
May 21 06:55:39.242: INFO: (9) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 9.853839ms)
May 21 06:55:39.242: INFO: (9) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 10.577752ms)
May 21 06:55:39.243: INFO: (9) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 11.152919ms)
May 21 06:55:39.243: INFO: (9) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 11.291118ms)
May 21 06:55:39.243: INFO: (9) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 11.524818ms)
May 21 06:55:39.247: INFO: (10) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 3.658873ms)
May 21 06:55:39.251: INFO: (10) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 7.00836ms)
May 21 06:55:39.251: INFO: (10) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 7.532083ms)
May 21 06:55:39.251: INFO: (10) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 7.300018ms)
May 21 06:55:39.251: INFO: (10) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 7.48818ms)
May 21 06:55:39.251: INFO: (10) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 7.683175ms)
May 21 06:55:39.251: INFO: (10) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 7.763072ms)
May 21 06:55:39.251: INFO: (10) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 7.15039ms)
May 21 06:55:39.251: INFO: (10) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 7.755092ms)
May 21 06:55:39.251: INFO: (10) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 8.067559ms)
May 21 06:55:39.252: INFO: (10) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 7.884283ms)
May 21 06:55:39.254: INFO: (10) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 10.469101ms)
May 21 06:55:39.254: INFO: (10) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 10.516843ms)
May 21 06:55:39.255: INFO: (10) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 11.196744ms)
May 21 06:55:39.256: INFO: (10) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 11.999504ms)
May 21 06:55:39.256: INFO: (10) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 12.26597ms)
May 21 06:55:39.261: INFO: (11) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 5.287691ms)
May 21 06:55:39.262: INFO: (11) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 5.562217ms)
May 21 06:55:39.262: INFO: (11) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 5.522252ms)
May 21 06:55:39.262: INFO: (11) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 5.513298ms)
May 21 06:55:39.267: INFO: (11) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 10.945287ms)
May 21 06:55:39.267: INFO: (11) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 10.68325ms)
May 21 06:55:39.268: INFO: (11) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 11.179291ms)
May 21 06:55:39.268: INFO: (11) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 11.514892ms)
May 21 06:55:39.268: INFO: (11) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 11.868431ms)
May 21 06:55:39.269: INFO: (11) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 12.208239ms)
May 21 06:55:39.269: INFO: (11) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 12.327818ms)
May 21 06:55:39.269: INFO: (11) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 12.337344ms)
May 21 06:55:39.269: INFO: (11) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 12.658706ms)
May 21 06:55:39.269: INFO: (11) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 12.867748ms)
May 21 06:55:39.270: INFO: (11) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 13.233182ms)
May 21 06:55:39.270: INFO: (11) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 13.177665ms)
May 21 06:55:39.275: INFO: (12) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 4.750413ms)
May 21 06:55:39.279: INFO: (12) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 8.530358ms)
May 21 06:55:39.279: INFO: (12) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 8.629745ms)
May 21 06:55:39.279: INFO: (12) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 8.944777ms)
May 21 06:55:39.279: INFO: (12) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.09034ms)
May 21 06:55:39.279: INFO: (12) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 9.441908ms)
May 21 06:55:39.280: INFO: (12) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 9.743177ms)
May 21 06:55:39.280: INFO: (12) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 9.387469ms)
May 21 06:55:39.280: INFO: (12) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.521592ms)
May 21 06:55:39.280: INFO: (12) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 9.691794ms)
May 21 06:55:39.280: INFO: (12) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 10.088881ms)
May 21 06:55:39.280: INFO: (12) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 9.932023ms)
May 21 06:55:39.280: INFO: (12) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 10.280745ms)
May 21 06:55:39.281: INFO: (12) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 10.115534ms)
May 21 06:55:39.281: INFO: (12) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 10.894101ms)
May 21 06:55:39.282: INFO: (12) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 11.708626ms)
May 21 06:55:39.286: INFO: (13) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 4.334061ms)
May 21 06:55:39.290: INFO: (13) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 8.101535ms)
May 21 06:55:39.290: INFO: (13) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 8.154158ms)
May 21 06:55:39.290: INFO: (13) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 7.922826ms)
May 21 06:55:39.290: INFO: (13) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 8.07085ms)
May 21 06:55:39.290: INFO: (13) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 8.30249ms)
May 21 06:55:39.291: INFO: (13) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 8.60723ms)
May 21 06:55:39.292: INFO: (13) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 9.702297ms)
May 21 06:55:39.292: INFO: (13) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 9.236712ms)
May 21 06:55:39.292: INFO: (13) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 10.32087ms)
May 21 06:55:39.292: INFO: (13) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 9.891229ms)
May 21 06:55:39.292: INFO: (13) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 10.111134ms)
May 21 06:55:39.293: INFO: (13) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 10.451247ms)
May 21 06:55:39.293: INFO: (13) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 10.56128ms)
May 21 06:55:39.293: INFO: (13) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 10.532112ms)
May 21 06:55:39.293: INFO: (13) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 10.859556ms)
May 21 06:55:39.296: INFO: (14) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 3.203224ms)
May 21 06:55:39.296: INFO: (14) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 3.618365ms)
May 21 06:55:39.302: INFO: (14) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 8.317752ms)
May 21 06:55:39.302: INFO: (14) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 8.736426ms)
May 21 06:55:39.302: INFO: (14) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 8.720182ms)
May 21 06:55:39.302: INFO: (14) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 8.814592ms)
May 21 06:55:39.302: INFO: (14) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.314314ms)
May 21 06:55:39.302: INFO: (14) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.168286ms)
May 21 06:55:39.303: INFO: (14) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 9.542144ms)
May 21 06:55:39.303: INFO: (14) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 9.366832ms)
May 21 06:55:39.303: INFO: (14) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 9.525409ms)
May 21 06:55:39.303: INFO: (14) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 9.755331ms)
May 21 06:55:39.303: INFO: (14) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 10.216773ms)
May 21 06:55:39.303: INFO: (14) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 9.815337ms)
May 21 06:55:39.304: INFO: (14) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 10.284318ms)
May 21 06:55:39.304: INFO: (14) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 10.505956ms)
May 21 06:55:39.312: INFO: (15) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 7.051252ms)
May 21 06:55:39.312: INFO: (15) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 7.534348ms)
May 21 06:55:39.312: INFO: (15) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 7.445529ms)
May 21 06:55:39.312: INFO: (15) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 7.527747ms)
May 21 06:55:39.312: INFO: (15) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 7.820138ms)
May 21 06:55:39.312: INFO: (15) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 7.359797ms)
May 21 06:55:39.312: INFO: (15) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 8.101179ms)
May 21 06:55:39.312: INFO: (15) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 8.04733ms)
May 21 06:55:39.313: INFO: (15) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 9.126867ms)
May 21 06:55:39.313: INFO: (15) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 8.674657ms)
May 21 06:55:39.313: INFO: (15) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 8.922156ms)
May 21 06:55:39.315: INFO: (15) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 10.094232ms)
May 21 06:55:39.315: INFO: (15) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 10.400573ms)
May 21 06:55:39.315: INFO: (15) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 10.313777ms)
May 21 06:55:39.315: INFO: (15) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 10.394819ms)
May 21 06:55:39.315: INFO: (15) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 11.093766ms)
May 21 06:55:39.321: INFO: (16) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 5.814651ms)
May 21 06:55:39.321: INFO: (16) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 6.039148ms)
May 21 06:55:39.321: INFO: (16) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 5.941967ms)
May 21 06:55:39.321: INFO: (16) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 6.270382ms)
May 21 06:55:39.321: INFO: (16) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 5.995524ms)
May 21 06:55:39.322: INFO: (16) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 7.018566ms)
May 21 06:55:39.323: INFO: (16) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 7.050499ms)
May 21 06:55:39.323: INFO: (16) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 8.103278ms)
May 21 06:55:39.324: INFO: (16) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 8.162066ms)
May 21 06:55:39.324: INFO: (16) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 8.786067ms)
May 21 06:55:39.325: INFO: (16) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 9.496508ms)
May 21 06:55:39.326: INFO: (16) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 10.643746ms)
May 21 06:55:39.326: INFO: (16) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 10.633011ms)
May 21 06:55:39.326: INFO: (16) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 10.887721ms)
May 21 06:55:39.326: INFO: (16) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 10.975383ms)
May 21 06:55:39.326: INFO: (16) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 10.882786ms)
May 21 06:55:39.330: INFO: (17) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 4.094291ms)
May 21 06:55:39.331: INFO: (17) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 3.994473ms)
May 21 06:55:39.333: INFO: (17) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 5.813508ms)
May 21 06:55:39.333: INFO: (17) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 6.825361ms)
May 21 06:55:39.334: INFO: (17) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 7.550395ms)
May 21 06:55:39.336: INFO: (17) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.380981ms)
May 21 06:55:39.337: INFO: (17) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.539921ms)
May 21 06:55:39.337: INFO: (17) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 9.67572ms)
May 21 06:55:39.337: INFO: (17) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 9.419484ms)
May 21 06:55:39.337: INFO: (17) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 9.865419ms)
May 21 06:55:39.337: INFO: (17) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 9.47732ms)
May 21 06:55:39.337: INFO: (17) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 9.675689ms)
May 21 06:55:39.337: INFO: (17) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.791158ms)
May 21 06:55:39.337: INFO: (17) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 9.847366ms)
May 21 06:55:39.337: INFO: (17) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 10.374253ms)
May 21 06:55:39.337: INFO: (17) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 10.415169ms)
May 21 06:55:39.345: INFO: (18) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 7.802134ms)
May 21 06:55:39.346: INFO: (18) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 7.798955ms)
May 21 06:55:39.346: INFO: (18) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 8.153581ms)
May 21 06:55:39.346: INFO: (18) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 8.3969ms)
May 21 06:55:39.346: INFO: (18) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 7.934078ms)
May 21 06:55:39.347: INFO: (18) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 9.725692ms)
May 21 06:55:39.347: INFO: (18) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 9.958952ms)
May 21 06:55:39.347: INFO: (18) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.811802ms)
May 21 06:55:39.348: INFO: (18) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 9.818872ms)
May 21 06:55:39.348: INFO: (18) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 10.465737ms)
May 21 06:55:39.348: INFO: (18) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 10.695606ms)
May 21 06:55:39.349: INFO: (18) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 11.06549ms)
May 21 06:55:39.349: INFO: (18) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 11.018385ms)
May 21 06:55:39.349: INFO: (18) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 11.242234ms)
May 21 06:55:39.349: INFO: (18) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 11.294594ms)
May 21 06:55:39.349: INFO: (18) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 11.271688ms)
May 21 06:55:39.363: INFO: (19) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:462/proxy/: tls qux (200; 13.519596ms)
May 21 06:55:39.363: INFO: (19) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:160/proxy/: foo (200; 13.610544ms)
May 21 06:55:39.363: INFO: (19) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:1080/proxy/rewriteme">test</... (200; 13.704379ms)
May 21 06:55:39.363: INFO: (19) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:162/proxy/: bar (200; 13.821572ms)
May 21 06:55:39.363: INFO: (19) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9/proxy/rewriteme">test</a> (200; 13.946115ms)
May 21 06:55:39.363: INFO: (19) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname2/proxy/: tls qux (200; 13.882312ms)
May 21 06:55:39.363: INFO: (19) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname1/proxy/: foo (200; 14.180872ms)
May 21 06:55:39.363: INFO: (19) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:443/proxy/tlsrewriteme... (200; 13.942525ms)
May 21 06:55:39.363: INFO: (19) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/: <a href="/api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:1080/proxy/rewriteme">t... (200; 14.107135ms)
May 21 06:55:39.363: INFO: (19) /api/v1/namespaces/proxy-308/pods/https:proxy-service-xq295-p2jf9:460/proxy/: tls baz (200; 14.112754ms)
May 21 06:55:39.364: INFO: (19) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname2/proxy/: bar (200; 14.491895ms)
May 21 06:55:39.364: INFO: (19) /api/v1/namespaces/proxy-308/pods/proxy-service-xq295-p2jf9:160/proxy/: foo (200; 14.461614ms)
May 21 06:55:39.364: INFO: (19) /api/v1/namespaces/proxy-308/services/proxy-service-xq295:portname2/proxy/: bar (200; 14.519519ms)
May 21 06:55:39.364: INFO: (19) /api/v1/namespaces/proxy-308/pods/http:proxy-service-xq295-p2jf9:162/proxy/: bar (200; 14.577209ms)
May 21 06:55:39.364: INFO: (19) /api/v1/namespaces/proxy-308/services/https:proxy-service-xq295:tlsportname1/proxy/: tls baz (200; 14.655138ms)
May 21 06:55:39.364: INFO: (19) /api/v1/namespaces/proxy-308/services/http:proxy-service-xq295:portname1/proxy/: foo (200; 14.841385ms)
STEP: deleting ReplicationController proxy-service-xq295 in namespace proxy-308, will wait for the garbage collector to delete the pods
May 21 06:55:39.421: INFO: Deleting ReplicationController proxy-service-xq295 took: 5.323202ms
May 21 06:55:40.021: INFO: Terminating ReplicationController proxy-service-xq295 pods took: 600.160337ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:55:52.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-308" for this suite.

• [SLOW TEST:17.426 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":281,"skipped":4819,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:55:52.427: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
May 21 06:55:52.462: INFO: Waiting up to 5m0s for pod "client-containers-6598dfed-6565-4d2f-8c1e-b9d8ad0b19ba" in namespace "containers-9055" to be "Succeeded or Failed"
May 21 06:55:52.463: INFO: Pod "client-containers-6598dfed-6565-4d2f-8c1e-b9d8ad0b19ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.356878ms
May 21 06:55:54.466: INFO: Pod "client-containers-6598dfed-6565-4d2f-8c1e-b9d8ad0b19ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00454588s
STEP: Saw pod success
May 21 06:55:54.466: INFO: Pod "client-containers-6598dfed-6565-4d2f-8c1e-b9d8ad0b19ba" satisfied condition "Succeeded or Failed"
May 21 06:55:54.468: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod client-containers-6598dfed-6565-4d2f-8c1e-b9d8ad0b19ba container agnhost-container: <nil>
STEP: delete the pod
May 21 06:55:54.487: INFO: Waiting for pod client-containers-6598dfed-6565-4d2f-8c1e-b9d8ad0b19ba to disappear
May 21 06:55:54.496: INFO: Pod client-containers-6598dfed-6565-4d2f-8c1e-b9d8ad0b19ba no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:55:54.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9055" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":282,"skipped":4824,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:55:54.503: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 21 06:55:54.537: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9171  6f442929-a1e3-44a6-9cae-def2c7f38d34 31680 0 2021-05-21 06:55:54 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-05-21 06:55:54 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msqc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msqc6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msqc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 06:55:54.557: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 21 06:55:56.560: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 21 06:55:56.560: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9171 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:55:56.560: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Verifying customized DNS server is configured on pod...
May 21 06:55:56.662: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9171 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 21 06:55:56.662: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
May 21 06:55:56.763: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:55:56.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9171" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":283,"skipped":4824,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:55:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:56:07.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3051" for this suite.

• [SLOW TEST:11.071 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":284,"skipped":4828,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:56:07.854: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 21 06:56:07.894: INFO: Waiting up to 5m0s for pod "pod-a2d65b78-7fe8-4602-9baf-baf90231905e" in namespace "emptydir-3711" to be "Succeeded or Failed"
May 21 06:56:07.901: INFO: Pod "pod-a2d65b78-7fe8-4602-9baf-baf90231905e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.816485ms
May 21 06:56:09.903: INFO: Pod "pod-a2d65b78-7fe8-4602-9baf-baf90231905e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009572742s
STEP: Saw pod success
May 21 06:56:09.904: INFO: Pod "pod-a2d65b78-7fe8-4602-9baf-baf90231905e" satisfied condition "Succeeded or Failed"
May 21 06:56:09.905: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-a2d65b78-7fe8-4602-9baf-baf90231905e container test-container: <nil>
STEP: delete the pod
May 21 06:56:09.916: INFO: Waiting for pod pod-a2d65b78-7fe8-4602-9baf-baf90231905e to disappear
May 21 06:56:09.925: INFO: Pod pod-a2d65b78-7fe8-4602-9baf-baf90231905e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:56:09.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3711" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:56:09.934: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
May 21 06:56:09.974: INFO: Waiting up to 5m0s for pod "test-pod-01763679-642c-44c6-844d-d747bad89343" in namespace "svcaccounts-6592" to be "Succeeded or Failed"
May 21 06:56:09.983: INFO: Pod "test-pod-01763679-642c-44c6-844d-d747bad89343": Phase="Pending", Reason="", readiness=false. Elapsed: 9.656169ms
May 21 06:56:11.987: INFO: Pod "test-pod-01763679-642c-44c6-844d-d747bad89343": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013058137s
STEP: Saw pod success
May 21 06:56:11.987: INFO: Pod "test-pod-01763679-642c-44c6-844d-d747bad89343" satisfied condition "Succeeded or Failed"
May 21 06:56:11.989: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod test-pod-01763679-642c-44c6-844d-d747bad89343 container agnhost-container: <nil>
STEP: delete the pod
May 21 06:56:12.003: INFO: Waiting for pod test-pod-01763679-642c-44c6-844d-d747bad89343 to disappear
May 21 06:56:12.010: INFO: Pod test-pod-01763679-642c-44c6-844d-d747bad89343 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:56:12.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6592" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":286,"skipped":4917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:56:12.018: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 21 06:56:12.066: INFO: Waiting up to 5m0s for pod "downward-api-658ec792-ad5a-467c-aef2-b3fa36bd97ec" in namespace "downward-api-9146" to be "Succeeded or Failed"
May 21 06:56:12.070: INFO: Pod "downward-api-658ec792-ad5a-467c-aef2-b3fa36bd97ec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.962181ms
May 21 06:56:14.073: INFO: Pod "downward-api-658ec792-ad5a-467c-aef2-b3fa36bd97ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006925034s
STEP: Saw pod success
May 21 06:56:14.073: INFO: Pod "downward-api-658ec792-ad5a-467c-aef2-b3fa36bd97ec" satisfied condition "Succeeded or Failed"
May 21 06:56:14.076: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downward-api-658ec792-ad5a-467c-aef2-b3fa36bd97ec container dapi-container: <nil>
STEP: delete the pod
May 21 06:56:14.090: INFO: Waiting for pod downward-api-658ec792-ad5a-467c-aef2-b3fa36bd97ec to disappear
May 21 06:56:14.096: INFO: Pod downward-api-658ec792-ad5a-467c-aef2-b3fa36bd97ec no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:56:14.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9146" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":4961,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:56:14.103: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 21 06:56:14.135: INFO: Waiting up to 5m0s for pod "downward-api-5264c10c-3e97-4d2f-946b-682e82d35a20" in namespace "downward-api-3630" to be "Succeeded or Failed"
May 21 06:56:14.136: INFO: Pod "downward-api-5264c10c-3e97-4d2f-946b-682e82d35a20": Phase="Pending", Reason="", readiness=false. Elapsed: 1.689781ms
May 21 06:56:16.140: INFO: Pod "downward-api-5264c10c-3e97-4d2f-946b-682e82d35a20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005380961s
STEP: Saw pod success
May 21 06:56:16.140: INFO: Pod "downward-api-5264c10c-3e97-4d2f-946b-682e82d35a20" satisfied condition "Succeeded or Failed"
May 21 06:56:16.142: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod downward-api-5264c10c-3e97-4d2f-946b-682e82d35a20 container dapi-container: <nil>
STEP: delete the pod
May 21 06:56:16.157: INFO: Waiting for pod downward-api-5264c10c-3e97-4d2f-946b-682e82d35a20 to disappear
May 21 06:56:16.163: INFO: Pod downward-api-5264c10c-3e97-4d2f-946b-682e82d35a20 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:56:16.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3630" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":288,"skipped":4973,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:56:16.169: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-ea289404-0fa9-4818-a108-8d1200ad283b
STEP: Creating secret with name s-test-opt-upd-001f7201-1783-4afd-a525-a34053b8b7f3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ea289404-0fa9-4818-a108-8d1200ad283b
STEP: Updating secret s-test-opt-upd-001f7201-1783-4afd-a525-a34053b8b7f3
STEP: Creating secret with name s-test-opt-create-e387989a-85fa-4808-990e-1baa70e2bc8f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:56:20.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6039" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":289,"skipped":4977,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:56:20.283: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4914
STEP: creating service affinity-clusterip in namespace services-4914
STEP: creating replication controller affinity-clusterip in namespace services-4914
I0521 06:56:20.332241      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4914, replica count: 3
I0521 06:56:23.383605      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:56:23.390: INFO: Creating new exec pod
May 21 06:56:26.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-4914 exec execpod-affinitycj9kq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
May 21 06:56:26.574: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 21 06:56:26.574: INFO: stdout: ""
May 21 06:56:26.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-4914 exec execpod-affinitycj9kq -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.209 80'
May 21 06:56:26.731: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.209 80\nConnection to 10.100.200.209 80 port [tcp/http] succeeded!\n"
May 21 06:56:26.731: INFO: stdout: ""
May 21 06:56:26.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-4914 exec execpod-affinitycj9kq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.200.209:80/ ; done'
May 21 06:56:26.970: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.209:80/\n"
May 21 06:56:26.970: INFO: stdout: "\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc\naffinity-clusterip-qtxvc"
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Received response from host: affinity-clusterip-qtxvc
May 21 06:56:26.970: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4914, will wait for the garbage collector to delete the pods
May 21 06:56:27.126: INFO: Deleting ReplicationController affinity-clusterip took: 9.484457ms
May 21 06:56:27.726: INFO: Terminating ReplicationController affinity-clusterip pods took: 600.222715ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:56:42.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4914" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:22.167 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":290,"skipped":4991,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:56:42.452: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 06:56:42.843: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 06:56:44.850: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757177002, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757177002, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757177002, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757177002, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 06:56:47.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:56:47.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1659" for this suite.
STEP: Destroying namespace "webhook-1659-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.520 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":291,"skipped":4993,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:56:47.972: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 21 06:56:48.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 create -f -'
May 21 06:56:48.259: INFO: stderr: ""
May 21 06:56:48.259: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 06:56:48.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 06:56:48.330: INFO: stderr: ""
May 21 06:56:48.330: INFO: stdout: "update-demo-nautilus-gs2hx update-demo-nautilus-gwzf7 "
May 21 06:56:48.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-gs2hx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 06:56:48.390: INFO: stderr: ""
May 21 06:56:48.390: INFO: stdout: ""
May 21 06:56:48.390: INFO: update-demo-nautilus-gs2hx is created but not running
May 21 06:56:53.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 06:56:53.465: INFO: stderr: ""
May 21 06:56:53.465: INFO: stdout: "update-demo-nautilus-gs2hx update-demo-nautilus-gwzf7 "
May 21 06:56:53.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-gs2hx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 06:56:53.527: INFO: stderr: ""
May 21 06:56:53.528: INFO: stdout: "true"
May 21 06:56:53.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-gs2hx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 06:56:53.590: INFO: stderr: ""
May 21 06:56:53.590: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 06:56:53.590: INFO: validating pod update-demo-nautilus-gs2hx
May 21 06:56:53.593: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 06:56:53.593: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 06:56:53.593: INFO: update-demo-nautilus-gs2hx is verified up and running
May 21 06:56:53.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-gwzf7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 06:56:53.673: INFO: stderr: ""
May 21 06:56:53.674: INFO: stdout: "true"
May 21 06:56:53.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-gwzf7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 06:56:53.735: INFO: stderr: ""
May 21 06:56:53.735: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 06:56:53.735: INFO: validating pod update-demo-nautilus-gwzf7
May 21 06:56:53.738: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 06:56:53.738: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 06:56:53.738: INFO: update-demo-nautilus-gwzf7 is verified up and running
STEP: scaling down the replication controller
May 21 06:56:53.740: INFO: scanned /root for discovery docs: <nil>
May 21 06:56:53.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 21 06:56:54.840: INFO: stderr: ""
May 21 06:56:54.840: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 06:56:54.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 06:56:54.907: INFO: stderr: ""
May 21 06:56:54.907: INFO: stdout: "update-demo-nautilus-gs2hx update-demo-nautilus-gwzf7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 21 06:56:59.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 06:56:59.978: INFO: stderr: ""
May 21 06:56:59.978: INFO: stdout: "update-demo-nautilus-gs2hx update-demo-nautilus-gwzf7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 21 06:57:04.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 06:57:05.050: INFO: stderr: ""
May 21 06:57:05.050: INFO: stdout: "update-demo-nautilus-gs2hx "
May 21 06:57:05.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-gs2hx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 06:57:05.121: INFO: stderr: ""
May 21 06:57:05.121: INFO: stdout: "true"
May 21 06:57:05.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-gs2hx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 06:57:05.184: INFO: stderr: ""
May 21 06:57:05.184: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 06:57:05.184: INFO: validating pod update-demo-nautilus-gs2hx
May 21 06:57:05.187: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 06:57:05.187: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 06:57:05.187: INFO: update-demo-nautilus-gs2hx is verified up and running
STEP: scaling up the replication controller
May 21 06:57:05.189: INFO: scanned /root for discovery docs: <nil>
May 21 06:57:05.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 21 06:57:06.275: INFO: stderr: ""
May 21 06:57:06.275: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 06:57:06.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 06:57:06.347: INFO: stderr: ""
May 21 06:57:06.347: INFO: stdout: "update-demo-nautilus-gs2hx update-demo-nautilus-s5rhm "
May 21 06:57:06.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-gs2hx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 06:57:06.413: INFO: stderr: ""
May 21 06:57:06.413: INFO: stdout: "true"
May 21 06:57:06.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-gs2hx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 06:57:06.473: INFO: stderr: ""
May 21 06:57:06.473: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 06:57:06.473: INFO: validating pod update-demo-nautilus-gs2hx
May 21 06:57:06.475: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 06:57:06.475: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 06:57:06.475: INFO: update-demo-nautilus-gs2hx is verified up and running
May 21 06:57:06.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-s5rhm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 06:57:06.549: INFO: stderr: ""
May 21 06:57:06.549: INFO: stdout: "true"
May 21 06:57:06.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods update-demo-nautilus-s5rhm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 06:57:06.615: INFO: stderr: ""
May 21 06:57:06.615: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 06:57:06.615: INFO: validating pod update-demo-nautilus-s5rhm
May 21 06:57:06.618: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 06:57:06.618: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 06:57:06.618: INFO: update-demo-nautilus-s5rhm is verified up and running
STEP: using delete to clean up resources
May 21 06:57:06.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 delete --grace-period=0 --force -f -'
May 21 06:57:06.696: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 06:57:06.696: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 21 06:57:06.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get rc,svc -l name=update-demo --no-headers'
May 21 06:57:06.763: INFO: stderr: "No resources found in kubectl-8339 namespace.\n"
May 21 06:57:06.763: INFO: stdout: ""
May 21 06:57:06.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-8339 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 06:57:06.832: INFO: stderr: ""
May 21 06:57:06.832: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:57:06.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8339" for this suite.

• [SLOW TEST:18.866 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":292,"skipped":5005,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:57:06.838: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9809
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-9809
May 21 06:57:06.904: INFO: Found 0 stateful pods, waiting for 1
May 21 06:57:16.908: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 06:57:16.922: INFO: Deleting all statefulset in ns statefulset-9809
May 21 06:57:16.931: INFO: Scaling statefulset ss to 0
May 21 06:57:46.974: INFO: Waiting for statefulset status.replicas updated to 0
May 21 06:57:46.976: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:57:46.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9809" for this suite.

• [SLOW TEST:40.165 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":293,"skipped":5019,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:57:47.003: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-6eb2813b-ad79-4d92-b0ec-4259fe75c83c
STEP: Creating secret with name secret-projected-all-test-volume-d7472563-bc2a-431f-92f9-bb20c6c25b85
STEP: Creating a pod to test Check all projections for projected volume plugin
May 21 06:57:47.272: INFO: Waiting up to 5m0s for pod "projected-volume-de315ff6-e5a4-47da-901f-7ca6d912ce4d" in namespace "projected-3674" to be "Succeeded or Failed"
May 21 06:57:47.328: INFO: Pod "projected-volume-de315ff6-e5a4-47da-901f-7ca6d912ce4d": Phase="Pending", Reason="", readiness=false. Elapsed: 55.983341ms
May 21 06:57:49.332: INFO: Pod "projected-volume-de315ff6-e5a4-47da-901f-7ca6d912ce4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.059368449s
STEP: Saw pod success
May 21 06:57:49.332: INFO: Pod "projected-volume-de315ff6-e5a4-47da-901f-7ca6d912ce4d" satisfied condition "Succeeded or Failed"
May 21 06:57:49.333: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod projected-volume-de315ff6-e5a4-47da-901f-7ca6d912ce4d container projected-all-volume-test: <nil>
STEP: delete the pod
May 21 06:57:49.371: INFO: Waiting for pod projected-volume-de315ff6-e5a4-47da-901f-7ca6d912ce4d to disappear
May 21 06:57:49.380: INFO: Pod projected-volume-de315ff6-e5a4-47da-901f-7ca6d912ce4d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:57:49.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3674" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":294,"skipped":5032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:57:49.386: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 21 06:57:49.452: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 06:57:49.462: INFO: Waiting for terminating namespaces to be deleted...
May 21 06:57:49.464: INFO: 
Logging pods the apiserver thinks is on node 16c19df1-13dd-4e26-99a1-a192204dc796 before test
May 21 06:57:49.472: INFO: fluent-bit-plrsb from pks-system started at 2021-05-21 06:48:05 +0000 UTC (2 container statuses recorded)
May 21 06:57:49.472: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 06:57:49.472: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:57:49.472: INFO: node-exporter-p6twh from pks-system started at 2021-05-21 06:48:02 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.472: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 06:57:49.472: INFO: telegraf-j4f48 from pks-system started at 2021-05-21 06:48:02 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.472: INFO: 	Container telegraf ready: true, restart count 0
May 21 06:57:49.472: INFO: sonobuoy from sonobuoy started at 2021-05-21 05:36:00 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.472: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 06:57:49.472: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-b9nwr from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:57:49.472: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 21 06:57:49.472: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 06:57:49.472: INFO: 
Logging pods the apiserver thinks is on node 53f495cf-0382-4128-b4b3-0979b69a209b before test
May 21 06:57:49.480: INFO: coredns-6d5447c5f4-fv5hd from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container coredns ready: true, restart count 0
May 21 06:57:49.480: INFO: coredns-6d5447c5f4-jms82 from kube-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container coredns ready: true, restart count 0
May 21 06:57:49.480: INFO: fluent-bit-vsrb2 from pks-system started at 2021-05-21 06:07:53 +0000 UTC (2 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 06:57:49.480: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:57:49.480: INFO: metric-controller-5754b4489d-c246d from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container metric-controller ready: true, restart count 0
May 21 06:57:49.480: INFO: node-exporter-fmcv8 from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 06:57:49.480: INFO: observability-manager-7f5c97895-xsj2f from pks-system started at 2021-05-21 04:30:14 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container observability-manager ready: true, restart count 0
May 21 06:57:49.480: INFO: telegraf-hlmgc from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container telegraf ready: true, restart count 0
May 21 06:57:49.480: INFO: telemetry-agent-8f56d9865-l6ntw from pks-system started at 2021-05-21 04:35:59 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
May 21 06:57:49.480: INFO: validator-54f6bbd67b-snnrv from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container validator ready: true, restart count 0
May 21 06:57:49.480: INFO: sonobuoy-e2e-job-a4c84655dab54066 from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container e2e ready: true, restart count 0
May 21 06:57:49.480: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 06:57:49.480: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-g25r8 from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:57:49.480: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 21 06:57:49.480: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 06:57:49.480: INFO: 
Logging pods the apiserver thinks is on node 973ef262-7e5e-4fd5-8bf3-6139f838e19b before test
May 21 06:57:49.487: INFO: coredns-6d5447c5f4-7dcmd from kube-system started at 2021-05-21 04:30:08 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.487: INFO: 	Container coredns ready: true, restart count 0
May 21 06:57:49.487: INFO: metrics-server-7d476fdfbd-phhm6 from kube-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.487: INFO: 	Container metrics-server ready: true, restart count 0
May 21 06:57:49.487: INFO: event-controller-55dd5b5777-4mn7d from pks-system started at 2021-05-21 04:30:17 +0000 UTC (2 container statuses recorded)
May 21 06:57:49.487: INFO: 	Container event-controller ready: true, restart count 0
May 21 06:57:49.487: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:57:49.487: INFO: fluent-bit-hnqwh from pks-system started at 2021-05-21 06:07:50 +0000 UTC (2 container statuses recorded)
May 21 06:57:49.487: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 06:57:49.487: INFO: 	Container ghostunnel ready: true, restart count 0
May 21 06:57:49.487: INFO: node-exporter-6k9jm from pks-system started at 2021-05-21 04:30:17 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.487: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May 21 06:57:49.487: INFO: sink-controller-5bf464ff96-4nbts from pks-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.487: INFO: 	Container sink-controller ready: true, restart count 0
May 21 06:57:49.487: INFO: telegraf-jqzbt from pks-system started at 2021-05-21 04:30:18 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.487: INFO: 	Container telegraf ready: true, restart count 0
May 21 06:57:49.487: INFO: wavefront-collector-5795f87754-596hz from pks-system started at 2021-05-21 06:07:34 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.487: INFO: 	Container wavefront-collector ready: true, restart count 0
May 21 06:57:49.487: INFO: wavefront-proxy-6446ff4bd7-lw9wt from pks-system started at 2021-05-21 04:32:41 +0000 UTC (1 container statuses recorded)
May 21 06:57:49.487: INFO: 	Container wavefront-proxy ready: true, restart count 0
May 21 06:57:49.487: INFO: sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-ppgpw from sonobuoy started at 2021-05-21 05:36:03 +0000 UTC (2 container statuses recorded)
May 21 06:57:49.487: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 21 06:57:49.487: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node 16c19df1-13dd-4e26-99a1-a192204dc796
STEP: verifying the node has the label node 53f495cf-0382-4128-b4b3-0979b69a209b
STEP: verifying the node has the label node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.535: INFO: Pod coredns-6d5447c5f4-7dcmd requesting resource cpu=100m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.535: INFO: Pod coredns-6d5447c5f4-fv5hd requesting resource cpu=100m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod coredns-6d5447c5f4-jms82 requesting resource cpu=100m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod metrics-server-7d476fdfbd-phhm6 requesting resource cpu=0m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.535: INFO: Pod event-controller-55dd5b5777-4mn7d requesting resource cpu=0m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.535: INFO: Pod fluent-bit-hnqwh requesting resource cpu=0m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.535: INFO: Pod fluent-bit-plrsb requesting resource cpu=0m on Node 16c19df1-13dd-4e26-99a1-a192204dc796
May 21 06:57:49.535: INFO: Pod fluent-bit-vsrb2 requesting resource cpu=0m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod metric-controller-5754b4489d-c246d requesting resource cpu=0m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod node-exporter-6k9jm requesting resource cpu=10m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.535: INFO: Pod node-exporter-fmcv8 requesting resource cpu=10m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod node-exporter-p6twh requesting resource cpu=10m on Node 16c19df1-13dd-4e26-99a1-a192204dc796
May 21 06:57:49.535: INFO: Pod observability-manager-7f5c97895-xsj2f requesting resource cpu=0m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod sink-controller-5bf464ff96-4nbts requesting resource cpu=0m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.535: INFO: Pod telegraf-hlmgc requesting resource cpu=0m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod telegraf-j4f48 requesting resource cpu=0m on Node 16c19df1-13dd-4e26-99a1-a192204dc796
May 21 06:57:49.535: INFO: Pod telegraf-jqzbt requesting resource cpu=0m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.535: INFO: Pod telemetry-agent-8f56d9865-l6ntw requesting resource cpu=0m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod validator-54f6bbd67b-snnrv requesting resource cpu=0m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod wavefront-collector-5795f87754-596hz requesting resource cpu=0m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.535: INFO: Pod wavefront-proxy-6446ff4bd7-lw9wt requesting resource cpu=0m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.535: INFO: Pod sonobuoy requesting resource cpu=0m on Node 16c19df1-13dd-4e26-99a1-a192204dc796
May 21 06:57:49.535: INFO: Pod sonobuoy-e2e-job-a4c84655dab54066 requesting resource cpu=0m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-b9nwr requesting resource cpu=0m on Node 16c19df1-13dd-4e26-99a1-a192204dc796
May 21 06:57:49.535: INFO: Pod sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-g25r8 requesting resource cpu=0m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
May 21 06:57:49.535: INFO: Pod sonobuoy-systemd-logs-daemon-set-90424842a1bf46f0-ppgpw requesting resource cpu=0m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
STEP: Starting Pods to consume most of the cluster CPU.
May 21 06:57:49.535: INFO: Creating a pod which consumes cpu=973m on Node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
May 21 06:57:49.539: INFO: Creating a pod which consumes cpu=1043m on Node 16c19df1-13dd-4e26-99a1-a192204dc796
May 21 06:57:49.566: INFO: Creating a pod which consumes cpu=903m on Node 53f495cf-0382-4128-b4b3-0979b69a209b
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57b6e386-4262-4af4-bc7a-b74612e64f47.168102a9e64ab083], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2722/filler-pod-57b6e386-4262-4af4-bc7a-b74612e64f47 to 973ef262-7e5e-4fd5-8bf3-6139f838e19b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57b6e386-4262-4af4-bc7a-b74612e64f47.168102aa1a52be3b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57b6e386-4262-4af4-bc7a-b74612e64f47.168102aa1f159c83], Reason = [Created], Message = [Created container filler-pod-57b6e386-4262-4af4-bc7a-b74612e64f47]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57b6e386-4262-4af4-bc7a-b74612e64f47.168102aa28cf9dd6], Reason = [Started], Message = [Started container filler-pod-57b6e386-4262-4af4-bc7a-b74612e64f47]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63a6cbc6-2924-4c43-be56-f6115520f125.168102a9e7b03f77], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2722/filler-pod-63a6cbc6-2924-4c43-be56-f6115520f125 to 53f495cf-0382-4128-b4b3-0979b69a209b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63a6cbc6-2924-4c43-be56-f6115520f125.168102aa1bae326a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63a6cbc6-2924-4c43-be56-f6115520f125.168102aa1f06067c], Reason = [Created], Message = [Created container filler-pod-63a6cbc6-2924-4c43-be56-f6115520f125]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63a6cbc6-2924-4c43-be56-f6115520f125.168102aa276251e2], Reason = [Started], Message = [Started container filler-pod-63a6cbc6-2924-4c43-be56-f6115520f125]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-728efb9d-e226-444c-abb8-e5c6b877459e.168102a9e741d01c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2722/filler-pod-728efb9d-e226-444c-abb8-e5c6b877459e to 16c19df1-13dd-4e26-99a1-a192204dc796]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-728efb9d-e226-444c-abb8-e5c6b877459e.168102aa1c503446], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-728efb9d-e226-444c-abb8-e5c6b877459e.168102aa202e903b], Reason = [Created], Message = [Created container filler-pod-728efb9d-e226-444c-abb8-e5c6b877459e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-728efb9d-e226-444c-abb8-e5c6b877459e.168102aa28c534bf], Reason = [Started], Message = [Started container filler-pod-728efb9d-e226-444c-abb8-e5c6b877459e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168102aa611550dc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168102aa638ed670], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 16c19df1-13dd-4e26-99a1-a192204dc796
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 53f495cf-0382-4128-b4b3-0979b69a209b
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 973ef262-7e5e-4fd5-8bf3-6139f838e19b
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:57:53.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2722" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":295,"skipped":5066,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:57:53.181: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
May 21 06:57:53.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=kubectl-6091 cluster-info'
May 21 06:57:53.280: INFO: stderr: ""
May 21 06:57:53.280: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.100.200.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:57:53.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6091" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":296,"skipped":5087,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:57:53.286: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:57:53.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4796" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":297,"skipped":5093,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:57:53.338: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:57:58.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2743" for this suite.

• [SLOW TEST:5.210 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":298,"skipped":5111,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:57:58.549: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0521 06:57:59.688323      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 06:57:59.688395      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 06:57:59.688411      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 06:57:59.688: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:57:59.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1064" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":299,"skipped":5131,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:57:59.694: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7525
STEP: creating service affinity-nodeport in namespace services-7525
STEP: creating replication controller affinity-nodeport in namespace services-7525
I0521 06:57:59.746830      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-7525, replica count: 3
I0521 06:58:02.797078      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:58:02.808: INFO: Creating new exec pod
May 21 06:58:07.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-7525 exec execpod-affinity2zk69 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
May 21 06:58:07.985: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 21 06:58:07.985: INFO: stdout: ""
May 21 06:58:07.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-7525 exec execpod-affinity2zk69 -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.231 80'
May 21 06:58:08.148: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.231 80\nConnection to 10.100.200.231 80 port [tcp/http] succeeded!\n"
May 21 06:58:08.148: INFO: stdout: ""
May 21 06:58:08.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-7525 exec execpod-affinity2zk69 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30598'
May 21 06:58:08.305: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30598\nConnection to 30.0.0.11 30598 port [tcp/30598] succeeded!\n"
May 21 06:58:08.305: INFO: stdout: ""
May 21 06:58:08.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-7525 exec execpod-affinity2zk69 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 30598'
May 21 06:58:08.458: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 30598\nConnection to 30.0.0.10 30598 port [tcp/30598] succeeded!\n"
May 21 06:58:08.458: INFO: stdout: ""
May 21 06:58:08.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-7525 exec execpod-affinity2zk69 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30598'
May 21 06:58:08.628: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30598\nConnection to 30.0.0.11 30598 port [tcp/30598] succeeded!\n"
May 21 06:58:08.628: INFO: stdout: ""
May 21 06:58:08.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-7525 exec execpod-affinity2zk69 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 30598'
May 21 06:58:08.783: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 30598\nConnection to 30.0.0.10 30598 port [tcp/30598] succeeded!\n"
May 21 06:58:08.783: INFO: stdout: ""
May 21 06:58:08.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-7525 exec execpod-affinity2zk69 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.0.0.12:30598/ ; done'
May 21 06:58:09.001: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:30598/\n"
May 21 06:58:09.001: INFO: stdout: "\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9\naffinity-nodeport-fqhn9"
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Received response from host: affinity-nodeport-fqhn9
May 21 06:58:09.001: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7525, will wait for the garbage collector to delete the pods
May 21 06:58:09.106: INFO: Deleting ReplicationController affinity-nodeport took: 9.06413ms
May 21 06:58:09.207: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.122217ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:58:23.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7525" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:23.447 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":300,"skipped":5152,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:58:23.141: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 21 06:58:23.183: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:58:24.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-987" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":301,"skipped":5164,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:58:24.209: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2071
STEP: creating service affinity-nodeport-transition in namespace services-2071
STEP: creating replication controller affinity-nodeport-transition in namespace services-2071
I0521 06:58:24.263233      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-2071, replica count: 3
I0521 06:58:27.313953      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 06:58:27.320: INFO: Creating new exec pod
May 21 06:58:30.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-2071 exec execpod-affinity4mwcn -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 21 06:58:30.539: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 21 06:58:30.539: INFO: stdout: ""
May 21 06:58:30.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-2071 exec execpod-affinity4mwcn -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.112 80'
May 21 06:58:30.709: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.112 80\nConnection to 10.100.200.112 80 port [tcp/http] succeeded!\n"
May 21 06:58:30.709: INFO: stdout: ""
May 21 06:58:30.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-2071 exec execpod-affinity4mwcn -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 31043'
May 21 06:58:30.865: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 31043\nConnection to 30.0.0.10 31043 port [tcp/31043] succeeded!\n"
May 21 06:58:30.865: INFO: stdout: ""
May 21 06:58:30.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-2071 exec execpod-affinity4mwcn -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 31043'
May 21 06:58:31.011: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 31043\nConnection to 30.0.0.12 31043 port [tcp/31043] succeeded!\n"
May 21 06:58:31.011: INFO: stdout: ""
May 21 06:58:31.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-2071 exec execpod-affinity4mwcn -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 31043'
May 21 06:58:31.180: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 31043\nConnection to 30.0.0.10 31043 port [tcp/31043] succeeded!\n"
May 21 06:58:31.180: INFO: stdout: ""
May 21 06:58:31.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-2071 exec execpod-affinity4mwcn -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 31043'
May 21 06:58:31.334: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 31043\nConnection to 30.0.0.12 31043 port [tcp/31043] succeeded!\n"
May 21 06:58:31.334: INFO: stdout: ""
May 21 06:58:31.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-2071 exec execpod-affinity4mwcn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.0.0.12:31043/ ; done'
May 21 06:58:31.575: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n"
May 21 06:58:31.575: INFO: stdout: "\naffinity-nodeport-transition-nnksg\naffinity-nodeport-transition-sr54f\naffinity-nodeport-transition-nnksg\naffinity-nodeport-transition-sr54f\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-nnksg\naffinity-nodeport-transition-nnksg\naffinity-nodeport-transition-sr54f\naffinity-nodeport-transition-sr54f\naffinity-nodeport-transition-nnksg\naffinity-nodeport-transition-nnksg\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-sr54f\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-nnksg"
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-nnksg
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-sr54f
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-nnksg
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-sr54f
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-nnksg
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-nnksg
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-sr54f
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-sr54f
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-nnksg
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-nnksg
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-sr54f
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.575: INFO: Received response from host: affinity-nodeport-transition-nnksg
May 21 06:58:31.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-149702678 --namespace=services-2071 exec execpod-affinity4mwcn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.0.0.12:31043/ ; done'
May 21 06:58:31.832: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.12:31043/\n"
May 21 06:58:31.832: INFO: stdout: "\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8\naffinity-nodeport-transition-bgwd8"
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Received response from host: affinity-nodeport-transition-bgwd8
May 21 06:58:31.832: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2071, will wait for the garbage collector to delete the pods
May 21 06:58:31.908: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.06804ms
May 21 06:58:32.008: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.159744ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:58:42.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2071" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:18.234 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":302,"skipped":5167,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:58:42.444: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 21 06:58:42.483: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:58:52.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4088" for this suite.

• [SLOW TEST:9.916 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5184,"failed":0}
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:58:52.360: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0521 06:58:53.419379      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 06:58:53.419397      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 06:58:53.419403      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 06:58:53.419: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:58:53.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9904" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":304,"skipped":5184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:58:53.429: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:58:57.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9176" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5243,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:58:57.478: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-71dcadf6-dec8-4cdf-83ca-4bf3aab6094a
STEP: Creating a pod to test consume secrets
May 21 06:58:57.521: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2479215f-b604-428a-a99f-aebedc83c6cf" in namespace "projected-4530" to be "Succeeded or Failed"
May 21 06:58:57.541: INFO: Pod "pod-projected-secrets-2479215f-b604-428a-a99f-aebedc83c6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 19.388676ms
May 21 06:58:59.545: INFO: Pod "pod-projected-secrets-2479215f-b604-428a-a99f-aebedc83c6cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023822019s
STEP: Saw pod success
May 21 06:58:59.545: INFO: Pod "pod-projected-secrets-2479215f-b604-428a-a99f-aebedc83c6cf" satisfied condition "Succeeded or Failed"
May 21 06:58:59.547: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod pod-projected-secrets-2479215f-b604-428a-a99f-aebedc83c6cf container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 06:58:59.571: INFO: Waiting for pod pod-projected-secrets-2479215f-b604-428a-a99f-aebedc83c6cf to disappear
May 21 06:58:59.579: INFO: Pod pod-projected-secrets-2479215f-b604-428a-a99f-aebedc83c6cf no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:58:59.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4530" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5253,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:58:59.585: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9b032136-e629-4859-b399-a31b1d6276e8
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-9b032136-e629-4859-b399-a31b1d6276e8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:59:03.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9713" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":307,"skipped":5275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:59:03.675: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
May 21 06:59:03.709: INFO: Waiting up to 5m0s for pod "var-expansion-bca039dd-8030-48fe-a8d3-6d510fb0b3f7" in namespace "var-expansion-9113" to be "Succeeded or Failed"
May 21 06:59:03.713: INFO: Pod "var-expansion-bca039dd-8030-48fe-a8d3-6d510fb0b3f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125328ms
May 21 06:59:05.717: INFO: Pod "var-expansion-bca039dd-8030-48fe-a8d3-6d510fb0b3f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007743332s
STEP: Saw pod success
May 21 06:59:05.717: INFO: Pod "var-expansion-bca039dd-8030-48fe-a8d3-6d510fb0b3f7" satisfied condition "Succeeded or Failed"
May 21 06:59:05.719: INFO: Trying to get logs from node 16c19df1-13dd-4e26-99a1-a192204dc796 pod var-expansion-bca039dd-8030-48fe-a8d3-6d510fb0b3f7 container dapi-container: <nil>
STEP: delete the pod
May 21 06:59:05.731: INFO: Waiting for pod var-expansion-bca039dd-8030-48fe-a8d3-6d510fb0b3f7 to disappear
May 21 06:59:05.738: INFO: Pod var-expansion-bca039dd-8030-48fe-a8d3-6d510fb0b3f7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 06:59:05.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9113" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5310,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 06:59:05.744: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
May 21 07:01:06.307: INFO: Successfully updated pod "var-expansion-514cb0ae-4c10-4697-8475-4785e175064f"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 21 07:01:08.321: INFO: Deleting pod "var-expansion-514cb0ae-4c10-4697-8475-4785e175064f" in namespace "var-expansion-3399"
May 21 07:01:08.327: INFO: Wait up to 5m0s for pod "var-expansion-514cb0ae-4c10-4697-8475-4785e175064f" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 07:01:44.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3399" for this suite.

• [SLOW TEST:158.600 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":309,"skipped":5311,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 07:01:44.345: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 21 07:01:44.398: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 07:01:44.398: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 07:01:44.412: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 07:01:44.412: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 07:01:44.454: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 07:01:44.454: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 07:01:44.523: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 07:01:44.523: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 21 07:01:45.851: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 21 07:01:45.851: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 21 07:01:46.035: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 21 07:01:46.074: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 0
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 2
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 2
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 2
May 21 07:01:46.075: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 2
May 21 07:01:46.118: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 2
May 21 07:01:46.118: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 2
May 21 07:01:46.172: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 2
May 21 07:01:46.172: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 2
May 21 07:01:46.212: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
STEP: listing Deployments
May 21 07:01:46.224: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 21 07:01:46.243: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 21 07:01:46.255: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1 and labels map[test-deployment:patched test-deployment-static:true]
May 21 07:01:46.255: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 07:01:46.275: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 07:01:46.312: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 07:01:46.353: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 07:01:46.369: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 07:01:46.394: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 21 07:01:46.407: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 21 07:01:48.541: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
May 21 07:01:48.541: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
May 21 07:01:48.541: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
May 21 07:01:48.541: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
May 21 07:01:48.541: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
May 21 07:01:48.541: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
May 21 07:01:48.541: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
May 21 07:01:48.541: INFO: observed Deployment test-deployment in namespace deployment-6661 with ReadyReplicas 1
STEP: deleting the Deployment
May 21 07:01:48.578: INFO: observed event type MODIFIED
May 21 07:01:48.578: INFO: observed event type MODIFIED
May 21 07:01:48.578: INFO: observed event type MODIFIED
May 21 07:01:48.578: INFO: observed event type MODIFIED
May 21 07:01:48.578: INFO: observed event type MODIFIED
May 21 07:01:48.578: INFO: observed event type MODIFIED
May 21 07:01:48.579: INFO: observed event type MODIFIED
May 21 07:01:48.579: INFO: observed event type MODIFIED
May 21 07:01:48.579: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 21 07:01:48.589: INFO: Log out all the ReplicaSets if there is no deployment created
May 21 07:01:48.605: INFO: ReplicaSet "test-deployment-7664fb7d98":
&ReplicaSet{ObjectMeta:{test-deployment-7664fb7d98  deployment-6661  90ac59c1-ecd0-43ee-a6f7-ed907151d9b6 33660 3 2021-05-21 07:01:46 +0000 UTC <nil> <nil> map[pod-template-hash:7664fb7d98 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e2170ced-cf3f-44d8-a76b-7eb2e1a3338e 0xc006c54307 0xc006c54308}] []  [{kube-controller-manager Update apps/v1 2021-05-21 07:01:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2170ced-cf3f-44d8-a76b-7eb2e1a3338e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7664fb7d98,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7664fb7d98 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006c54370 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

May 21 07:01:48.608: INFO: pod: "test-deployment-7664fb7d98-2gjvs":
&Pod{ObjectMeta:{test-deployment-7664fb7d98-2gjvs test-deployment-7664fb7d98- deployment-6661  a7208eb6-5c91-4f1d-ac01-975e5056a0e4 33661 0 2021-05-21 07:01:48 +0000 UTC <nil> <nil> map[pod-template-hash:7664fb7d98 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7664fb7d98 90ac59c1-ecd0-43ee-a6f7-ed907151d9b6 0xc006c114d7 0xc006c114d8}] []  [{kube-controller-manager Update v1 2021-05-21 07:01:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90ac59c1-ecd0-43ee-a6f7-ed907151d9b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 07:01:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74nlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74nlc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74nlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:,StartTime:2021-05-21 07:01:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 21 07:01:48.608: INFO: pod: "test-deployment-7664fb7d98-zsk8d":
&Pod{ObjectMeta:{test-deployment-7664fb7d98-zsk8d test-deployment-7664fb7d98- deployment-6661  550a2ec6-b079-4c38-bd70-49f9a9053293 33643 0 2021-05-21 07:01:46 +0000 UTC <nil> <nil> map[pod-template-hash:7664fb7d98 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7664fb7d98 90ac59c1-ecd0-43ee-a6f7-ed907151d9b6 0xc006c11657 0xc006c11658}] []  [{kube-controller-manager Update v1 2021-05-21 07:01:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90ac59c1-ecd0-43ee-a6f7-ed907151d9b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 07:01:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.88.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74nlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74nlc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74nlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.88.126,StartTime:2021-05-21 07:01:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 07:01:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3d6407b903f86d836d03d96ff936510be1987c735eaa0e9f74b5088437658da7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.88.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 21 07:01:48.608: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-6661  53ce9818-e8eb-4e00-8933-6f7260fa4762 33662 4 2021-05-21 07:01:46 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e2170ced-cf3f-44d8-a76b-7eb2e1a3338e 0xc006c543d7 0xc006c543d8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 07:01:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2170ced-cf3f-44d8-a76b-7eb2e1a3338e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006c54458 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 21 07:01:48.612: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-6661  9797993b-ca8e-40cd-bdf9-8032df0f8f38 33595 2 2021-05-21 07:01:44 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e2170ced-cf3f-44d8-a76b-7eb2e1a3338e 0xc006c544b7 0xc006c544b8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 07:01:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2170ced-cf3f-44d8-a76b-7eb2e1a3338e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006c54520 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

May 21 07:01:48.614: INFO: pod: "test-deployment-8b6954bfb-sv4jp":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-sv4jp test-deployment-8b6954bfb- deployment-6661  85b378cf-cd9f-4257-b88a-d1ba2ec7ee7d 33568 0 2021-05-21 07:01:44 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 9797993b-ca8e-40cd-bdf9-8032df0f8f38 0xc0043d1617 0xc0043d1618}] []  [{kube-controller-manager Update v1 2021-05-21 07:01:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9797993b-ca8e-40cd-bdf9-8032df0f8f38\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 07:01:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.88.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74nlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74nlc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74nlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16c19df1-13dd-4e26-99a1-a192204dc796,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 07:01:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.88.122,StartTime:2021-05-21 07:01:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 07:01:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://0245420a8b944c90ff528371541be001b1ecd94091676cacefc06fd8321d77ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.88.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 07:01:48.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6661" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":310,"skipped":5316,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 07:01:48.623: INFO: >>> kubeConfig: /tmp/kubeconfig-149702678
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-fe2220d2-66c8-4db5-bece-a9daea3278fa
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 07:01:48.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9495" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":311,"skipped":5355,"failed":0}
SMay 21 07:01:48.662: INFO: Running AfterSuite actions on all nodes
May 21 07:01:48.662: INFO: Running AfterSuite actions on node 1
May 21 07:01:48.662: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 5130.021 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h25m31.883891524s
Test Suite Passed
