Conformance test: not doing test setup.
I0722 19:54:56.741917    5567 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0722 19:54:56.742043    5567 e2e.go:129] Starting e2e run "e38bdbce-4603-41fc-9dd5-9f9a0aec799e" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1626983695 - Will randomize all specs
Will run 311 of 5668 specs

Jul 22 19:54:57.039: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 19:54:57.041: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 22 19:54:57.123: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
Jul 22 19:54:57.189: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 22 19:54:57.189: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
Jul 22 19:54:57.189: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 22 19:54:57.212: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
Jul 22 19:54:57.212: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul 22 19:54:57.212: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 22 19:54:57.212: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jul 22 19:54:57.212: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Jul 22 19:54:57.212: INFO: e2e test version: v1.20.8
Jul 22 19:54:57.226: INFO: kube-apiserver version: v1.20.8
Jul 22 19:54:57.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 19:54:57.240: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:54:57.241: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
Jul 22 19:54:57.300: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jul 22 19:54:57.338: INFO: PSP annotation exists on dry run pod: "gardener.kube-system.apiserver-proxy"; assuming PodSecurityPolicy is enabled
Jul 22 19:54:57.378: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul 22 19:55:37.645: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 22 19:55:37.645: INFO: Deleting pod "simpletest.rc-25bz9" in namespace "gc-2360"
W0722 19:55:37.645641    5567 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 19:55:37.645675    5567 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 19:55:37.645680    5567 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 22 19:55:37.661: INFO: Deleting pod "simpletest.rc-4vtzt" in namespace "gc-2360"
Jul 22 19:55:37.681: INFO: Deleting pod "simpletest.rc-9vvtw" in namespace "gc-2360"
Jul 22 19:55:37.702: INFO: Deleting pod "simpletest.rc-bvgfc" in namespace "gc-2360"
Jul 22 19:55:37.726: INFO: Deleting pod "simpletest.rc-dqlx5" in namespace "gc-2360"
Jul 22 19:55:37.744: INFO: Deleting pod "simpletest.rc-hc2w6" in namespace "gc-2360"
Jul 22 19:55:37.761: INFO: Deleting pod "simpletest.rc-lqndb" in namespace "gc-2360"
Jul 22 19:55:37.776: INFO: Deleting pod "simpletest.rc-r4hwt" in namespace "gc-2360"
Jul 22 19:55:37.797: INFO: Deleting pod "simpletest.rc-xkjm7" in namespace "gc-2360"
Jul 22 19:55:37.819: INFO: Deleting pod "simpletest.rc-xt6qf" in namespace "gc-2360"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:55:37.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2360" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":1,"skipped":29,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:55:37.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-3885
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3885 to expose endpoints map[]
Jul 22 19:55:38.114: INFO: successfully validated that service multi-endpoint-test in namespace services-3885 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3885
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3885 to expose endpoints map[pod1:[100]]
Jul 22 19:55:42.171: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Jul 22 19:55:47.172: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Jul 22 19:55:49.194: INFO: successfully validated that service multi-endpoint-test in namespace services-3885 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3885
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3885 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 22 19:55:53.274: INFO: Unexpected endpoints: found map[5fd24bc2-3852-465e-a976-90c1a077170d:[100]], expected map[pod1:[100] pod2:[101]], will retry
Jul 22 19:55:58.298: INFO: successfully validated that service multi-endpoint-test in namespace services-3885 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-3885
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3885 to expose endpoints map[pod2:[101]]
Jul 22 19:55:58.367: INFO: successfully validated that service multi-endpoint-test in namespace services-3885 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3885
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3885 to expose endpoints map[]
Jul 22 19:55:58.425: INFO: successfully validated that service multi-endpoint-test in namespace services-3885 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:55:58.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3885" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":2,"skipped":30,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:55:58.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3007
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3007
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-3007
Jul 22 19:55:58.719: INFO: Found 0 stateful pods, waiting for 1
Jul 22 19:56:08.732: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jul 22 19:56:18.732: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 22 19:56:18.803: INFO: Deleting all statefulset in ns statefulset-3007
Jul 22 19:56:18.815: INFO: Scaling statefulset ss to 0
Jul 22 19:56:28.867: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 19:56:28.879: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:56:28.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3007" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":3,"skipped":48,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:56:28.962: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jul 22 19:56:29.158: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 19:56:29.183: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 19:56:29.195: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 before test
Jul 22 19:56:29.220: INFO: addons-nginx-ingress-controller-5f6b8d6b9b-kk4sh from kube-system started at 2021-07-22 19:51:18 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 22 19:56:29.220: INFO: apiserver-proxy-b49r5 from kube-system started at 2021-07-22 19:48:59 +0000 UTC (2 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container proxy ready: true, restart count 0
Jul 22 19:56:29.220: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 19:56:29.220: INFO: blackbox-exporter-859b5d9c8c-nf6n7 from kube-system started at 2021-07-22 19:55:18 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul 22 19:56:29.220: INFO: calico-node-jxrzv from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 19:56:29.220: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-n4zsh from kube-system started at 2021-07-22 19:49:46 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 19:56:29.220: INFO: coredns-7589655f7c-qkrtx from kube-system started at 2021-07-22 19:49:18 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container coredns ready: true, restart count 0
Jul 22 19:56:29.220: INFO: kube-proxy-nbdq9 from kube-system started at 2021-07-22 19:51:32 +0000 UTC (2 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 19:56:29.220: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 19:56:29.220: INFO: metrics-server-7fcbc9df99-xsdqd from kube-system started at 2021-07-22 19:49:46 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 19:56:29.220: INFO: node-exporter-bxzmq from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 19:56:29.220: INFO: node-problem-detector-vtbfr from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 19:56:29.220: INFO: vpn-shoot-6c79f97679-65l7b from kube-system started at 2021-07-22 19:49:46 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul 22 19:56:29.220: INFO: dashboard-metrics-scraper-5fc7d79f9-xvgk7 from kubernetes-dashboard started at 2021-07-22 19:49:46 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.220: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 22 19:56:29.220: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv before test
Jul 22 19:56:29.251: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-799f5cb4df-cnbrj from kube-system started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.251: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul 22 19:56:29.252: INFO: apiserver-proxy-lgksb from kube-system started at 2021-07-22 19:48:53 +0000 UTC (2 container statuses recorded)
Jul 22 19:56:29.252: INFO: 	Container proxy ready: true, restart count 0
Jul 22 19:56:29.252: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 19:56:29.252: INFO: calico-node-96j6z from kube-system started at 2021-07-22 19:50:28 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.252: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 19:56:29.252: INFO: calico-node-vertical-autoscaler-785b5f968-dl6lj from kube-system started at 2021-07-22 19:49:15 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.252: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 19:56:29.252: INFO: calico-typha-deploy-59966cd68c-kk766 from kube-system started at 2021-07-22 19:49:04 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.252: INFO: 	Container calico-typha ready: true, restart count 0
Jul 22 19:56:29.252: INFO: calico-typha-vertical-autoscaler-5c9655cddd-9t9nc from kube-system started at 2021-07-22 19:49:15 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.252: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 19:56:29.252: INFO: coredns-7589655f7c-8gplg from kube-system started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.252: INFO: 	Container coredns ready: true, restart count 0
Jul 22 19:56:29.252: INFO: kube-proxy-r86fj from kube-system started at 2021-07-22 19:51:22 +0000 UTC (2 container statuses recorded)
Jul 22 19:56:29.252: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 19:56:29.252: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 19:56:29.252: INFO: node-exporter-lkd2s from kube-system started at 2021-07-22 19:48:53 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.252: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 19:56:29.252: INFO: node-problem-detector-w4wrl from kube-system started at 2021-07-22 19:48:53 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.252: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 19:56:29.252: INFO: kubernetes-dashboard-775d7d55c5-q9hcp from kubernetes-dashboard started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 19:56:29.252: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16943522e16dd00a], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16943522e1b6f3a2], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:56:30.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2539" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":4,"skipped":63,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:56:30.365: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4193
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:56:30.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4193" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":5,"skipped":92,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:56:30.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 19:56:30.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b74c114-7634-4767-b6ba-cd0e555807db" in namespace "downward-api-6834" to be "Succeeded or Failed"
Jul 22 19:56:30.964: INFO: Pod "downwardapi-volume-2b74c114-7634-4767-b6ba-cd0e555807db": Phase="Pending", Reason="", readiness=false. Elapsed: 11.252122ms
Jul 22 19:56:32.976: INFO: Pod "downwardapi-volume-2b74c114-7634-4767-b6ba-cd0e555807db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023609626s
Jul 22 19:56:34.988: INFO: Pod "downwardapi-volume-2b74c114-7634-4767-b6ba-cd0e555807db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035594749s
STEP: Saw pod success
Jul 22 19:56:34.988: INFO: Pod "downwardapi-volume-2b74c114-7634-4767-b6ba-cd0e555807db" satisfied condition "Succeeded or Failed"
Jul 22 19:56:35.001: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-2b74c114-7634-4767-b6ba-cd0e555807db container client-container: <nil>
STEP: delete the pod
Jul 22 19:56:50.469: INFO: Waiting for pod downwardapi-volume-2b74c114-7634-4767-b6ba-cd0e555807db to disappear
Jul 22 19:56:50.480: INFO: Pod downwardapi-volume-2b74c114-7634-4767-b6ba-cd0e555807db no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:56:50.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6834" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:56:50.521: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 22 19:56:57.289: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-3977 pod-service-account-9a57ab30-ab3d-411c-9752-a08cb08f513e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 22 19:57:02.827: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-3977 pod-service-account-9a57ab30-ab3d-411c-9752-a08cb08f513e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 22 19:57:03.343: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-3977 pod-service-account-9a57ab30-ab3d-411c-9752-a08cb08f513e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:57:03.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3977" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":7,"skipped":144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:57:03.928: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Jul 22 19:57:04.137: INFO: Waiting up to 5m0s for pod "client-containers-5419ff7e-d89c-4b49-a62b-42867f6dd3bd" in namespace "containers-7898" to be "Succeeded or Failed"
Jul 22 19:57:04.157: INFO: Pod "client-containers-5419ff7e-d89c-4b49-a62b-42867f6dd3bd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.052733ms
Jul 22 19:57:06.173: INFO: Pod "client-containers-5419ff7e-d89c-4b49-a62b-42867f6dd3bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035438741s
Jul 22 19:57:08.186: INFO: Pod "client-containers-5419ff7e-d89c-4b49-a62b-42867f6dd3bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04872306s
STEP: Saw pod success
Jul 22 19:57:08.186: INFO: Pod "client-containers-5419ff7e-d89c-4b49-a62b-42867f6dd3bd" satisfied condition "Succeeded or Failed"
Jul 22 19:57:08.200: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv pod client-containers-5419ff7e-d89c-4b49-a62b-42867f6dd3bd container agnhost-container: <nil>
STEP: delete the pod
Jul 22 19:57:08.290: INFO: Waiting for pod client-containers-5419ff7e-d89c-4b49-a62b-42867f6dd3bd to disappear
Jul 22 19:57:08.302: INFO: Pod client-containers-5419ff7e-d89c-4b49-a62b-42867f6dd3bd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:57:08.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7898" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":171,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:57:08.350: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 19:57:08.577: INFO: Waiting up to 5m0s for pod "busybox-user-65534-448da1c6-6c06-4288-a10b-5995b5f36d9d" in namespace "security-context-test-6810" to be "Succeeded or Failed"
Jul 22 19:57:08.591: INFO: Pod "busybox-user-65534-448da1c6-6c06-4288-a10b-5995b5f36d9d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.51884ms
Jul 22 19:57:10.604: INFO: Pod "busybox-user-65534-448da1c6-6c06-4288-a10b-5995b5f36d9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026627499s
Jul 22 19:57:12.633: INFO: Pod "busybox-user-65534-448da1c6-6c06-4288-a10b-5995b5f36d9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056172793s
Jul 22 19:57:14.646: INFO: Pod "busybox-user-65534-448da1c6-6c06-4288-a10b-5995b5f36d9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069128728s
Jul 22 19:57:14.646: INFO: Pod "busybox-user-65534-448da1c6-6c06-4288-a10b-5995b5f36d9d" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:57:14.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6810" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":9,"skipped":176,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:57:14.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 19:57:18.922: INFO: Deleting pod "var-expansion-611fc2e8-d72b-4daa-bbf5-bcb595d44d9b" in namespace "var-expansion-6141"
Jul 22 19:57:18.936: INFO: Wait up to 5m0s for pod "var-expansion-611fc2e8-d72b-4daa-bbf5-bcb595d44d9b" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:57:22.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6141" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":10,"skipped":209,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:57:22.996: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6291
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:57:23.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6291" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":11,"skipped":213,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:57:23.300: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3837
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3837.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3837.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3837.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3837.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3837.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3837.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 19:57:38.141: INFO: DNS probes using dns-3837/dns-test-9ccc7c75-985e-42be-a6b2-9ac2ca29c40e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:57:38.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3837" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":12,"skipped":217,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:57:38.214: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-6070
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 19:57:38.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating first CR 
Jul 22 19:57:38.553: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T19:57:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T19:57:38Z]] name:name1 resourceVersion:4261 uid:7a9d55b6-cc25-4b4b-bc2a-405cb6b683af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul 22 19:57:48.567: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T19:57:48Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T19:57:48Z]] name:name2 resourceVersion:4313 uid:78da220a-45d8-41d1-8909-b88133f3a5cf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul 22 19:57:58.583: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T19:57:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T19:57:58Z]] name:name1 resourceVersion:4356 uid:7a9d55b6-cc25-4b4b-bc2a-405cb6b683af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul 22 19:58:08.598: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T19:57:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T19:58:08Z]] name:name2 resourceVersion:4399 uid:78da220a-45d8-41d1-8909-b88133f3a5cf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul 22 19:58:18.613: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T19:57:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T19:57:58Z]] name:name1 resourceVersion:4460 uid:7a9d55b6-cc25-4b4b-bc2a-405cb6b683af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul 22 19:58:28.630: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T19:57:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T19:58:08Z]] name:name2 resourceVersion:4504 uid:78da220a-45d8-41d1-8909-b88133f3a5cf] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:58:39.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6070" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":13,"skipped":228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:58:39.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-482
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-f9d9bb2b-f41c-4ba4-99e2-6e68d7ac752a
STEP: Creating secret with name s-test-opt-upd-08b31c4d-46b5-4034-82f8-d1940fb6cb24
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f9d9bb2b-f41c-4ba4-99e2-6e68d7ac752a
STEP: Updating secret s-test-opt-upd-08b31c4d-46b5-4034-82f8-d1940fb6cb24
STEP: Creating secret with name s-test-opt-create-36d8e996-aa6e-47b7-81f5-aa795714ecad
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:58:46.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-482" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":265,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:58:46.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4907
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 19:58:46.314: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 19:58:46.355: INFO: Number of nodes with available pods: 0
Jul 22 19:58:46.355: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 19:58:47.390: INFO: Number of nodes with available pods: 0
Jul 22 19:58:47.390: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 19:58:48.391: INFO: Number of nodes with available pods: 0
Jul 22 19:58:48.391: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 19:58:49.389: INFO: Number of nodes with available pods: 1
Jul 22 19:58:49.389: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 19:58:50.390: INFO: Number of nodes with available pods: 1
Jul 22 19:58:50.390: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 19:58:51.389: INFO: Number of nodes with available pods: 1
Jul 22 19:58:51.389: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 19:58:52.400: INFO: Number of nodes with available pods: 1
Jul 22 19:58:52.400: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 19:58:53.392: INFO: Number of nodes with available pods: 1
Jul 22 19:58:53.392: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 19:58:54.395: INFO: Number of nodes with available pods: 1
Jul 22 19:58:54.395: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 19:58:55.390: INFO: Number of nodes with available pods: 1
Jul 22 19:58:55.390: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 19:58:56.390: INFO: Number of nodes with available pods: 2
Jul 22 19:58:56.390: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 22 19:58:56.479: INFO: Wrong image for pod: daemon-set-vrb8q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:58:56.479: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:58:57.504: INFO: Wrong image for pod: daemon-set-vrb8q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:58:57.504: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:58:58.503: INFO: Wrong image for pod: daemon-set-vrb8q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:58:58.503: INFO: Pod daemon-set-vrb8q is not available
Jul 22 19:58:58.503: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:58:59.503: INFO: Pod daemon-set-8dh5f is not available
Jul 22 19:58:59.503: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:00.506: INFO: Pod daemon-set-8dh5f is not available
Jul 22 19:59:00.506: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:01.510: INFO: Pod daemon-set-8dh5f is not available
Jul 22 19:59:01.510: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:02.504: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:03.504: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:03.504: INFO: Pod daemon-set-zczth is not available
Jul 22 19:59:04.503: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:04.504: INFO: Pod daemon-set-zczth is not available
Jul 22 19:59:05.504: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:05.504: INFO: Pod daemon-set-zczth is not available
Jul 22 19:59:06.504: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:06.504: INFO: Pod daemon-set-zczth is not available
Jul 22 19:59:07.504: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:07.504: INFO: Pod daemon-set-zczth is not available
Jul 22 19:59:08.526: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:08.526: INFO: Pod daemon-set-zczth is not available
Jul 22 19:59:09.503: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:09.503: INFO: Pod daemon-set-zczth is not available
Jul 22 19:59:10.504: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:10.504: INFO: Pod daemon-set-zczth is not available
Jul 22 19:59:11.505: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:11.505: INFO: Pod daemon-set-zczth is not available
Jul 22 19:59:12.504: INFO: Wrong image for pod: daemon-set-zczth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jul 22 19:59:12.504: INFO: Pod daemon-set-zczth is not available
Jul 22 19:59:13.504: INFO: Pod daemon-set-bncmc is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 22 19:59:13.553: INFO: Number of nodes with available pods: 1
Jul 22 19:59:13.553: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 19:59:14.588: INFO: Number of nodes with available pods: 1
Jul 22 19:59:14.588: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 19:59:15.588: INFO: Number of nodes with available pods: 2
Jul 22 19:59:15.588: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4907, will wait for the garbage collector to delete the pods
Jul 22 19:59:15.729: INFO: Deleting DaemonSet.extensions daemon-set took: 16.157951ms
Jul 22 19:59:16.530: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.276926ms
Jul 22 19:59:22.742: INFO: Number of nodes with available pods: 0
Jul 22 19:59:22.742: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 19:59:22.754: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4857"},"items":null}

Jul 22 19:59:22.767: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4857"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:59:22.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4907" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":15,"skipped":280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:59:22.840: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 22 19:59:23.274: INFO: starting watch
STEP: patching
STEP: updating
Jul 22 19:59:23.310: INFO: waiting for watch events with expected annotations
Jul 22 19:59:23.310: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 19:59:23.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-217" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":16,"skipped":310,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 19:59:23.403: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jul 22 19:59:23.592: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 19:59:23.618: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 19:59:23.630: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 before test
Jul 22 19:59:23.655: INFO: addons-nginx-ingress-controller-5f6b8d6b9b-kk4sh from kube-system started at 2021-07-22 19:51:18 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 22 19:59:23.655: INFO: apiserver-proxy-b49r5 from kube-system started at 2021-07-22 19:48:59 +0000 UTC (2 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container proxy ready: true, restart count 0
Jul 22 19:59:23.655: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 19:59:23.655: INFO: blackbox-exporter-859b5d9c8c-nf6n7 from kube-system started at 2021-07-22 19:55:18 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul 22 19:59:23.655: INFO: calico-node-jxrzv from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 19:59:23.655: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-n4zsh from kube-system started at 2021-07-22 19:49:46 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 19:59:23.655: INFO: coredns-7589655f7c-qkrtx from kube-system started at 2021-07-22 19:49:18 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container coredns ready: true, restart count 0
Jul 22 19:59:23.655: INFO: kube-proxy-nbdq9 from kube-system started at 2021-07-22 19:51:32 +0000 UTC (2 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 19:59:23.655: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 19:59:23.655: INFO: metrics-server-7fcbc9df99-xsdqd from kube-system started at 2021-07-22 19:49:46 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 19:59:23.655: INFO: node-exporter-bxzmq from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 19:59:23.655: INFO: node-problem-detector-vtbfr from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 19:59:23.655: INFO: vpn-shoot-6c79f97679-65l7b from kube-system started at 2021-07-22 19:49:46 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul 22 19:59:23.655: INFO: dashboard-metrics-scraper-5fc7d79f9-xvgk7 from kubernetes-dashboard started at 2021-07-22 19:49:46 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.655: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 22 19:59:23.655: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv before test
Jul 22 19:59:23.671: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-799f5cb4df-cnbrj from kube-system started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul 22 19:59:23.671: INFO: apiserver-proxy-lgksb from kube-system started at 2021-07-22 19:48:53 +0000 UTC (2 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container proxy ready: true, restart count 0
Jul 22 19:59:23.671: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 19:59:23.671: INFO: calico-node-96j6z from kube-system started at 2021-07-22 19:50:28 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 19:59:23.671: INFO: calico-node-vertical-autoscaler-785b5f968-dl6lj from kube-system started at 2021-07-22 19:49:15 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 19:59:23.671: INFO: calico-typha-deploy-59966cd68c-kk766 from kube-system started at 2021-07-22 19:49:04 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container calico-typha ready: true, restart count 0
Jul 22 19:59:23.671: INFO: calico-typha-vertical-autoscaler-5c9655cddd-9t9nc from kube-system started at 2021-07-22 19:49:15 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 19:59:23.671: INFO: coredns-7589655f7c-8gplg from kube-system started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container coredns ready: true, restart count 0
Jul 22 19:59:23.671: INFO: kube-proxy-r86fj from kube-system started at 2021-07-22 19:51:22 +0000 UTC (2 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 19:59:23.671: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 19:59:23.671: INFO: node-exporter-lkd2s from kube-system started at 2021-07-22 19:48:53 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 19:59:23.671: INFO: node-problem-detector-w4wrl from kube-system started at 2021-07-22 19:48:53 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 19:59:23.671: INFO: kubernetes-dashboard-775d7d55c5-q9hcp from kubernetes-dashboard started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 19:59:23.671: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-968f6a6d-275f-4943-a8b2-c19d89bdbb51 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.250.0.4 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.250.0.4 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jul 22 19:59:43.983: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.0.4 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 19:59:43.983: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.4, port: 54321
Jul 22 19:59:44.423: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.0.4:54321/hostname] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 19:59:44.423: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.4, port: 54321 UDP
Jul 22 19:59:44.926: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.250.0.4 54321] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 19:59:44.926: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jul 22 19:59:50.351: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.0.4 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 19:59:50.351: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.4, port: 54321
Jul 22 19:59:50.742: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.0.4:54321/hostname] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 19:59:50.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.4, port: 54321 UDP
Jul 22 19:59:51.233: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.250.0.4 54321] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 19:59:51.234: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jul 22 19:59:56.649: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.0.4 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 19:59:56.649: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.4, port: 54321
Jul 22 19:59:57.053: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.0.4:54321/hostname] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 19:59:57.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.4, port: 54321 UDP
Jul 22 19:59:57.465: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.250.0.4 54321] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 19:59:57.465: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jul 22 20:00:02.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.0.4 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:00:02.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.4, port: 54321
Jul 22 20:00:03.265: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.0.4:54321/hostname] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:00:03.265: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.4, port: 54321 UDP
Jul 22 20:00:03.689: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.250.0.4 54321] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:00:03.689: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jul 22 20:00:09.420: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.0.4 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:00:09.420: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.4, port: 54321
Jul 22 20:00:09.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.0.4:54321/hostname] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:00:09.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.4, port: 54321 UDP
Jul 22 20:00:10.342: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.250.0.4 54321] Namespace:sched-pred-6218 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:00:10.342: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: removing the label kubernetes.io/e2e-968f6a6d-275f-4943-a8b2-c19d89bdbb51 off the node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-968f6a6d-275f-4943-a8b2-c19d89bdbb51
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:00:15.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6218" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":17,"skipped":313,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:00:15.863: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-434
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 20:00:20.158: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:00:20.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-434" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":18,"skipped":371,"failed":0}
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:00:20.225: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-6201
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 22 20:00:20.418: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Jul 22 20:00:20.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:00:22.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:00:24.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:00:26.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:00:28.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:00:30.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:00:32.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:00:34.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:00:36.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580820, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:00:40.378: INFO: Waited 1.382724769s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:00:41.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6201" for this suite.
•{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":19,"skipped":374,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:00:41.603: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-5691
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5691
STEP: Deleting pre-stop pod
Jul 22 20:00:55.074: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:00:55.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5691" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":20,"skipped":377,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:00:55.151: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-3f2d3894-7c49-4a6d-a978-97404214e847
STEP: Creating a pod to test consume configMaps
Jul 22 20:00:55.378: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-620f31a9-df6a-4afb-aecd-45873aa5419e" in namespace "projected-2961" to be "Succeeded or Failed"
Jul 22 20:00:55.391: INFO: Pod "pod-projected-configmaps-620f31a9-df6a-4afb-aecd-45873aa5419e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.788144ms
Jul 22 20:00:57.404: INFO: Pod "pod-projected-configmaps-620f31a9-df6a-4afb-aecd-45873aa5419e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026110071s
Jul 22 20:00:59.417: INFO: Pod "pod-projected-configmaps-620f31a9-df6a-4afb-aecd-45873aa5419e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038909685s
STEP: Saw pod success
Jul 22 20:00:59.417: INFO: Pod "pod-projected-configmaps-620f31a9-df6a-4afb-aecd-45873aa5419e" satisfied condition "Succeeded or Failed"
Jul 22 20:00:59.429: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-configmaps-620f31a9-df6a-4afb-aecd-45873aa5419e container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:00:59.579: INFO: Waiting for pod pod-projected-configmaps-620f31a9-df6a-4afb-aecd-45873aa5419e to disappear
Jul 22 20:00:59.596: INFO: Pod pod-projected-configmaps-620f31a9-df6a-4afb-aecd-45873aa5419e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:00:59.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2961" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":21,"skipped":378,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:00:59.636: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 22 20:01:04.432: INFO: Successfully updated pod "pod-update-activedeadlineseconds-373b03a0-a957-4b60-a6c9-57142b8a41d7"
Jul 22 20:01:04.432: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-373b03a0-a957-4b60-a6c9-57142b8a41d7" in namespace "pods-8408" to be "terminated due to deadline exceeded"
Jul 22 20:01:04.446: INFO: Pod "pod-update-activedeadlineseconds-373b03a0-a957-4b60-a6c9-57142b8a41d7": Phase="Running", Reason="", readiness=true. Elapsed: 13.919569ms
Jul 22 20:01:06.458: INFO: Pod "pod-update-activedeadlineseconds-373b03a0-a957-4b60-a6c9-57142b8a41d7": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.026076578s
Jul 22 20:01:06.458: INFO: Pod "pod-update-activedeadlineseconds-373b03a0-a957-4b60-a6c9-57142b8a41d7" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:01:06.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8408" for this suite.
•{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":389,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:01:06.493: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 22 20:01:06.724: INFO: Waiting up to 5m0s for pod "pod-6811f4fe-c251-45fe-963d-6f74f4435d46" in namespace "emptydir-7914" to be "Succeeded or Failed"
Jul 22 20:01:06.736: INFO: Pod "pod-6811f4fe-c251-45fe-963d-6f74f4435d46": Phase="Pending", Reason="", readiness=false. Elapsed: 11.683019ms
Jul 22 20:01:08.748: INFO: Pod "pod-6811f4fe-c251-45fe-963d-6f74f4435d46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023923531s
Jul 22 20:01:10.760: INFO: Pod "pod-6811f4fe-c251-45fe-963d-6f74f4435d46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036245437s
STEP: Saw pod success
Jul 22 20:01:10.760: INFO: Pod "pod-6811f4fe-c251-45fe-963d-6f74f4435d46" satisfied condition "Succeeded or Failed"
Jul 22 20:01:10.773: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-6811f4fe-c251-45fe-963d-6f74f4435d46 container test-container: <nil>
STEP: delete the pod
Jul 22 20:01:10.845: INFO: Waiting for pod pod-6811f4fe-c251-45fe-963d-6f74f4435d46 to disappear
Jul 22 20:01:10.857: INFO: Pod pod-6811f4fe-c251-45fe-963d-6f74f4435d46 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:01:10.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7914" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":409,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:01:10.892: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:01:11.089: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9235 create -f -'
Jul 22 20:01:11.396: INFO: stderr: ""
Jul 22 20:01:11.396: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul 22 20:01:11.396: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9235 create -f -'
Jul 22 20:01:11.630: INFO: stderr: ""
Jul 22 20:01:11.630: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 22 20:01:12.652: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 20:01:12.652: INFO: Found 0 / 1
Jul 22 20:01:13.644: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 20:01:13.644: INFO: Found 0 / 1
Jul 22 20:01:14.643: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 20:01:14.643: INFO: Found 1 / 1
Jul 22 20:01:14.643: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 22 20:01:14.661: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 20:01:14.661: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 20:01:14.661: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9235 describe pod agnhost-primary-7bwl9'
Jul 22 20:01:14.798: INFO: stderr: ""
Jul 22 20:01:14.799: INFO: stdout: "Name:         agnhost-primary-7bwl9\nNamespace:    kubectl-9235\nPriority:     0\nNode:         shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9/10.250.0.4\nStart Time:   Thu, 22 Jul 2021 20:01:11 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 100.96.1.36/32\n              cni.projectcalico.org/podIPs: 100.96.1.36/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           100.96.1.36\nIPs:\n  IP:           100.96.1.36\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://b2236fbb93009d74ff12a29d7767964e51b0a59adb9f0c86b82bfeaa782d84e2\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 22 Jul 2021 20:01:13 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tkqc9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tkqc9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tkqc9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-9235/agnhost-primary-7bwl9 to shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jul 22 20:01:14.799: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9235 describe rc agnhost-primary'
Jul 22 20:01:14.946: INFO: stderr: ""
Jul 22 20:01:14.946: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9235\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-7bwl9\n"
Jul 22 20:01:14.946: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9235 describe service agnhost-primary'
Jul 22 20:01:15.085: INFO: stderr: ""
Jul 22 20:01:15.085: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9235\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                100.64.33.79\nIPs:               100.64.33.79\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.96.1.36:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 22 20:01:15.108: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9235 describe node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9'
Jul 22 20:01:15.314: INFO: stderr: ""
Jul 22 20:01:15.314: INFO: stdout: "Name:               shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_DS2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=northeurope\n                    failure-domain.beta.kubernetes.io/zone=0\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=Standard_DS2_v2\n                    node.kubernetes.io/role=node\n                    topology.kubernetes.io/region=northeurope\n                    topology.kubernetes.io/zone=0\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/pool=worker-1\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 393c14fe397fa50099d68084d1a340424dff642b64234299dc82d425cfa8a3a9\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"node.kubernetes.io/role\":\"node\",\"worker.garden.sapcloud.io/group\":\"worker-1\",\"worker.gard...\n                    projectcalico.org/IPv4Address: 10.250.0.4/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.96.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 22 Jul 2021 19:48:55 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 22 Jul 2021 20:01:11 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  ReadonlyFilesystem            False   Thu, 22 Jul 2021 20:00:16 +0000   Thu, 22 Jul 2021 19:50:13 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentKubeletRestart        False   Thu, 22 Jul 2021 20:00:16 +0000   Thu, 22 Jul 2021 19:50:13 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Thu, 22 Jul 2021 20:00:16 +0000   Thu, 22 Jul 2021 19:50:13 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Thu, 22 Jul 2021 20:00:16 +0000   Thu, 22 Jul 2021 19:50:13 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  CorruptDockerOverlay2         False   Thu, 22 Jul 2021 20:00:16 +0000   Thu, 22 Jul 2021 19:50:14 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Thu, 22 Jul 2021 20:00:16 +0000   Thu, 22 Jul 2021 19:50:13 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  KernelDeadlock                False   Thu, 22 Jul 2021 20:00:16 +0000   Thu, 22 Jul 2021 19:50:13 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  NetworkUnavailable            False   Thu, 22 Jul 2021 19:49:25 +0000   Thu, 22 Jul 2021 19:49:25 +0000   RouteCreated                    RouteController created a route\n  MemoryPressure                False   Thu, 22 Jul 2021 20:01:11 +0000   Thu, 22 Jul 2021 19:48:55 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Thu, 22 Jul 2021 20:01:11 +0000   Thu, 22 Jul 2021 19:48:55 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Thu, 22 Jul 2021 20:01:11 +0000   Thu, 22 Jul 2021 19:48:55 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Thu, 22 Jul 2021 20:01:11 +0000   Thu, 22 Jul 2021 19:49:15 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9\n  InternalIP:  10.250.0.4\nCapacity:\n  attachable-volumes-azure-disk:  8\n  cpu:                            2\n  ephemeral-storage:              35011340Ki\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         7126176Ki\n  pods:                           110\nAllocatable:\n  attachable-volumes-azure-disk:  8\n  cpu:                            1920m\n  ephemeral-storage:              34059031526\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         5975200Ki\n  pods:                           110\nSystem Info:\n  Machine ID:                 9d72cc0a98ff406796b3f9531dc079b9\n  System UUID:                95177684-2726-fe47-afc0-520c70530d0c\n  Boot ID:                    40caa161-e615-46b9-a1f7-72dfb8eba9e1\n  Kernel Version:             5.4.0-6-cloud-amd64\n  OS Image:                   Garden Linux 318.8\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.5+dfsg1\n  Kubelet Version:            v1.20.8\n  Kube-Proxy Version:         v1.20.8\nPodCIDR:                      100.96.1.0/24\nPodCIDRs:                     100.96.1.0/24\nProviderID:                   azure:///subscriptions/0b9904be-2a50-4fda-a947-c5f1b1d07666/resourceGroups/shoot--it--tmi8i-6ya/providers/Microsoft.Compute/virtualMachines/shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                   ------------  ----------  ---------------  -------------  ---\n  kube-system                 addons-nginx-ingress-controller-5f6b8d6b9b-kk4sh       100m (5%)     400m (20%)  128Mi (2%)       512Mi (8%)     9m57s\n  kube-system                 apiserver-proxy-b49r5                                  40m (2%)      400m (20%)  40Mi (0%)        500Mi (8%)     12m\n  kube-system                 blackbox-exporter-859b5d9c8c-nf6n7                     11m (0%)      44m (2%)    23574998 (0%)    94299992 (1%)  5m57s\n  kube-system                 calico-node-jxrzv                                      250m (13%)    800m (41%)  100Mi (1%)       700Mi (11%)    12m\n  kube-system                 calico-typha-horizontal-autoscaler-5b58bb446c-n4zsh    10m (0%)      10m (0%)    50Mi (0%)        50Mi (0%)      12m\n  kube-system                 coredns-7589655f7c-qkrtx                               50m (2%)      250m (13%)  15Mi (0%)        500Mi (8%)     11m\n  kube-system                 kube-proxy-nbdq9                                       22m (1%)      44m (2%)    35074998 (0%)    94299992 (1%)  9m43s\n  kube-system                 metrics-server-7fcbc9df99-xsdqd                        50m (2%)      500m (26%)  150Mi (2%)       1Gi (17%)      12m\n  kube-system                 node-exporter-bxzmq                                    50m (2%)      50m (2%)    50Mi (0%)        50Mi (0%)      12m\n  kube-system                 node-problem-detector-vtbfr                            20m (1%)      80m (4%)    20Mi (0%)        80Mi (1%)      12m\n  kube-system                 vpn-shoot-6c79f97679-65l7b                             100m (5%)     400m (20%)  100Mi (1%)       400Mi (6%)     12m\n  kubectl-9235                agnhost-primary-7bwl9                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  kubernetes-dashboard        dashboard-metrics-scraper-5fc7d79f9-xvgk7              0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests         Limits\n  --------                       --------         ------\n  cpu                            703m (36%)       2978m (155%)\n  memory                         743370124 (12%)  4189966000 (68%)\n  ephemeral-storage              0 (0%)           0 (0%)\n  hugepages-1Gi                  0 (0%)           0 (0%)\n  hugepages-2Mi                  0 (0%)           0 (0%)\n  attachable-volumes-azure-disk  0                0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   NodeHasSufficientMemory  12m (x8 over 12m)  kubelet          Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    12m (x8 over 12m)  kubelet          Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 status is now: NodeHasNoDiskPressure\n  Normal   Starting                 11m                kube-proxy       Starting kube-proxy.\n  Warning  DockerStart              10m (x2 over 10m)  systemd-monitor  Starting Docker Application Container Engine...\n  Normal   Starting                 9m37s              kube-proxy       Starting kube-proxy.\n"
Jul 22 20:01:15.314: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9235 describe namespace kubectl-9235'
Jul 22 20:01:15.449: INFO: stderr: ""
Jul 22 20:01:15.449: INFO: stdout: "Name:         kubectl-9235\nLabels:       e2e-framework=kubectl\n              e2e-run=e38bdbce-4603-41fc-9dd5-9f9a0aec799e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:01:15.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9235" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":24,"skipped":419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:01:15.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:01:19.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3826" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":25,"skipped":453,"failed":0}

------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:01:19.804: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-81b7a137-311a-4307-ab76-150822e4b2e5 in namespace container-probe-5994
Jul 22 20:01:24.039: INFO: Started pod busybox-81b7a137-311a-4307-ab76-150822e4b2e5 in namespace container-probe-5994
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:01:24.051: INFO: Initial restart count of pod busybox-81b7a137-311a-4307-ab76-150822e4b2e5 is 0
Jul 22 20:02:10.393: INFO: Restart count of pod container-probe-5994/busybox-81b7a137-311a-4307-ab76-150822e4b2e5 is now 1 (46.342313604s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:02:10.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5994" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":453,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:02:10.445: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-af1b9020-fe74-46ba-ab2c-63bc0db5b366-5749
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:02:10.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4666" for this suite.
STEP: Destroying namespace "nspatchtest-af1b9020-fe74-46ba-ab2c-63bc0db5b366-5749" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":27,"skipped":464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:02:10.888: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 20:02:15.172: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:02:15.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4424" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":28,"skipped":486,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:02:15.243: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:02:15.462: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 20:02:19.487: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 22 20:02:21.500: INFO: Creating deployment "test-rollover-deployment"
Jul 22 20:02:21.533: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 22 20:02:23.558: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 22 20:02:23.582: INFO: Ensure that both replica sets have 1 created replica
Jul 22 20:02:23.607: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 22 20:02:23.632: INFO: Updating deployment test-rollover-deployment
Jul 22 20:02:23.632: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 22 20:02:25.660: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 22 20:02:25.687: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 22 20:02:25.711: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:02:25.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580943, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:02:27.735: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:02:27.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580946, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:02:29.737: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:02:29.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580946, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:02:31.736: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:02:31.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580946, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:02:33.736: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:02:33.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580946, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:02:35.735: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:02:35.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580946, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762580941, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:02:37.740: INFO: 
Jul 22 20:02:37.740: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jul 22 20:02:37.777: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6454  62f6ff1c-91f4-4e4c-81d4-f7aef3cc9744 6197 2 2021-07-22 20:02:21 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-22 20:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026571f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-22 20:02:21 +0000 UTC,LastTransitionTime:2021-07-22 20:02:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-07-22 20:02:36 +0000 UTC,LastTransitionTime:2021-07-22 20:02:21 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 20:02:37.790: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-6454  188e02d2-940a-402e-8de9-2ab38834a72b 6190 2 2021-07-22 20:02:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 62f6ff1c-91f4-4e4c-81d4-f7aef3cc9744 0xc002657637 0xc002657638}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62f6ff1c-91f4-4e4c-81d4-f7aef3cc9744\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026576c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:02:37.790: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 22 20:02:37.790: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6454  a5f068b7-422b-4fe1-b446-16240e9b34ab 6196 2 2021-07-22 20:02:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 62f6ff1c-91f4-4e4c-81d4-f7aef3cc9744 0xc00265752f 0xc002657540}] []  [{e2e.test Update apps/v1 2021-07-22 20:02:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62f6ff1c-91f4-4e4c-81d4-f7aef3cc9744\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0026575d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:02:37.791: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-6454  0047e0f8-9866-4c9a-a2e6-96995dbfe341 6128 2 2021-07-22 20:02:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 62f6ff1c-91f4-4e4c-81d4-f7aef3cc9744 0xc002657727 0xc002657728}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62f6ff1c-91f4-4e4c-81d4-f7aef3cc9744\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026577b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:02:37.803: INFO: Pod "test-rollover-deployment-668db69979-hl4g6" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-hl4g6 test-rollover-deployment-668db69979- deployment-6454  5a81fc76-b6ba-45cb-8a9c-bc691b65e309 6149 0 2021-07-22 20:02:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:100.96.0.23/32 cni.projectcalico.org/podIPs:100.96.0.23/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 188e02d2-940a-402e-8de9-2ab38834a72b 0xc002657ca7 0xc002657ca8}] []  [{kube-controller-manager Update v1 2021-07-22 20:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"188e02d2-940a-402e-8de9-2ab38834a72b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:02:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7rnvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7rnvm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7rnvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:02:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:02:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.0.23,StartTime:2021-07-22 20:02:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:02:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://ba908b6f6ee792c5a050a9ce1b6c88526576872883b790955f8a6fc59f248b29,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:02:37.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6454" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":29,"skipped":501,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:02:37.853: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8486
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8486
STEP: Creating statefulset with conflicting port in namespace statefulset-8486
STEP: Waiting until pod test-pod will start running in namespace statefulset-8486
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8486
Jul 22 20:02:42.137: INFO: Observed stateful pod in namespace: statefulset-8486, name: ss-0, uid: 643bfc9a-e536-4fe3-a2a2-b684c47a1d97, status phase: Pending. Waiting for statefulset controller to delete.
Jul 22 20:02:42.155: INFO: Observed stateful pod in namespace: statefulset-8486, name: ss-0, uid: 643bfc9a-e536-4fe3-a2a2-b684c47a1d97, status phase: Failed. Waiting for statefulset controller to delete.
Jul 22 20:02:42.167: INFO: Observed stateful pod in namespace: statefulset-8486, name: ss-0, uid: 643bfc9a-e536-4fe3-a2a2-b684c47a1d97, status phase: Failed. Waiting for statefulset controller to delete.
Jul 22 20:02:42.169: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8486
STEP: Removing pod with conflicting port in namespace statefulset-8486
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8486 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 22 20:02:46.226: INFO: Deleting all statefulset in ns statefulset-8486
Jul 22 20:02:46.239: INFO: Scaling statefulset ss to 0
Jul 22 20:02:56.289: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:02:56.301: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:02:56.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8486" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":30,"skipped":514,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:02:56.375: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:02:56.775: INFO: Waiting up to 5m0s for pod "downwardapi-volume-756adc72-4365-4fde-8f84-0e602462590b" in namespace "projected-3325" to be "Succeeded or Failed"
Jul 22 20:02:56.786: INFO: Pod "downwardapi-volume-756adc72-4365-4fde-8f84-0e602462590b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.434809ms
Jul 22 20:02:58.806: INFO: Pod "downwardapi-volume-756adc72-4365-4fde-8f84-0e602462590b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03132574s
Jul 22 20:03:00.818: INFO: Pod "downwardapi-volume-756adc72-4365-4fde-8f84-0e602462590b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043596162s
STEP: Saw pod success
Jul 22 20:03:00.818: INFO: Pod "downwardapi-volume-756adc72-4365-4fde-8f84-0e602462590b" satisfied condition "Succeeded or Failed"
Jul 22 20:03:00.830: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-756adc72-4365-4fde-8f84-0e602462590b container client-container: <nil>
STEP: delete the pod
Jul 22 20:03:01.040: INFO: Waiting for pod downwardapi-volume-756adc72-4365-4fde-8f84-0e602462590b to disappear
Jul 22 20:03:01.052: INFO: Pod downwardapi-volume-756adc72-4365-4fde-8f84-0e602462590b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:03:01.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3325" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":526,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:03:01.089: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jul 22 20:03:01.344: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:03:01.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9417" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":32,"skipped":588,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:03:01.408: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:03:17.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6826" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":33,"skipped":598,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:03:17.991: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jul 22 20:03:28.303: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0722 20:03:28.303196    5567 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 20:03:28.303228    5567 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 20:03:28.303234    5567 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:03:28.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2896" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":34,"skipped":617,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:03:28.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7945
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jul 22 20:03:28.541: INFO: Waiting up to 5m0s for pod "downward-api-dba311f7-bd7c-4a03-bc18-3aa85a340964" in namespace "downward-api-7945" to be "Succeeded or Failed"
Jul 22 20:03:28.552: INFO: Pod "downward-api-dba311f7-bd7c-4a03-bc18-3aa85a340964": Phase="Pending", Reason="", readiness=false. Elapsed: 11.310134ms
Jul 22 20:03:30.564: INFO: Pod "downward-api-dba311f7-bd7c-4a03-bc18-3aa85a340964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023328403s
Jul 22 20:03:32.577: INFO: Pod "downward-api-dba311f7-bd7c-4a03-bc18-3aa85a340964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035978695s
STEP: Saw pod success
Jul 22 20:03:32.577: INFO: Pod "downward-api-dba311f7-bd7c-4a03-bc18-3aa85a340964" satisfied condition "Succeeded or Failed"
Jul 22 20:03:32.589: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downward-api-dba311f7-bd7c-4a03-bc18-3aa85a340964 container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:03:32.674: INFO: Waiting for pod downward-api-dba311f7-bd7c-4a03-bc18-3aa85a340964 to disappear
Jul 22 20:03:32.685: INFO: Pod downward-api-dba311f7-bd7c-4a03-bc18-3aa85a340964 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:03:32.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7945" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":617,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:03:32.725: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-e6be72a8-2227-4e4b-8ed2-3d7d09a99d23
STEP: Creating a pod to test consume configMaps
Jul 22 20:03:32.950: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fa164d64-d472-4ed0-8636-4acd5a450db3" in namespace "projected-5331" to be "Succeeded or Failed"
Jul 22 20:03:32.962: INFO: Pod "pod-projected-configmaps-fa164d64-d472-4ed0-8636-4acd5a450db3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.138418ms
Jul 22 20:03:34.975: INFO: Pod "pod-projected-configmaps-fa164d64-d472-4ed0-8636-4acd5a450db3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024953203s
Jul 22 20:03:36.987: INFO: Pod "pod-projected-configmaps-fa164d64-d472-4ed0-8636-4acd5a450db3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037526942s
STEP: Saw pod success
Jul 22 20:03:36.987: INFO: Pod "pod-projected-configmaps-fa164d64-d472-4ed0-8636-4acd5a450db3" satisfied condition "Succeeded or Failed"
Jul 22 20:03:37.000: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-configmaps-fa164d64-d472-4ed0-8636-4acd5a450db3 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:03:37.069: INFO: Waiting for pod pod-projected-configmaps-fa164d64-d472-4ed0-8636-4acd5a450db3 to disappear
Jul 22 20:03:37.082: INFO: Pod pod-projected-configmaps-fa164d64-d472-4ed0-8636-4acd5a450db3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:03:37.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5331" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:03:37.117: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8706
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 22 20:03:37.343: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:03:37.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8706" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":37,"skipped":657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:03:37.450: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jul 22 20:03:37.637: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:03:44.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5467" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":38,"skipped":679,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:03:44.230: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-ca639fd7-a05a-4f87-9c13-7963f1860b19 in namespace container-probe-3798
Jul 22 20:03:48.470: INFO: Started pod test-webserver-ca639fd7-a05a-4f87-9c13-7963f1860b19 in namespace container-probe-3798
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:03:48.483: INFO: Initial restart count of pod test-webserver-ca639fd7-a05a-4f87-9c13-7963f1860b19 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:07:50.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3798" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":39,"skipped":689,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:07:50.225: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-908
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:07:50.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94cc59db-93ea-42f1-8080-8bac537b8e12" in namespace "projected-908" to be "Succeeded or Failed"
Jul 22 20:07:50.457: INFO: Pod "downwardapi-volume-94cc59db-93ea-42f1-8080-8bac537b8e12": Phase="Pending", Reason="", readiness=false. Elapsed: 11.6276ms
Jul 22 20:07:52.470: INFO: Pod "downwardapi-volume-94cc59db-93ea-42f1-8080-8bac537b8e12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024893081s
Jul 22 20:07:54.531: INFO: Pod "downwardapi-volume-94cc59db-93ea-42f1-8080-8bac537b8e12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085617544s
STEP: Saw pod success
Jul 22 20:07:54.531: INFO: Pod "downwardapi-volume-94cc59db-93ea-42f1-8080-8bac537b8e12" satisfied condition "Succeeded or Failed"
Jul 22 20:07:54.543: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-94cc59db-93ea-42f1-8080-8bac537b8e12 container client-container: <nil>
STEP: delete the pod
Jul 22 20:07:54.657: INFO: Waiting for pod downwardapi-volume-94cc59db-93ea-42f1-8080-8bac537b8e12 to disappear
Jul 22 20:07:54.668: INFO: Pod downwardapi-volume-94cc59db-93ea-42f1-8080-8bac537b8e12 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:07:54.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-908" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":709,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:07:54.704: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3436
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7323
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:08:10.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-569" for this suite.
STEP: Destroying namespace "nsdeletetest-3436" for this suite.
Jul 22 20:08:10.389: INFO: Namespace nsdeletetest-3436 was already deleted
STEP: Destroying namespace "nsdeletetest-7323" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":41,"skipped":732,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:08:10.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5099
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5099
I0722 20:08:10.668243    5567 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5099, replica count: 2
I0722 20:08:13.718578    5567 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:08:16.719: INFO: Creating new exec pod
I0722 20:08:16.719798    5567 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:08:21.772: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5099 exec execpodx6k5f -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 22 20:08:23.115: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 20:08:23.115: INFO: stdout: ""
Jul 22 20:08:23.115: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5099 exec execpodx6k5f -- /bin/sh -x -c nc -zv -t -w 2 100.64.206.251 80'
Jul 22 20:08:23.658: INFO: stderr: "+ nc -zv -t -w 2 100.64.206.251 80\nConnection to 100.64.206.251 80 port [tcp/http] succeeded!\n"
Jul 22 20:08:23.658: INFO: stdout: ""
Jul 22 20:08:23.658: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:08:23.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5099" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":42,"skipped":742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:08:23.721: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8660
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-8660
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8660
Jul 22 20:08:23.950: INFO: Found 0 stateful pods, waiting for 1
Jul 22 20:08:33.964: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 22 20:08:33.976: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 20:08:34.606: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 20:08:34.606: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 20:08:34.606: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 20:08:34.618: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 22 20:08:44.632: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 20:08:44.632: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:08:44.683: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:08:44.683: INFO: ss-0  shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:23 +0000 UTC  }]
Jul 22 20:08:44.683: INFO: ss-1                                             Pending         []
Jul 22 20:08:44.683: INFO: 
Jul 22 20:08:44.683: INFO: StatefulSet ss has not reached scale 3, at 2
Jul 22 20:08:45.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987867572s
Jul 22 20:08:46.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971411127s
Jul 22 20:08:47.726: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.958154905s
Jul 22 20:08:48.739: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.944722921s
Jul 22 20:08:49.752: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.931937609s
Jul 22 20:08:50.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.918928802s
Jul 22 20:08:51.778: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.905550441s
Jul 22 20:08:52.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.892646313s
Jul 22 20:08:53.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 872.92358ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8660
Jul 22 20:08:54.823: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:08:55.391: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 20:08:55.391: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 20:08:55.391: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 20:08:55.391: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:08:55.987: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 22 20:08:55.987: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 20:08:55.987: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 20:08:55.987: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:08:56.530: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 22 20:08:56.530: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 20:08:56.530: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 20:08:56.543: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:08:56.543: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:08:56.543: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 22 20:08:56.556: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 20:08:57.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 20:08:57.014: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 20:08:57.014: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 20:08:57.014: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 20:08:57.568: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 20:08:57.568: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 20:08:57.568: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 20:08:57.568: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 20:08:58.117: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 20:08:58.117: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 20:08:58.117: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 20:08:58.117: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:08:58.129: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 22 20:09:08.154: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 20:09:08.154: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 20:09:08.154: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 20:09:08.198: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:09:08.198: INFO: ss-0  shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:23 +0000 UTC  }]
Jul 22 20:09:08.198: INFO: ss-1  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:08.198: INFO: ss-2  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:08.198: INFO: 
Jul 22 20:09:08.198: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 20:09:09.214: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:09:09.214: INFO: ss-0  shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:23 +0000 UTC  }]
Jul 22 20:09:09.214: INFO: ss-1  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:09.214: INFO: ss-2  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:09.214: INFO: 
Jul 22 20:09:09.214: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 20:09:10.228: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:09:10.228: INFO: ss-0  shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:23 +0000 UTC  }]
Jul 22 20:09:10.228: INFO: ss-1  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:10.228: INFO: ss-2  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:10.228: INFO: 
Jul 22 20:09:10.228: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 20:09:11.316: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:09:11.316: INFO: ss-1  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:11.316: INFO: ss-2  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:11.316: INFO: 
Jul 22 20:09:11.316: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 22 20:09:12.330: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:09:12.330: INFO: ss-1  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:12.330: INFO: ss-2  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:12.330: INFO: 
Jul 22 20:09:12.330: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 22 20:09:13.343: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:09:13.343: INFO: ss-1  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:13.343: INFO: ss-2  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:13.343: INFO: 
Jul 22 20:09:13.343: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 22 20:09:14.356: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:09:14.356: INFO: ss-1  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:14.356: INFO: ss-2  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:14.356: INFO: 
Jul 22 20:09:14.356: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 22 20:09:15.369: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:09:15.369: INFO: ss-1  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:15.369: INFO: ss-2  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:15.369: INFO: 
Jul 22 20:09:15.369: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 22 20:09:16.382: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:09:16.382: INFO: ss-1  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:16.382: INFO: ss-2  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:16.382: INFO: 
Jul 22 20:09:16.382: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 22 20:09:17.395: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jul 22 20:09:17.395: INFO: ss-1  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:17.395: INFO: ss-2  shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:08:44 +0000 UTC  }]
Jul 22 20:09:17.395: INFO: 
Jul 22 20:09:17.395: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8660
Jul 22 20:09:18.415: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:09:18.769: INFO: rc: 1
Jul 22 20:09:18.769: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jul 22 20:09:28.769: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:09:28.932: INFO: rc: 1
Jul 22 20:09:28.933: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:09:38.933: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:09:39.059: INFO: rc: 1
Jul 22 20:09:39.059: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:09:49.059: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:09:49.185: INFO: rc: 1
Jul 22 20:09:49.185: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:09:59.185: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:09:59.324: INFO: rc: 1
Jul 22 20:09:59.324: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:10:09.324: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:10:09.440: INFO: rc: 1
Jul 22 20:10:09.440: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:10:19.440: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:10:19.558: INFO: rc: 1
Jul 22 20:10:19.558: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:10:29.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:10:29.684: INFO: rc: 1
Jul 22 20:10:29.684: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:10:39.685: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:10:39.836: INFO: rc: 1
Jul 22 20:10:39.836: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:10:49.836: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:10:49.984: INFO: rc: 1
Jul 22 20:10:49.984: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:10:59.984: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:11:00.154: INFO: rc: 1
Jul 22 20:11:00.154: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:11:10.155: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:11:10.300: INFO: rc: 1
Jul 22 20:11:10.300: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:11:20.301: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:11:20.436: INFO: rc: 1
Jul 22 20:11:20.436: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:11:30.436: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:11:30.544: INFO: rc: 1
Jul 22 20:11:30.544: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:11:40.544: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:11:40.676: INFO: rc: 1
Jul 22 20:11:40.676: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:11:50.676: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:11:50.816: INFO: rc: 1
Jul 22 20:11:50.816: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:12:00.816: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:12:00.995: INFO: rc: 1
Jul 22 20:12:00.995: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:12:10.995: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:12:11.145: INFO: rc: 1
Jul 22 20:12:11.145: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:12:21.145: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:12:21.305: INFO: rc: 1
Jul 22 20:12:21.305: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:12:31.305: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:12:31.475: INFO: rc: 1
Jul 22 20:12:31.476: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:12:41.476: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:12:41.631: INFO: rc: 1
Jul 22 20:12:41.631: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:12:51.631: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:12:51.764: INFO: rc: 1
Jul 22 20:12:51.764: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:13:01.765: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:13:01.918: INFO: rc: 1
Jul 22 20:13:01.918: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:13:11.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:13:12.121: INFO: rc: 1
Jul 22 20:13:12.121: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:13:22.121: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:13:22.253: INFO: rc: 1
Jul 22 20:13:22.253: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:13:32.254: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:13:32.388: INFO: rc: 1
Jul 22 20:13:32.388: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:13:42.388: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:13:42.551: INFO: rc: 1
Jul 22 20:13:42.551: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:13:52.551: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:13:52.678: INFO: rc: 1
Jul 22 20:13:52.678: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:14:02.678: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:14:02.817: INFO: rc: 1
Jul 22 20:14:02.817: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:14:12.818: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:14:12.941: INFO: rc: 1
Jul 22 20:14:12.941: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 22 20:14:22.941: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-8660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:14:23.055: INFO: rc: 1
Jul 22 20:14:23.055: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Jul 22 20:14:23.055: INFO: Scaling statefulset ss to 0
Jul 22 20:14:23.117: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 22 20:14:23.128: INFO: Deleting all statefulset in ns statefulset-8660
Jul 22 20:14:23.139: INFO: Scaling statefulset ss to 0
Jul 22 20:14:23.173: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:14:23.185: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:14:23.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8660" for this suite.

• [SLOW TEST:359.546 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":43,"skipped":823,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:14:23.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-khj6b in namespace proxy-1259
I0722 20:14:23.491348    5567 runners.go:190] Created replication controller with name: proxy-service-khj6b, namespace: proxy-1259, replica count: 1
I0722 20:14:24.541755    5567 runners.go:190] proxy-service-khj6b Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 20:14:25.541946    5567 runners.go:190] proxy-service-khj6b Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 20:14:26.542630    5567 runners.go:190] proxy-service-khj6b Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 20:14:27.542834    5567 runners.go:190] proxy-service-khj6b Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 20:14:28.543044    5567 runners.go:190] proxy-service-khj6b Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 20:14:29.543249    5567 runners.go:190] proxy-service-khj6b Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 20:14:30.543469    5567 runners.go:190] proxy-service-khj6b Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 20:14:31.543717    5567 runners.go:190] proxy-service-khj6b Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 20:14:32.543930    5567 runners.go:190] proxy-service-khj6b Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:14:32.556: INFO: setup took 9.100469797s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 22 20:14:32.636: INFO: (0) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 79.312199ms)
Jul 22 20:14:32.636: INFO: (0) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 79.440172ms)
Jul 22 20:14:32.636: INFO: (0) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 79.465833ms)
Jul 22 20:14:32.636: INFO: (0) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 79.69673ms)
Jul 22 20:14:32.636: INFO: (0) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 79.765523ms)
Jul 22 20:14:32.636: INFO: (0) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 79.855616ms)
Jul 22 20:14:32.636: INFO: (0) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 79.859738ms)
Jul 22 20:14:32.640: INFO: (0) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 83.559648ms)
Jul 22 20:14:32.640: INFO: (0) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 83.559587ms)
Jul 22 20:14:32.643: INFO: (0) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 86.468334ms)
Jul 22 20:14:32.643: INFO: (0) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 86.555162ms)
Jul 22 20:14:32.656: INFO: (0) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 99.678274ms)
Jul 22 20:14:32.666: INFO: (0) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 109.647983ms)
Jul 22 20:14:32.666: INFO: (0) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 109.624826ms)
Jul 22 20:14:32.666: INFO: (0) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 109.674272ms)
Jul 22 20:14:32.666: INFO: (0) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 109.699237ms)
Jul 22 20:14:32.697: INFO: (1) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 30.761666ms)
Jul 22 20:14:32.697: INFO: (1) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 30.73922ms)
Jul 22 20:14:32.697: INFO: (1) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 30.928718ms)
Jul 22 20:14:32.697: INFO: (1) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 30.858383ms)
Jul 22 20:14:32.697: INFO: (1) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 30.819577ms)
Jul 22 20:14:32.697: INFO: (1) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 30.95112ms)
Jul 22 20:14:32.697: INFO: (1) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 30.79107ms)
Jul 22 20:14:32.697: INFO: (1) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 30.898298ms)
Jul 22 20:14:32.697: INFO: (1) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 30.814481ms)
Jul 22 20:14:32.703: INFO: (1) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 36.917937ms)
Jul 22 20:14:32.703: INFO: (1) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 36.767686ms)
Jul 22 20:14:32.703: INFO: (1) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 36.816815ms)
Jul 22 20:14:32.715: INFO: (1) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 48.655539ms)
Jul 22 20:14:32.715: INFO: (1) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 48.740841ms)
Jul 22 20:14:32.715: INFO: (1) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 48.670748ms)
Jul 22 20:14:32.720: INFO: (1) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 53.350559ms)
Jul 22 20:14:32.751: INFO: (2) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 31.557691ms)
Jul 22 20:14:32.751: INFO: (2) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 31.676952ms)
Jul 22 20:14:32.751: INFO: (2) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 31.522721ms)
Jul 22 20:14:32.751: INFO: (2) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 31.547512ms)
Jul 22 20:14:32.751: INFO: (2) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 31.699453ms)
Jul 22 20:14:32.751: INFO: (2) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 31.638353ms)
Jul 22 20:14:32.751: INFO: (2) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 31.675349ms)
Jul 22 20:14:32.751: INFO: (2) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 31.548619ms)
Jul 22 20:14:32.751: INFO: (2) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 31.728844ms)
Jul 22 20:14:32.757: INFO: (2) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 37.610923ms)
Jul 22 20:14:32.757: INFO: (2) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 37.575336ms)
Jul 22 20:14:32.757: INFO: (2) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 37.570168ms)
Jul 22 20:14:32.775: INFO: (2) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 54.928876ms)
Jul 22 20:14:32.775: INFO: (2) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 54.814893ms)
Jul 22 20:14:32.775: INFO: (2) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 55.004417ms)
Jul 22 20:14:32.791: INFO: (2) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 71.409387ms)
Jul 22 20:14:32.828: INFO: (3) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 36.78431ms)
Jul 22 20:14:32.828: INFO: (3) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 36.742803ms)
Jul 22 20:14:32.828: INFO: (3) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 36.735159ms)
Jul 22 20:14:32.828: INFO: (3) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 36.755715ms)
Jul 22 20:14:32.828: INFO: (3) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 36.855431ms)
Jul 22 20:14:32.828: INFO: (3) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 36.753617ms)
Jul 22 20:14:32.828: INFO: (3) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 36.936305ms)
Jul 22 20:14:32.842: INFO: (3) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 51.144244ms)
Jul 22 20:14:32.842: INFO: (3) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 51.104321ms)
Jul 22 20:14:32.849: INFO: (3) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 57.565279ms)
Jul 22 20:14:32.849: INFO: (3) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 57.602977ms)
Jul 22 20:14:32.849: INFO: (3) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 57.506301ms)
Jul 22 20:14:32.849: INFO: (3) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 57.542251ms)
Jul 22 20:14:32.860: INFO: (3) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 68.667415ms)
Jul 22 20:14:32.860: INFO: (3) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 68.793117ms)
Jul 22 20:14:32.893: INFO: (3) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 102.010373ms)
Jul 22 20:14:32.925: INFO: (4) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 31.50477ms)
Jul 22 20:14:32.925: INFO: (4) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 31.464049ms)
Jul 22 20:14:32.925: INFO: (4) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 31.607322ms)
Jul 22 20:14:32.925: INFO: (4) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 31.626783ms)
Jul 22 20:14:32.925: INFO: (4) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 31.564453ms)
Jul 22 20:14:32.926: INFO: (4) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 32.075286ms)
Jul 22 20:14:32.926: INFO: (4) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 32.154923ms)
Jul 22 20:14:32.926: INFO: (4) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 32.11509ms)
Jul 22 20:14:32.926: INFO: (4) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 32.081761ms)
Jul 22 20:14:32.926: INFO: (4) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 32.196655ms)
Jul 22 20:14:32.926: INFO: (4) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 32.140431ms)
Jul 22 20:14:32.926: INFO: (4) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 32.578137ms)
Jul 22 20:14:32.946: INFO: (4) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 52.208019ms)
Jul 22 20:14:32.962: INFO: (4) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 68.630784ms)
Jul 22 20:14:32.983: INFO: (4) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 89.861049ms)
Jul 22 20:14:32.983: INFO: (4) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 89.801011ms)
Jul 22 20:14:33.014: INFO: (5) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 30.804714ms)
Jul 22 20:14:33.014: INFO: (5) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 30.874495ms)
Jul 22 20:14:33.015: INFO: (5) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 31.702471ms)
Jul 22 20:14:33.015: INFO: (5) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 31.524399ms)
Jul 22 20:14:33.018: INFO: (5) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 34.530749ms)
Jul 22 20:14:33.018: INFO: (5) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 34.606ms)
Jul 22 20:14:33.018: INFO: (5) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 34.566592ms)
Jul 22 20:14:33.018: INFO: (5) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 34.628035ms)
Jul 22 20:14:33.018: INFO: (5) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 34.693639ms)
Jul 22 20:14:33.037: INFO: (5) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 53.233739ms)
Jul 22 20:14:33.037: INFO: (5) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 53.353986ms)
Jul 22 20:14:33.037: INFO: (5) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 53.313961ms)
Jul 22 20:14:33.037: INFO: (5) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 53.242498ms)
Jul 22 20:14:33.037: INFO: (5) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 53.317458ms)
Jul 22 20:14:33.053: INFO: (5) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 69.350398ms)
Jul 22 20:14:33.053: INFO: (5) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 69.431961ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 33.499395ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 33.486167ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 33.508039ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 33.526145ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 33.744358ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 33.694362ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 33.522898ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 33.563037ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 33.552833ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 33.67934ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 33.618979ms)
Jul 22 20:14:33.087: INFO: (6) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 33.603771ms)
Jul 22 20:14:33.105: INFO: (6) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 51.50783ms)
Jul 22 20:14:33.105: INFO: (6) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 51.588671ms)
Jul 22 20:14:33.105: INFO: (6) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 51.57915ms)
Jul 22 20:14:33.120: INFO: (6) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 67.192635ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 39.511395ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 39.65572ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 39.586463ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 39.591276ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 39.627359ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 39.6384ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 39.589071ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 39.680038ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 39.644881ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 39.607447ms)
Jul 22 20:14:33.160: INFO: (7) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 39.612925ms)
Jul 22 20:14:33.171: INFO: (7) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 50.684106ms)
Jul 22 20:14:33.178: INFO: (7) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 57.624749ms)
Jul 22 20:14:33.178: INFO: (7) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 57.771625ms)
Jul 22 20:14:33.178: INFO: (7) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 57.883757ms)
Jul 22 20:14:33.189: INFO: (7) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 67.957028ms)
Jul 22 20:14:33.230: INFO: (8) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 40.821631ms)
Jul 22 20:14:33.230: INFO: (8) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 40.842928ms)
Jul 22 20:14:33.230: INFO: (8) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 40.913095ms)
Jul 22 20:14:33.230: INFO: (8) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 40.889664ms)
Jul 22 20:14:33.230: INFO: (8) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 41.036181ms)
Jul 22 20:14:33.240: INFO: (8) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 50.71114ms)
Jul 22 20:14:33.240: INFO: (8) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 50.778388ms)
Jul 22 20:14:33.240: INFO: (8) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 50.878253ms)
Jul 22 20:14:33.240: INFO: (8) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 50.794472ms)
Jul 22 20:14:33.240: INFO: (8) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 50.881979ms)
Jul 22 20:14:33.240: INFO: (8) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 50.848099ms)
Jul 22 20:14:33.257: INFO: (8) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 68.13226ms)
Jul 22 20:14:33.257: INFO: (8) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 68.150715ms)
Jul 22 20:14:33.257: INFO: (8) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 68.253129ms)
Jul 22 20:14:33.257: INFO: (8) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 68.099686ms)
Jul 22 20:14:33.301: INFO: (8) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 112.473346ms)
Jul 22 20:14:33.336: INFO: (9) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 34.436706ms)
Jul 22 20:14:33.336: INFO: (9) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 34.491079ms)
Jul 22 20:14:33.336: INFO: (9) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 34.420447ms)
Jul 22 20:14:33.336: INFO: (9) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 34.464949ms)
Jul 22 20:14:33.336: INFO: (9) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 34.465127ms)
Jul 22 20:14:33.352: INFO: (9) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 50.517486ms)
Jul 22 20:14:33.352: INFO: (9) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 50.649918ms)
Jul 22 20:14:33.352: INFO: (9) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 50.618964ms)
Jul 22 20:14:33.352: INFO: (9) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 50.581199ms)
Jul 22 20:14:33.352: INFO: (9) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 50.432421ms)
Jul 22 20:14:33.354: INFO: (9) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 52.581607ms)
Jul 22 20:14:33.354: INFO: (9) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 52.67841ms)
Jul 22 20:14:33.354: INFO: (9) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 52.696507ms)
Jul 22 20:14:33.354: INFO: (9) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 52.627809ms)
Jul 22 20:14:33.369: INFO: (9) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 67.508322ms)
Jul 22 20:14:33.386: INFO: (9) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 84.598061ms)
Jul 22 20:14:33.417: INFO: (10) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 31.066258ms)
Jul 22 20:14:33.417: INFO: (10) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 31.021533ms)
Jul 22 20:14:33.417: INFO: (10) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 31.165913ms)
Jul 22 20:14:33.417: INFO: (10) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 31.102676ms)
Jul 22 20:14:33.417: INFO: (10) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 31.080873ms)
Jul 22 20:14:33.419: INFO: (10) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 32.739458ms)
Jul 22 20:14:33.419: INFO: (10) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 32.787483ms)
Jul 22 20:14:33.439: INFO: (10) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 52.874106ms)
Jul 22 20:14:33.439: INFO: (10) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 53.053417ms)
Jul 22 20:14:33.439: INFO: (10) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 53.002812ms)
Jul 22 20:14:33.439: INFO: (10) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 52.976273ms)
Jul 22 20:14:33.439: INFO: (10) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 52.982436ms)
Jul 22 20:14:33.439: INFO: (10) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 52.908003ms)
Jul 22 20:14:33.439: INFO: (10) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 52.942307ms)
Jul 22 20:14:33.453: INFO: (10) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 67.118185ms)
Jul 22 20:14:33.487: INFO: (10) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 100.608742ms)
Jul 22 20:14:33.520: INFO: (11) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 32.989343ms)
Jul 22 20:14:33.520: INFO: (11) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 32.910276ms)
Jul 22 20:14:33.520: INFO: (11) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 32.93361ms)
Jul 22 20:14:33.520: INFO: (11) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 32.898716ms)
Jul 22 20:14:33.520: INFO: (11) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 33.772384ms)
Jul 22 20:14:33.520: INFO: (11) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 33.737855ms)
Jul 22 20:14:33.520: INFO: (11) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 33.720228ms)
Jul 22 20:14:33.521: INFO: (11) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 33.861043ms)
Jul 22 20:14:33.538: INFO: (11) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 50.830008ms)
Jul 22 20:14:33.538: INFO: (11) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 50.942765ms)
Jul 22 20:14:33.538: INFO: (11) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 50.876014ms)
Jul 22 20:14:33.538: INFO: (11) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 50.871507ms)
Jul 22 20:14:33.555: INFO: (11) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 68.001386ms)
Jul 22 20:14:33.555: INFO: (11) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 68.025265ms)
Jul 22 20:14:33.571: INFO: (11) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 84.240119ms)
Jul 22 20:14:33.571: INFO: (11) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 84.28211ms)
Jul 22 20:14:33.610: INFO: (12) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 39.197353ms)
Jul 22 20:14:33.610: INFO: (12) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 39.268222ms)
Jul 22 20:14:33.611: INFO: (12) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 39.208587ms)
Jul 22 20:14:33.611: INFO: (12) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 39.386476ms)
Jul 22 20:14:33.611: INFO: (12) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 39.267015ms)
Jul 22 20:14:33.611: INFO: (12) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 39.269202ms)
Jul 22 20:14:33.611: INFO: (12) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 39.411922ms)
Jul 22 20:14:33.611: INFO: (12) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 39.761859ms)
Jul 22 20:14:33.611: INFO: (12) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 39.684708ms)
Jul 22 20:14:33.614: INFO: (12) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 42.294988ms)
Jul 22 20:14:33.710: INFO: (12) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 139.190728ms)
Jul 22 20:14:33.710: INFO: (12) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 139.149915ms)
Jul 22 20:14:33.710: INFO: (12) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 139.160907ms)
Jul 22 20:14:33.710: INFO: (12) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 139.281925ms)
Jul 22 20:14:33.710: INFO: (12) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 139.259326ms)
Jul 22 20:14:33.715: INFO: (12) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 143.701965ms)
Jul 22 20:14:33.828: INFO: (13) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 113.485582ms)
Jul 22 20:14:33.828: INFO: (13) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 113.505165ms)
Jul 22 20:14:33.828: INFO: (13) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 113.482129ms)
Jul 22 20:14:33.829: INFO: (13) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 113.57466ms)
Jul 22 20:14:33.829: INFO: (13) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 113.55375ms)
Jul 22 20:14:33.837: INFO: (13) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 122.563049ms)
Jul 22 20:14:33.838: INFO: (13) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 122.5047ms)
Jul 22 20:14:33.838: INFO: (13) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 122.635077ms)
Jul 22 20:14:33.838: INFO: (13) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 122.556578ms)
Jul 22 20:14:33.838: INFO: (13) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 122.550013ms)
Jul 22 20:14:33.838: INFO: (13) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 122.553123ms)
Jul 22 20:14:33.838: INFO: (13) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 122.545929ms)
Jul 22 20:14:33.862: INFO: (13) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 147.304513ms)
Jul 22 20:14:33.862: INFO: (13) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 147.287564ms)
Jul 22 20:14:33.862: INFO: (13) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 147.344405ms)
Jul 22 20:14:33.863: INFO: (13) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 147.547324ms)
Jul 22 20:14:33.893: INFO: (14) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 30.006425ms)
Jul 22 20:14:33.893: INFO: (14) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 29.874521ms)
Jul 22 20:14:33.893: INFO: (14) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 29.958096ms)
Jul 22 20:14:33.910: INFO: (14) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 47.086139ms)
Jul 22 20:14:33.910: INFO: (14) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 47.054719ms)
Jul 22 20:14:33.910: INFO: (14) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 47.203221ms)
Jul 22 20:14:33.910: INFO: (14) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 47.128947ms)
Jul 22 20:14:33.910: INFO: (14) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 47.11981ms)
Jul 22 20:14:33.910: INFO: (14) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 47.116569ms)
Jul 22 20:14:33.910: INFO: (14) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 47.25123ms)
Jul 22 20:14:33.934: INFO: (14) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 71.237262ms)
Jul 22 20:14:33.934: INFO: (14) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 71.312228ms)
Jul 22 20:14:33.934: INFO: (14) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 71.207846ms)
Jul 22 20:14:33.934: INFO: (14) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 71.259184ms)
Jul 22 20:14:33.969: INFO: (14) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 106.452282ms)
Jul 22 20:14:33.969: INFO: (14) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 106.392105ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 32.462858ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 32.564327ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 32.49894ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 32.718499ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 32.822677ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 32.671396ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 32.649465ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 32.763095ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 32.765868ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 32.858631ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 32.762416ms)
Jul 22 20:14:34.002: INFO: (15) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 32.745882ms)
Jul 22 20:14:34.036: INFO: (15) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 66.405238ms)
Jul 22 20:14:34.036: INFO: (15) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 66.448386ms)
Jul 22 20:14:34.036: INFO: (15) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 66.398551ms)
Jul 22 20:14:34.039: INFO: (15) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 70.02282ms)
Jul 22 20:14:34.075: INFO: (16) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 35.277377ms)
Jul 22 20:14:34.075: INFO: (16) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 35.383755ms)
Jul 22 20:14:34.075: INFO: (16) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 35.472144ms)
Jul 22 20:14:34.075: INFO: (16) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 35.574046ms)
Jul 22 20:14:34.075: INFO: (16) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 35.506268ms)
Jul 22 20:14:34.075: INFO: (16) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 35.641579ms)
Jul 22 20:14:34.075: INFO: (16) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 35.459123ms)
Jul 22 20:14:34.075: INFO: (16) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 35.477441ms)
Jul 22 20:14:34.075: INFO: (16) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 35.422703ms)
Jul 22 20:14:34.075: INFO: (16) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 35.485498ms)
Jul 22 20:14:34.083: INFO: (16) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 42.857117ms)
Jul 22 20:14:34.083: INFO: (16) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 43.069663ms)
Jul 22 20:14:34.098: INFO: (16) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 57.986947ms)
Jul 22 20:14:34.098: INFO: (16) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 58.060369ms)
Jul 22 20:14:34.098: INFO: (16) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 58.13199ms)
Jul 22 20:14:34.098: INFO: (16) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 57.962526ms)
Jul 22 20:14:34.134: INFO: (17) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 36.514066ms)
Jul 22 20:14:34.134: INFO: (17) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 36.413684ms)
Jul 22 20:14:34.134: INFO: (17) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 36.440153ms)
Jul 22 20:14:34.143: INFO: (17) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 45.114535ms)
Jul 22 20:14:34.145: INFO: (17) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 47.501576ms)
Jul 22 20:14:34.146: INFO: (17) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 47.750657ms)
Jul 22 20:14:34.146: INFO: (17) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 47.72039ms)
Jul 22 20:14:34.146: INFO: (17) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 47.698131ms)
Jul 22 20:14:34.146: INFO: (17) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 47.819229ms)
Jul 22 20:14:34.146: INFO: (17) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 47.784698ms)
Jul 22 20:14:34.164: INFO: (17) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 65.802884ms)
Jul 22 20:14:34.164: INFO: (17) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 65.850091ms)
Jul 22 20:14:34.164: INFO: (17) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 65.826679ms)
Jul 22 20:14:34.164: INFO: (17) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 65.967123ms)
Jul 22 20:14:34.197: INFO: (17) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 99.146317ms)
Jul 22 20:14:34.197: INFO: (17) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 99.142461ms)
Jul 22 20:14:34.229: INFO: (18) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 31.782556ms)
Jul 22 20:14:34.229: INFO: (18) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 31.688809ms)
Jul 22 20:14:34.229: INFO: (18) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 31.841599ms)
Jul 22 20:14:34.229: INFO: (18) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 31.688243ms)
Jul 22 20:14:34.232: INFO: (18) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 35.317761ms)
Jul 22 20:14:34.232: INFO: (18) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 35.334368ms)
Jul 22 20:14:34.233: INFO: (18) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 35.281314ms)
Jul 22 20:14:34.233: INFO: (18) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 35.29372ms)
Jul 22 20:14:34.233: INFO: (18) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 35.324557ms)
Jul 22 20:14:34.233: INFO: (18) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 35.368862ms)
Jul 22 20:14:34.234: INFO: (18) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 37.311301ms)
Jul 22 20:14:34.249: INFO: (18) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 51.63647ms)
Jul 22 20:14:34.249: INFO: (18) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 51.530519ms)
Jul 22 20:14:34.249: INFO: (18) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 51.540198ms)
Jul 22 20:14:34.265: INFO: (18) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 68.164732ms)
Jul 22 20:14:34.284: INFO: (18) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 86.584339ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 31.745452ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">... (200; 31.764301ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname2/proxy/: bar (200; 31.911594ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8/proxy/rewriteme">test</a> (200; 31.832407ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:443/proxy/tlsrewritem... (200; 31.879622ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:460/proxy/: tls baz (200; 31.935869ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/pods/https:proxy-service-khj6b-rd5q8:462/proxy/: tls qux (200; 31.911919ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname1/proxy/: tls baz (200; 31.901809ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/services/https:proxy-service-khj6b:tlsportname2/proxy/: tls qux (200; 32.058337ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname1/proxy/: foo (200; 32.037459ms)
Jul 22 20:14:34.316: INFO: (19) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/: <a href="/api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:1080/proxy/rewriteme">test<... (200; 32.001643ms)
Jul 22 20:14:34.317: INFO: (19) /api/v1/namespaces/proxy-1259/pods/proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 32.540506ms)
Jul 22 20:14:34.354: INFO: (19) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:162/proxy/: bar (200; 69.662175ms)
Jul 22 20:14:34.354: INFO: (19) /api/v1/namespaces/proxy-1259/services/proxy-service-khj6b:portname1/proxy/: foo (200; 69.522583ms)
Jul 22 20:14:34.354: INFO: (19) /api/v1/namespaces/proxy-1259/pods/http:proxy-service-khj6b-rd5q8:160/proxy/: foo (200; 69.556919ms)
Jul 22 20:14:34.370: INFO: (19) /api/v1/namespaces/proxy-1259/services/http:proxy-service-khj6b:portname2/proxy/: bar (200; 85.637557ms)
STEP: deleting ReplicationController proxy-service-khj6b in namespace proxy-1259, will wait for the garbage collector to delete the pods
Jul 22 20:14:34.446: INFO: Deleting ReplicationController proxy-service-khj6b took: 14.558043ms
Jul 22 20:14:35.247: INFO: Terminating ReplicationController proxy-service-khj6b pods took: 800.279137ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:14:42.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1259" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":44,"skipped":839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:14:42.888: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 20:14:43.175: INFO: Number of nodes with available pods: 0
Jul 22 20:14:43.175: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:14:44.208: INFO: Number of nodes with available pods: 0
Jul 22 20:14:44.208: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:14:45.208: INFO: Number of nodes with available pods: 0
Jul 22 20:14:45.208: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:14:46.209: INFO: Number of nodes with available pods: 2
Jul 22 20:14:46.209: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 22 20:14:46.276: INFO: Number of nodes with available pods: 1
Jul 22 20:14:46.276: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:14:47.310: INFO: Number of nodes with available pods: 1
Jul 22 20:14:47.310: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:14:48.309: INFO: Number of nodes with available pods: 1
Jul 22 20:14:48.309: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:14:49.315: INFO: Number of nodes with available pods: 2
Jul 22 20:14:49.315: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6844, will wait for the garbage collector to delete the pods
Jul 22 20:14:49.413: INFO: Deleting DaemonSet.extensions daemon-set took: 13.632358ms
Jul 22 20:14:49.513: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.234029ms
Jul 22 20:15:02.824: INFO: Number of nodes with available pods: 0
Jul 22 20:15:02.824: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 20:15:02.835: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10358"},"items":null}

Jul 22 20:15:02.846: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10358"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:15:02.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6844" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":45,"skipped":901,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:15:02.915: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:15:03.127: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99834b4f-fb56-4315-b45a-4f678c3fdffc" in namespace "downward-api-4872" to be "Succeeded or Failed"
Jul 22 20:15:03.138: INFO: Pod "downwardapi-volume-99834b4f-fb56-4315-b45a-4f678c3fdffc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.950414ms
Jul 22 20:15:05.150: INFO: Pod "downwardapi-volume-99834b4f-fb56-4315-b45a-4f678c3fdffc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022770638s
Jul 22 20:15:07.163: INFO: Pod "downwardapi-volume-99834b4f-fb56-4315-b45a-4f678c3fdffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035140968s
STEP: Saw pod success
Jul 22 20:15:07.163: INFO: Pod "downwardapi-volume-99834b4f-fb56-4315-b45a-4f678c3fdffc" satisfied condition "Succeeded or Failed"
Jul 22 20:15:07.174: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-99834b4f-fb56-4315-b45a-4f678c3fdffc container client-container: <nil>
STEP: delete the pod
Jul 22 20:15:07.246: INFO: Waiting for pod downwardapi-volume-99834b4f-fb56-4315-b45a-4f678c3fdffc to disappear
Jul 22 20:15:07.257: INFO: Pod downwardapi-volume-99834b4f-fb56-4315-b45a-4f678c3fdffc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:15:07.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4872" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":46,"skipped":915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:15:07.290: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-6642f706-e168-44ab-8019-1a8eea7c785f
STEP: Creating a pod to test consume configMaps
Jul 22 20:15:07.512: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1c655ae-5c8c-4a93-b9c6-f6ef593c60ec" in namespace "configmap-4077" to be "Succeeded or Failed"
Jul 22 20:15:07.534: INFO: Pod "pod-configmaps-e1c655ae-5c8c-4a93-b9c6-f6ef593c60ec": Phase="Pending", Reason="", readiness=false. Elapsed: 21.68687ms
Jul 22 20:15:09.545: INFO: Pod "pod-configmaps-e1c655ae-5c8c-4a93-b9c6-f6ef593c60ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032830502s
Jul 22 20:15:11.557: INFO: Pod "pod-configmaps-e1c655ae-5c8c-4a93-b9c6-f6ef593c60ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044466641s
STEP: Saw pod success
Jul 22 20:15:11.607: INFO: Pod "pod-configmaps-e1c655ae-5c8c-4a93-b9c6-f6ef593c60ec" satisfied condition "Succeeded or Failed"
Jul 22 20:15:11.623: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-configmaps-e1c655ae-5c8c-4a93-b9c6-f6ef593c60ec container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:15:11.689: INFO: Waiting for pod pod-configmaps-e1c655ae-5c8c-4a93-b9c6-f6ef593c60ec to disappear
Jul 22 20:15:11.709: INFO: Pod pod-configmaps-e1c655ae-5c8c-4a93-b9c6-f6ef593c60ec no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:15:11.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4077" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":47,"skipped":952,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:15:11.745: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-663
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-663.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-663.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-663.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-663.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-663.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-663.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:15:16.616: INFO: DNS probes using dns-663/dns-test-e42e5719-46b5-49ed-8045-cdbc6452eeb2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:15:16.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-663" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":48,"skipped":972,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:15:16.667: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 22 20:15:16.905: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 20:16:17.005: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Jul 22 20:16:17.053: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 22 20:16:17.085: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:16:47.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6419" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":49,"skipped":984,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:16:47.321: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:16:47.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-514" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":50,"skipped":987,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:16:47.615: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-0f39b3c6-9d5e-4b44-b2ca-93a373a9f9da in namespace container-probe-1486
Jul 22 20:16:51.843: INFO: Started pod liveness-0f39b3c6-9d5e-4b44-b2ca-93a373a9f9da in namespace container-probe-1486
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:16:51.854: INFO: Initial restart count of pod liveness-0f39b3c6-9d5e-4b44-b2ca-93a373a9f9da is 0
Jul 22 20:17:07.962: INFO: Restart count of pod container-probe-1486/liveness-0f39b3c6-9d5e-4b44-b2ca-93a373a9f9da is now 1 (16.107980005s elapsed)
Jul 22 20:17:28.120: INFO: Restart count of pod container-probe-1486/liveness-0f39b3c6-9d5e-4b44-b2ca-93a373a9f9da is now 2 (36.265975652s elapsed)
Jul 22 20:17:48.248: INFO: Restart count of pod container-probe-1486/liveness-0f39b3c6-9d5e-4b44-b2ca-93a373a9f9da is now 3 (56.393987667s elapsed)
Jul 22 20:18:08.386: INFO: Restart count of pod container-probe-1486/liveness-0f39b3c6-9d5e-4b44-b2ca-93a373a9f9da is now 4 (1m16.531516798s elapsed)
Jul 22 20:19:10.811: INFO: Restart count of pod container-probe-1486/liveness-0f39b3c6-9d5e-4b44-b2ca-93a373a9f9da is now 5 (2m18.95720772s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:19:10.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1486" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":993,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:19:10.874: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-qfdg
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 20:19:11.149: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qfdg" in namespace "subpath-3974" to be "Succeeded or Failed"
Jul 22 20:19:11.161: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.623403ms
Jul 22 20:19:13.173: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023786198s
Jul 22 20:19:15.185: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Running", Reason="", readiness=true. Elapsed: 4.035964188s
Jul 22 20:19:17.197: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Running", Reason="", readiness=true. Elapsed: 6.048080182s
Jul 22 20:19:19.209: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Running", Reason="", readiness=true. Elapsed: 8.060228307s
Jul 22 20:19:21.222: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Running", Reason="", readiness=true. Elapsed: 10.072893253s
Jul 22 20:19:23.235: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Running", Reason="", readiness=true. Elapsed: 12.085408533s
Jul 22 20:19:25.246: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Running", Reason="", readiness=true. Elapsed: 14.097044945s
Jul 22 20:19:27.259: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Running", Reason="", readiness=true. Elapsed: 16.110044345s
Jul 22 20:19:29.271: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Running", Reason="", readiness=true. Elapsed: 18.121746759s
Jul 22 20:19:31.283: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Running", Reason="", readiness=true. Elapsed: 20.133619242s
Jul 22 20:19:33.298: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Running", Reason="", readiness=true. Elapsed: 22.149128365s
Jul 22 20:19:35.311: INFO: Pod "pod-subpath-test-downwardapi-qfdg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.161272891s
STEP: Saw pod success
Jul 22 20:19:35.311: INFO: Pod "pod-subpath-test-downwardapi-qfdg" satisfied condition "Succeeded or Failed"
Jul 22 20:19:35.322: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-subpath-test-downwardapi-qfdg container test-container-subpath-downwardapi-qfdg: <nil>
STEP: delete the pod
Jul 22 20:19:35.388: INFO: Waiting for pod pod-subpath-test-downwardapi-qfdg to disappear
Jul 22 20:19:35.399: INFO: Pod pod-subpath-test-downwardapi-qfdg no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qfdg
Jul 22 20:19:35.399: INFO: Deleting pod "pod-subpath-test-downwardapi-qfdg" in namespace "subpath-3974"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:19:35.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3974" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":52,"skipped":993,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:19:35.454: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9186
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:19:35.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 22 20:19:39.821: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9186 --namespace=crd-publish-openapi-9186 create -f -'
Jul 22 20:19:41.249: INFO: stderr: ""
Jul 22 20:19:41.249: INFO: stdout: "e2e-test-crd-publish-openapi-2883-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 22 20:19:41.249: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9186 --namespace=crd-publish-openapi-9186 delete e2e-test-crd-publish-openapi-2883-crds test-cr'
Jul 22 20:19:41.413: INFO: stderr: ""
Jul 22 20:19:41.413: INFO: stdout: "e2e-test-crd-publish-openapi-2883-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 22 20:19:41.413: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9186 --namespace=crd-publish-openapi-9186 apply -f -'
Jul 22 20:19:41.756: INFO: stderr: ""
Jul 22 20:19:41.756: INFO: stdout: "e2e-test-crd-publish-openapi-2883-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 22 20:19:41.756: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9186 --namespace=crd-publish-openapi-9186 delete e2e-test-crd-publish-openapi-2883-crds test-cr'
Jul 22 20:19:41.935: INFO: stderr: ""
Jul 22 20:19:41.935: INFO: stdout: "e2e-test-crd-publish-openapi-2883-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul 22 20:19:41.935: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9186 explain e2e-test-crd-publish-openapi-2883-crds'
Jul 22 20:19:42.289: INFO: stderr: ""
Jul 22 20:19:42.289: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2883-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:19:46.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9186" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":53,"skipped":993,"failed":0}
SS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:19:46.907: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:19:47.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5068" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":54,"skipped":995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:19:47.141: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6224 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6224;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6224 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6224;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6224.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6224.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6224.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6224.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6224.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6224.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6224.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6224.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6224.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6224.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6224.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6224.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6224.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 212.95.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.95.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.95.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.95.212_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6224 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6224;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6224 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6224;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6224.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6224.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6224.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6224.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6224.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6224.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6224.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6224.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6224.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6224.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6224.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6224.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6224.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 212.95.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.95.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.95.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.95.212_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:19:51.618: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:51.666: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:51.721: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:51.778: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:51.808: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:51.845: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:52.439: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:52.472: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:52.503: INFO: Unable to read jessie_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:52.534: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:52.564: INFO: Unable to read jessie_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:52.595: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:53.195: INFO: Lookups using dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6224 wheezy_tcp@dns-test-service.dns-6224 wheezy_udp@dns-test-service.dns-6224.svc wheezy_tcp@dns-test-service.dns-6224.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6224 jessie_tcp@dns-test-service.dns-6224 jessie_udp@dns-test-service.dns-6224.svc jessie_tcp@dns-test-service.dns-6224.svc]

Jul 22 20:19:58.228: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:58.286: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:58.318: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:58.349: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:58.380: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:58.410: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:59.018: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:59.057: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:59.090: INFO: Unable to read jessie_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:59.121: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:59.182: INFO: Unable to read jessie_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:59.213: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:19:59.812: INFO: Lookups using dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6224 wheezy_tcp@dns-test-service.dns-6224 wheezy_udp@dns-test-service.dns-6224.svc wheezy_tcp@dns-test-service.dns-6224.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6224 jessie_tcp@dns-test-service.dns-6224 jessie_udp@dns-test-service.dns-6224.svc jessie_tcp@dns-test-service.dns-6224.svc]

Jul 22 20:20:03.226: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:03.264: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:03.293: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:03.325: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:03.356: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:03.392: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:04.134: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:04.165: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:04.196: INFO: Unable to read jessie_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:04.227: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:04.257: INFO: Unable to read jessie_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:04.288: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:04.894: INFO: Lookups using dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6224 wheezy_tcp@dns-test-service.dns-6224 wheezy_udp@dns-test-service.dns-6224.svc wheezy_tcp@dns-test-service.dns-6224.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6224 jessie_tcp@dns-test-service.dns-6224 jessie_udp@dns-test-service.dns-6224.svc jessie_tcp@dns-test-service.dns-6224.svc]

Jul 22 20:20:08.227: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:08.287: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:08.317: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:08.351: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:08.381: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:08.412: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:09.031: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:09.065: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:09.103: INFO: Unable to read jessie_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:09.136: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:09.184: INFO: Unable to read jessie_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:09.215: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:09.909: INFO: Lookups using dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6224 wheezy_tcp@dns-test-service.dns-6224 wheezy_udp@dns-test-service.dns-6224.svc wheezy_tcp@dns-test-service.dns-6224.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6224 jessie_tcp@dns-test-service.dns-6224 jessie_udp@dns-test-service.dns-6224.svc jessie_tcp@dns-test-service.dns-6224.svc]

Jul 22 20:20:13.225: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:13.257: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:13.318: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:13.348: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:13.377: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:13.410: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:14.024: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:14.082: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:14.117: INFO: Unable to read jessie_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:14.154: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:14.185: INFO: Unable to read jessie_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:14.215: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:14.816: INFO: Lookups using dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6224 wheezy_tcp@dns-test-service.dns-6224 wheezy_udp@dns-test-service.dns-6224.svc wheezy_tcp@dns-test-service.dns-6224.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6224 jessie_tcp@dns-test-service.dns-6224 jessie_udp@dns-test-service.dns-6224.svc jessie_tcp@dns-test-service.dns-6224.svc]

Jul 22 20:20:18.226: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:18.286: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:18.317: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:18.348: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:18.379: INFO: Unable to read wheezy_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:18.412: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:19.087: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:19.118: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:19.149: INFO: Unable to read jessie_udp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:19.179: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224 from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:19.213: INFO: Unable to read jessie_udp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:19.244: INFO: Unable to read jessie_tcp@dns-test-service.dns-6224.svc from pod dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5: the server could not find the requested resource (get pods dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5)
Jul 22 20:20:19.865: INFO: Lookups using dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6224 wheezy_tcp@dns-test-service.dns-6224 wheezy_udp@dns-test-service.dns-6224.svc wheezy_tcp@dns-test-service.dns-6224.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6224 jessie_tcp@dns-test-service.dns-6224 jessie_udp@dns-test-service.dns-6224.svc jessie_tcp@dns-test-service.dns-6224.svc]

Jul 22 20:20:25.234: INFO: DNS probes using dns-6224/dns-test-bba09d76-b3e2-43fc-a547-29e90222a5f5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:20:25.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6224" for this suite.
•{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":55,"skipped":1019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:20:25.320: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5648
STEP: creating service affinity-clusterip in namespace services-5648
STEP: creating replication controller affinity-clusterip in namespace services-5648
I0722 20:20:25.543327    5567 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-5648, replica count: 3
I0722 20:20:28.593809    5567 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 20:20:31.593997    5567 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:20:31.621: INFO: Creating new exec pod
Jul 22 20:20:36.672: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5648 exec execpod-affinity6k49n -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jul 22 20:20:37.257: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul 22 20:20:37.257: INFO: stdout: ""
Jul 22 20:20:37.257: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5648 exec execpod-affinity6k49n -- /bin/sh -x -c nc -zv -t -w 2 100.70.143.224 80'
Jul 22 20:20:37.780: INFO: stderr: "+ nc -zv -t -w 2 100.70.143.224 80\nConnection to 100.70.143.224 80 port [tcp/http] succeeded!\n"
Jul 22 20:20:37.780: INFO: stdout: ""
Jul 22 20:20:37.780: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5648 exec execpod-affinity6k49n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.70.143.224:80/ ; done'
Jul 22 20:20:38.483: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.143.224:80/\n"
Jul 22 20:20:38.483: INFO: stdout: "\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m\naffinity-clusterip-kgb9m"
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Received response from host: affinity-clusterip-kgb9m
Jul 22 20:20:38.483: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5648, will wait for the garbage collector to delete the pods
Jul 22 20:20:38.579: INFO: Deleting ReplicationController affinity-clusterip took: 15.149487ms
Jul 22 20:20:39.379: INFO: Terminating ReplicationController affinity-clusterip pods took: 800.28324ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:20:52.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5648" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":56,"skipped":1063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:20:52.840: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5852
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-0c937b6f-a544-450c-a64a-89d462795e99
STEP: Creating a pod to test consume configMaps
Jul 22 20:20:53.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0689e930-f258-41e3-b677-e43365ba871f" in namespace "projected-5852" to be "Succeeded or Failed"
Jul 22 20:20:53.086: INFO: Pod "pod-projected-configmaps-0689e930-f258-41e3-b677-e43365ba871f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.36265ms
Jul 22 20:20:55.098: INFO: Pod "pod-projected-configmaps-0689e930-f258-41e3-b677-e43365ba871f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0236944s
Jul 22 20:20:57.110: INFO: Pod "pod-projected-configmaps-0689e930-f258-41e3-b677-e43365ba871f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0358275s
STEP: Saw pod success
Jul 22 20:20:57.110: INFO: Pod "pod-projected-configmaps-0689e930-f258-41e3-b677-e43365ba871f" satisfied condition "Succeeded or Failed"
Jul 22 20:20:57.122: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-configmaps-0689e930-f258-41e3-b677-e43365ba871f container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:20:57.200: INFO: Waiting for pod pod-projected-configmaps-0689e930-f258-41e3-b677-e43365ba871f to disappear
Jul 22 20:20:57.212: INFO: Pod pod-projected-configmaps-0689e930-f258-41e3-b677-e43365ba871f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:20:57.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5852" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":57,"skipped":1092,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:20:57.248: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5440
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul 22 20:20:57.441: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:21:01.509: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:21:18.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5440" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":58,"skipped":1110,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:21:18.219: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3942
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:21:18.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:21:22.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3942" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":1124,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:21:22.581: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 22 20:21:22.829: INFO: Waiting up to 5m0s for pod "pod-c85d8dc4-5518-47d5-be6b-fd15bbacb5ac" in namespace "emptydir-8620" to be "Succeeded or Failed"
Jul 22 20:21:22.840: INFO: Pod "pod-c85d8dc4-5518-47d5-be6b-fd15bbacb5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 11.256679ms
Jul 22 20:21:24.852: INFO: Pod "pod-c85d8dc4-5518-47d5-be6b-fd15bbacb5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023243448s
Jul 22 20:21:26.867: INFO: Pod "pod-c85d8dc4-5518-47d5-be6b-fd15bbacb5ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037797487s
STEP: Saw pod success
Jul 22 20:21:26.867: INFO: Pod "pod-c85d8dc4-5518-47d5-be6b-fd15bbacb5ac" satisfied condition "Succeeded or Failed"
Jul 22 20:21:26.878: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv pod pod-c85d8dc4-5518-47d5-be6b-fd15bbacb5ac container test-container: <nil>
STEP: delete the pod
Jul 22 20:21:26.990: INFO: Waiting for pod pod-c85d8dc4-5518-47d5-be6b-fd15bbacb5ac to disappear
Jul 22 20:21:27.002: INFO: Pod pod-c85d8dc4-5518-47d5-be6b-fd15bbacb5ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:21:27.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8620" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":1129,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:21:27.039: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:21:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9905" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":61,"skipped":1133,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:21:32.512: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1579
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul 22 20:21:32.709: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul 22 20:21:47.865: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:21:52.269: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:22:07.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1579" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":62,"skipped":1164,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:22:07.281: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7774
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:22:18.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7774" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":63,"skipped":1175,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:22:18.609: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5925
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-c819e7a1-935d-48cc-bc68-ee6641a7ddc8
STEP: Creating a pod to test consume configMaps
Jul 22 20:22:18.830: INFO: Waiting up to 5m0s for pod "pod-configmaps-492e0ea7-4cbb-4e31-b0c0-d122d314456a" in namespace "configmap-5925" to be "Succeeded or Failed"
Jul 22 20:22:18.840: INFO: Pod "pod-configmaps-492e0ea7-4cbb-4e31-b0c0-d122d314456a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.552339ms
Jul 22 20:22:20.853: INFO: Pod "pod-configmaps-492e0ea7-4cbb-4e31-b0c0-d122d314456a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023150792s
Jul 22 20:22:22.866: INFO: Pod "pod-configmaps-492e0ea7-4cbb-4e31-b0c0-d122d314456a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035696395s
STEP: Saw pod success
Jul 22 20:22:22.866: INFO: Pod "pod-configmaps-492e0ea7-4cbb-4e31-b0c0-d122d314456a" satisfied condition "Succeeded or Failed"
Jul 22 20:22:22.879: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-configmaps-492e0ea7-4cbb-4e31-b0c0-d122d314456a container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:22:22.965: INFO: Waiting for pod pod-configmaps-492e0ea7-4cbb-4e31-b0c0-d122d314456a to disappear
Jul 22 20:22:22.983: INFO: Pod pod-configmaps-492e0ea7-4cbb-4e31-b0c0-d122d314456a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:22:22.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5925" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":64,"skipped":1182,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:22:23.017: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 22 20:22:31.327: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:22:31.338: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:22:33.339: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:22:33.351: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:22:35.339: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:22:35.351: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:22:35.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9664" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":1183,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:22:35.433: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:22:35.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e1cbe0f-71a6-4c2f-a2b0-f8bc054c6dd3" in namespace "projected-9499" to be "Succeeded or Failed"
Jul 22 20:22:35.655: INFO: Pod "downwardapi-volume-4e1cbe0f-71a6-4c2f-a2b0-f8bc054c6dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.089349ms
Jul 22 20:22:37.668: INFO: Pod "downwardapi-volume-4e1cbe0f-71a6-4c2f-a2b0-f8bc054c6dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023186231s
Jul 22 20:22:39.680: INFO: Pod "downwardapi-volume-4e1cbe0f-71a6-4c2f-a2b0-f8bc054c6dd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035551085s
STEP: Saw pod success
Jul 22 20:22:39.680: INFO: Pod "downwardapi-volume-4e1cbe0f-71a6-4c2f-a2b0-f8bc054c6dd3" satisfied condition "Succeeded or Failed"
Jul 22 20:22:39.691: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv pod downwardapi-volume-4e1cbe0f-71a6-4c2f-a2b0-f8bc054c6dd3 container client-container: <nil>
STEP: delete the pod
Jul 22 20:22:39.770: INFO: Waiting for pod downwardapi-volume-4e1cbe0f-71a6-4c2f-a2b0-f8bc054c6dd3 to disappear
Jul 22 20:22:39.781: INFO: Pod downwardapi-volume-4e1cbe0f-71a6-4c2f-a2b0-f8bc054c6dd3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:22:39.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9499" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":1202,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:22:39.815: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:22:45.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8696" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":67,"skipped":1203,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:22:46.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:22:46.649: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582166, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582166, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582166, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582166, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:22:48.661: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582166, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582166, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582166, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582166, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:22:51.685: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:22:51.703: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3147-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:22:52.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2302" for this suite.
STEP: Destroying namespace "webhook-2302-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":68,"skipped":1203,"failed":0}
SSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:22:52.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7497
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-9bc8c943-e7d1-46ce-b209-d3a361624769
STEP: Creating secret with name secret-projected-all-test-volume-3f30205c-61e6-48de-825e-e55bb6aea600
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 22 20:22:53.157: INFO: Waiting up to 5m0s for pod "projected-volume-6a215d3d-206f-4273-ad86-81b159b93158" in namespace "projected-7497" to be "Succeeded or Failed"
Jul 22 20:22:53.168: INFO: Pod "projected-volume-6a215d3d-206f-4273-ad86-81b159b93158": Phase="Pending", Reason="", readiness=false. Elapsed: 11.131787ms
Jul 22 20:22:55.214: INFO: Pod "projected-volume-6a215d3d-206f-4273-ad86-81b159b93158": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057251878s
Jul 22 20:22:57.227: INFO: Pod "projected-volume-6a215d3d-206f-4273-ad86-81b159b93158": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070341326s
Jul 22 20:22:59.239: INFO: Pod "projected-volume-6a215d3d-206f-4273-ad86-81b159b93158": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08246927s
STEP: Saw pod success
Jul 22 20:22:59.239: INFO: Pod "projected-volume-6a215d3d-206f-4273-ad86-81b159b93158" satisfied condition "Succeeded or Failed"
Jul 22 20:22:59.251: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod projected-volume-6a215d3d-206f-4273-ad86-81b159b93158 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 22 20:22:59.367: INFO: Waiting for pod projected-volume-6a215d3d-206f-4273-ad86-81b159b93158 to disappear
Jul 22 20:22:59.379: INFO: Pod projected-volume-6a215d3d-206f-4273-ad86-81b159b93158 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:22:59.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7497" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":69,"skipped":1207,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:22:59.414: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jul 22 20:22:59.623: INFO: Waiting up to 5m0s for pod "downward-api-7bb5648e-4074-4eed-b8e7-f593f6e08c92" in namespace "downward-api-1828" to be "Succeeded or Failed"
Jul 22 20:22:59.634: INFO: Pod "downward-api-7bb5648e-4074-4eed-b8e7-f593f6e08c92": Phase="Pending", Reason="", readiness=false. Elapsed: 11.194798ms
Jul 22 20:23:01.652: INFO: Pod "downward-api-7bb5648e-4074-4eed-b8e7-f593f6e08c92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028930957s
Jul 22 20:23:03.665: INFO: Pod "downward-api-7bb5648e-4074-4eed-b8e7-f593f6e08c92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041766431s
STEP: Saw pod success
Jul 22 20:23:03.665: INFO: Pod "downward-api-7bb5648e-4074-4eed-b8e7-f593f6e08c92" satisfied condition "Succeeded or Failed"
Jul 22 20:23:03.676: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv pod downward-api-7bb5648e-4074-4eed-b8e7-f593f6e08c92 container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:23:03.745: INFO: Waiting for pod downward-api-7bb5648e-4074-4eed-b8e7-f593f6e08c92 to disappear
Jul 22 20:23:03.756: INFO: Pod downward-api-7bb5648e-4074-4eed-b8e7-f593f6e08c92 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:23:03.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1828" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:23:03.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8272
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jul 22 20:23:08.036: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8272 PodName:pod-sharedvolume-792f290d-3c9d-4275-91b5-b6d7f1a4d87c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:23:08.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:23:08.433: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:23:08.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8272" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":71,"skipped":1281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:23:08.502: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 20:23:11.764: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:23:11.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4584" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":72,"skipped":1343,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:23:11.835: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-222
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 22 20:23:12.123: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 22 20:23:12.144: INFO: starting watch
STEP: patching
STEP: updating
Jul 22 20:23:12.192: INFO: waiting for watch events with expected annotations
Jul 22 20:23:12.192: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:23:12.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-222" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":73,"skipped":1360,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:23:12.343: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1367
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-1367
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1367 to expose endpoints map[]
Jul 22 20:23:12.585: INFO: successfully validated that service endpoint-test2 in namespace services-1367 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1367
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1367 to expose endpoints map[pod1:[80]]
Jul 22 20:23:15.666: INFO: successfully validated that service endpoint-test2 in namespace services-1367 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-1367
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1367 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 22 20:23:18.753: INFO: successfully validated that service endpoint-test2 in namespace services-1367 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-1367
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1367 to expose endpoints map[pod2:[80]]
Jul 22 20:23:18.818: INFO: successfully validated that service endpoint-test2 in namespace services-1367 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-1367
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1367 to expose endpoints map[]
Jul 22 20:23:18.867: INFO: successfully validated that service endpoint-test2 in namespace services-1367 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:23:18.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1367" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":74,"skipped":1364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:23:18.926: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4066
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:23:19.112: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 22 20:23:22.821: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4066 --namespace=crd-publish-openapi-4066 create -f -'
Jul 22 20:23:23.508: INFO: stderr: ""
Jul 22 20:23:23.508: INFO: stdout: "e2e-test-crd-publish-openapi-7020-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 22 20:23:23.508: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4066 --namespace=crd-publish-openapi-4066 delete e2e-test-crd-publish-openapi-7020-crds test-cr'
Jul 22 20:23:23.635: INFO: stderr: ""
Jul 22 20:23:23.635: INFO: stdout: "e2e-test-crd-publish-openapi-7020-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 22 20:23:23.635: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4066 --namespace=crd-publish-openapi-4066 apply -f -'
Jul 22 20:23:23.892: INFO: stderr: ""
Jul 22 20:23:23.892: INFO: stdout: "e2e-test-crd-publish-openapi-7020-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 22 20:23:23.892: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4066 --namespace=crd-publish-openapi-4066 delete e2e-test-crd-publish-openapi-7020-crds test-cr'
Jul 22 20:23:24.008: INFO: stderr: ""
Jul 22 20:23:24.008: INFO: stdout: "e2e-test-crd-publish-openapi-7020-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 22 20:23:24.008: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4066 explain e2e-test-crd-publish-openapi-7020-crds'
Jul 22 20:23:24.229: INFO: stderr: ""
Jul 22 20:23:24.229: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7020-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:23:27.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4066" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":75,"skipped":1390,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:23:27.952: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:23:28.847: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582208, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582208, loc:(*time.Location)(0x7980f00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582208, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582208, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:23:30.861: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582208, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582208, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582208, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582208, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:23:33.881: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:23:34.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-176" for this suite.
STEP: Destroying namespace "webhook-176-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":76,"skipped":1391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:23:34.531: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-8374
STEP: Creating secret with name secret-test-f28237e3-8298-47ab-b4da-f0e6526804a6
STEP: Creating a pod to test consume secrets
Jul 22 20:23:34.956: INFO: Waiting up to 5m0s for pod "pod-secrets-6e8c778e-e4c5-453f-a462-1e23c3b20f13" in namespace "secrets-502" to be "Succeeded or Failed"
Jul 22 20:23:34.967: INFO: Pod "pod-secrets-6e8c778e-e4c5-453f-a462-1e23c3b20f13": Phase="Pending", Reason="", readiness=false. Elapsed: 10.995391ms
Jul 22 20:23:36.981: INFO: Pod "pod-secrets-6e8c778e-e4c5-453f-a462-1e23c3b20f13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024475937s
Jul 22 20:23:38.993: INFO: Pod "pod-secrets-6e8c778e-e4c5-453f-a462-1e23c3b20f13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036629819s
STEP: Saw pod success
Jul 22 20:23:38.993: INFO: Pod "pod-secrets-6e8c778e-e4c5-453f-a462-1e23c3b20f13" satisfied condition "Succeeded or Failed"
Jul 22 20:23:39.004: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv pod pod-secrets-6e8c778e-e4c5-453f-a462-1e23c3b20f13 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:23:39.082: INFO: Waiting for pod pod-secrets-6e8c778e-e4c5-453f-a462-1e23c3b20f13 to disappear
Jul 22 20:23:39.094: INFO: Pod pod-secrets-6e8c778e-e4c5-453f-a462-1e23c3b20f13 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:23:39.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-502" for this suite.
STEP: Destroying namespace "secret-namespace-8374" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1415,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:23:39.142: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-7190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jul 22 20:23:39.335: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 20:24:39.429: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:24:39.441: INFO: Starting informer...
STEP: Starting pods...
Jul 22 20:24:39.493: INFO: Pod1 is running on shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9. Tainting Node
Jul 22 20:24:43.556: INFO: Pod2 is running on shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul 22 20:25:02.755: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 22 20:25:12.758: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:25:12.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7190" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":78,"skipped":1418,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:25:12.831: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul 22 20:25:13.049: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8643  aa476550-fda5-413a-b065-333596aee2f4 14438 0 2021-07-22 20:25:13 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-07-22 20:25:13 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-znftr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-znftr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-znftr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:25:13.060: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:25:15.073: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:25:17.073: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jul 22 20:25:17.073: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8643 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:25:17.073: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Verifying customized DNS server is configured on pod...
Jul 22 20:25:17.592: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8643 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:25:17.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:25:18.024: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:25:18.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8643" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":79,"skipped":1418,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:25:18.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:25:18.281: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f13615c4-c451-4a7f-8afb-ce96742ffaab" in namespace "projected-8468" to be "Succeeded or Failed"
Jul 22 20:25:18.293: INFO: Pod "downwardapi-volume-f13615c4-c451-4a7f-8afb-ce96742ffaab": Phase="Pending", Reason="", readiness=false. Elapsed: 11.589687ms
Jul 22 20:25:20.307: INFO: Pod "downwardapi-volume-f13615c4-c451-4a7f-8afb-ce96742ffaab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025531969s
Jul 22 20:25:22.319: INFO: Pod "downwardapi-volume-f13615c4-c451-4a7f-8afb-ce96742ffaab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038185661s
STEP: Saw pod success
Jul 22 20:25:22.319: INFO: Pod "downwardapi-volume-f13615c4-c451-4a7f-8afb-ce96742ffaab" satisfied condition "Succeeded or Failed"
Jul 22 20:25:22.331: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-f13615c4-c451-4a7f-8afb-ce96742ffaab container client-container: <nil>
STEP: delete the pod
Jul 22 20:25:23.500: INFO: Waiting for pod downwardapi-volume-f13615c4-c451-4a7f-8afb-ce96742ffaab to disappear
Jul 22 20:25:23.511: INFO: Pod downwardapi-volume-f13615c4-c451-4a7f-8afb-ce96742ffaab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:25:23.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8468" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":80,"skipped":1425,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:25:23.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jul 22 20:25:23.733: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:25:29.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-238" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":81,"skipped":1429,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:25:29.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3665
Jul 22 20:25:31.450: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3665 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 22 20:25:32.019: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 22 20:25:32.019: INFO: stdout: "iptables"
Jul 22 20:25:32.019: INFO: proxyMode: iptables
Jul 22 20:25:32.038: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 22 20:25:32.049: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3665
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3665
I0722 20:25:32.090048    5567 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3665, replica count: 3
I0722 20:25:35.140813    5567 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 20:25:38.141004    5567 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:25:38.187: INFO: Creating new exec pod
Jul 22 20:25:43.275: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3665 exec execpod-affinity5p4v4 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jul 22 20:25:43.866: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jul 22 20:25:43.866: INFO: stdout: ""
Jul 22 20:25:43.867: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3665 exec execpod-affinity5p4v4 -- /bin/sh -x -c nc -zv -t -w 2 100.64.53.145 80'
Jul 22 20:25:44.434: INFO: stderr: "+ nc -zv -t -w 2 100.64.53.145 80\nConnection to 100.64.53.145 80 port [tcp/http] succeeded!\n"
Jul 22 20:25:44.434: INFO: stdout: ""
Jul 22 20:25:44.434: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3665 exec execpod-affinity5p4v4 -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.4 32368'
Jul 22 20:25:44.990: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.4 32368\nConnection to 10.250.0.4 32368 port [tcp/32368] succeeded!\n"
Jul 22 20:25:44.990: INFO: stdout: ""
Jul 22 20:25:44.990: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3665 exec execpod-affinity5p4v4 -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.5 32368'
Jul 22 20:25:45.574: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.5 32368\nConnection to 10.250.0.5 32368 port [tcp/32368] succeeded!\n"
Jul 22 20:25:45.574: INFO: stdout: ""
Jul 22 20:25:45.574: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3665 exec execpod-affinity5p4v4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.4:32368/ ; done'
Jul 22 20:25:46.248: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n"
Jul 22 20:25:46.248: INFO: stdout: "\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz\naffinity-nodeport-timeout-vhtwz"
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Received response from host: affinity-nodeport-timeout-vhtwz
Jul 22 20:25:46.248: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3665 exec execpod-affinity5p4v4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.0.4:32368/'
Jul 22 20:25:46.816: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n"
Jul 22 20:25:46.816: INFO: stdout: "affinity-nodeport-timeout-vhtwz"
Jul 22 20:26:06.816: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3665 exec execpod-affinity5p4v4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.0.4:32368/'
Jul 22 20:26:07.418: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.0.4:32368/\n"
Jul 22 20:26:07.418: INFO: stdout: "affinity-nodeport-timeout-n2nwt"
Jul 22 20:26:07.418: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3665, will wait for the garbage collector to delete the pods
Jul 22 20:26:07.518: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 13.26823ms
Jul 22 20:26:08.318: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 800.195343ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:26:22.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3665" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":82,"skipped":1430,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:26:22.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:26:23.128: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 22 20:26:23.154: INFO: Number of nodes with available pods: 0
Jul 22 20:26:23.154: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 22 20:26:23.225: INFO: Number of nodes with available pods: 0
Jul 22 20:26:23.225: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:26:24.238: INFO: Number of nodes with available pods: 0
Jul 22 20:26:24.238: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:26:25.238: INFO: Number of nodes with available pods: 0
Jul 22 20:26:25.238: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:26:26.237: INFO: Number of nodes with available pods: 1
Jul 22 20:26:26.237: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 22 20:26:26.306: INFO: Number of nodes with available pods: 0
Jul 22 20:26:26.306: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 22 20:26:26.330: INFO: Number of nodes with available pods: 0
Jul 22 20:26:26.330: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:26:27.343: INFO: Number of nodes with available pods: 0
Jul 22 20:26:27.343: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:26:28.342: INFO: Number of nodes with available pods: 0
Jul 22 20:26:28.342: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:26:29.342: INFO: Number of nodes with available pods: 0
Jul 22 20:26:29.342: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:26:30.345: INFO: Number of nodes with available pods: 0
Jul 22 20:26:30.345: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:26:31.342: INFO: Number of nodes with available pods: 0
Jul 22 20:26:31.342: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:26:32.347: INFO: Number of nodes with available pods: 0
Jul 22 20:26:32.347: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 20:26:33.342: INFO: Number of nodes with available pods: 1
Jul 22 20:26:33.342: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8261, will wait for the garbage collector to delete the pods
Jul 22 20:26:33.442: INFO: Deleting DaemonSet.extensions daemon-set took: 14.45617ms
Jul 22 20:26:34.242: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.248044ms
Jul 22 20:26:42.854: INFO: Number of nodes with available pods: 0
Jul 22 20:26:42.854: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 20:26:42.865: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15041"},"items":null}

Jul 22 20:26:42.876: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15041"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:26:42.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8261" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":83,"skipped":1441,"failed":0}

------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:26:42.980: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3174
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-3174/configmap-test-2c51dea4-ad7f-424a-8344-481c9441c397
STEP: Creating a pod to test consume configMaps
Jul 22 20:26:43.199: INFO: Waiting up to 5m0s for pod "pod-configmaps-198b4d54-0ace-46d8-901c-2683c3d1d8d4" in namespace "configmap-3174" to be "Succeeded or Failed"
Jul 22 20:26:43.212: INFO: Pod "pod-configmaps-198b4d54-0ace-46d8-901c-2683c3d1d8d4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.768347ms
Jul 22 20:26:45.227: INFO: Pod "pod-configmaps-198b4d54-0ace-46d8-901c-2683c3d1d8d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028614784s
Jul 22 20:26:47.240: INFO: Pod "pod-configmaps-198b4d54-0ace-46d8-901c-2683c3d1d8d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041085366s
STEP: Saw pod success
Jul 22 20:26:47.240: INFO: Pod "pod-configmaps-198b4d54-0ace-46d8-901c-2683c3d1d8d4" satisfied condition "Succeeded or Failed"
Jul 22 20:26:47.251: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-configmaps-198b4d54-0ace-46d8-901c-2683c3d1d8d4 container env-test: <nil>
STEP: delete the pod
Jul 22 20:26:47.321: INFO: Waiting for pod pod-configmaps-198b4d54-0ace-46d8-901c-2683c3d1d8d4 to disappear
Jul 22 20:26:47.333: INFO: Pod pod-configmaps-198b4d54-0ace-46d8-901c-2683c3d1d8d4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:26:47.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3174" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":84,"skipped":1441,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:26:47.368: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1967
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-79055d43-22ee-4181-8dc9-f03a75cdf32d
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-79055d43-22ee-4181-8dc9-f03a75cdf32d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:28:07.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1967" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:28:07.930: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:28:21.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5644" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":86,"skipped":1488,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:28:21.484: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:28:21.689: INFO: Waiting up to 5m0s for pod "downwardapi-volume-406b97ad-eac3-4b9f-b4d9-d548de2aa3b8" in namespace "downward-api-6905" to be "Succeeded or Failed"
Jul 22 20:28:21.702: INFO: Pod "downwardapi-volume-406b97ad-eac3-4b9f-b4d9-d548de2aa3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.05567ms
Jul 22 20:28:23.715: INFO: Pod "downwardapi-volume-406b97ad-eac3-4b9f-b4d9-d548de2aa3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025397905s
Jul 22 20:28:25.727: INFO: Pod "downwardapi-volume-406b97ad-eac3-4b9f-b4d9-d548de2aa3b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03736452s
STEP: Saw pod success
Jul 22 20:28:25.727: INFO: Pod "downwardapi-volume-406b97ad-eac3-4b9f-b4d9-d548de2aa3b8" satisfied condition "Succeeded or Failed"
Jul 22 20:28:25.738: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-406b97ad-eac3-4b9f-b4d9-d548de2aa3b8 container client-container: <nil>
STEP: delete the pod
Jul 22 20:28:25.808: INFO: Waiting for pod downwardapi-volume-406b97ad-eac3-4b9f-b4d9-d548de2aa3b8 to disappear
Jul 22 20:28:25.820: INFO: Pod downwardapi-volume-406b97ad-eac3-4b9f-b4d9-d548de2aa3b8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:28:25.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6905" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":87,"skipped":1488,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:28:25.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:28:26.750: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582506, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582506, loc:(*time.Location)(0x7980f00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582506, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582506, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:28:28.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582506, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582506, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582506, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582506, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:28:31.784: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:28:31.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9884-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:28:33.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9778" for this suite.
STEP: Destroying namespace "webhook-9778-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":88,"skipped":1501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:28:33.166: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4773
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4773
I0722 20:28:33.458719    5567 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4773, replica count: 2
I0722 20:28:36.509150    5567 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:28:39.509: INFO: Creating new exec pod
I0722 20:28:39.509359    5567 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:28:44.584: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4773 exec execpodmhnl5 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 22 20:28:45.113: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 20:28:45.113: INFO: stdout: ""
Jul 22 20:28:45.114: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4773 exec execpodmhnl5 -- /bin/sh -x -c nc -zv -t -w 2 100.67.152.148 80'
Jul 22 20:28:45.671: INFO: stderr: "+ nc -zv -t -w 2 100.67.152.148 80\nConnection to 100.67.152.148 80 port [tcp/http] succeeded!\n"
Jul 22 20:28:45.671: INFO: stdout: ""
Jul 22 20:28:45.671: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4773 exec execpodmhnl5 -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.4 31151'
Jul 22 20:28:46.205: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.4 31151\nConnection to 10.250.0.4 31151 port [tcp/31151] succeeded!\n"
Jul 22 20:28:46.205: INFO: stdout: ""
Jul 22 20:28:46.205: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4773 exec execpodmhnl5 -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.5 31151'
Jul 22 20:28:46.717: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.5 31151\nConnection to 10.250.0.5 31151 port [tcp/31151] succeeded!\n"
Jul 22 20:28:46.717: INFO: stdout: ""
Jul 22 20:28:46.717: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:28:46.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4773" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":89,"skipped":1537,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:28:46.785: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:28:46.977: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:28:51.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9955" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:28:51.265: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-4134
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jul 22 20:28:51.476: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jul 22 20:28:51.498: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 22 20:28:51.498: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jul 22 20:28:51.530: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 22 20:28:51.530: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jul 22 20:28:51.560: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul 22 20:28:51.560: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jul 22 20:28:58.670: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:28:58.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4134" for this suite.
•{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":91,"skipped":1587,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:28:58.723: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:28:58.917: INFO: Creating deployment "test-recreate-deployment"
Jul 22 20:28:58.930: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 22 20:28:58.959: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 22 20:28:58.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582538, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582538, loc:(*time.Location)(0x7980f00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-786dd7c454\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582538, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582538, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:29:00.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582538, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582538, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582538, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582538, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:29:02.983: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 22 20:29:03.007: INFO: Updating deployment test-recreate-deployment
Jul 22 20:29:03.007: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jul 22 20:29:03.117: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8905  89f9170c-256c-4c69-8bd4-e762b015171a 15968 2 2021-07-22 20:28:58 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-22 20:29:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:29:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00516c688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-22 20:29:03 +0000 UTC,LastTransitionTime:2021-07-22 20:29:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-07-22 20:29:03 +0000 UTC,LastTransitionTime:2021-07-22 20:28:58 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 22 20:29:03.129: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-8905  0d3518ea-efb6-48df-a65a-93705a97a2a8 15967 1 2021-07-22 20:29:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 89f9170c-256c-4c69-8bd4-e762b015171a 0xc00516caf0 0xc00516caf1}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:29:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89f9170c-256c-4c69-8bd4-e762b015171a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00516cb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:29:03.129: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 22 20:29:03.129: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-8905  587f1670-f1aa-44ac-b9a9-22d7bbcc60d3 15960 2 2021-07-22 20:28:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 89f9170c-256c-4c69-8bd4-e762b015171a 0xc00516c9f7 0xc00516c9f8}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:29:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89f9170c-256c-4c69-8bd4-e762b015171a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00516ca88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:29:03.141: INFO: Pod "test-recreate-deployment-f79dd4667-zjc6v" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-zjc6v test-recreate-deployment-f79dd4667- deployment-8905  24171956-6ce7-4c95-abff-450bfbdc8bec 15969 0 2021-07-22 20:29:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 0d3518ea-efb6-48df-a65a-93705a97a2a8 0xc00516cf70 0xc00516cf71}] []  [{kube-controller-manager Update v1 2021-07-22 20:29:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0d3518ea-efb6-48df-a65a-93705a97a2a8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:29:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sw2hf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sw2hf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sw2hf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:29:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:29:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:29:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:29:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 20:29:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:29:03.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8905" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":92,"skipped":1606,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:29:03.180: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:29:03.401: INFO: Waiting up to 5m0s for pod "downwardapi-volume-583f9458-658f-4c1d-9505-733dc32f90db" in namespace "projected-8747" to be "Succeeded or Failed"
Jul 22 20:29:03.413: INFO: Pod "downwardapi-volume-583f9458-658f-4c1d-9505-733dc32f90db": Phase="Pending", Reason="", readiness=false. Elapsed: 11.223541ms
Jul 22 20:29:05.424: INFO: Pod "downwardapi-volume-583f9458-658f-4c1d-9505-733dc32f90db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022755172s
Jul 22 20:29:07.437: INFO: Pod "downwardapi-volume-583f9458-658f-4c1d-9505-733dc32f90db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035608521s
STEP: Saw pod success
Jul 22 20:29:07.437: INFO: Pod "downwardapi-volume-583f9458-658f-4c1d-9505-733dc32f90db" satisfied condition "Succeeded or Failed"
Jul 22 20:29:07.449: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-583f9458-658f-4c1d-9505-733dc32f90db container client-container: <nil>
STEP: delete the pod
Jul 22 20:29:07.581: INFO: Waiting for pod downwardapi-volume-583f9458-658f-4c1d-9505-733dc32f90db to disappear
Jul 22 20:29:07.592: INFO: Pod downwardapi-volume-583f9458-658f-4c1d-9505-733dc32f90db no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:29:07.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8747" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":93,"skipped":1606,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:29:07.631: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2864
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-k6lw
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 20:29:07.861: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-k6lw" in namespace "subpath-2864" to be "Succeeded or Failed"
Jul 22 20:29:07.873: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Pending", Reason="", readiness=false. Elapsed: 11.490011ms
Jul 22 20:29:09.885: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023921504s
Jul 22 20:29:11.898: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Running", Reason="", readiness=true. Elapsed: 4.037000912s
Jul 22 20:29:13.911: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Running", Reason="", readiness=true. Elapsed: 6.049085273s
Jul 22 20:29:15.923: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Running", Reason="", readiness=true. Elapsed: 8.061486259s
Jul 22 20:29:17.935: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Running", Reason="", readiness=true. Elapsed: 10.073157371s
Jul 22 20:29:19.947: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Running", Reason="", readiness=true. Elapsed: 12.085853273s
Jul 22 20:29:21.960: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Running", Reason="", readiness=true. Elapsed: 14.098977346s
Jul 22 20:29:23.974: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Running", Reason="", readiness=true. Elapsed: 16.112111175s
Jul 22 20:29:25.986: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Running", Reason="", readiness=true. Elapsed: 18.12474064s
Jul 22 20:29:27.999: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Running", Reason="", readiness=true. Elapsed: 20.137017634s
Jul 22 20:29:30.013: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Running", Reason="", readiness=true. Elapsed: 22.151097148s
Jul 22 20:29:32.026: INFO: Pod "pod-subpath-test-projected-k6lw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.16402403s
STEP: Saw pod success
Jul 22 20:29:32.026: INFO: Pod "pod-subpath-test-projected-k6lw" satisfied condition "Succeeded or Failed"
Jul 22 20:29:32.037: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-subpath-test-projected-k6lw container test-container-subpath-projected-k6lw: <nil>
STEP: delete the pod
Jul 22 20:29:32.109: INFO: Waiting for pod pod-subpath-test-projected-k6lw to disappear
Jul 22 20:29:32.121: INFO: Pod pod-subpath-test-projected-k6lw no longer exists
STEP: Deleting pod pod-subpath-test-projected-k6lw
Jul 22 20:29:32.121: INFO: Deleting pod "pod-subpath-test-projected-k6lw" in namespace "subpath-2864"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:29:32.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2864" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":94,"skipped":1607,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:29:32.175: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Jul 22 20:29:32.375: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8815 cluster-info'
Jul 22 20:29:32.485: INFO: stderr: ""
Jul 22 20:29:32.485: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:29:32.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8815" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":95,"skipped":1610,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:29:32.512: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1309
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-d609f99f-15c2-42f9-b53a-db139491c670
STEP: Creating a pod to test consume secrets
Jul 22 20:29:32.731: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-96aae18a-6136-42d8-a92c-77c43f520e2b" in namespace "projected-1309" to be "Succeeded or Failed"
Jul 22 20:29:32.741: INFO: Pod "pod-projected-secrets-96aae18a-6136-42d8-a92c-77c43f520e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.858835ms
Jul 22 20:29:34.753: INFO: Pod "pod-projected-secrets-96aae18a-6136-42d8-a92c-77c43f520e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022629363s
Jul 22 20:29:36.774: INFO: Pod "pod-projected-secrets-96aae18a-6136-42d8-a92c-77c43f520e2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043389423s
STEP: Saw pod success
Jul 22 20:29:36.774: INFO: Pod "pod-projected-secrets-96aae18a-6136-42d8-a92c-77c43f520e2b" satisfied condition "Succeeded or Failed"
Jul 22 20:29:36.786: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-secrets-96aae18a-6136-42d8-a92c-77c43f520e2b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:29:36.858: INFO: Waiting for pod pod-projected-secrets-96aae18a-6136-42d8-a92c-77c43f520e2b to disappear
Jul 22 20:29:36.869: INFO: Pod pod-projected-secrets-96aae18a-6136-42d8-a92c-77c43f520e2b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:29:36.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1309" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":96,"skipped":1613,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:29:36.904: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Jul 22 20:29:37.097: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3060 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 22 20:29:37.209: INFO: stderr: ""
Jul 22 20:29:37.209: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Jul 22 20:29:37.209: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 22 20:29:37.209: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3060" to be "running and ready, or succeeded"
Jul 22 20:29:37.228: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 18.56509ms
Jul 22 20:29:39.241: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031478388s
Jul 22 20:29:41.256: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.046415423s
Jul 22 20:29:41.256: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 22 20:29:41.256: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul 22 20:29:41.256: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3060 logs logs-generator logs-generator'
Jul 22 20:29:41.415: INFO: stderr: ""
Jul 22 20:29:41.415: INFO: stdout: "I0722 20:29:39.112677       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/ffz 334\nI0722 20:29:39.312816       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/9887 500\nI0722 20:29:39.512803       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/w9g 286\nI0722 20:29:39.712798       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/vsx 231\nI0722 20:29:39.912849       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/zl6h 406\nI0722 20:29:40.113012       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/2444 334\nI0722 20:29:40.312815       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/ndh6 430\nI0722 20:29:40.512820       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/fzh 203\nI0722 20:29:40.712767       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/f92t 566\nI0722 20:29:40.913500       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/m6x 359\nI0722 20:29:41.112817       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/m2q 523\nI0722 20:29:41.312839       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/mpq 387\n"
STEP: limiting log lines
Jul 22 20:29:41.415: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3060 logs logs-generator logs-generator --tail=1'
Jul 22 20:29:41.616: INFO: stderr: ""
Jul 22 20:29:41.616: INFO: stdout: "I0722 20:29:41.512854       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/fj8 324\n"
Jul 22 20:29:41.616: INFO: got output "I0722 20:29:41.512854       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/fj8 324\n"
STEP: limiting log bytes
Jul 22 20:29:41.616: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3060 logs logs-generator logs-generator --limit-bytes=1'
Jul 22 20:29:41.767: INFO: stderr: ""
Jul 22 20:29:41.767: INFO: stdout: "I"
Jul 22 20:29:41.767: INFO: got output "I"
STEP: exposing timestamps
Jul 22 20:29:41.767: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3060 logs logs-generator logs-generator --tail=1 --timestamps'
Jul 22 20:29:41.919: INFO: stderr: ""
Jul 22 20:29:41.919: INFO: stdout: "2021-07-22T20:29:41.712952987Z I0722 20:29:41.712842       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/jswf 347\n"
Jul 22 20:29:41.919: INFO: got output "2021-07-22T20:29:41.712952987Z I0722 20:29:41.712842       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/jswf 347\n"
STEP: restricting to a time range
Jul 22 20:29:44.419: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3060 logs logs-generator logs-generator --since=1s'
Jul 22 20:29:44.574: INFO: stderr: ""
Jul 22 20:29:44.574: INFO: stdout: "I0722 20:29:43.712848       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/wx7d 440\nI0722 20:29:43.912810       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/vmk 523\nI0722 20:29:44.112881       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/bslr 212\nI0722 20:29:44.312849       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/vzc 285\nI0722 20:29:44.512709       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/g9c 272\n"
Jul 22 20:29:44.574: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3060 logs logs-generator logs-generator --since=24h'
Jul 22 20:29:44.731: INFO: stderr: ""
Jul 22 20:29:44.731: INFO: stdout: "I0722 20:29:39.112677       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/ffz 334\nI0722 20:29:39.312816       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/9887 500\nI0722 20:29:39.512803       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/w9g 286\nI0722 20:29:39.712798       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/vsx 231\nI0722 20:29:39.912849       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/zl6h 406\nI0722 20:29:40.113012       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/2444 334\nI0722 20:29:40.312815       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/ndh6 430\nI0722 20:29:40.512820       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/fzh 203\nI0722 20:29:40.712767       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/f92t 566\nI0722 20:29:40.913500       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/m6x 359\nI0722 20:29:41.112817       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/m2q 523\nI0722 20:29:41.312839       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/mpq 387\nI0722 20:29:41.512854       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/fj8 324\nI0722 20:29:41.712842       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/jswf 347\nI0722 20:29:41.912824       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/f54j 412\nI0722 20:29:42.113348       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/r9k 368\nI0722 20:29:42.312851       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/rgg 423\nI0722 20:29:42.512859       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/44j 293\nI0722 20:29:42.712762       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/j7lt 376\nI0722 20:29:42.912751       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/6w4 324\nI0722 20:29:43.112922       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/f2xn 240\nI0722 20:29:43.312947       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/smnt 257\nI0722 20:29:43.512787       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/c5k 521\nI0722 20:29:43.712848       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/wx7d 440\nI0722 20:29:43.912810       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/vmk 523\nI0722 20:29:44.112881       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/bslr 212\nI0722 20:29:44.312849       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/vzc 285\nI0722 20:29:44.512709       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/g9c 272\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Jul 22 20:29:44.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3060 delete pod logs-generator'
Jul 22 20:29:52.750: INFO: stderr: ""
Jul 22 20:29:52.750: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:29:52.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3060" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":97,"skipped":1619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:29:52.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8387
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 22 20:29:53.058: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8387  059811e5-0878-46fa-93a3-8df5bd47e14d 16345 0 2021-07-22 20:29:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:29:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:29:53.058: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8387  059811e5-0878-46fa-93a3-8df5bd47e14d 16346 0 2021-07-22 20:29:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:29:53.058: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8387  059811e5-0878-46fa-93a3-8df5bd47e14d 16347 0 2021-07-22 20:29:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 22 20:30:03.154: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8387  059811e5-0878-46fa-93a3-8df5bd47e14d 16398 0 2021-07-22 20:29:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:30:03.154: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8387  059811e5-0878-46fa-93a3-8df5bd47e14d 16399 0 2021-07-22 20:29:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:30:03.155: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8387  059811e5-0878-46fa-93a3-8df5bd47e14d 16400 0 2021-07-22 20:29:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:30:03.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8387" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":98,"skipped":1659,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:30:03.189: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jul 22 20:30:03.404: INFO: Waiting up to 5m0s for pod "downward-api-a4773976-a756-4854-961b-89ead9a87705" in namespace "downward-api-5534" to be "Succeeded or Failed"
Jul 22 20:30:03.417: INFO: Pod "downward-api-a4773976-a756-4854-961b-89ead9a87705": Phase="Pending", Reason="", readiness=false. Elapsed: 13.54609ms
Jul 22 20:30:05.429: INFO: Pod "downward-api-a4773976-a756-4854-961b-89ead9a87705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025811734s
Jul 22 20:30:07.442: INFO: Pod "downward-api-a4773976-a756-4854-961b-89ead9a87705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038332306s
STEP: Saw pod success
Jul 22 20:30:07.442: INFO: Pod "downward-api-a4773976-a756-4854-961b-89ead9a87705" satisfied condition "Succeeded or Failed"
Jul 22 20:30:07.458: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downward-api-a4773976-a756-4854-961b-89ead9a87705 container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:30:07.540: INFO: Waiting for pod downward-api-a4773976-a756-4854-961b-89ead9a87705 to disappear
Jul 22 20:30:07.552: INFO: Pod downward-api-a4773976-a756-4854-961b-89ead9a87705 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:30:07.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5534" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":99,"skipped":1662,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:30:07.586: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-371ad791-4bc6-4a61-884e-a055f0fc31e5
STEP: Creating a pod to test consume secrets
Jul 22 20:30:07.805: INFO: Waiting up to 5m0s for pod "pod-secrets-22dfb946-8ff9-4eab-93e3-e2d1c2d52b26" in namespace "secrets-6005" to be "Succeeded or Failed"
Jul 22 20:30:07.818: INFO: Pod "pod-secrets-22dfb946-8ff9-4eab-93e3-e2d1c2d52b26": Phase="Pending", Reason="", readiness=false. Elapsed: 12.564346ms
Jul 22 20:30:09.830: INFO: Pod "pod-secrets-22dfb946-8ff9-4eab-93e3-e2d1c2d52b26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024884553s
Jul 22 20:30:11.842: INFO: Pod "pod-secrets-22dfb946-8ff9-4eab-93e3-e2d1c2d52b26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037004299s
STEP: Saw pod success
Jul 22 20:30:11.842: INFO: Pod "pod-secrets-22dfb946-8ff9-4eab-93e3-e2d1c2d52b26" satisfied condition "Succeeded or Failed"
Jul 22 20:30:11.854: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-secrets-22dfb946-8ff9-4eab-93e3-e2d1c2d52b26 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:30:11.924: INFO: Waiting for pod pod-secrets-22dfb946-8ff9-4eab-93e3-e2d1c2d52b26 to disappear
Jul 22 20:30:11.935: INFO: Pod pod-secrets-22dfb946-8ff9-4eab-93e3-e2d1c2d52b26 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:30:11.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6005" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":100,"skipped":1666,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:30:11.973: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9508.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9508.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:30:16.357: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:16.413: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:16.497: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:16.603: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:16.771: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:16.803: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:16.834: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:16.866: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:17.043: INFO: Lookups using dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local]

Jul 22 20:30:22.076: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:22.113: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:22.176: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:22.207: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:22.414: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:22.446: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:22.481: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:22.513: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:22.688: INFO: Lookups using dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local]

Jul 22 20:30:27.079: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:27.111: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:27.153: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:27.186: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:27.393: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:27.425: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:27.461: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:27.495: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:27.666: INFO: Lookups using dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local]

Jul 22 20:30:32.077: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:32.140: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:32.172: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:32.203: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:32.413: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:32.491: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:32.557: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:32.589: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:32.723: INFO: Lookups using dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local]

Jul 22 20:30:37.076: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:37.108: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:37.143: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:37.175: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:37.379: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:37.411: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:37.444: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:37.476: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:37.650: INFO: Lookups using dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local]

Jul 22 20:30:42.081: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:42.120: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:42.154: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:42.187: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:42.400: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:42.467: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:42.501: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:42.533: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local from pod dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a: the server could not find the requested resource (get pods dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a)
Jul 22 20:30:42.715: INFO: Lookups using dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9508.svc.cluster.local jessie_udp@dns-test-service-2.dns-9508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9508.svc.cluster.local]

Jul 22 20:30:47.878: INFO: DNS probes using dns-9508/dns-test-5b2f8acf-b715-4012-8ce3-8bc9d605ab6a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:30:47.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9508" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":101,"skipped":1683,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:30:47.959: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:30:52.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1035" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":1692,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:30:52.228: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:30:52.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582652, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582652, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582652, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582652, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:30:54.925: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582652, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582652, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582652, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582652, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:30:57.948: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:30:58.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3295" for this suite.
STEP: Destroying namespace "webhook-3295-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":103,"skipped":1708,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:30:58.424: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:31:02.650: INFO: Deleting pod "var-expansion-855f8631-6ad6-46e1-941d-90033e478014" in namespace "var-expansion-3427"
Jul 22 20:31:02.664: INFO: Wait up to 5m0s for pod "var-expansion-855f8631-6ad6-46e1-941d-90033e478014" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:31:14.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3427" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":104,"skipped":1720,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:31:14.723: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Jul 22 20:31:14.926: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 create -f -'
Jul 22 20:31:15.233: INFO: stderr: ""
Jul 22 20:31:15.233: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 20:31:15.233: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:31:15.342: INFO: stderr: ""
Jul 22 20:31:15.342: INFO: stdout: "update-demo-nautilus-697s6 update-demo-nautilus-bns28 "
Jul 22 20:31:15.342: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-697s6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:31:15.460: INFO: stderr: ""
Jul 22 20:31:15.460: INFO: stdout: ""
Jul 22 20:31:15.460: INFO: update-demo-nautilus-697s6 is created but not running
Jul 22 20:31:20.460: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:31:20.574: INFO: stderr: ""
Jul 22 20:31:20.574: INFO: stdout: "update-demo-nautilus-697s6 update-demo-nautilus-bns28 "
Jul 22 20:31:20.574: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-697s6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:31:20.676: INFO: stderr: ""
Jul 22 20:31:20.676: INFO: stdout: "true"
Jul 22 20:31:20.676: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-697s6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:31:20.777: INFO: stderr: ""
Jul 22 20:31:20.777: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 20:31:20.777: INFO: validating pod update-demo-nautilus-697s6
Jul 22 20:31:20.854: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:31:20.854: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:31:20.854: INFO: update-demo-nautilus-697s6 is verified up and running
Jul 22 20:31:20.854: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-bns28 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:31:20.953: INFO: stderr: ""
Jul 22 20:31:20.953: INFO: stdout: "true"
Jul 22 20:31:20.954: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-bns28 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:31:21.054: INFO: stderr: ""
Jul 22 20:31:21.054: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 20:31:21.054: INFO: validating pod update-demo-nautilus-bns28
Jul 22 20:31:21.179: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:31:21.179: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:31:21.179: INFO: update-demo-nautilus-bns28 is verified up and running
STEP: scaling down the replication controller
Jul 22 20:31:21.181: INFO: scanned /root for discovery docs: <nil>
Jul 22 20:31:21.181: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jul 22 20:31:21.309: INFO: stderr: ""
Jul 22 20:31:21.309: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 20:31:21.309: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:31:21.414: INFO: stderr: ""
Jul 22 20:31:21.414: INFO: stdout: "update-demo-nautilus-697s6 update-demo-nautilus-bns28 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 22 20:31:26.414: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:31:26.524: INFO: stderr: ""
Jul 22 20:31:26.524: INFO: stdout: "update-demo-nautilus-697s6 update-demo-nautilus-bns28 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 22 20:31:31.525: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:31:31.629: INFO: stderr: ""
Jul 22 20:31:31.629: INFO: stdout: "update-demo-nautilus-697s6 update-demo-nautilus-bns28 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 22 20:31:36.629: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:31:36.739: INFO: stderr: ""
Jul 22 20:31:36.739: INFO: stdout: "update-demo-nautilus-697s6 "
Jul 22 20:31:36.739: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-697s6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:31:36.855: INFO: stderr: ""
Jul 22 20:31:36.855: INFO: stdout: "true"
Jul 22 20:31:36.855: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-697s6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:31:36.954: INFO: stderr: ""
Jul 22 20:31:36.954: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 20:31:36.954: INFO: validating pod update-demo-nautilus-697s6
Jul 22 20:31:37.004: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:31:37.004: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:31:37.004: INFO: update-demo-nautilus-697s6 is verified up and running
STEP: scaling up the replication controller
Jul 22 20:31:37.006: INFO: scanned /root for discovery docs: <nil>
Jul 22 20:31:37.006: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jul 22 20:31:38.161: INFO: stderr: ""
Jul 22 20:31:38.161: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 20:31:38.161: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:31:38.265: INFO: stderr: ""
Jul 22 20:31:38.265: INFO: stdout: "update-demo-nautilus-697s6 update-demo-nautilus-s6c4w "
Jul 22 20:31:38.265: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-697s6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:31:38.363: INFO: stderr: ""
Jul 22 20:31:38.363: INFO: stdout: "true"
Jul 22 20:31:38.363: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-697s6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:31:38.464: INFO: stderr: ""
Jul 22 20:31:38.464: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 20:31:38.464: INFO: validating pod update-demo-nautilus-697s6
Jul 22 20:31:38.555: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:31:38.555: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:31:38.555: INFO: update-demo-nautilus-697s6 is verified up and running
Jul 22 20:31:38.555: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-s6c4w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:31:38.657: INFO: stderr: ""
Jul 22 20:31:38.658: INFO: stdout: ""
Jul 22 20:31:38.658: INFO: update-demo-nautilus-s6c4w is created but not running
Jul 22 20:31:43.658: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:31:43.780: INFO: stderr: ""
Jul 22 20:31:43.780: INFO: stdout: "update-demo-nautilus-697s6 update-demo-nautilus-s6c4w "
Jul 22 20:31:43.780: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-697s6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:31:43.879: INFO: stderr: ""
Jul 22 20:31:43.879: INFO: stdout: "true"
Jul 22 20:31:43.879: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-697s6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:31:43.977: INFO: stderr: ""
Jul 22 20:31:43.977: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 20:31:43.977: INFO: validating pod update-demo-nautilus-697s6
Jul 22 20:31:44.073: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:31:44.073: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:31:44.073: INFO: update-demo-nautilus-697s6 is verified up and running
Jul 22 20:31:44.073: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-s6c4w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:31:44.172: INFO: stderr: ""
Jul 22 20:31:44.172: INFO: stdout: "true"
Jul 22 20:31:44.172: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods update-demo-nautilus-s6c4w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:31:44.289: INFO: stderr: ""
Jul 22 20:31:44.289: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 20:31:44.289: INFO: validating pod update-demo-nautilus-s6c4w
Jul 22 20:31:44.410: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:31:44.410: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:31:44.410: INFO: update-demo-nautilus-s6c4w is verified up and running
STEP: using delete to clean up resources
Jul 22 20:31:44.410: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 delete --grace-period=0 --force -f -'
Jul 22 20:31:44.536: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 20:31:44.536: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 22 20:31:44.536: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get rc,svc -l name=update-demo --no-headers'
Jul 22 20:31:44.653: INFO: stderr: "No resources found in kubectl-7672 namespace.\n"
Jul 22 20:31:44.653: INFO: stdout: ""
Jul 22 20:31:44.653: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 20:31:44.765: INFO: stderr: ""
Jul 22 20:31:44.765: INFO: stdout: "update-demo-nautilus-697s6\nupdate-demo-nautilus-s6c4w\n"
Jul 22 20:31:45.266: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get rc,svc -l name=update-demo --no-headers'
Jul 22 20:31:45.380: INFO: stderr: "No resources found in kubectl-7672 namespace.\n"
Jul 22 20:31:45.380: INFO: stdout: ""
Jul 22 20:31:45.380: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7672 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 20:31:45.491: INFO: stderr: ""
Jul 22 20:31:45.491: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:31:45.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7672" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":105,"skipped":1727,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:31:45.529: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-np87
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 20:31:45.763: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-np87" in namespace "subpath-6662" to be "Succeeded or Failed"
Jul 22 20:31:45.774: INFO: Pod "pod-subpath-test-secret-np87": Phase="Pending", Reason="", readiness=false. Elapsed: 11.208609ms
Jul 22 20:31:47.786: INFO: Pod "pod-subpath-test-secret-np87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023305814s
Jul 22 20:31:49.799: INFO: Pod "pod-subpath-test-secret-np87": Phase="Running", Reason="", readiness=true. Elapsed: 4.035786725s
Jul 22 20:31:51.811: INFO: Pod "pod-subpath-test-secret-np87": Phase="Running", Reason="", readiness=true. Elapsed: 6.048185204s
Jul 22 20:31:53.823: INFO: Pod "pod-subpath-test-secret-np87": Phase="Running", Reason="", readiness=true. Elapsed: 8.060364659s
Jul 22 20:31:55.836: INFO: Pod "pod-subpath-test-secret-np87": Phase="Running", Reason="", readiness=true. Elapsed: 10.072591343s
Jul 22 20:31:57.847: INFO: Pod "pod-subpath-test-secret-np87": Phase="Running", Reason="", readiness=true. Elapsed: 12.084563296s
Jul 22 20:31:59.860: INFO: Pod "pod-subpath-test-secret-np87": Phase="Running", Reason="", readiness=true. Elapsed: 14.097412477s
Jul 22 20:32:01.872: INFO: Pod "pod-subpath-test-secret-np87": Phase="Running", Reason="", readiness=true. Elapsed: 16.10926718s
Jul 22 20:32:03.891: INFO: Pod "pod-subpath-test-secret-np87": Phase="Running", Reason="", readiness=true. Elapsed: 18.128520219s
Jul 22 20:32:05.905: INFO: Pod "pod-subpath-test-secret-np87": Phase="Running", Reason="", readiness=true. Elapsed: 20.141940416s
Jul 22 20:32:07.917: INFO: Pod "pod-subpath-test-secret-np87": Phase="Running", Reason="", readiness=true. Elapsed: 22.153910874s
Jul 22 20:32:09.930: INFO: Pod "pod-subpath-test-secret-np87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.166932622s
STEP: Saw pod success
Jul 22 20:32:09.930: INFO: Pod "pod-subpath-test-secret-np87" satisfied condition "Succeeded or Failed"
Jul 22 20:32:09.941: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-subpath-test-secret-np87 container test-container-subpath-secret-np87: <nil>
STEP: delete the pod
Jul 22 20:32:10.088: INFO: Waiting for pod pod-subpath-test-secret-np87 to disappear
Jul 22 20:32:10.099: INFO: Pod pod-subpath-test-secret-np87 no longer exists
STEP: Deleting pod pod-subpath-test-secret-np87
Jul 22 20:32:10.099: INFO: Deleting pod "pod-subpath-test-secret-np87" in namespace "subpath-6662"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:32:10.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6662" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":106,"skipped":1747,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:32:10.144: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4444
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:32:38.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4444" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":107,"skipped":1749,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:32:38.483: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:32:38.697: INFO: Waiting up to 5m0s for pod "downwardapi-volume-609ff8c3-657f-4903-95ea-9d5a008880eb" in namespace "downward-api-4439" to be "Succeeded or Failed"
Jul 22 20:32:38.708: INFO: Pod "downwardapi-volume-609ff8c3-657f-4903-95ea-9d5a008880eb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.117047ms
Jul 22 20:32:40.721: INFO: Pod "downwardapi-volume-609ff8c3-657f-4903-95ea-9d5a008880eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024134978s
Jul 22 20:32:42.733: INFO: Pod "downwardapi-volume-609ff8c3-657f-4903-95ea-9d5a008880eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036719592s
STEP: Saw pod success
Jul 22 20:32:42.733: INFO: Pod "downwardapi-volume-609ff8c3-657f-4903-95ea-9d5a008880eb" satisfied condition "Succeeded or Failed"
Jul 22 20:32:42.746: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-609ff8c3-657f-4903-95ea-9d5a008880eb container client-container: <nil>
STEP: delete the pod
Jul 22 20:32:42.855: INFO: Waiting for pod downwardapi-volume-609ff8c3-657f-4903-95ea-9d5a008880eb to disappear
Jul 22 20:32:42.866: INFO: Pod downwardapi-volume-609ff8c3-657f-4903-95ea-9d5a008880eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:32:42.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4439" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":108,"skipped":1754,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:32:42.901: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-e1cfb2e5-b2a3-4d66-8b15-1d72c9c0f6bb in namespace container-probe-127
Jul 22 20:32:47.131: INFO: Started pod liveness-e1cfb2e5-b2a3-4d66-8b15-1d72c9c0f6bb in namespace container-probe-127
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:32:47.142: INFO: Initial restart count of pod liveness-e1cfb2e5-b2a3-4d66-8b15-1d72c9c0f6bb is 0
Jul 22 20:33:11.313: INFO: Restart count of pod container-probe-127/liveness-e1cfb2e5-b2a3-4d66-8b15-1d72c9c0f6bb is now 1 (24.170390883s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:33:11.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-127" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":109,"skipped":1767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:33:11.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-1894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:33:11.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1894" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":110,"skipped":1815,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:33:11.689: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:33:11.874: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 22 20:33:11.905: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 20:33:15.928: INFO: Creating deployment "test-rolling-update-deployment"
Jul 22 20:33:15.941: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 22 20:33:15.965: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Jul 22 20:33:17.991: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 22 20:33:18.008: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582795, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582795, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582795, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582795, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:33:20.027: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jul 22 20:33:20.062: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6329  a08ed89c-6900-4f08-884d-564fdc079be8 17702 1 2021-07-22 20:33:15 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-07-22 20:33:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00767d798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-22 20:33:15 +0000 UTC,LastTransitionTime:2021-07-22 20:33:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-07-22 20:33:18 +0000 UTC,LastTransitionTime:2021-07-22 20:33:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 20:33:20.075: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-6329  fa55c0ea-d74b-4fcf-b67b-4ff9228e0110 17695 1 2021-07-22 20:33:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a08ed89c-6900-4f08-884d-564fdc079be8 0xc00767dc37 0xc00767dc38}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a08ed89c-6900-4f08-884d-564fdc079be8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00767dcc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:33:20.075: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 22 20:33:20.075: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6329  ac2ce3e3-7ad0-4f68-bf4b-10f8cbcaf8a7 17701 2 2021-07-22 20:33:11 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a08ed89c-6900-4f08-884d-564fdc079be8 0xc00767db27 0xc00767db28}] []  [{e2e.test Update apps/v1 2021-07-22 20:33:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a08ed89c-6900-4f08-884d-564fdc079be8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00767dbc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:33:20.087: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-8n4r6" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-8n4r6 test-rolling-update-deployment-6b6bf9df46- deployment-6329  45f95717-1570-491a-8f84-9b550ecf73ae 17694 0 2021-07-22 20:33:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:100.96.1.113/32 cni.projectcalico.org/podIPs:100.96.1.113/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 fa55c0ea-d74b-4fcf-b67b-4ff9228e0110 0xc003114117 0xc003114118}] []  [{kube-controller-manager Update v1 2021-07-22 20:33:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55c0ea-d74b-4fcf-b67b-4ff9228e0110\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:33:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n56kx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n56kx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n56kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:33:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:33:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:33:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:33:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.1.113,StartTime:2021-07-22 20:33:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:33:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://be5e55564f5b21a7948035f4070e33ed7045023be673de076e43d6dc2331611a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:33:20.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6329" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":111,"skipped":1828,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:33:20.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-6766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:33:20.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6766" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":112,"skipped":1848,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:33:20.355: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9849
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:33:32.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9849" for this suite.
•{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":113,"skipped":1891,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:33:32.604: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jul 22 20:33:32.791: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 20:33:32.815: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 20:33:32.828: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 before test
Jul 22 20:33:32.852: INFO: fail-once-local-9s6xg from job-9849 started at 2021-07-22 20:33:20 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.852: INFO: 	Container c ready: false, restart count 1
Jul 22 20:33:32.852: INFO: fail-once-local-ffpb4 from job-9849 started at 2021-07-22 20:33:20 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.852: INFO: 	Container c ready: false, restart count 1
Jul 22 20:33:32.852: INFO: fail-once-local-gqp5r from job-9849 started at 2021-07-22 20:33:25 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.852: INFO: 	Container c ready: false, restart count 1
Jul 22 20:33:32.852: INFO: fail-once-local-svftz from job-9849 started at 2021-07-22 20:33:26 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.852: INFO: 	Container c ready: false, restart count 1
Jul 22 20:33:32.852: INFO: apiserver-proxy-b49r5 from kube-system started at 2021-07-22 19:48:59 +0000 UTC (2 container statuses recorded)
Jul 22 20:33:32.852: INFO: 	Container proxy ready: true, restart count 0
Jul 22 20:33:32.852: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 20:33:32.852: INFO: blackbox-exporter-859b5d9c8c-nf6n7 from kube-system started at 2021-07-22 19:55:18 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.852: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul 22 20:33:32.852: INFO: calico-node-jxrzv from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.852: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 20:33:32.852: INFO: kube-proxy-nbdq9 from kube-system started at 2021-07-22 19:51:32 +0000 UTC (2 container statuses recorded)
Jul 22 20:33:32.852: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 20:33:32.852: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:33:32.852: INFO: node-exporter-bxzmq from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.852: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 20:33:32.852: INFO: node-problem-detector-vlbxm from kube-system started at 2021-07-22 20:15:32 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.852: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 20:33:32.852: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv before test
Jul 22 20:33:32.868: INFO: addons-nginx-ingress-controller-5f6b8d6b9b-z5p47 from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 22 20:33:32.868: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-799f5cb4df-cnbrj from kube-system started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul 22 20:33:32.868: INFO: apiserver-proxy-lgksb from kube-system started at 2021-07-22 19:48:53 +0000 UTC (2 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container proxy ready: true, restart count 0
Jul 22 20:33:32.868: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 20:33:32.868: INFO: calico-node-96j6z from kube-system started at 2021-07-22 19:50:28 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 20:33:32.868: INFO: calico-node-vertical-autoscaler-785b5f968-dl6lj from kube-system started at 2021-07-22 19:49:15 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:33:32.868: INFO: calico-typha-deploy-59966cd68c-kk766 from kube-system started at 2021-07-22 19:49:04 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container calico-typha ready: true, restart count 0
Jul 22 20:33:32.868: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-5dvs2 from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:33:32.868: INFO: calico-typha-vertical-autoscaler-5c9655cddd-9t9nc from kube-system started at 2021-07-22 19:49:15 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:33:32.868: INFO: coredns-7589655f7c-8gplg from kube-system started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:33:32.868: INFO: coredns-7589655f7c-cck5m from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:33:32.868: INFO: kube-proxy-r86fj from kube-system started at 2021-07-22 19:51:22 +0000 UTC (2 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 20:33:32.868: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:33:32.868: INFO: metrics-server-7fcbc9df99-qm94n from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 20:33:32.868: INFO: node-exporter-lkd2s from kube-system started at 2021-07-22 19:48:53 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 20:33:32.868: INFO: node-problem-detector-kj6cn from kube-system started at 2021-07-22 20:15:21 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 20:33:32.868: INFO: vpn-shoot-6c79f97679-hz7lk from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul 22 20:33:32.868: INFO: dashboard-metrics-scraper-5fc7d79f9-tk49g from kubernetes-dashboard started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 22 20:33:32.868: INFO: kubernetes-dashboard-775d7d55c5-q9hcp from kubernetes-dashboard started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:33:32.868: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-41da547a-07ed-4865-a371-478b4de9eefa 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.0.4 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-41da547a-07ed-4865-a371-478b4de9eefa off the node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-41da547a-07ed-4865-a371-478b4de9eefa
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:38:41.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4844" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:308.567 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":114,"skipped":1891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:38:41.171: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2291
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 22 20:38:41.418: INFO: Waiting up to 5m0s for pod "pod-81e1137a-0801-45b5-99bc-117b1a3ebe65" in namespace "emptydir-2291" to be "Succeeded or Failed"
Jul 22 20:38:41.430: INFO: Pod "pod-81e1137a-0801-45b5-99bc-117b1a3ebe65": Phase="Pending", Reason="", readiness=false. Elapsed: 11.761409ms
Jul 22 20:38:43.443: INFO: Pod "pod-81e1137a-0801-45b5-99bc-117b1a3ebe65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024386928s
Jul 22 20:38:45.466: INFO: Pod "pod-81e1137a-0801-45b5-99bc-117b1a3ebe65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047821236s
STEP: Saw pod success
Jul 22 20:38:45.466: INFO: Pod "pod-81e1137a-0801-45b5-99bc-117b1a3ebe65" satisfied condition "Succeeded or Failed"
Jul 22 20:38:45.478: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-81e1137a-0801-45b5-99bc-117b1a3ebe65 container test-container: <nil>
STEP: delete the pod
Jul 22 20:38:45.600: INFO: Waiting for pod pod-81e1137a-0801-45b5-99bc-117b1a3ebe65 to disappear
Jul 22 20:38:45.612: INFO: Pod pod-81e1137a-0801-45b5-99bc-117b1a3ebe65 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:38:45.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2291" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":115,"skipped":1926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:38:45.671: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-ad83a341-f24f-4e46-850c-7209535f6494
STEP: Creating a pod to test consume secrets
Jul 22 20:38:45.899: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e1fd4d38-8d47-4ab9-b225-9223a2e99d5e" in namespace "projected-442" to be "Succeeded or Failed"
Jul 22 20:38:45.911: INFO: Pod "pod-projected-secrets-e1fd4d38-8d47-4ab9-b225-9223a2e99d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.829727ms
Jul 22 20:38:47.927: INFO: Pod "pod-projected-secrets-e1fd4d38-8d47-4ab9-b225-9223a2e99d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027005245s
Jul 22 20:38:49.939: INFO: Pod "pod-projected-secrets-e1fd4d38-8d47-4ab9-b225-9223a2e99d5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039190466s
STEP: Saw pod success
Jul 22 20:38:49.939: INFO: Pod "pod-projected-secrets-e1fd4d38-8d47-4ab9-b225-9223a2e99d5e" satisfied condition "Succeeded or Failed"
Jul 22 20:38:49.950: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-secrets-e1fd4d38-8d47-4ab9-b225-9223a2e99d5e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:38:50.028: INFO: Waiting for pod pod-projected-secrets-e1fd4d38-8d47-4ab9-b225-9223a2e99d5e to disappear
Jul 22 20:38:50.040: INFO: Pod pod-projected-secrets-e1fd4d38-8d47-4ab9-b225-9223a2e99d5e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:38:50.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-442" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":2011,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:38:50.075: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5112
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:38:50.311: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 22 20:38:54.884: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5112 --namespace=crd-publish-openapi-5112 create -f -'
Jul 22 20:38:55.821: INFO: stderr: ""
Jul 22 20:38:55.821: INFO: stdout: "e2e-test-crd-publish-openapi-8348-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 22 20:38:55.821: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5112 --namespace=crd-publish-openapi-5112 delete e2e-test-crd-publish-openapi-8348-crds test-cr'
Jul 22 20:38:56.010: INFO: stderr: ""
Jul 22 20:38:56.010: INFO: stdout: "e2e-test-crd-publish-openapi-8348-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 22 20:38:56.010: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5112 --namespace=crd-publish-openapi-5112 apply -f -'
Jul 22 20:38:56.346: INFO: stderr: ""
Jul 22 20:38:56.346: INFO: stdout: "e2e-test-crd-publish-openapi-8348-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 22 20:38:56.346: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5112 --namespace=crd-publish-openapi-5112 delete e2e-test-crd-publish-openapi-8348-crds test-cr'
Jul 22 20:38:56.512: INFO: stderr: ""
Jul 22 20:38:56.512: INFO: stdout: "e2e-test-crd-publish-openapi-8348-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 22 20:38:56.512: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5112 explain e2e-test-crd-publish-openapi-8348-crds'
Jul 22 20:38:56.817: INFO: stderr: ""
Jul 22 20:38:56.817: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8348-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:39:00.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5112" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":117,"skipped":2016,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:39:00.919: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:39:05.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6478" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":2018,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:39:05.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:39:05.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1319" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":2025,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:39:05.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-3575d39a-6f20-4f84-9016-4532d93084a3
STEP: Creating a pod to test consume secrets
Jul 22 20:39:05.949: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b37178ae-0c93-4c85-9907-f2c1f734ced3" in namespace "projected-6879" to be "Succeeded or Failed"
Jul 22 20:39:05.962: INFO: Pod "pod-projected-secrets-b37178ae-0c93-4c85-9907-f2c1f734ced3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.086059ms
Jul 22 20:39:07.975: INFO: Pod "pod-projected-secrets-b37178ae-0c93-4c85-9907-f2c1f734ced3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025501965s
Jul 22 20:39:09.988: INFO: Pod "pod-projected-secrets-b37178ae-0c93-4c85-9907-f2c1f734ced3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038196813s
STEP: Saw pod success
Jul 22 20:39:09.988: INFO: Pod "pod-projected-secrets-b37178ae-0c93-4c85-9907-f2c1f734ced3" satisfied condition "Succeeded or Failed"
Jul 22 20:39:10.000: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-secrets-b37178ae-0c93-4c85-9907-f2c1f734ced3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:39:10.073: INFO: Waiting for pod pod-projected-secrets-b37178ae-0c93-4c85-9907-f2c1f734ced3 to disappear
Jul 22 20:39:10.085: INFO: Pod pod-projected-secrets-b37178ae-0c93-4c85-9907-f2c1f734ced3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:39:10.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6879" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":2034,"failed":0}
SSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:39:10.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-3911
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jul 22 20:39:10.398: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jul 22 20:39:10.486: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:39:10.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3911" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":121,"skipped":2037,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:39:10.579: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3794
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-81d258a8-6921-453c-84ac-73b117ac3649
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-81d258a8-6921-453c-84ac-73b117ac3649
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:39:17.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3794" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2038,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:39:17.045: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3111
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3111
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3111
STEP: creating replication controller externalsvc in namespace services-3111
I0722 20:39:17.293478    5567 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3111, replica count: 2
I0722 20:39:20.343856    5567 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul 22 20:39:20.387: INFO: Creating new exec pod
Jul 22 20:39:24.427: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3111 exec execpodprx2w -- /bin/sh -x -c nslookup nodeport-service.services-3111.svc.cluster.local'
Jul 22 20:39:25.090: INFO: stderr: "+ nslookup nodeport-service.services-3111.svc.cluster.local\n"
Jul 22 20:39:25.090: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nnodeport-service.services-3111.svc.cluster.local\tcanonical name = externalsvc.services-3111.svc.cluster.local.\nName:\texternalsvc.services-3111.svc.cluster.local\nAddress: 100.68.235.243\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3111, will wait for the garbage collector to delete the pods
Jul 22 20:39:25.167: INFO: Deleting ReplicationController externalsvc took: 14.113518ms
Jul 22 20:39:25.267: INFO: Terminating ReplicationController externalsvc pods took: 100.241695ms
Jul 22 20:39:32.787: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:39:32.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3111" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":123,"skipped":2071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:39:32.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4590
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:39:50.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4590" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":124,"skipped":2173,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:39:50.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4243
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-185d0ac6-1d1b-4a10-9245-c157a1787a29
STEP: Creating configMap with name cm-test-opt-upd-2cd84d6c-d7c8-4a7f-9958-fd961da5526c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-185d0ac6-1d1b-4a10-9245-c157a1787a29
STEP: Updating configmap cm-test-opt-upd-2cd84d6c-d7c8-4a7f-9958-fd961da5526c
STEP: Creating configMap with name cm-test-opt-create-de308f6a-352f-4adf-92a6-ee2c5d9ef83b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:41:23.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4243" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":125,"skipped":2190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:41:23.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4839
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:41:27.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4839" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":126,"skipped":2237,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:41:28.023: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-1586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 22 20:41:29.384: INFO: starting watch
STEP: patching
STEP: updating
Jul 22 20:41:29.422: INFO: waiting for watch events with expected annotations
Jul 22 20:41:29.422: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:41:29.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1586" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":127,"skipped":2256,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:41:29.610: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Jul 22 20:41:29.817: INFO: created test-pod-1
Jul 22 20:41:29.836: INFO: created test-pod-2
Jul 22 20:41:29.857: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:41:29.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2963" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":128,"skipped":2273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:41:29.939: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4007
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:41:30.128: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul 22 20:41:34.066: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 --namespace=crd-publish-openapi-4007 create -f -'
Jul 22 20:41:35.433: INFO: stderr: ""
Jul 22 20:41:35.433: INFO: stdout: "e2e-test-crd-publish-openapi-7523-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 22 20:41:35.434: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 --namespace=crd-publish-openapi-4007 delete e2e-test-crd-publish-openapi-7523-crds test-foo'
Jul 22 20:41:35.715: INFO: stderr: ""
Jul 22 20:41:35.715: INFO: stdout: "e2e-test-crd-publish-openapi-7523-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 22 20:41:35.715: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 --namespace=crd-publish-openapi-4007 apply -f -'
Jul 22 20:41:36.027: INFO: stderr: ""
Jul 22 20:41:36.027: INFO: stdout: "e2e-test-crd-publish-openapi-7523-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 22 20:41:36.027: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 --namespace=crd-publish-openapi-4007 delete e2e-test-crd-publish-openapi-7523-crds test-foo'
Jul 22 20:41:36.157: INFO: stderr: ""
Jul 22 20:41:36.158: INFO: stdout: "e2e-test-crd-publish-openapi-7523-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul 22 20:41:36.158: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 --namespace=crd-publish-openapi-4007 create -f -'
Jul 22 20:41:36.411: INFO: rc: 1
Jul 22 20:41:36.411: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 --namespace=crd-publish-openapi-4007 apply -f -'
Jul 22 20:41:36.668: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul 22 20:41:36.668: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 --namespace=crd-publish-openapi-4007 create -f -'
Jul 22 20:41:36.914: INFO: rc: 1
Jul 22 20:41:36.914: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 --namespace=crd-publish-openapi-4007 apply -f -'
Jul 22 20:41:37.166: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul 22 20:41:37.166: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 explain e2e-test-crd-publish-openapi-7523-crds'
Jul 22 20:41:37.412: INFO: stderr: ""
Jul 22 20:41:37.413: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7523-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul 22 20:41:37.413: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 explain e2e-test-crd-publish-openapi-7523-crds.metadata'
Jul 22 20:41:37.699: INFO: stderr: ""
Jul 22 20:41:37.699: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7523-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 22 20:41:37.699: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 explain e2e-test-crd-publish-openapi-7523-crds.spec'
Jul 22 20:41:37.993: INFO: stderr: ""
Jul 22 20:41:37.993: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7523-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 22 20:41:37.993: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 explain e2e-test-crd-publish-openapi-7523-crds.spec.bars'
Jul 22 20:41:38.276: INFO: stderr: ""
Jul 22 20:41:38.276: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7523-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul 22 20:41:38.276: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4007 explain e2e-test-crd-publish-openapi-7523-crds.spec.bars2'
Jul 22 20:41:38.703: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:41:43.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4007" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":129,"skipped":2332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:41:43.340: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:41:43.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8583" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":130,"skipped":2357,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:41:43.766: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5943
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jul 22 20:41:48.640: INFO: Successfully updated pod "annotationupdate4935e50e-f31c-4dde-80aa-7d72154b27dd"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:41:50.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5943" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:41:50.815: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 20:41:55.115: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:41:55.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3650" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":132,"skipped":2394,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:41:55.187: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:41:55.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7482" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":133,"skipped":2411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:41:55.443: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:41:55.655: INFO: Waiting up to 5m0s for pod "downwardapi-volume-733fb63b-035f-4f13-acf0-9fa379024530" in namespace "downward-api-1752" to be "Succeeded or Failed"
Jul 22 20:41:55.667: INFO: Pod "downwardapi-volume-733fb63b-035f-4f13-acf0-9fa379024530": Phase="Pending", Reason="", readiness=false. Elapsed: 11.547966ms
Jul 22 20:41:57.679: INFO: Pod "downwardapi-volume-733fb63b-035f-4f13-acf0-9fa379024530": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0239807s
Jul 22 20:41:59.693: INFO: Pod "downwardapi-volume-733fb63b-035f-4f13-acf0-9fa379024530": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038095162s
Jul 22 20:42:01.706: INFO: Pod "downwardapi-volume-733fb63b-035f-4f13-acf0-9fa379024530": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050505245s
STEP: Saw pod success
Jul 22 20:42:01.706: INFO: Pod "downwardapi-volume-733fb63b-035f-4f13-acf0-9fa379024530" satisfied condition "Succeeded or Failed"
Jul 22 20:42:01.718: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-733fb63b-035f-4f13-acf0-9fa379024530 container client-container: <nil>
STEP: delete the pod
Jul 22 20:42:01.810: INFO: Waiting for pod downwardapi-volume-733fb63b-035f-4f13-acf0-9fa379024530 to disappear
Jul 22 20:42:01.821: INFO: Pod downwardapi-volume-733fb63b-035f-4f13-acf0-9fa379024530 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:42:01.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1752" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2436,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:42:01.856: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9546
STEP: creating service affinity-nodeport in namespace services-9546
STEP: creating replication controller affinity-nodeport in namespace services-9546
I0722 20:42:02.083179    5567 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-9546, replica count: 3
I0722 20:42:05.133622    5567 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 20:42:08.133831    5567 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:42:08.180: INFO: Creating new exec pod
Jul 22 20:42:15.356: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9546 exec execpod-affinityfrr87 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jul 22 20:42:15.918: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul 22 20:42:15.918: INFO: stdout: ""
Jul 22 20:42:15.918: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9546 exec execpod-affinityfrr87 -- /bin/sh -x -c nc -zv -t -w 2 100.65.229.156 80'
Jul 22 20:42:16.422: INFO: stderr: "+ nc -zv -t -w 2 100.65.229.156 80\nConnection to 100.65.229.156 80 port [tcp/http] succeeded!\n"
Jul 22 20:42:16.422: INFO: stdout: ""
Jul 22 20:42:16.422: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9546 exec execpod-affinityfrr87 -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.4 30614'
Jul 22 20:42:16.976: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.4 30614\nConnection to 10.250.0.4 30614 port [tcp/30614] succeeded!\n"
Jul 22 20:42:16.976: INFO: stdout: ""
Jul 22 20:42:16.976: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9546 exec execpod-affinityfrr87 -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.5 30614'
Jul 22 20:42:17.640: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.5 30614\nConnection to 10.250.0.5 30614 port [tcp/30614] succeeded!\n"
Jul 22 20:42:17.640: INFO: stdout: ""
Jul 22 20:42:17.640: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9546 exec execpod-affinityfrr87 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.4:30614/ ; done'
Jul 22 20:42:18.181: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:30614/\n"
Jul 22 20:42:18.182: INFO: stdout: "\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj\naffinity-nodeport-8j7fj"
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Received response from host: affinity-nodeport-8j7fj
Jul 22 20:42:18.182: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9546, will wait for the garbage collector to delete the pods
Jul 22 20:42:18.277: INFO: Deleting ReplicationController affinity-nodeport took: 15.255232ms
Jul 22 20:42:18.378: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.262676ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:42:32.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9546" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":135,"skipped":2440,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:42:32.946: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5042
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Jul 22 20:42:33.717: INFO: created pod pod-service-account-defaultsa
Jul 22 20:42:33.717: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 22 20:42:33.733: INFO: created pod pod-service-account-mountsa
Jul 22 20:42:33.733: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 22 20:42:33.749: INFO: created pod pod-service-account-nomountsa
Jul 22 20:42:33.749: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 22 20:42:33.765: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 22 20:42:33.765: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 22 20:42:33.786: INFO: created pod pod-service-account-mountsa-mountspec
Jul 22 20:42:33.786: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 22 20:42:33.803: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 22 20:42:33.803: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 22 20:42:33.818: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 22 20:42:33.818: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 22 20:42:33.834: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 22 20:42:33.834: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 22 20:42:33.855: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 22 20:42:33.855: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:42:33.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5042" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":136,"skipped":2460,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:42:33.896: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:42:34.655: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 22 20:42:36.691: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:42:38.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:42:40.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:42:42.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583354, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:42:45.722: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:42:45.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:42:47.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7481" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":137,"skipped":2463,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:42:47.231: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Jul 22 20:42:47.517: INFO: Waiting up to 5m0s for pod "var-expansion-a0422f4d-cd9d-4909-96e9-f83905fc730f" in namespace "var-expansion-6165" to be "Succeeded or Failed"
Jul 22 20:42:47.529: INFO: Pod "var-expansion-a0422f4d-cd9d-4909-96e9-f83905fc730f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.786308ms
Jul 22 20:42:49.541: INFO: Pod "var-expansion-a0422f4d-cd9d-4909-96e9-f83905fc730f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023824443s
Jul 22 20:42:51.554: INFO: Pod "var-expansion-a0422f4d-cd9d-4909-96e9-f83905fc730f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036413158s
STEP: Saw pod success
Jul 22 20:42:51.554: INFO: Pod "var-expansion-a0422f4d-cd9d-4909-96e9-f83905fc730f" satisfied condition "Succeeded or Failed"
Jul 22 20:42:51.565: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod var-expansion-a0422f4d-cd9d-4909-96e9-f83905fc730f container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:42:51.634: INFO: Waiting for pod var-expansion-a0422f4d-cd9d-4909-96e9-f83905fc730f to disappear
Jul 22 20:42:51.645: INFO: Pod var-expansion-a0422f4d-cd9d-4909-96e9-f83905fc730f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:42:51.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6165" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":138,"skipped":2471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:42:51.680: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3441
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-d1da775f-37a7-4650-8f47-7770356894b5
STEP: Creating a pod to test consume configMaps
Jul 22 20:42:51.906: INFO: Waiting up to 5m0s for pod "pod-configmaps-5cf45190-a8c7-490e-85e2-a121c14bb805" in namespace "configmap-3441" to be "Succeeded or Failed"
Jul 22 20:42:51.921: INFO: Pod "pod-configmaps-5cf45190-a8c7-490e-85e2-a121c14bb805": Phase="Pending", Reason="", readiness=false. Elapsed: 14.298375ms
Jul 22 20:42:53.933: INFO: Pod "pod-configmaps-5cf45190-a8c7-490e-85e2-a121c14bb805": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02646152s
Jul 22 20:42:55.945: INFO: Pod "pod-configmaps-5cf45190-a8c7-490e-85e2-a121c14bb805": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038898293s
STEP: Saw pod success
Jul 22 20:42:55.945: INFO: Pod "pod-configmaps-5cf45190-a8c7-490e-85e2-a121c14bb805" satisfied condition "Succeeded or Failed"
Jul 22 20:42:55.957: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-configmaps-5cf45190-a8c7-490e-85e2-a121c14bb805 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 20:42:56.067: INFO: Waiting for pod pod-configmaps-5cf45190-a8c7-490e-85e2-a121c14bb805 to disappear
Jul 22 20:42:56.089: INFO: Pod pod-configmaps-5cf45190-a8c7-490e-85e2-a121c14bb805 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:42:56.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3441" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":139,"skipped":2505,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:42:56.124: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3216
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3216
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3216
Jul 22 20:42:56.365: INFO: Found 0 stateful pods, waiting for 1
Jul 22 20:43:06.379: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 22 20:43:06.392: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 20:43:06.936: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 20:43:06.936: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 20:43:06.936: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 20:43:06.949: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 22 20:43:16.961: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 20:43:16.961: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:43:17.013: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999959s
Jul 22 20:43:18.027: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988000626s
Jul 22 20:43:19.040: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.974414477s
Jul 22 20:43:20.052: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.961406554s
Jul 22 20:43:21.065: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.948878654s
Jul 22 20:43:22.077: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.936290614s
Jul 22 20:43:23.092: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.924483065s
Jul 22 20:43:24.104: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.909636621s
Jul 22 20:43:25.119: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.896837819s
Jul 22 20:43:26.136: INFO: Verifying statefulset ss doesn't scale past 1 for another 882.280721ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3216
Jul 22 20:43:27.148: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:43:27.653: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 20:43:27.653: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 20:43:27.653: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 20:43:27.665: INFO: Found 1 stateful pods, waiting for 3
Jul 22 20:43:37.678: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:43:37.679: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:43:37.679: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 22 20:43:37.702: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 20:43:38.215: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 20:43:38.215: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 20:43:38.215: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 20:43:38.215: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 20:43:38.777: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 20:43:38.777: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 20:43:38.777: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 20:43:38.777: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 20:43:39.348: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 20:43:39.348: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 20:43:39.348: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 20:43:39.348: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:43:39.366: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 22 20:43:49.392: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 20:43:49.392: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 20:43:49.392: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 20:43:49.428: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999633s
Jul 22 20:43:50.446: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988169472s
Jul 22 20:43:51.459: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.969872476s
Jul 22 20:43:52.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.957424404s
Jul 22 20:43:53.483: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945249922s
Jul 22 20:43:54.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.932916352s
Jul 22 20:43:55.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.910798204s
Jul 22 20:43:56.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.897007291s
Jul 22 20:43:57.546: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.8838704s
Jul 22 20:43:58.559: INFO: Verifying statefulset ss doesn't scale past 3 for another 870.036178ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3216
Jul 22 20:43:59.572: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:44:00.114: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 20:44:00.114: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 20:44:00.114: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 20:44:00.114: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:44:00.622: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 20:44:00.622: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 20:44:00.622: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 20:44:00.622: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:44:01.388: INFO: rc: 1
Jul 22 20:44:01.388: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container not running (3e7f44f76654d2090c3756509faffba9fdc767d07159474bb07130212e4b1a60)

error:
exit status 1
Jul 22 20:44:11.389: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:44:11.710: INFO: rc: 1
Jul 22 20:44:11.710: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jul 22 20:44:21.710: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:44:21.840: INFO: rc: 1
Jul 22 20:44:21.840: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:44:31.841: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:44:31.973: INFO: rc: 1
Jul 22 20:44:31.973: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:44:41.974: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:44:42.126: INFO: rc: 1
Jul 22 20:44:42.126: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:44:52.127: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:44:52.236: INFO: rc: 1
Jul 22 20:44:52.236: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:45:02.237: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:45:02.354: INFO: rc: 1
Jul 22 20:45:02.354: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:45:12.355: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:45:12.487: INFO: rc: 1
Jul 22 20:45:12.487: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:45:22.488: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:45:22.603: INFO: rc: 1
Jul 22 20:45:22.604: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:45:32.604: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:45:32.741: INFO: rc: 1
Jul 22 20:45:32.741: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:45:42.742: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:45:42.855: INFO: rc: 1
Jul 22 20:45:42.855: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:45:52.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:45:52.985: INFO: rc: 1
Jul 22 20:45:52.985: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:46:02.985: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:46:03.095: INFO: rc: 1
Jul 22 20:46:03.095: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:46:13.095: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:46:13.218: INFO: rc: 1
Jul 22 20:46:13.218: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:46:23.219: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:46:23.342: INFO: rc: 1
Jul 22 20:46:23.342: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:46:33.342: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:46:33.456: INFO: rc: 1
Jul 22 20:46:33.456: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:46:43.456: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:46:43.570: INFO: rc: 1
Jul 22 20:46:43.571: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:46:53.571: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:46:53.689: INFO: rc: 1
Jul 22 20:46:53.689: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:47:03.690: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:47:03.811: INFO: rc: 1
Jul 22 20:47:03.811: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:47:13.811: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:47:13.930: INFO: rc: 1
Jul 22 20:47:13.930: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:47:23.932: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:47:24.082: INFO: rc: 1
Jul 22 20:47:24.082: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:47:34.083: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:47:34.199: INFO: rc: 1
Jul 22 20:47:34.199: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:47:44.200: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:47:44.336: INFO: rc: 1
Jul 22 20:47:44.336: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:47:54.336: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:47:54.454: INFO: rc: 1
Jul 22 20:47:54.454: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:48:04.454: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:48:04.616: INFO: rc: 1
Jul 22 20:48:04.616: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:48:14.617: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:48:14.770: INFO: rc: 1
Jul 22 20:48:14.770: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:48:24.831: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:48:24.993: INFO: rc: 1
Jul 22 20:48:24.993: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:48:34.993: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:48:35.150: INFO: rc: 1
Jul 22 20:48:35.150: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:48:45.150: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:48:45.313: INFO: rc: 1
Jul 22 20:48:45.313: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:48:55.313: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:48:55.484: INFO: rc: 1
Jul 22 20:48:55.484: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 22 20:49:05.484: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3216 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 20:49:05.654: INFO: rc: 1
Jul 22 20:49:05.654: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jul 22 20:49:05.654: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 22 20:49:05.700: INFO: Deleting all statefulset in ns statefulset-3216
Jul 22 20:49:05.712: INFO: Scaling statefulset ss to 0
Jul 22 20:49:05.748: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:49:05.760: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:05.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3216" for this suite.

• [SLOW TEST:369.715 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":140,"skipped":2520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:05.840: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Jul 22 20:49:06.033: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6550 proxy --unix-socket=/tmp/kubectl-proxy-unix225902214/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:06.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6550" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":141,"skipped":2562,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:06.147: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-cba15a3c-aefa-43f7-8be1-4251ac733350
STEP: Creating a pod to test consume configMaps
Jul 22 20:49:06.387: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e2fcf22-4c35-4b7d-9375-9264e688f0cf" in namespace "configmap-6996" to be "Succeeded or Failed"
Jul 22 20:49:06.399: INFO: Pod "pod-configmaps-5e2fcf22-4c35-4b7d-9375-9264e688f0cf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.822106ms
Jul 22 20:49:08.411: INFO: Pod "pod-configmaps-5e2fcf22-4c35-4b7d-9375-9264e688f0cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024358386s
Jul 22 20:49:10.424: INFO: Pod "pod-configmaps-5e2fcf22-4c35-4b7d-9375-9264e688f0cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037012118s
STEP: Saw pod success
Jul 22 20:49:10.424: INFO: Pod "pod-configmaps-5e2fcf22-4c35-4b7d-9375-9264e688f0cf" satisfied condition "Succeeded or Failed"
Jul 22 20:49:10.439: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-configmaps-5e2fcf22-4c35-4b7d-9375-9264e688f0cf container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:49:10.557: INFO: Waiting for pod pod-configmaps-5e2fcf22-4c35-4b7d-9375-9264e688f0cf to disappear
Jul 22 20:49:10.568: INFO: Pod pod-configmaps-5e2fcf22-4c35-4b7d-9375-9264e688f0cf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:10.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6996" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":142,"skipped":2565,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:10.604: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:10.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8289" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":143,"skipped":2584,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:10.908: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-06589715-4c00-42b6-87a4-d707110b1100
STEP: Creating a pod to test consume secrets
Jul 22 20:49:11.121: INFO: Waiting up to 5m0s for pod "pod-secrets-c4e8c09f-acc7-484e-ba27-09e58768887b" in namespace "secrets-5411" to be "Succeeded or Failed"
Jul 22 20:49:11.142: INFO: Pod "pod-secrets-c4e8c09f-acc7-484e-ba27-09e58768887b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.927626ms
Jul 22 20:49:13.155: INFO: Pod "pod-secrets-c4e8c09f-acc7-484e-ba27-09e58768887b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033523631s
Jul 22 20:49:15.167: INFO: Pod "pod-secrets-c4e8c09f-acc7-484e-ba27-09e58768887b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046333487s
STEP: Saw pod success
Jul 22 20:49:15.167: INFO: Pod "pod-secrets-c4e8c09f-acc7-484e-ba27-09e58768887b" satisfied condition "Succeeded or Failed"
Jul 22 20:49:15.179: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-secrets-c4e8c09f-acc7-484e-ba27-09e58768887b container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:49:15.316: INFO: Waiting for pod pod-secrets-c4e8c09f-acc7-484e-ba27-09e58768887b to disappear
Jul 22 20:49:15.327: INFO: Pod pod-secrets-c4e8c09f-acc7-484e-ba27-09e58768887b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:15.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5411" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":144,"skipped":2603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:15.362: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 22 20:49:15.724: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:23.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9260" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2630,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:23.212: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 22 20:49:23.456: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4209  534ee34b-8a19-42fa-b63e-e4f3da2c5e23 23234 0 2021-07-22 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-22 20:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:49:23.456: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4209  534ee34b-8a19-42fa-b63e-e4f3da2c5e23 23237 0 2021-07-22 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-22 20:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 22 20:49:23.504: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4209  534ee34b-8a19-42fa-b63e-e4f3da2c5e23 23238 0 2021-07-22 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-22 20:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:49:23.504: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4209  534ee34b-8a19-42fa-b63e-e4f3da2c5e23 23239 0 2021-07-22 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-22 20:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:23.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4209" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":146,"skipped":2645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:23.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9375
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-5d122956-6895-4be7-9a6b-7a4a96fd4330
STEP: Creating a pod to test consume configMaps
Jul 22 20:49:23.753: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43efbf2a-bb56-41f0-8a4e-caf9062cb034" in namespace "projected-9375" to be "Succeeded or Failed"
Jul 22 20:49:23.765: INFO: Pod "pod-projected-configmaps-43efbf2a-bb56-41f0-8a4e-caf9062cb034": Phase="Pending", Reason="", readiness=false. Elapsed: 11.492822ms
Jul 22 20:49:25.777: INFO: Pod "pod-projected-configmaps-43efbf2a-bb56-41f0-8a4e-caf9062cb034": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023774877s
Jul 22 20:49:27.789: INFO: Pod "pod-projected-configmaps-43efbf2a-bb56-41f0-8a4e-caf9062cb034": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036105689s
STEP: Saw pod success
Jul 22 20:49:27.789: INFO: Pod "pod-projected-configmaps-43efbf2a-bb56-41f0-8a4e-caf9062cb034" satisfied condition "Succeeded or Failed"
Jul 22 20:49:27.801: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-configmaps-43efbf2a-bb56-41f0-8a4e-caf9062cb034 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:49:27.961: INFO: Waiting for pod pod-projected-configmaps-43efbf2a-bb56-41f0-8a4e-caf9062cb034 to disappear
Jul 22 20:49:27.972: INFO: Pod pod-projected-configmaps-43efbf2a-bb56-41f0-8a4e-caf9062cb034 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:27.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9375" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:28.009: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 22 20:49:28.215: INFO: Waiting up to 5m0s for pod "pod-fa613321-601e-46d9-a38c-563f78267fd8" in namespace "emptydir-6283" to be "Succeeded or Failed"
Jul 22 20:49:28.226: INFO: Pod "pod-fa613321-601e-46d9-a38c-563f78267fd8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.198435ms
Jul 22 20:49:30.240: INFO: Pod "pod-fa613321-601e-46d9-a38c-563f78267fd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024657642s
Jul 22 20:49:32.252: INFO: Pod "pod-fa613321-601e-46d9-a38c-563f78267fd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036963494s
STEP: Saw pod success
Jul 22 20:49:32.252: INFO: Pod "pod-fa613321-601e-46d9-a38c-563f78267fd8" satisfied condition "Succeeded or Failed"
Jul 22 20:49:32.264: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-fa613321-601e-46d9-a38c-563f78267fd8 container test-container: <nil>
STEP: delete the pod
Jul 22 20:49:32.335: INFO: Waiting for pod pod-fa613321-601e-46d9-a38c-563f78267fd8 to disappear
Jul 22 20:49:32.347: INFO: Pod pod-fa613321-601e-46d9-a38c-563f78267fd8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:32.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6283" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":148,"skipped":2745,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:32.383: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jul 22 20:49:34.029: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0722 20:49:34.029181    5567 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 20:49:34.029221    5567 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 20:49:34.029229    5567 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:34.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4420" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":149,"skipped":2760,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:34.057: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-7264
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 20:49:34.250: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 22 20:49:34.325: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:49:36.338: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:49:38.338: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:49:40.338: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:49:42.339: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:49:44.345: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:49:46.338: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:49:48.344: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:49:50.338: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:49:52.338: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:49:54.337: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 22 20:49:54.365: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 22 20:49:58.430: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 22 20:49:58.430: INFO: Breadth first check of 100.96.1.155 on host 10.250.0.4...
Jul 22 20:49:58.442: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.156:9080/dial?request=hostname&protocol=http&host=100.96.1.155&port=8080&tries=1'] Namespace:pod-network-test-7264 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:49:58.442: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:49:58.912: INFO: Waiting for responses: map[]
Jul 22 20:49:58.912: INFO: reached 100.96.1.155 after 0/1 tries
Jul 22 20:49:58.912: INFO: Breadth first check of 100.96.0.56 on host 10.250.0.5...
Jul 22 20:49:58.925: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.156:9080/dial?request=hostname&protocol=http&host=100.96.0.56&port=8080&tries=1'] Namespace:pod-network-test-7264 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:49:58.925: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:49:59.351: INFO: Waiting for responses: map[]
Jul 22 20:49:59.351: INFO: reached 100.96.0.56 after 0/1 tries
Jul 22 20:49:59.351: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:49:59.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7264" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2760,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:49:59.387: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9822
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-3933188d-ca3f-4a05-a078-3c33023a82c7
STEP: Creating a pod to test consume configMaps
Jul 22 20:49:59.621: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bbc061dd-8c52-4233-8daf-412ae4bd3ee2" in namespace "projected-9822" to be "Succeeded or Failed"
Jul 22 20:49:59.638: INFO: Pod "pod-projected-configmaps-bbc061dd-8c52-4233-8daf-412ae4bd3ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.860065ms
Jul 22 20:50:01.650: INFO: Pod "pod-projected-configmaps-bbc061dd-8c52-4233-8daf-412ae4bd3ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029328602s
Jul 22 20:50:03.663: INFO: Pod "pod-projected-configmaps-bbc061dd-8c52-4233-8daf-412ae4bd3ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041694089s
STEP: Saw pod success
Jul 22 20:50:03.663: INFO: Pod "pod-projected-configmaps-bbc061dd-8c52-4233-8daf-412ae4bd3ee2" satisfied condition "Succeeded or Failed"
Jul 22 20:50:03.674: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-configmaps-bbc061dd-8c52-4233-8daf-412ae4bd3ee2 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:50:03.790: INFO: Waiting for pod pod-projected-configmaps-bbc061dd-8c52-4233-8daf-412ae4bd3ee2 to disappear
Jul 22 20:50:03.802: INFO: Pod pod-projected-configmaps-bbc061dd-8c52-4233-8daf-412ae4bd3ee2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:50:03.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9822" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":151,"skipped":2765,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:50:03.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:50:04.512: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:50:06.524: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:50:08.532: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583804, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:50:11.544: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:50:11.557: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5081-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:50:13.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4874" for this suite.
STEP: Destroying namespace "webhook-4874-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":152,"skipped":2783,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:50:13.340: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-9271
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:50:13.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9271" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":153,"skipped":2791,"failed":0}
SSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:50:14.013: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-4382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Jul 22 20:50:14.258: INFO: created test-podtemplate-1
Jul 22 20:50:14.270: INFO: created test-podtemplate-2
Jul 22 20:50:14.282: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jul 22 20:50:14.294: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jul 22 20:50:14.324: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:50:14.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4382" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":154,"skipped":2794,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:50:14.364: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:50:14.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c73000e8-896d-4931-9707-e94ec8c24ac6" in namespace "downward-api-4046" to be "Succeeded or Failed"
Jul 22 20:50:14.599: INFO: Pod "downwardapi-volume-c73000e8-896d-4931-9707-e94ec8c24ac6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.326148ms
Jul 22 20:50:16.612: INFO: Pod "downwardapi-volume-c73000e8-896d-4931-9707-e94ec8c24ac6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024236525s
Jul 22 20:50:18.713: INFO: Pod "downwardapi-volume-c73000e8-896d-4931-9707-e94ec8c24ac6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.125052277s
STEP: Saw pod success
Jul 22 20:50:18.713: INFO: Pod "downwardapi-volume-c73000e8-896d-4931-9707-e94ec8c24ac6" satisfied condition "Succeeded or Failed"
Jul 22 20:50:18.724: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-c73000e8-896d-4931-9707-e94ec8c24ac6 container client-container: <nil>
STEP: delete the pod
Jul 22 20:50:18.873: INFO: Waiting for pod downwardapi-volume-c73000e8-896d-4931-9707-e94ec8c24ac6 to disappear
Jul 22 20:50:18.884: INFO: Pod downwardapi-volume-c73000e8-896d-4931-9707-e94ec8c24ac6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:50:18.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4046" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:50:18.919: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-5911
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Jul 22 20:50:19.130: INFO: Major version: 1
STEP: Confirm minor version
Jul 22 20:50:19.130: INFO: cleanMinorVersion: 20
Jul 22 20:50:19.130: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:50:19.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5911" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":156,"skipped":2837,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:50:19.155: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7036
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-29fa561f-00d5-4348-87d9-10f837cf5ab9
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:50:23.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7036" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":157,"skipped":2858,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:50:23.757: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:50:24.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583824, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583824, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583824, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583824, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:50:26.579: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583824, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583824, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583824, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762583824, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:50:29.600: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:50:29.614: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:50:31.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6771" for this suite.
STEP: Destroying namespace "webhook-6771-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":158,"skipped":2858,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:50:31.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8292
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0722 20:50:38.011188    5567 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 20:50:38.011235    5567 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 20:50:38.011243    5567 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 22 20:50:38.011: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:50:38.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8292" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":159,"skipped":2875,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:50:38.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Jul 22 20:50:38.255: INFO: Waiting up to 5m0s for pod "client-containers-77d0c960-1f83-414b-a7cf-8d4613eeb271" in namespace "containers-4956" to be "Succeeded or Failed"
Jul 22 20:50:38.266: INFO: Pod "client-containers-77d0c960-1f83-414b-a7cf-8d4613eeb271": Phase="Pending", Reason="", readiness=false. Elapsed: 11.595229ms
Jul 22 20:50:40.279: INFO: Pod "client-containers-77d0c960-1f83-414b-a7cf-8d4613eeb271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024424438s
Jul 22 20:50:42.291: INFO: Pod "client-containers-77d0c960-1f83-414b-a7cf-8d4613eeb271": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03670111s
Jul 22 20:50:44.304: INFO: Pod "client-containers-77d0c960-1f83-414b-a7cf-8d4613eeb271": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049109686s
STEP: Saw pod success
Jul 22 20:50:44.304: INFO: Pod "client-containers-77d0c960-1f83-414b-a7cf-8d4613eeb271" satisfied condition "Succeeded or Failed"
Jul 22 20:50:44.315: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod client-containers-77d0c960-1f83-414b-a7cf-8d4613eeb271 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:50:44.385: INFO: Waiting for pod client-containers-77d0c960-1f83-414b-a7cf-8d4613eeb271 to disappear
Jul 22 20:50:44.396: INFO: Pod client-containers-77d0c960-1f83-414b-a7cf-8d4613eeb271 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:50:44.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4956" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":2887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:50:44.431: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-4bzj
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 20:50:44.679: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4bzj" in namespace "subpath-8645" to be "Succeeded or Failed"
Jul 22 20:50:44.690: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Pending", Reason="", readiness=false. Elapsed: 11.201662ms
Jul 22 20:50:46.702: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023256675s
Jul 22 20:50:48.714: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Running", Reason="", readiness=true. Elapsed: 4.03573187s
Jul 22 20:50:50.728: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Running", Reason="", readiness=true. Elapsed: 6.049529787s
Jul 22 20:50:52.741: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Running", Reason="", readiness=true. Elapsed: 8.062487808s
Jul 22 20:50:54.754: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Running", Reason="", readiness=true. Elapsed: 10.074923269s
Jul 22 20:50:56.766: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Running", Reason="", readiness=true. Elapsed: 12.087642696s
Jul 22 20:50:58.779: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Running", Reason="", readiness=true. Elapsed: 14.099833089s
Jul 22 20:51:00.799: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Running", Reason="", readiness=true. Elapsed: 16.120595204s
Jul 22 20:51:02.813: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Running", Reason="", readiness=true. Elapsed: 18.133760815s
Jul 22 20:51:04.840: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Running", Reason="", readiness=true. Elapsed: 20.161456059s
Jul 22 20:51:06.853: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Running", Reason="", readiness=true. Elapsed: 22.174122173s
Jul 22 20:51:08.872: INFO: Pod "pod-subpath-test-configmap-4bzj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.193424842s
STEP: Saw pod success
Jul 22 20:51:08.872: INFO: Pod "pod-subpath-test-configmap-4bzj" satisfied condition "Succeeded or Failed"
Jul 22 20:51:08.884: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-subpath-test-configmap-4bzj container test-container-subpath-configmap-4bzj: <nil>
STEP: delete the pod
Jul 22 20:51:08.958: INFO: Waiting for pod pod-subpath-test-configmap-4bzj to disappear
Jul 22 20:51:08.970: INFO: Pod pod-subpath-test-configmap-4bzj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4bzj
Jul 22 20:51:08.970: INFO: Deleting pod "pod-subpath-test-configmap-4bzj" in namespace "subpath-8645"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:51:08.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8645" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":161,"skipped":2916,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:51:09.022: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1132
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 22 20:51:09.231: INFO: Waiting up to 5m0s for pod "pod-641794eb-6d4e-402f-93f1-41deb10fa44b" in namespace "emptydir-1132" to be "Succeeded or Failed"
Jul 22 20:51:09.242: INFO: Pod "pod-641794eb-6d4e-402f-93f1-41deb10fa44b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.317743ms
Jul 22 20:51:11.255: INFO: Pod "pod-641794eb-6d4e-402f-93f1-41deb10fa44b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0237216s
Jul 22 20:51:13.267: INFO: Pod "pod-641794eb-6d4e-402f-93f1-41deb10fa44b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035966269s
STEP: Saw pod success
Jul 22 20:51:13.267: INFO: Pod "pod-641794eb-6d4e-402f-93f1-41deb10fa44b" satisfied condition "Succeeded or Failed"
Jul 22 20:51:13.278: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-641794eb-6d4e-402f-93f1-41deb10fa44b container test-container: <nil>
STEP: delete the pod
Jul 22 20:51:13.336: INFO: Waiting for pod pod-641794eb-6d4e-402f-93f1-41deb10fa44b to disappear
Jul 22 20:51:13.349: INFO: Pod pod-641794eb-6d4e-402f-93f1-41deb10fa44b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:51:13.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1132" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":2920,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:51:13.383: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:51:13.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3073fad2-8d94-48d1-bbae-72220614e104" in namespace "downward-api-7236" to be "Succeeded or Failed"
Jul 22 20:51:13.600: INFO: Pod "downwardapi-volume-3073fad2-8d94-48d1-bbae-72220614e104": Phase="Pending", Reason="", readiness=false. Elapsed: 12.234947ms
Jul 22 20:51:15.612: INFO: Pod "downwardapi-volume-3073fad2-8d94-48d1-bbae-72220614e104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024308152s
Jul 22 20:51:17.626: INFO: Pod "downwardapi-volume-3073fad2-8d94-48d1-bbae-72220614e104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038238509s
STEP: Saw pod success
Jul 22 20:51:17.626: INFO: Pod "downwardapi-volume-3073fad2-8d94-48d1-bbae-72220614e104" satisfied condition "Succeeded or Failed"
Jul 22 20:51:17.638: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-3073fad2-8d94-48d1-bbae-72220614e104 container client-container: <nil>
STEP: delete the pod
Jul 22 20:51:17.999: INFO: Waiting for pod downwardapi-volume-3073fad2-8d94-48d1-bbae-72220614e104 to disappear
Jul 22 20:51:18.011: INFO: Pod downwardapi-volume-3073fad2-8d94-48d1-bbae-72220614e104 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:51:18.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7236" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":163,"skipped":2937,"failed":0}

------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:51:18.045: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-8af41b29-3b5d-464e-9cdf-8db9b5392475 in namespace container-probe-2020
Jul 22 20:51:22.276: INFO: Started pod liveness-8af41b29-3b5d-464e-9cdf-8db9b5392475 in namespace container-probe-2020
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:51:22.288: INFO: Initial restart count of pod liveness-8af41b29-3b5d-464e-9cdf-8db9b5392475 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:55:23.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2020" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":164,"skipped":2937,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:55:23.983: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-699/configmap-test-1079e89e-a93a-4e90-923f-bacab4f0b99c
STEP: Creating a pod to test consume configMaps
Jul 22 20:55:24.214: INFO: Waiting up to 5m0s for pod "pod-configmaps-77f2a7ca-72cc-40d2-a99c-ab30596059eb" in namespace "configmap-699" to be "Succeeded or Failed"
Jul 22 20:55:24.226: INFO: Pod "pod-configmaps-77f2a7ca-72cc-40d2-a99c-ab30596059eb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.039944ms
Jul 22 20:55:26.244: INFO: Pod "pod-configmaps-77f2a7ca-72cc-40d2-a99c-ab30596059eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029767629s
Jul 22 20:55:28.258: INFO: Pod "pod-configmaps-77f2a7ca-72cc-40d2-a99c-ab30596059eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043418702s
STEP: Saw pod success
Jul 22 20:55:28.258: INFO: Pod "pod-configmaps-77f2a7ca-72cc-40d2-a99c-ab30596059eb" satisfied condition "Succeeded or Failed"
Jul 22 20:55:28.269: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-configmaps-77f2a7ca-72cc-40d2-a99c-ab30596059eb container env-test: <nil>
STEP: delete the pod
Jul 22 20:55:28.359: INFO: Waiting for pod pod-configmaps-77f2a7ca-72cc-40d2-a99c-ab30596059eb to disappear
Jul 22 20:55:28.370: INFO: Pod pod-configmaps-77f2a7ca-72cc-40d2-a99c-ab30596059eb no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:55:28.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-699" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":165,"skipped":2941,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:55:28.408: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 22 20:55:28.635: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 20:56:28.744: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Jul 22 20:56:28.794: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 22 20:56:28.830: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:56:46.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6618" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":166,"skipped":2951,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:56:47.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8342
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 20:56:47.491: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:56:48.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8342" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":167,"skipped":2954,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:56:48.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jul 22 20:56:48.547: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 20:56:48.572: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 20:56:48.585: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 before test
Jul 22 20:56:48.601: INFO: apiserver-proxy-b49r5 from kube-system started at 2021-07-22 19:48:59 +0000 UTC (2 container statuses recorded)
Jul 22 20:56:48.601: INFO: 	Container proxy ready: true, restart count 0
Jul 22 20:56:48.601: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 20:56:48.601: INFO: blackbox-exporter-859b5d9c8c-nf6n7 from kube-system started at 2021-07-22 19:55:18 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.601: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul 22 20:56:48.601: INFO: calico-node-jxrzv from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.601: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 20:56:48.601: INFO: kube-proxy-nbdq9 from kube-system started at 2021-07-22 19:51:32 +0000 UTC (2 container statuses recorded)
Jul 22 20:56:48.601: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 20:56:48.601: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:56:48.601: INFO: node-exporter-bxzmq from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.601: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 20:56:48.601: INFO: node-problem-detector-vlbxm from kube-system started at 2021-07-22 20:15:32 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.601: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 20:56:48.601: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv before test
Jul 22 20:56:48.632: INFO: addons-nginx-ingress-controller-5f6b8d6b9b-z5p47 from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 22 20:56:48.632: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-799f5cb4df-cnbrj from kube-system started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul 22 20:56:48.632: INFO: apiserver-proxy-lgksb from kube-system started at 2021-07-22 19:48:53 +0000 UTC (2 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container proxy ready: true, restart count 0
Jul 22 20:56:48.632: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 20:56:48.632: INFO: calico-node-96j6z from kube-system started at 2021-07-22 19:50:28 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 20:56:48.632: INFO: calico-node-vertical-autoscaler-785b5f968-dl6lj from kube-system started at 2021-07-22 19:49:15 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:56:48.632: INFO: calico-typha-deploy-59966cd68c-kk766 from kube-system started at 2021-07-22 19:49:04 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container calico-typha ready: true, restart count 0
Jul 22 20:56:48.632: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-5dvs2 from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:56:48.632: INFO: calico-typha-vertical-autoscaler-5c9655cddd-9t9nc from kube-system started at 2021-07-22 19:49:15 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:56:48.632: INFO: coredns-7589655f7c-8gplg from kube-system started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:56:48.632: INFO: coredns-7589655f7c-cck5m from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:56:48.632: INFO: kube-proxy-r86fj from kube-system started at 2021-07-22 19:51:22 +0000 UTC (2 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 20:56:48.632: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:56:48.632: INFO: metrics-server-7fcbc9df99-qm94n from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 20:56:48.632: INFO: node-exporter-lkd2s from kube-system started at 2021-07-22 19:48:53 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 20:56:48.632: INFO: node-problem-detector-kj6cn from kube-system started at 2021-07-22 20:15:21 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 20:56:48.632: INFO: vpn-shoot-6c79f97679-hz7lk from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul 22 20:56:48.632: INFO: dashboard-metrics-scraper-5fc7d79f9-tk49g from kubernetes-dashboard started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 22 20:56:48.632: INFO: kubernetes-dashboard-775d7d55c5-q9hcp from kubernetes-dashboard started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
Jul 22 20:56:48.632: INFO: pod1-sched-preemption-medium-priority from sched-preemption-6618 started at 2021-07-22 20:56:28 +0000 UTC (1 container statuses recorded)
Jul 22 20:56:48.632: INFO: 	Container pod1-sched-preemption-medium-priority ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c52c271a-7ff7-4f83-9437-bb9ce4da6609 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c52c271a-7ff7-4f83-9437-bb9ce4da6609 off the node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c52c271a-7ff7-4f83-9437-bb9ce4da6609
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 20:56:56.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9444" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":168,"skipped":2962,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 20:56:56.882: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-13dd07bd-5246-47da-99af-1c87f30ff6ca in namespace container-probe-9528
Jul 22 20:57:01.125: INFO: Started pod busybox-13dd07bd-5246-47da-99af-1c87f30ff6ca in namespace container-probe-9528
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:57:01.140: INFO: Initial restart count of pod busybox-13dd07bd-5246-47da-99af-1c87f30ff6ca is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:01:02.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9528" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":169,"skipped":2974,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:01:02.952: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4530
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Jul 22 21:03:03.746: INFO: Successfully updated pod "var-expansion-3ecdc537-4e31-4d6d-8f89-0806879eb825"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jul 22 21:03:05.822: INFO: Deleting pod "var-expansion-3ecdc537-4e31-4d6d-8f89-0806879eb825" in namespace "var-expansion-4530"
Jul 22 21:03:05.835: INFO: Wait up to 5m0s for pod "var-expansion-3ecdc537-4e31-4d6d-8f89-0806879eb825" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:03:43.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4530" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":170,"skipped":2981,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:03:43.901: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 22 21:03:44.136: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 21:04:44.252: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:04:44.264: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-7730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jul 22 21:04:48.557: INFO: found a healthy node: shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:05:02.777: INFO: pods created so far: [1 1 1]
Jul 22 21:05:02.777: INFO: length of pods created so far: 3
Jul 22 21:05:18.811: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:05:25.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7730" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:05:25.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6189" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":171,"skipped":2982,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:05:26.040: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Jul 22 21:05:26.234: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6166 create -f -'
Jul 22 21:05:27.086: INFO: stderr: ""
Jul 22 21:05:27.086: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 22 21:05:28.099: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:05:28.099: INFO: Found 0 / 1
Jul 22 21:05:29.098: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:05:29.098: INFO: Found 0 / 1
Jul 22 21:05:30.099: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:05:30.099: INFO: Found 1 / 1
Jul 22 21:05:30.099: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 22 21:05:30.111: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:05:30.111: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 21:05:30.111: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6166 patch pod agnhost-primary-n4bnj -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 22 21:05:30.306: INFO: stderr: ""
Jul 22 21:05:30.306: INFO: stdout: "pod/agnhost-primary-n4bnj patched\n"
STEP: checking annotations
Jul 22 21:05:30.318: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:05:30.318: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:05:30.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6166" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":172,"skipped":2992,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:05:30.355: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-514
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:05:30.556: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:05:31.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-514" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":173,"skipped":3015,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:05:31.321: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jul 22 21:05:31.652: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:05:37.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3643" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":174,"skipped":3024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:05:37.051: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:05:37.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afad5e70-01d5-4038-876a-eb09357bc9ea" in namespace "downward-api-7393" to be "Succeeded or Failed"
Jul 22 21:05:37.278: INFO: Pod "downwardapi-volume-afad5e70-01d5-4038-876a-eb09357bc9ea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.460227ms
Jul 22 21:05:39.293: INFO: Pod "downwardapi-volume-afad5e70-01d5-4038-876a-eb09357bc9ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02670779s
Jul 22 21:05:41.305: INFO: Pod "downwardapi-volume-afad5e70-01d5-4038-876a-eb09357bc9ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038765596s
STEP: Saw pod success
Jul 22 21:05:41.305: INFO: Pod "downwardapi-volume-afad5e70-01d5-4038-876a-eb09357bc9ea" satisfied condition "Succeeded or Failed"
Jul 22 21:05:41.321: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-afad5e70-01d5-4038-876a-eb09357bc9ea container client-container: <nil>
STEP: delete the pod
Jul 22 21:05:41.429: INFO: Waiting for pod downwardapi-volume-afad5e70-01d5-4038-876a-eb09357bc9ea to disappear
Jul 22 21:05:41.442: INFO: Pod downwardapi-volume-afad5e70-01d5-4038-876a-eb09357bc9ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:05:41.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7393" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":175,"skipped":3061,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:05:41.479: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:05:42.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584742, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584742, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584742, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584742, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:05:44.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584742, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584742, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584742, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584742, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:05:47.249: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:05:47.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-694" for this suite.
STEP: Destroying namespace "webhook-694-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":176,"skipped":3064,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:05:47.896: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4493
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:05:48.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:05:48.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4493" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":177,"skipped":3076,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:05:48.237: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:05:48.622: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6267
I0722 21:05:48.719904    5567 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6267, replica count: 1
I0722 21:05:49.770295    5567 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 21:05:50.770505    5567 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 21:05:51.770759    5567 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 21:05:52.770940    5567 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:05:52.927: INFO: Created: latency-svc-zr464
Jul 22 21:05:52.927: INFO: Got endpoints: latency-svc-zr464 [56.695291ms]
Jul 22 21:05:52.948: INFO: Created: latency-svc-6tb5w
Jul 22 21:05:52.951: INFO: Got endpoints: latency-svc-6tb5w [23.629055ms]
Jul 22 21:05:52.956: INFO: Created: latency-svc-67s2k
Jul 22 21:05:52.959: INFO: Got endpoints: latency-svc-67s2k [31.326411ms]
Jul 22 21:05:53.014: INFO: Created: latency-svc-bsnqv
Jul 22 21:05:53.018: INFO: Created: latency-svc-vnsbs
Jul 22 21:05:53.026: INFO: Got endpoints: latency-svc-vnsbs [97.502668ms]
Jul 22 21:05:53.026: INFO: Got endpoints: latency-svc-bsnqv [97.595053ms]
Jul 22 21:05:53.028: INFO: Created: latency-svc-qzdq8
Jul 22 21:05:53.034: INFO: Got endpoints: latency-svc-qzdq8 [105.749916ms]
Jul 22 21:05:53.038: INFO: Created: latency-svc-v8pwp
Jul 22 21:05:53.045: INFO: Got endpoints: latency-svc-v8pwp [116.493224ms]
Jul 22 21:05:53.047: INFO: Created: latency-svc-wkbk4
Jul 22 21:05:53.053: INFO: Got endpoints: latency-svc-wkbk4 [124.190256ms]
Jul 22 21:05:53.053: INFO: Created: latency-svc-9kcnz
Jul 22 21:05:53.056: INFO: Got endpoints: latency-svc-9kcnz [127.967265ms]
Jul 22 21:05:53.063: INFO: Created: latency-svc-zxdpd
Jul 22 21:05:53.066: INFO: Got endpoints: latency-svc-zxdpd [136.666539ms]
Jul 22 21:05:53.112: INFO: Created: latency-svc-6wgmh
Jul 22 21:05:53.117: INFO: Created: latency-svc-gl9z4
Jul 22 21:05:53.117: INFO: Got endpoints: latency-svc-6wgmh [188.833581ms]
Jul 22 21:05:53.124: INFO: Got endpoints: latency-svc-gl9z4 [195.411445ms]
Jul 22 21:05:53.125: INFO: Created: latency-svc-j8v9j
Jul 22 21:05:53.130: INFO: Got endpoints: latency-svc-j8v9j [201.664134ms]
Jul 22 21:05:53.131: INFO: Created: latency-svc-brgmr
Jul 22 21:05:53.136: INFO: Got endpoints: latency-svc-brgmr [206.922364ms]
Jul 22 21:05:53.139: INFO: Created: latency-svc-jklrk
Jul 22 21:05:53.145: INFO: Got endpoints: latency-svc-jklrk [215.746456ms]
Jul 22 21:05:53.145: INFO: Created: latency-svc-x4vdt
Jul 22 21:05:53.154: INFO: Got endpoints: latency-svc-x4vdt [224.381534ms]
Jul 22 21:05:53.159: INFO: Created: latency-svc-mntzm
Jul 22 21:05:53.212: INFO: Created: latency-svc-wkrtr
Jul 22 21:05:53.213: INFO: Got endpoints: latency-svc-mntzm [261.711867ms]
Jul 22 21:05:53.218: INFO: Created: latency-svc-64v2z
Jul 22 21:05:53.218: INFO: Got endpoints: latency-svc-wkrtr [259.241835ms]
Jul 22 21:05:53.221: INFO: Got endpoints: latency-svc-64v2z [195.078655ms]
Jul 22 21:05:53.226: INFO: Created: latency-svc-vvf4p
Jul 22 21:05:53.233: INFO: Created: latency-svc-j9j4l
Jul 22 21:05:53.243: INFO: Got endpoints: latency-svc-vvf4p [215.312217ms]
Jul 22 21:05:53.245: INFO: Got endpoints: latency-svc-j9j4l [210.748221ms]
Jul 22 21:05:53.248: INFO: Created: latency-svc-rhnft
Jul 22 21:05:53.250: INFO: Got endpoints: latency-svc-rhnft [205.274271ms]
Jul 22 21:05:53.311: INFO: Created: latency-svc-nd7s7
Jul 22 21:05:53.317: INFO: Created: latency-svc-hlzcx
Jul 22 21:05:53.317: INFO: Got endpoints: latency-svc-nd7s7 [264.374315ms]
Jul 22 21:05:53.323: INFO: Created: latency-svc-pr6tl
Jul 22 21:05:53.323: INFO: Got endpoints: latency-svc-hlzcx [266.711497ms]
Jul 22 21:05:53.326: INFO: Got endpoints: latency-svc-pr6tl [259.855736ms]
Jul 22 21:05:53.339: INFO: Created: latency-svc-hkhvx
Jul 22 21:05:53.339: INFO: Got endpoints: latency-svc-hkhvx [221.0856ms]
Jul 22 21:05:53.351: INFO: Created: latency-svc-mn6fr
Jul 22 21:05:53.358: INFO: Got endpoints: latency-svc-mn6fr [233.785703ms]
Jul 22 21:05:53.411: INFO: Created: latency-svc-djcdw
Jul 22 21:05:53.417: INFO: Got endpoints: latency-svc-djcdw [287.175319ms]
Jul 22 21:05:53.418: INFO: Created: latency-svc-xnxml
Jul 22 21:05:53.420: INFO: Got endpoints: latency-svc-xnxml [284.329585ms]
Jul 22 21:05:53.429: INFO: Created: latency-svc-mvpw2
Jul 22 21:05:53.435: INFO: Got endpoints: latency-svc-mvpw2 [290.46586ms]
Jul 22 21:05:53.436: INFO: Created: latency-svc-2stnx
Jul 22 21:05:53.446: INFO: Created: latency-svc-w567r
Jul 22 21:05:53.446: INFO: Got endpoints: latency-svc-2stnx [292.814777ms]
Jul 22 21:05:53.453: INFO: Got endpoints: latency-svc-w567r [239.576624ms]
Jul 22 21:05:53.458: INFO: Created: latency-svc-khbp8
Jul 22 21:05:53.512: INFO: Got endpoints: latency-svc-khbp8 [293.609993ms]
Jul 22 21:05:53.519: INFO: Created: latency-svc-hpz2v
Jul 22 21:05:53.522: INFO: Got endpoints: latency-svc-hpz2v [300.910017ms]
Jul 22 21:05:53.528: INFO: Created: latency-svc-pxlbl
Jul 22 21:05:53.530: INFO: Got endpoints: latency-svc-pxlbl [287.134544ms]
Jul 22 21:05:53.536: INFO: Created: latency-svc-phzw2
Jul 22 21:05:53.540: INFO: Got endpoints: latency-svc-phzw2 [295.019778ms]
Jul 22 21:05:53.546: INFO: Created: latency-svc-smvlb
Jul 22 21:05:53.552: INFO: Got endpoints: latency-svc-smvlb [301.285708ms]
Jul 22 21:05:53.552: INFO: Created: latency-svc-zlczw
Jul 22 21:05:53.554: INFO: Got endpoints: latency-svc-zlczw [236.850866ms]
Jul 22 21:05:53.564: INFO: Created: latency-svc-dzll2
Jul 22 21:05:53.611: INFO: Got endpoints: latency-svc-dzll2 [287.625252ms]
Jul 22 21:05:53.612: INFO: Created: latency-svc-pdxtx
Jul 22 21:05:53.618: INFO: Got endpoints: latency-svc-pdxtx [292.709813ms]
Jul 22 21:05:53.618: INFO: Created: latency-svc-h9l92
Jul 22 21:05:53.633: INFO: Got endpoints: latency-svc-h9l92 [294.819811ms]
Jul 22 21:05:53.647: INFO: Created: latency-svc-n5gs8
Jul 22 21:05:53.652: INFO: Got endpoints: latency-svc-n5gs8 [293.658547ms]
Jul 22 21:05:53.655: INFO: Created: latency-svc-qdmrc
Jul 22 21:05:53.658: INFO: Got endpoints: latency-svc-qdmrc [240.257625ms]
Jul 22 21:05:53.663: INFO: Created: latency-svc-tsjtz
Jul 22 21:05:53.665: INFO: Got endpoints: latency-svc-tsjtz [245.092504ms]
Jul 22 21:05:53.712: INFO: Created: latency-svc-vsn86
Jul 22 21:05:53.719: INFO: Got endpoints: latency-svc-vsn86 [283.659428ms]
Jul 22 21:05:53.719: INFO: Created: latency-svc-52w7z
Jul 22 21:05:53.730: INFO: Got endpoints: latency-svc-52w7z [283.602644ms]
Jul 22 21:05:53.731: INFO: Created: latency-svc-zwz9n
Jul 22 21:05:53.748: INFO: Created: latency-svc-mn2s8
Jul 22 21:05:53.749: INFO: Created: latency-svc-td5q6
Jul 22 21:05:53.754: INFO: Created: latency-svc-l7w8r
Jul 22 21:05:53.767: INFO: Created: latency-svc-9w9n5
Jul 22 21:05:53.813: INFO: Created: latency-svc-nnhvf
Jul 22 21:05:53.816: INFO: Got endpoints: latency-svc-zwz9n [363.697362ms]
Jul 22 21:05:53.826: INFO: Got endpoints: latency-svc-mn2s8 [313.487852ms]
Jul 22 21:05:53.826: INFO: Created: latency-svc-ndg8g
Jul 22 21:05:53.840: INFO: Created: latency-svc-xfv9f
Jul 22 21:05:53.848: INFO: Created: latency-svc-ttwlf
Jul 22 21:05:53.854: INFO: Created: latency-svc-kv8nx
Jul 22 21:05:53.860: INFO: Created: latency-svc-gtrrk
Jul 22 21:05:53.868: INFO: Created: latency-svc-zt86q
Jul 22 21:05:53.879: INFO: Created: latency-svc-8lcsz
Jul 22 21:05:53.879: INFO: Got endpoints: latency-svc-td5q6 [357.00968ms]
Jul 22 21:05:53.884: INFO: Created: latency-svc-rssm7
Jul 22 21:05:53.913: INFO: Created: latency-svc-gcpn4
Jul 22 21:05:53.918: INFO: Created: latency-svc-zzj7j
Jul 22 21:05:53.926: INFO: Created: latency-svc-hkn22
Jul 22 21:05:53.926: INFO: Got endpoints: latency-svc-l7w8r [396.516074ms]
Jul 22 21:05:53.932: INFO: Created: latency-svc-txxct
Jul 22 21:05:53.946: INFO: Created: latency-svc-6vlr2
Jul 22 21:05:53.976: INFO: Got endpoints: latency-svc-9w9n5 [436.455685ms]
Jul 22 21:05:53.995: INFO: Created: latency-svc-gj95n
Jul 22 21:05:54.026: INFO: Got endpoints: latency-svc-nnhvf [474.048444ms]
Jul 22 21:05:54.047: INFO: Created: latency-svc-9h48l
Jul 22 21:05:54.082: INFO: Got endpoints: latency-svc-ndg8g [528.074923ms]
Jul 22 21:05:54.104: INFO: Created: latency-svc-btnqx
Jul 22 21:05:54.126: INFO: Got endpoints: latency-svc-xfv9f [515.417118ms]
Jul 22 21:05:54.150: INFO: Created: latency-svc-tt9ns
Jul 22 21:05:54.176: INFO: Got endpoints: latency-svc-ttwlf [542.191321ms]
Jul 22 21:05:54.196: INFO: Created: latency-svc-d2x75
Jul 22 21:05:54.225: INFO: Got endpoints: latency-svc-kv8nx [591.391339ms]
Jul 22 21:05:54.252: INFO: Created: latency-svc-fjjl9
Jul 22 21:05:54.277: INFO: Got endpoints: latency-svc-gtrrk [625.041715ms]
Jul 22 21:05:54.297: INFO: Created: latency-svc-9gwx6
Jul 22 21:05:54.338: INFO: Got endpoints: latency-svc-zt86q [679.995811ms]
Jul 22 21:05:54.359: INFO: Created: latency-svc-5lk58
Jul 22 21:05:54.375: INFO: Got endpoints: latency-svc-8lcsz [710.009882ms]
Jul 22 21:05:54.394: INFO: Created: latency-svc-wrqv5
Jul 22 21:05:54.425: INFO: Got endpoints: latency-svc-rssm7 [706.242956ms]
Jul 22 21:05:54.444: INFO: Created: latency-svc-rftqk
Jul 22 21:05:54.482: INFO: Got endpoints: latency-svc-gcpn4 [752.209545ms]
Jul 22 21:05:54.503: INFO: Created: latency-svc-nqdxd
Jul 22 21:05:54.529: INFO: Got endpoints: latency-svc-zzj7j [712.63502ms]
Jul 22 21:05:54.550: INFO: Created: latency-svc-j2wb2
Jul 22 21:05:54.575: INFO: Got endpoints: latency-svc-hkn22 [749.674795ms]
Jul 22 21:05:54.594: INFO: Created: latency-svc-mfxnd
Jul 22 21:05:54.626: INFO: Got endpoints: latency-svc-txxct [746.741887ms]
Jul 22 21:05:54.644: INFO: Created: latency-svc-g457d
Jul 22 21:05:54.675: INFO: Got endpoints: latency-svc-6vlr2 [748.372601ms]
Jul 22 21:05:54.698: INFO: Created: latency-svc-9mx2z
Jul 22 21:05:54.726: INFO: Got endpoints: latency-svc-gj95n [749.345049ms]
Jul 22 21:05:54.745: INFO: Created: latency-svc-cj4jh
Jul 22 21:05:54.774: INFO: Got endpoints: latency-svc-9h48l [748.608636ms]
Jul 22 21:05:54.794: INFO: Created: latency-svc-pw4w9
Jul 22 21:05:54.825: INFO: Got endpoints: latency-svc-btnqx [742.547241ms]
Jul 22 21:05:54.847: INFO: Created: latency-svc-rd7pj
Jul 22 21:05:54.876: INFO: Got endpoints: latency-svc-tt9ns [749.315557ms]
Jul 22 21:05:54.894: INFO: Created: latency-svc-hfnvn
Jul 22 21:05:54.927: INFO: Got endpoints: latency-svc-d2x75 [750.998992ms]
Jul 22 21:05:54.945: INFO: Created: latency-svc-s4954
Jul 22 21:05:54.979: INFO: Got endpoints: latency-svc-fjjl9 [754.03978ms]
Jul 22 21:05:54.997: INFO: Created: latency-svc-s4dsq
Jul 22 21:05:55.026: INFO: Got endpoints: latency-svc-9gwx6 [749.23074ms]
Jul 22 21:05:55.047: INFO: Created: latency-svc-w8zxz
Jul 22 21:05:55.076: INFO: Got endpoints: latency-svc-5lk58 [737.95528ms]
Jul 22 21:05:55.095: INFO: Created: latency-svc-dzj67
Jul 22 21:05:55.129: INFO: Got endpoints: latency-svc-wrqv5 [753.424939ms]
Jul 22 21:05:55.149: INFO: Created: latency-svc-76k7w
Jul 22 21:05:55.175: INFO: Got endpoints: latency-svc-rftqk [749.796786ms]
Jul 22 21:05:55.200: INFO: Created: latency-svc-bzkw9
Jul 22 21:05:55.228: INFO: Got endpoints: latency-svc-nqdxd [745.945369ms]
Jul 22 21:05:55.249: INFO: Created: latency-svc-mfkp2
Jul 22 21:05:55.275: INFO: Got endpoints: latency-svc-j2wb2 [745.746945ms]
Jul 22 21:05:55.308: INFO: Created: latency-svc-zlgcw
Jul 22 21:05:55.325: INFO: Got endpoints: latency-svc-mfxnd [749.96447ms]
Jul 22 21:05:55.345: INFO: Created: latency-svc-2mtzf
Jul 22 21:05:55.376: INFO: Got endpoints: latency-svc-g457d [749.960485ms]
Jul 22 21:05:55.394: INFO: Created: latency-svc-q5qgl
Jul 22 21:05:55.425: INFO: Got endpoints: latency-svc-9mx2z [750.634773ms]
Jul 22 21:05:55.445: INFO: Created: latency-svc-mg55k
Jul 22 21:05:55.475: INFO: Got endpoints: latency-svc-cj4jh [748.975552ms]
Jul 22 21:05:55.493: INFO: Created: latency-svc-4sbw2
Jul 22 21:05:55.528: INFO: Got endpoints: latency-svc-pw4w9 [753.704158ms]
Jul 22 21:05:55.548: INFO: Created: latency-svc-kpr27
Jul 22 21:05:55.576: INFO: Got endpoints: latency-svc-rd7pj [751.161696ms]
Jul 22 21:05:55.595: INFO: Created: latency-svc-9pfhg
Jul 22 21:05:55.626: INFO: Got endpoints: latency-svc-hfnvn [750.420315ms]
Jul 22 21:05:55.645: INFO: Created: latency-svc-8t5dh
Jul 22 21:05:55.675: INFO: Got endpoints: latency-svc-s4954 [748.72848ms]
Jul 22 21:05:55.694: INFO: Created: latency-svc-7ktp6
Jul 22 21:05:55.728: INFO: Got endpoints: latency-svc-s4dsq [748.571335ms]
Jul 22 21:05:55.746: INFO: Created: latency-svc-wt89w
Jul 22 21:05:55.775: INFO: Got endpoints: latency-svc-w8zxz [749.040061ms]
Jul 22 21:05:55.795: INFO: Created: latency-svc-pw9ns
Jul 22 21:05:55.826: INFO: Got endpoints: latency-svc-dzj67 [750.357321ms]
Jul 22 21:05:55.847: INFO: Created: latency-svc-k65xb
Jul 22 21:05:55.880: INFO: Got endpoints: latency-svc-76k7w [750.990927ms]
Jul 22 21:05:55.900: INFO: Created: latency-svc-k88vr
Jul 22 21:05:55.925: INFO: Got endpoints: latency-svc-bzkw9 [749.908262ms]
Jul 22 21:05:55.946: INFO: Created: latency-svc-f2trn
Jul 22 21:05:55.975: INFO: Got endpoints: latency-svc-mfkp2 [746.568993ms]
Jul 22 21:05:55.997: INFO: Created: latency-svc-vsx5j
Jul 22 21:05:56.025: INFO: Got endpoints: latency-svc-zlgcw [750.039735ms]
Jul 22 21:05:56.047: INFO: Created: latency-svc-j7llf
Jul 22 21:05:56.075: INFO: Got endpoints: latency-svc-2mtzf [749.670474ms]
Jul 22 21:05:56.096: INFO: Created: latency-svc-t9klb
Jul 22 21:05:56.125: INFO: Got endpoints: latency-svc-q5qgl [749.736371ms]
Jul 22 21:05:56.144: INFO: Created: latency-svc-vsg6v
Jul 22 21:05:56.175: INFO: Got endpoints: latency-svc-mg55k [749.473489ms]
Jul 22 21:05:56.194: INFO: Created: latency-svc-q7bgp
Jul 22 21:05:56.230: INFO: Got endpoints: latency-svc-4sbw2 [755.073501ms]
Jul 22 21:05:56.254: INFO: Created: latency-svc-qvhdr
Jul 22 21:05:56.276: INFO: Got endpoints: latency-svc-kpr27 [747.650155ms]
Jul 22 21:05:56.295: INFO: Created: latency-svc-njfbd
Jul 22 21:05:56.325: INFO: Got endpoints: latency-svc-9pfhg [748.889577ms]
Jul 22 21:05:56.349: INFO: Created: latency-svc-kk7jg
Jul 22 21:05:56.381: INFO: Got endpoints: latency-svc-8t5dh [754.356711ms]
Jul 22 21:05:56.410: INFO: Created: latency-svc-49qkv
Jul 22 21:05:56.426: INFO: Got endpoints: latency-svc-7ktp6 [750.585363ms]
Jul 22 21:05:56.445: INFO: Created: latency-svc-fct2r
Jul 22 21:05:56.479: INFO: Got endpoints: latency-svc-wt89w [751.0816ms]
Jul 22 21:05:56.499: INFO: Created: latency-svc-bc42r
Jul 22 21:05:56.525: INFO: Got endpoints: latency-svc-pw9ns [749.88804ms]
Jul 22 21:05:56.546: INFO: Created: latency-svc-6dbt8
Jul 22 21:05:56.576: INFO: Got endpoints: latency-svc-k65xb [749.974554ms]
Jul 22 21:05:56.602: INFO: Created: latency-svc-wfr4j
Jul 22 21:05:56.626: INFO: Got endpoints: latency-svc-k88vr [745.995892ms]
Jul 22 21:05:56.646: INFO: Created: latency-svc-dghz2
Jul 22 21:05:56.675: INFO: Got endpoints: latency-svc-f2trn [749.739026ms]
Jul 22 21:05:56.700: INFO: Created: latency-svc-7dcvp
Jul 22 21:05:56.725: INFO: Got endpoints: latency-svc-vsx5j [750.348348ms]
Jul 22 21:05:56.749: INFO: Created: latency-svc-k7dc9
Jul 22 21:05:56.812: INFO: Got endpoints: latency-svc-j7llf [787.204034ms]
Jul 22 21:05:56.914: INFO: Got endpoints: latency-svc-t9klb [838.554529ms]
Jul 22 21:05:56.920: INFO: Got endpoints: latency-svc-vsg6v [794.164888ms]
Jul 22 21:05:56.931: INFO: Got endpoints: latency-svc-q7bgp [756.352183ms]
Jul 22 21:05:56.931: INFO: Created: latency-svc-fwk29
Jul 22 21:05:57.013: INFO: Got endpoints: latency-svc-qvhdr [783.207705ms]
Jul 22 21:05:57.020: INFO: Created: latency-svc-tm9kx
Jul 22 21:05:57.026: INFO: Created: latency-svc-thn59
Jul 22 21:05:57.032: INFO: Created: latency-svc-lfrrg
Jul 22 21:05:57.032: INFO: Got endpoints: latency-svc-njfbd [756.378606ms]
Jul 22 21:05:57.039: INFO: Created: latency-svc-zkhqs
Jul 22 21:05:57.052: INFO: Created: latency-svc-d5h7p
Jul 22 21:05:57.119: INFO: Got endpoints: latency-svc-kk7jg [793.946816ms]
Jul 22 21:05:57.124: INFO: Got endpoints: latency-svc-49qkv [743.511552ms]
Jul 22 21:05:57.140: INFO: Created: latency-svc-h2m48
Jul 22 21:05:57.154: INFO: Created: latency-svc-lfqbz
Jul 22 21:05:57.175: INFO: Got endpoints: latency-svc-fct2r [748.443704ms]
Jul 22 21:05:57.218: INFO: Created: latency-svc-w7vkc
Jul 22 21:05:57.230: INFO: Got endpoints: latency-svc-bc42r [751.169835ms]
Jul 22 21:05:57.250: INFO: Created: latency-svc-rc7nh
Jul 22 21:05:57.276: INFO: Got endpoints: latency-svc-6dbt8 [750.67328ms]
Jul 22 21:05:57.295: INFO: Created: latency-svc-z2sbr
Jul 22 21:05:57.326: INFO: Got endpoints: latency-svc-wfr4j [749.856617ms]
Jul 22 21:05:57.348: INFO: Created: latency-svc-gwjjc
Jul 22 21:05:57.379: INFO: Got endpoints: latency-svc-dghz2 [753.312876ms]
Jul 22 21:05:57.400: INFO: Created: latency-svc-fkxbt
Jul 22 21:05:57.424: INFO: Got endpoints: latency-svc-7dcvp [749.401622ms]
Jul 22 21:05:57.453: INFO: Created: latency-svc-mnlx2
Jul 22 21:05:57.476: INFO: Got endpoints: latency-svc-k7dc9 [750.339181ms]
Jul 22 21:05:57.496: INFO: Created: latency-svc-ftljp
Jul 22 21:05:57.530: INFO: Got endpoints: latency-svc-fwk29 [717.419964ms]
Jul 22 21:05:57.548: INFO: Created: latency-svc-rpjtf
Jul 22 21:05:57.575: INFO: Got endpoints: latency-svc-tm9kx [654.898627ms]
Jul 22 21:05:57.595: INFO: Created: latency-svc-5xw4d
Jul 22 21:05:57.628: INFO: Got endpoints: latency-svc-thn59 [696.544414ms]
Jul 22 21:05:57.651: INFO: Created: latency-svc-m7tmf
Jul 22 21:05:57.679: INFO: Got endpoints: latency-svc-lfrrg [765.562898ms]
Jul 22 21:05:57.697: INFO: Created: latency-svc-qvwh7
Jul 22 21:05:57.749: INFO: Got endpoints: latency-svc-zkhqs [735.424285ms]
Jul 22 21:05:57.783: INFO: Created: latency-svc-84z97
Jul 22 21:05:57.783: INFO: Got endpoints: latency-svc-d5h7p [751.13871ms]
Jul 22 21:05:57.802: INFO: Created: latency-svc-w7tjt
Jul 22 21:05:57.825: INFO: Got endpoints: latency-svc-h2m48 [705.823465ms]
Jul 22 21:05:57.843: INFO: Created: latency-svc-jczlg
Jul 22 21:05:57.875: INFO: Got endpoints: latency-svc-lfqbz [750.452411ms]
Jul 22 21:05:57.894: INFO: Created: latency-svc-ndg28
Jul 22 21:05:57.925: INFO: Got endpoints: latency-svc-w7vkc [750.183181ms]
Jul 22 21:05:57.943: INFO: Created: latency-svc-lgmpj
Jul 22 21:05:57.977: INFO: Got endpoints: latency-svc-rc7nh [746.576182ms]
Jul 22 21:05:57.999: INFO: Created: latency-svc-4mbt8
Jul 22 21:05:58.028: INFO: Got endpoints: latency-svc-z2sbr [752.00641ms]
Jul 22 21:05:58.046: INFO: Created: latency-svc-6hc2r
Jul 22 21:05:58.075: INFO: Got endpoints: latency-svc-gwjjc [748.610261ms]
Jul 22 21:05:58.101: INFO: Created: latency-svc-dnvkx
Jul 22 21:05:58.134: INFO: Got endpoints: latency-svc-fkxbt [754.149142ms]
Jul 22 21:05:58.153: INFO: Created: latency-svc-9rc7v
Jul 22 21:05:58.179: INFO: Got endpoints: latency-svc-mnlx2 [754.642083ms]
Jul 22 21:05:58.199: INFO: Created: latency-svc-crrnm
Jul 22 21:05:58.225: INFO: Got endpoints: latency-svc-ftljp [749.319218ms]
Jul 22 21:05:58.246: INFO: Created: latency-svc-whprn
Jul 22 21:05:58.275: INFO: Got endpoints: latency-svc-rpjtf [744.872816ms]
Jul 22 21:05:58.301: INFO: Created: latency-svc-xfz4x
Jul 22 21:05:58.334: INFO: Got endpoints: latency-svc-5xw4d [759.732298ms]
Jul 22 21:05:58.354: INFO: Created: latency-svc-l5kh9
Jul 22 21:05:58.376: INFO: Got endpoints: latency-svc-m7tmf [747.790648ms]
Jul 22 21:05:58.396: INFO: Created: latency-svc-m7xv6
Jul 22 21:05:58.425: INFO: Got endpoints: latency-svc-qvwh7 [745.310417ms]
Jul 22 21:05:58.446: INFO: Created: latency-svc-vhzjt
Jul 22 21:05:58.475: INFO: Got endpoints: latency-svc-84z97 [726.104658ms]
Jul 22 21:05:58.493: INFO: Created: latency-svc-v9jrz
Jul 22 21:05:58.532: INFO: Got endpoints: latency-svc-w7tjt [748.131854ms]
Jul 22 21:05:58.551: INFO: Created: latency-svc-p2mvz
Jul 22 21:05:58.575: INFO: Got endpoints: latency-svc-jczlg [749.676101ms]
Jul 22 21:05:58.595: INFO: Created: latency-svc-f7njv
Jul 22 21:05:58.625: INFO: Got endpoints: latency-svc-ndg28 [750.265679ms]
Jul 22 21:05:58.645: INFO: Created: latency-svc-nghv6
Jul 22 21:05:58.678: INFO: Got endpoints: latency-svc-lgmpj [753.349314ms]
Jul 22 21:05:58.696: INFO: Created: latency-svc-8w5qr
Jul 22 21:05:58.725: INFO: Got endpoints: latency-svc-4mbt8 [748.342129ms]
Jul 22 21:05:58.746: INFO: Created: latency-svc-4vjmx
Jul 22 21:05:58.775: INFO: Got endpoints: latency-svc-6hc2r [747.121696ms]
Jul 22 21:05:58.795: INFO: Created: latency-svc-wnwjb
Jul 22 21:05:58.825: INFO: Got endpoints: latency-svc-dnvkx [750.122014ms]
Jul 22 21:05:58.844: INFO: Created: latency-svc-746gr
Jul 22 21:05:58.876: INFO: Got endpoints: latency-svc-9rc7v [742.325539ms]
Jul 22 21:05:58.896: INFO: Created: latency-svc-6cq64
Jul 22 21:05:58.924: INFO: Got endpoints: latency-svc-crrnm [745.315213ms]
Jul 22 21:05:58.943: INFO: Created: latency-svc-mbqjt
Jul 22 21:05:58.976: INFO: Got endpoints: latency-svc-whprn [751.005301ms]
Jul 22 21:05:58.998: INFO: Created: latency-svc-q46m4
Jul 22 21:05:59.025: INFO: Got endpoints: latency-svc-xfz4x [750.731949ms]
Jul 22 21:05:59.049: INFO: Created: latency-svc-x6jjn
Jul 22 21:05:59.078: INFO: Got endpoints: latency-svc-l5kh9 [743.971872ms]
Jul 22 21:05:59.101: INFO: Created: latency-svc-pnz5k
Jul 22 21:05:59.127: INFO: Got endpoints: latency-svc-m7xv6 [751.218649ms]
Jul 22 21:05:59.148: INFO: Created: latency-svc-fklvt
Jul 22 21:05:59.175: INFO: Got endpoints: latency-svc-vhzjt [750.277005ms]
Jul 22 21:05:59.197: INFO: Created: latency-svc-bnz6d
Jul 22 21:05:59.225: INFO: Got endpoints: latency-svc-v9jrz [749.984471ms]
Jul 22 21:05:59.243: INFO: Created: latency-svc-qxwdc
Jul 22 21:05:59.279: INFO: Got endpoints: latency-svc-p2mvz [747.649783ms]
Jul 22 21:05:59.308: INFO: Created: latency-svc-wkt59
Jul 22 21:05:59.326: INFO: Got endpoints: latency-svc-f7njv [750.997652ms]
Jul 22 21:05:59.346: INFO: Created: latency-svc-8rb4r
Jul 22 21:05:59.378: INFO: Got endpoints: latency-svc-nghv6 [752.696277ms]
Jul 22 21:05:59.402: INFO: Created: latency-svc-qg98n
Jul 22 21:05:59.425: INFO: Got endpoints: latency-svc-8w5qr [747.192458ms]
Jul 22 21:05:59.445: INFO: Created: latency-svc-2kp56
Jul 22 21:05:59.480: INFO: Got endpoints: latency-svc-4vjmx [755.030195ms]
Jul 22 21:05:59.499: INFO: Created: latency-svc-62rfl
Jul 22 21:05:59.527: INFO: Got endpoints: latency-svc-wnwjb [751.989237ms]
Jul 22 21:05:59.548: INFO: Created: latency-svc-nh5t8
Jul 22 21:05:59.575: INFO: Got endpoints: latency-svc-746gr [749.583431ms]
Jul 22 21:05:59.602: INFO: Created: latency-svc-fhlmt
Jul 22 21:05:59.625: INFO: Got endpoints: latency-svc-6cq64 [749.161775ms]
Jul 22 21:05:59.646: INFO: Created: latency-svc-tcbqh
Jul 22 21:05:59.675: INFO: Got endpoints: latency-svc-mbqjt [750.513932ms]
Jul 22 21:05:59.696: INFO: Created: latency-svc-xbxd6
Jul 22 21:05:59.725: INFO: Got endpoints: latency-svc-q46m4 [748.928672ms]
Jul 22 21:05:59.744: INFO: Created: latency-svc-l479v
Jul 22 21:05:59.776: INFO: Got endpoints: latency-svc-x6jjn [750.190388ms]
Jul 22 21:05:59.797: INFO: Created: latency-svc-5lzjc
Jul 22 21:05:59.833: INFO: Got endpoints: latency-svc-pnz5k [754.44148ms]
Jul 22 21:05:59.854: INFO: Created: latency-svc-s7bwb
Jul 22 21:05:59.877: INFO: Got endpoints: latency-svc-fklvt [749.362155ms]
Jul 22 21:05:59.896: INFO: Created: latency-svc-fksdd
Jul 22 21:05:59.926: INFO: Got endpoints: latency-svc-bnz6d [751.14056ms]
Jul 22 21:05:59.945: INFO: Created: latency-svc-k9cx9
Jul 22 21:05:59.975: INFO: Got endpoints: latency-svc-qxwdc [750.29782ms]
Jul 22 21:05:59.997: INFO: Created: latency-svc-fnkh7
Jul 22 21:06:00.030: INFO: Got endpoints: latency-svc-wkt59 [750.314501ms]
Jul 22 21:06:00.049: INFO: Created: latency-svc-tbcpp
Jul 22 21:06:00.076: INFO: Got endpoints: latency-svc-8rb4r [750.231539ms]
Jul 22 21:06:00.097: INFO: Created: latency-svc-wwvrx
Jul 22 21:06:00.126: INFO: Got endpoints: latency-svc-qg98n [748.347319ms]
Jul 22 21:06:00.146: INFO: Created: latency-svc-wmhgj
Jul 22 21:06:00.184: INFO: Got endpoints: latency-svc-2kp56 [758.688937ms]
Jul 22 21:06:00.204: INFO: Created: latency-svc-gb7f9
Jul 22 21:06:00.226: INFO: Got endpoints: latency-svc-62rfl [745.581545ms]
Jul 22 21:06:00.254: INFO: Created: latency-svc-l4vtx
Jul 22 21:06:00.275: INFO: Got endpoints: latency-svc-nh5t8 [747.462383ms]
Jul 22 21:06:00.295: INFO: Created: latency-svc-8b6m4
Jul 22 21:06:00.329: INFO: Got endpoints: latency-svc-fhlmt [753.813603ms]
Jul 22 21:06:00.360: INFO: Created: latency-svc-gzg2w
Jul 22 21:06:00.380: INFO: Got endpoints: latency-svc-tcbqh [754.960913ms]
Jul 22 21:06:00.400: INFO: Created: latency-svc-ttkvd
Jul 22 21:06:00.425: INFO: Got endpoints: latency-svc-xbxd6 [750.45593ms]
Jul 22 21:06:00.445: INFO: Created: latency-svc-hcnl7
Jul 22 21:06:00.476: INFO: Got endpoints: latency-svc-l479v [750.936712ms]
Jul 22 21:06:00.495: INFO: Created: latency-svc-j2bfr
Jul 22 21:06:00.525: INFO: Got endpoints: latency-svc-5lzjc [749.590714ms]
Jul 22 21:06:00.546: INFO: Created: latency-svc-htx7w
Jul 22 21:06:00.575: INFO: Got endpoints: latency-svc-s7bwb [741.607132ms]
Jul 22 21:06:00.594: INFO: Created: latency-svc-527ss
Jul 22 21:06:00.626: INFO: Got endpoints: latency-svc-fksdd [749.117875ms]
Jul 22 21:06:00.645: INFO: Created: latency-svc-8m4sg
Jul 22 21:06:00.676: INFO: Got endpoints: latency-svc-k9cx9 [749.248207ms]
Jul 22 21:06:00.694: INFO: Created: latency-svc-9l7zw
Jul 22 21:06:00.726: INFO: Got endpoints: latency-svc-fnkh7 [750.977512ms]
Jul 22 21:06:00.746: INFO: Created: latency-svc-bf5bf
Jul 22 21:06:00.782: INFO: Got endpoints: latency-svc-tbcpp [752.667084ms]
Jul 22 21:06:00.829: INFO: Got endpoints: latency-svc-wwvrx [752.71259ms]
Jul 22 21:06:00.876: INFO: Got endpoints: latency-svc-wmhgj [749.607879ms]
Jul 22 21:06:00.926: INFO: Got endpoints: latency-svc-gb7f9 [741.551542ms]
Jul 22 21:06:00.976: INFO: Got endpoints: latency-svc-l4vtx [750.09326ms]
Jul 22 21:06:01.033: INFO: Got endpoints: latency-svc-8b6m4 [758.061779ms]
Jul 22 21:06:01.075: INFO: Got endpoints: latency-svc-gzg2w [746.769622ms]
Jul 22 21:06:01.133: INFO: Got endpoints: latency-svc-ttkvd [752.628555ms]
Jul 22 21:06:01.179: INFO: Got endpoints: latency-svc-hcnl7 [753.280456ms]
Jul 22 21:06:01.226: INFO: Got endpoints: latency-svc-j2bfr [749.986204ms]
Jul 22 21:06:01.275: INFO: Got endpoints: latency-svc-htx7w [749.526501ms]
Jul 22 21:06:01.332: INFO: Got endpoints: latency-svc-527ss [757.200605ms]
Jul 22 21:06:01.376: INFO: Got endpoints: latency-svc-8m4sg [749.943513ms]
Jul 22 21:06:01.432: INFO: Got endpoints: latency-svc-9l7zw [756.206109ms]
Jul 22 21:06:01.476: INFO: Got endpoints: latency-svc-bf5bf [749.479162ms]
Jul 22 21:06:01.476: INFO: Latencies: [23.629055ms 31.326411ms 97.502668ms 97.595053ms 105.749916ms 116.493224ms 124.190256ms 127.967265ms 136.666539ms 188.833581ms 195.078655ms 195.411445ms 201.664134ms 205.274271ms 206.922364ms 210.748221ms 215.312217ms 215.746456ms 221.0856ms 224.381534ms 233.785703ms 236.850866ms 239.576624ms 240.257625ms 245.092504ms 259.241835ms 259.855736ms 261.711867ms 264.374315ms 266.711497ms 283.602644ms 283.659428ms 284.329585ms 287.134544ms 287.175319ms 287.625252ms 290.46586ms 292.709813ms 292.814777ms 293.609993ms 293.658547ms 294.819811ms 295.019778ms 300.910017ms 301.285708ms 313.487852ms 357.00968ms 363.697362ms 396.516074ms 436.455685ms 474.048444ms 515.417118ms 528.074923ms 542.191321ms 591.391339ms 625.041715ms 654.898627ms 679.995811ms 696.544414ms 705.823465ms 706.242956ms 710.009882ms 712.63502ms 717.419964ms 726.104658ms 735.424285ms 737.95528ms 741.551542ms 741.607132ms 742.325539ms 742.547241ms 743.511552ms 743.971872ms 744.872816ms 745.310417ms 745.315213ms 745.581545ms 745.746945ms 745.945369ms 745.995892ms 746.568993ms 746.576182ms 746.741887ms 746.769622ms 747.121696ms 747.192458ms 747.462383ms 747.649783ms 747.650155ms 747.790648ms 748.131854ms 748.342129ms 748.347319ms 748.372601ms 748.443704ms 748.571335ms 748.608636ms 748.610261ms 748.72848ms 748.889577ms 748.928672ms 748.975552ms 749.040061ms 749.117875ms 749.161775ms 749.23074ms 749.248207ms 749.315557ms 749.319218ms 749.345049ms 749.362155ms 749.401622ms 749.473489ms 749.479162ms 749.526501ms 749.583431ms 749.590714ms 749.607879ms 749.670474ms 749.674795ms 749.676101ms 749.736371ms 749.739026ms 749.796786ms 749.856617ms 749.88804ms 749.908262ms 749.943513ms 749.960485ms 749.96447ms 749.974554ms 749.984471ms 749.986204ms 750.039735ms 750.09326ms 750.122014ms 750.183181ms 750.190388ms 750.231539ms 750.265679ms 750.277005ms 750.29782ms 750.314501ms 750.339181ms 750.348348ms 750.357321ms 750.420315ms 750.452411ms 750.45593ms 750.513932ms 750.585363ms 750.634773ms 750.67328ms 750.731949ms 750.936712ms 750.977512ms 750.990927ms 750.997652ms 750.998992ms 751.005301ms 751.0816ms 751.13871ms 751.14056ms 751.161696ms 751.169835ms 751.218649ms 751.989237ms 752.00641ms 752.209545ms 752.628555ms 752.667084ms 752.696277ms 752.71259ms 753.280456ms 753.312876ms 753.349314ms 753.424939ms 753.704158ms 753.813603ms 754.03978ms 754.149142ms 754.356711ms 754.44148ms 754.642083ms 754.960913ms 755.030195ms 755.073501ms 756.206109ms 756.352183ms 756.378606ms 757.200605ms 758.061779ms 758.688937ms 759.732298ms 765.562898ms 783.207705ms 787.204034ms 793.946816ms 794.164888ms 838.554529ms]
Jul 22 21:06:01.476: INFO: 50 %ile: 748.928672ms
Jul 22 21:06:01.476: INFO: 90 %ile: 754.149142ms
Jul 22 21:06:01.476: INFO: 99 %ile: 794.164888ms
Jul 22 21:06:01.476: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:06:01.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6267" for this suite.
•{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":178,"skipped":3084,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:06:01.513: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1476.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1476.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1476.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1476.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1476.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1476.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1476.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1476.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1476.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1476.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1476.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1476.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 229.215.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.215.229_udp@PTR;check="$$(dig +tcp +noall +answer +search 229.215.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.215.229_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1476.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1476.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1476.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1476.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1476.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1476.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1476.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1476.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1476.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1476.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1476.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1476.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1476.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 229.215.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.215.229_udp@PTR;check="$$(dig +tcp +noall +answer +search 229.215.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.215.229_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:06:05.930: INFO: Unable to read wheezy_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:06.017: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:06.122: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:06.636: INFO: Unable to read jessie_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:06.677: INFO: Unable to read jessie_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:06.786: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:07.244: INFO: Lookups using dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078 failed for: [wheezy_udp@dns-test-service.dns-1476.svc.cluster.local wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1476.svc.cluster.local jessie_udp@dns-test-service.dns-1476.svc.cluster.local jessie_tcp@dns-test-service.dns-1476.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1476.svc.cluster.local]

Jul 22 21:06:12.283: INFO: Unable to read wheezy_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:12.314: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:12.962: INFO: Unable to read jessie_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:12.993: INFO: Unable to read jessie_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:13.589: INFO: Lookups using dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078 failed for: [wheezy_udp@dns-test-service.dns-1476.svc.cluster.local wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local jessie_udp@dns-test-service.dns-1476.svc.cluster.local jessie_tcp@dns-test-service.dns-1476.svc.cluster.local]

Jul 22 21:06:17.277: INFO: Unable to read wheezy_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:17.311: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:17.956: INFO: Unable to read jessie_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:17.986: INFO: Unable to read jessie_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:18.585: INFO: Lookups using dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078 failed for: [wheezy_udp@dns-test-service.dns-1476.svc.cluster.local wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local jessie_udp@dns-test-service.dns-1476.svc.cluster.local jessie_tcp@dns-test-service.dns-1476.svc.cluster.local]

Jul 22 21:06:22.277: INFO: Unable to read wheezy_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:22.309: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:22.984: INFO: Unable to read jessie_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:23.014: INFO: Unable to read jessie_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:23.615: INFO: Lookups using dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078 failed for: [wheezy_udp@dns-test-service.dns-1476.svc.cluster.local wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local jessie_udp@dns-test-service.dns-1476.svc.cluster.local jessie_tcp@dns-test-service.dns-1476.svc.cluster.local]

Jul 22 21:06:27.275: INFO: Unable to read wheezy_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:27.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:27.977: INFO: Unable to read jessie_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:28.039: INFO: Unable to read jessie_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:28.647: INFO: Lookups using dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078 failed for: [wheezy_udp@dns-test-service.dns-1476.svc.cluster.local wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local jessie_udp@dns-test-service.dns-1476.svc.cluster.local jessie_tcp@dns-test-service.dns-1476.svc.cluster.local]

Jul 22 21:06:32.276: INFO: Unable to read wheezy_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:32.313: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:32.942: INFO: Unable to read jessie_udp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:32.973: INFO: Unable to read jessie_tcp@dns-test-service.dns-1476.svc.cluster.local from pod dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078: the server could not find the requested resource (get pods dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078)
Jul 22 21:06:33.599: INFO: Lookups using dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078 failed for: [wheezy_udp@dns-test-service.dns-1476.svc.cluster.local wheezy_tcp@dns-test-service.dns-1476.svc.cluster.local jessie_udp@dns-test-service.dns-1476.svc.cluster.local jessie_tcp@dns-test-service.dns-1476.svc.cluster.local]

Jul 22 21:06:38.721: INFO: DNS probes using dns-1476/dns-test-06e0e20c-375b-4b50-8e86-95072d4a9078 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:06:38.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1476" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":179,"skipped":3098,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:06:38.825: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:06:39.839: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 22 21:06:41.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584799, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584799, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584799, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584799, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:06:43.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584799, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584799, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584799, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584799, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:06:46.921: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul 22 21:06:51.141: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=webhook-4702 attach --namespace=webhook-4702 to-be-attached-pod -i -c=container1'
Jul 22 21:06:51.544: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:06:51.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4702" for this suite.
STEP: Destroying namespace "webhook-4702-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":180,"skipped":3115,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:06:51.692: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Jul 22 21:06:51.893: INFO: namespace kubectl-4820
Jul 22 21:06:51.893: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4820 create -f -'
Jul 22 21:06:52.313: INFO: stderr: ""
Jul 22 21:06:52.313: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 22 21:06:53.326: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:06:53.326: INFO: Found 0 / 1
Jul 22 21:06:54.327: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:06:54.327: INFO: Found 0 / 1
Jul 22 21:06:55.326: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:06:55.326: INFO: Found 0 / 1
Jul 22 21:06:56.328: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:06:56.328: INFO: Found 1 / 1
Jul 22 21:06:56.328: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 22 21:06:56.340: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:06:56.341: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 21:06:56.341: INFO: wait on agnhost-primary startup in kubectl-4820 
Jul 22 21:06:56.341: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4820 logs agnhost-primary-78xmb agnhost-primary'
Jul 22 21:06:56.547: INFO: stderr: ""
Jul 22 21:06:56.547: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul 22 21:06:56.547: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4820 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jul 22 21:06:56.700: INFO: stderr: ""
Jul 22 21:06:56.700: INFO: stdout: "service/rm2 exposed\n"
Jul 22 21:06:56.711: INFO: Service rm2 in namespace kubectl-4820 found.
STEP: exposing service
Jul 22 21:06:58.736: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4820 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jul 22 21:06:58.892: INFO: stderr: ""
Jul 22 21:06:58.892: INFO: stdout: "service/rm3 exposed\n"
Jul 22 21:06:58.904: INFO: Service rm3 in namespace kubectl-4820 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:07:00.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4820" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":181,"skipped":3132,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:07:00.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Jul 22 21:07:01.162: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1208 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:07:01.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1208" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":182,"skipped":3134,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:07:01.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-1c9365e8-6cb2-47b5-bde5-e4981874bf3c
STEP: Creating a pod to test consume secrets
Jul 22 21:07:01.542: INFO: Waiting up to 5m0s for pod "pod-secrets-26d8a34a-a447-4cc3-8f1d-29ee5260eea4" in namespace "secrets-8407" to be "Succeeded or Failed"
Jul 22 21:07:01.555: INFO: Pod "pod-secrets-26d8a34a-a447-4cc3-8f1d-29ee5260eea4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.499072ms
Jul 22 21:07:03.568: INFO: Pod "pod-secrets-26d8a34a-a447-4cc3-8f1d-29ee5260eea4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025684415s
Jul 22 21:07:05.581: INFO: Pod "pod-secrets-26d8a34a-a447-4cc3-8f1d-29ee5260eea4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038211712s
STEP: Saw pod success
Jul 22 21:07:05.581: INFO: Pod "pod-secrets-26d8a34a-a447-4cc3-8f1d-29ee5260eea4" satisfied condition "Succeeded or Failed"
Jul 22 21:07:05.594: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-secrets-26d8a34a-a447-4cc3-8f1d-29ee5260eea4 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:07:05.752: INFO: Waiting for pod pod-secrets-26d8a34a-a447-4cc3-8f1d-29ee5260eea4 to disappear
Jul 22 21:07:05.763: INFO: Pod pod-secrets-26d8a34a-a447-4cc3-8f1d-29ee5260eea4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:07:05.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8407" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":183,"skipped":3154,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:07:05.804: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Jul 22 21:07:05.990: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2528 create -f -'
Jul 22 21:07:06.271: INFO: stderr: ""
Jul 22 21:07:06.271: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jul 22 21:07:06.271: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2528 diff -f -'
Jul 22 21:07:06.682: INFO: rc: 1
Jul 22 21:07:06.682: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2528 delete -f -'
Jul 22 21:07:06.825: INFO: stderr: ""
Jul 22 21:07:06.825: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:07:06.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2528" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":184,"skipped":3171,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:07:06.861: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:07:07.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1efa6bc-04f8-48a6-a3cc-8fe269713a07" in namespace "projected-5310" to be "Succeeded or Failed"
Jul 22 21:07:07.098: INFO: Pod "downwardapi-volume-d1efa6bc-04f8-48a6-a3cc-8fe269713a07": Phase="Pending", Reason="", readiness=false. Elapsed: 28.162338ms
Jul 22 21:07:09.115: INFO: Pod "downwardapi-volume-d1efa6bc-04f8-48a6-a3cc-8fe269713a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044851178s
Jul 22 21:07:11.217: INFO: Pod "downwardapi-volume-d1efa6bc-04f8-48a6-a3cc-8fe269713a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.147750415s
STEP: Saw pod success
Jul 22 21:07:11.218: INFO: Pod "downwardapi-volume-d1efa6bc-04f8-48a6-a3cc-8fe269713a07" satisfied condition "Succeeded or Failed"
Jul 22 21:07:11.229: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-d1efa6bc-04f8-48a6-a3cc-8fe269713a07 container client-container: <nil>
STEP: delete the pod
Jul 22 21:07:11.596: INFO: Waiting for pod downwardapi-volume-d1efa6bc-04f8-48a6-a3cc-8fe269713a07 to disappear
Jul 22 21:07:11.607: INFO: Pod downwardapi-volume-d1efa6bc-04f8-48a6-a3cc-8fe269713a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:07:11.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5310" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":185,"skipped":3177,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:07:11.662: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jul 22 21:07:15.910: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9655 PodName:var-expansion-08c57340-a2ad-4e81-89f4-f9d014a721c4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:07:15.910: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: test for file in mounted path
Jul 22 21:07:16.330: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9655 PodName:var-expansion-08c57340-a2ad-4e81-89f4-f9d014a721c4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:07:16.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: updating the annotation value
Jul 22 21:07:17.257: INFO: Successfully updated pod "var-expansion-08c57340-a2ad-4e81-89f4-f9d014a721c4"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jul 22 21:07:17.270: INFO: Deleting pod "var-expansion-08c57340-a2ad-4e81-89f4-f9d014a721c4" in namespace "var-expansion-9655"
Jul 22 21:07:17.285: INFO: Wait up to 5m0s for pod "var-expansion-08c57340-a2ad-4e81-89f4-f9d014a721c4" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:07:53.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9655" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":186,"skipped":3181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:07:53.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5402
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Jul 22 21:07:53.558: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:08:15.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5402" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":187,"skipped":3207,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:08:15.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-b03c56c1-72b4-4ce1-8740-273f77f1a0cc
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:08:15.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2056" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":188,"skipped":3226,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:08:15.429: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 22 21:08:15.653: INFO: Waiting up to 5m0s for pod "pod-d26bd7eb-47d8-4774-8b67-f9773804889e" in namespace "emptydir-7164" to be "Succeeded or Failed"
Jul 22 21:08:15.665: INFO: Pod "pod-d26bd7eb-47d8-4774-8b67-f9773804889e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.818677ms
Jul 22 21:08:17.684: INFO: Pod "pod-d26bd7eb-47d8-4774-8b67-f9773804889e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030684352s
Jul 22 21:08:19.696: INFO: Pod "pod-d26bd7eb-47d8-4774-8b67-f9773804889e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043325216s
STEP: Saw pod success
Jul 22 21:08:19.696: INFO: Pod "pod-d26bd7eb-47d8-4774-8b67-f9773804889e" satisfied condition "Succeeded or Failed"
Jul 22 21:08:19.709: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-d26bd7eb-47d8-4774-8b67-f9773804889e container test-container: <nil>
STEP: delete the pod
Jul 22 21:08:19.778: INFO: Waiting for pod pod-d26bd7eb-47d8-4774-8b67-f9773804889e to disappear
Jul 22 21:08:19.795: INFO: Pod pod-d26bd7eb-47d8-4774-8b67-f9773804889e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:08:19.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7164" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3261,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:08:19.833: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-9913
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 21:08:20.036: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 22 21:08:20.115: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:08:22.128: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:08:24.128: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:08:26.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:08:28.128: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:08:30.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:08:32.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:08:34.128: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 22 21:08:34.154: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 22 21:08:36.168: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 22 21:08:38.167: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 22 21:08:40.167: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 22 21:08:44.284: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 22 21:08:44.284: INFO: Going to poll 100.96.1.199 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Jul 22 21:08:44.296: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.199:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9913 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:08:44.296: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:08:44.709: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 22 21:08:44.709: INFO: Going to poll 100.96.0.62 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Jul 22 21:08:44.722: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.62:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9913 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:08:44.722: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:08:45.177: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:08:45.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9913" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:08:45.214: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7596
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:08:45.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7596" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":191,"skipped":3317,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:08:45.482: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:08:45.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7623" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":192,"skipped":3322,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:08:45.804: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 22 21:08:50.590: INFO: Successfully updated pod "pod-update-1ae48eee-783b-443e-b191-a9736da8c273"
STEP: verifying the updated pod is in kubernetes
Jul 22 21:08:50.617: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:08:50.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5271" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":193,"skipped":3323,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:08:50.651: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2808
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:08:50.848: INFO: Creating deployment "webserver-deployment"
Jul 22 21:08:50.861: INFO: Waiting for observed generation 1
Jul 22 21:08:52.886: INFO: Waiting for all required pods to come up
Jul 22 21:08:52.921: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 22 21:09:00.947: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 22 21:09:00.970: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 22 21:09:00.996: INFO: Updating deployment webserver-deployment
Jul 22 21:09:00.996: INFO: Waiting for observed generation 2
Jul 22 21:09:03.022: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 22 21:09:03.035: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 22 21:09:03.046: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 22 21:09:03.081: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 22 21:09:03.081: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 22 21:09:03.092: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 22 21:09:03.114: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 22 21:09:03.114: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 22 21:09:03.143: INFO: Updating deployment webserver-deployment
Jul 22 21:09:03.143: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 22 21:09:03.168: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 22 21:09:05.225: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jul 22 21:09:05.249: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2808  4aa49306-974d-47a5-aeb3-7ab13161b4ab 32031 3 2021-07-22 21:08:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-22 21:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b4cd78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-22 21:09:03 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-07-22 21:09:03 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 22 21:09:05.261: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-2808  22d203a3-bdc8-4da9-941c-c587647bb34f 32029 3 2021-07-22 21:09:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4aa49306-974d-47a5-aeb3-7ab13161b4ab 0xc003b4d247 0xc003b4d248}] []  [{kube-controller-manager Update apps/v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4aa49306-974d-47a5-aeb3-7ab13161b4ab\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b4d2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 21:09:05.261: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 22 21:09:05.261: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-2808  b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 32018 3 2021-07-22 21:08:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4aa49306-974d-47a5-aeb3-7ab13161b4ab 0xc003b4d357 0xc003b4d358}] []  [{kube-controller-manager Update apps/v1 2021-07-22 21:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4aa49306-974d-47a5-aeb3-7ab13161b4ab\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b4d3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 22 21:09:05.295: INFO: Pod "webserver-deployment-795d758f88-5shqj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5shqj webserver-deployment-795d758f88- deployment-2808  2bd98958-58a1-4fc1-890a-9c95c61e901d 32034 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003b778c7 0xc003b778c8}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.296: INFO: Pod "webserver-deployment-795d758f88-6qg8f" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6qg8f webserver-deployment-795d758f88- deployment-2808  88be2c56-7865-478a-a27e-a3efadf5892d 31994 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003b77a70 0xc003b77a71}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.296: INFO: Pod "webserver-deployment-795d758f88-8jk8b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8jk8b webserver-deployment-795d758f88- deployment-2808  be088f78-63ea-4cab-9303-294f91f76fc6 32044 0 2021-07-22 21:09:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.210/32 cni.projectcalico.org/podIPs:100.96.1.210/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003b77c20 0xc003b77c21}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-22 21:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.296: INFO: Pod "webserver-deployment-795d758f88-976tq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-976tq webserver-deployment-795d758f88- deployment-2808  b2e4167e-db4b-4e91-a33b-2df080980287 32039 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003b77de0 0xc003b77de1}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.296: INFO: Pod "webserver-deployment-795d758f88-9jwn7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9jwn7 webserver-deployment-795d758f88- deployment-2808  249e900c-df2d-4162-868e-ba7fc66614d0 31965 0 2021-07-22 21:09:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.0.67/32 cni.projectcalico.org/podIPs:100.96.0.67/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003b77fd0 0xc003b77fd1}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-22 21:09:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.296: INFO: Pod "webserver-deployment-795d758f88-bpdgd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bpdgd webserver-deployment-795d758f88- deployment-2808  b238a2a0-201c-461d-9bf1-74f2201af5a9 32042 0 2021-07-22 21:09:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.208/32 cni.projectcalico.org/podIPs:100.96.1.208/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003bac250 0xc003bac251}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.296: INFO: Pod "webserver-deployment-795d758f88-dlxz7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dlxz7 webserver-deployment-795d758f88- deployment-2808  e05e89b4-4635-4523-a82c-0eee8a0fef6e 32026 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003bac480 0xc003bac481}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.296: INFO: Pod "webserver-deployment-795d758f88-ghz6f" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ghz6f webserver-deployment-795d758f88- deployment-2808  30fc52b6-33e4-4eff-9871-0e46d43d6872 32037 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003bac650 0xc003bac651}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.297: INFO: Pod "webserver-deployment-795d758f88-hxvkx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hxvkx webserver-deployment-795d758f88- deployment-2808  e60d4769-2347-4602-bd9c-521e805e0330 31966 0 2021-07-22 21:09:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.0.68/32 cni.projectcalico.org/podIPs:100.96.0.68/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003bac860 0xc003bac861}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.297: INFO: Pod "webserver-deployment-795d758f88-jdwc2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jdwc2 webserver-deployment-795d758f88- deployment-2808  599d458f-d7d0-46a9-a585-f5a24b14f76f 32043 0 2021-07-22 21:09:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.209/32 cni.projectcalico.org/podIPs:100.96.1.209/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003bacaa0 0xc003bacaa1}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.297: INFO: Pod "webserver-deployment-795d758f88-qg4fm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qg4fm webserver-deployment-795d758f88- deployment-2808  b0031910-e055-4dd5-8bd5-0cee659128e8 32038 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003bacce0 0xc003bacce1}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.297: INFO: Pod "webserver-deployment-795d758f88-wlsm6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wlsm6 webserver-deployment-795d758f88- deployment-2808  519f9920-d488-4b3f-9b70-951f960fe77e 32030 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003bacec0 0xc003bacec1}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.297: INFO: Pod "webserver-deployment-795d758f88-xc5nc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xc5nc webserver-deployment-795d758f88- deployment-2808  f7b3bd7d-44b4-4385-b002-f8dcb48ddf20 32033 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 22d203a3-bdc8-4da9-941c-c587647bb34f 0xc003bad060 0xc003bad061}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d203a3-bdc8-4da9-941c-c587647bb34f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.297: INFO: Pod "webserver-deployment-dd94f59b7-2dk9m" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2dk9m webserver-deployment-dd94f59b7- deployment-2808  5d7c8512-41d9-4c39-8add-513fda0768f5 32036 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bad200 0xc003bad201}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.297: INFO: Pod "webserver-deployment-dd94f59b7-44drx" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-44drx webserver-deployment-dd94f59b7- deployment-2808  7595337c-4c67-4c78-b012-eabe16991cdf 31853 0 2021-07-22 21:08:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.0.63/32 cni.projectcalico.org/podIPs:100.96.0.63/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bad390 0xc003bad391}] []  [{kube-controller-manager Update v1 2021-07-22 21:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 21:08:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 21:08:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.0.63,StartTime:2021-07-22 21:08:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 21:08:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://04e29b0c28e740cb00e7fe451d0a9fd3924b7c85177cac4ba1d6d995602aa7d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.298: INFO: Pod "webserver-deployment-dd94f59b7-55r5b" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-55r5b webserver-deployment-dd94f59b7- deployment-2808  6f978c4d-f963-42cb-9624-958716119c51 31856 0 2021-07-22 21:08:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.0.65/32 cni.projectcalico.org/podIPs:100.96.0.65/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bad570 0xc003bad571}] []  [{kube-controller-manager Update v1 2021-07-22 21:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 21:08:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 21:08:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.0.65,StartTime:2021-07-22 21:08:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 21:08:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a3165117f681839cea043db95aede119baff9527756b323fa7c0d88e3965475e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.298: INFO: Pod "webserver-deployment-dd94f59b7-6mvmd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6mvmd webserver-deployment-dd94f59b7- deployment-2808  bd543be7-a9b6-4e87-b4a9-4f199f87e303 32028 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bad720 0xc003bad721}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.298: INFO: Pod "webserver-deployment-dd94f59b7-9g6tv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9g6tv webserver-deployment-dd94f59b7- deployment-2808  c7c03e0d-46e9-44fc-be0f-16d4d77fd187 31863 0 2021-07-22 21:08:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.0.64/32 cni.projectcalico.org/podIPs:100.96.0.64/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bad900 0xc003bad901}] []  [{kube-controller-manager Update v1 2021-07-22 21:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 21:08:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 21:08:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.0.64,StartTime:2021-07-22 21:08:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 21:08:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3db35e54c4b92783c51c2d2a91222f936d8073bce8f2b3037dda96c91823f679,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.298: INFO: Pod "webserver-deployment-dd94f59b7-cdspk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cdspk webserver-deployment-dd94f59b7- deployment-2808  da857105-6be4-499d-a391-0f0551f9d936 32035 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003badaf0 0xc003badaf1}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.298: INFO: Pod "webserver-deployment-dd94f59b7-dcm8q" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dcm8q webserver-deployment-dd94f59b7- deployment-2808  04afe5a8-9afa-4ec9-8025-fefabcbbf67f 32025 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003badc70 0xc003badc71}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.298: INFO: Pod "webserver-deployment-dd94f59b7-jnpmh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jnpmh webserver-deployment-dd94f59b7- deployment-2808  c58d29c8-28b0-4b07-95f1-d3c9ca7fd42e 32024 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003badde0 0xc003badde1}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.298: INFO: Pod "webserver-deployment-dd94f59b7-m77dh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-m77dh webserver-deployment-dd94f59b7- deployment-2808  ee5b4d69-3310-436c-bbf2-6851d2ab4fd3 32027 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003badf50 0xc003badf51}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.298: INFO: Pod "webserver-deployment-dd94f59b7-mdn5q" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mdn5q webserver-deployment-dd94f59b7- deployment-2808  45849995-8a6f-4804-9212-5d0ea8f7c426 32005 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bce120 0xc003bce121}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.299: INFO: Pod "webserver-deployment-dd94f59b7-nlq28" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nlq28 webserver-deployment-dd94f59b7- deployment-2808  47d8fbc5-28fb-4c80-a57c-5d135102200a 32019 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bce2c0 0xc003bce2c1}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.299: INFO: Pod "webserver-deployment-dd94f59b7-r965t" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-r965t webserver-deployment-dd94f59b7- deployment-2808  cf14c2de-714d-4471-9743-ce956a4e4b8c 32032 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bce440 0xc003bce441}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.299: INFO: Pod "webserver-deployment-dd94f59b7-rtwpm" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rtwpm webserver-deployment-dd94f59b7- deployment-2808  e5c3b012-3d94-4650-bfd6-935f6e5f8f52 31899 0 2021-07-22 21:08:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.1.207/32 cni.projectcalico.org/podIPs:100.96.1.207/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bce660 0xc003bce661}] []  [{kube-controller-manager Update v1 2021-07-22 21:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 21:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 21:08:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.1.207,StartTime:2021-07-22 21:08:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 21:08:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://87f6e0ea908f558dec0d6799a3f5b347940b95addacc9b343a71be30aff72024,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.299: INFO: Pod "webserver-deployment-dd94f59b7-tcbqv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tcbqv webserver-deployment-dd94f59b7- deployment-2808  7dbaf50b-11b7-438d-bf22-6be6efb30b79 32020 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bce830 0xc003bce831}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.299: INFO: Pod "webserver-deployment-dd94f59b7-v8qrr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v8qrr webserver-deployment-dd94f59b7- deployment-2808  800528b7-0774-4d09-a4f7-72d98fa64ab9 31889 0 2021-07-22 21:08:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.1.202/32 cni.projectcalico.org/podIPs:100.96.1.202/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bcea50 0xc003bcea51}] []  [{kube-controller-manager Update v1 2021-07-22 21:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 21:08:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 21:08:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.1.202,StartTime:2021-07-22 21:08:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 21:08:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2157698670ca5d07c2936172b240e84dbbb9712626e86eef907bf52cc9304d0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.299: INFO: Pod "webserver-deployment-dd94f59b7-wfz8z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wfz8z webserver-deployment-dd94f59b7- deployment-2808  fafa7c06-8ca7-4511-8a5c-4fa3550462e6 31886 0 2021-07-22 21:08:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.1.203/32 cni.projectcalico.org/podIPs:100.96.1.203/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bcec60 0xc003bcec61}] []  [{kube-controller-manager Update v1 2021-07-22 21:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 21:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 21:08:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.1.203,StartTime:2021-07-22 21:08:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 21:08:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://28dc2e76d0c8e3869af206bd5b11e3ed04ba014e0e00c6056f9c8c2a44d2d64f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.299: INFO: Pod "webserver-deployment-dd94f59b7-wvwqx" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wvwqx webserver-deployment-dd94f59b7- deployment-2808  d7db2e6d-0e22-4858-a22e-eda51b591fd1 32007 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bcee10 0xc003bcee11}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.299: INFO: Pod "webserver-deployment-dd94f59b7-ww9q5" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ww9q5 webserver-deployment-dd94f59b7- deployment-2808  cb679ea4-16d9-4366-8edc-2d0041700e4d 31859 0 2021-07-22 21:08:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.0.66/32 cni.projectcalico.org/podIPs:100.96.0.66/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bcf080 0xc003bcf081}] []  [{kube-controller-manager Update v1 2021-07-22 21:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 21:08:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 21:08:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.0.66,StartTime:2021-07-22 21:08:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 21:08:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6bb595267bcbb41afc3a19da79cfb6a92db3ab7ccbb61143c0971db1b401b324,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.300: INFO: Pod "webserver-deployment-dd94f59b7-x8zr8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-x8zr8 webserver-deployment-dd94f59b7- deployment-2808  a023df21-b000-47bd-b8b1-be50e92beaed 31894 0 2021-07-22 21:08:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.1.204/32 cni.projectcalico.org/podIPs:100.96.1.204/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bcf300 0xc003bcf301}] []  [{kube-controller-manager Update v1 2021-07-22 21:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 21:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 21:08:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:08:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.1.204,StartTime:2021-07-22 21:08:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 21:08:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8484fa4f08ddd12e45b4673d8c3232767948ccc0a4e08d9129678c4932ec135c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 21:09:05.300: INFO: Pod "webserver-deployment-dd94f59b7-ztnh8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ztnh8 webserver-deployment-dd94f59b7- deployment-2808  767a0224-0c9b-43fe-acf4-e6a0226063bf 31993 0 2021-07-22 21:09:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680 0xc003bcf590 0xc003bcf591}] []  [{kube-controller-manager Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5fe1bc1-88fe-40f2-8a90-ef5f3d44c680\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 21:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ggxp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ggxp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ggxp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2021-07-22 21:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:09:05.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2808" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":194,"skipped":3343,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:09:05.328: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:09:05.523: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3118 version'
Jul 22 21:09:05.637: INFO: stderr: ""
Jul 22 21:09:05.637: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.8\", GitCommit:\"5575935422cc1cf5169dfc8847cb587aa47bac5a\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T13:00:45Z\", GoVersion:\"go1.15.13\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.8\", GitCommit:\"5575935422cc1cf5169dfc8847cb587aa47bac5a\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T12:53:07Z\", GoVersion:\"go1.15.13\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:09:05.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3118" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":195,"skipped":3346,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:09:05.664: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1206
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jul 22 21:09:05.879: INFO: Waiting up to 5m0s for pod "downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7" in namespace "downward-api-1206" to be "Succeeded or Failed"
Jul 22 21:09:05.890: INFO: Pod "downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.406484ms
Jul 22 21:09:07.902: INFO: Pod "downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02346175s
Jul 22 21:09:09.914: INFO: Pod "downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035418891s
Jul 22 21:09:11.928: INFO: Pod "downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04911474s
Jul 22 21:09:13.940: INFO: Pod "downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.06162864s
Jul 22 21:09:15.953: INFO: Pod "downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.074070232s
Jul 22 21:09:17.965: INFO: Pod "downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.086469421s
Jul 22 21:09:19.978: INFO: Pod "downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.099218202s
STEP: Saw pod success
Jul 22 21:09:19.978: INFO: Pod "downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7" satisfied condition "Succeeded or Failed"
Jul 22 21:09:19.990: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7 container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:09:20.064: INFO: Waiting for pod downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7 to disappear
Jul 22 21:09:20.075: INFO: Pod downward-api-83fe5ad1-73d4-4c57-b266-4cae204975d7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:09:20.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1206" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3354,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:09:20.111: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:09:20.706: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584960, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584960, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584960, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584960, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:09:22.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584960, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584960, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584960, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584960, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:09:25.917: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:09:36.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1617" for this suite.
STEP: Destroying namespace "webhook-1617-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":197,"skipped":3359,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:09:37.106: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-b0272085-0cd4-4a63-a9bc-ec61ab1338dc
STEP: Creating a pod to test consume configMaps
Jul 22 21:09:37.337: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac38ea48-ed98-4d0f-ae3b-3b62a682cf36" in namespace "configmap-2314" to be "Succeeded or Failed"
Jul 22 21:09:37.349: INFO: Pod "pod-configmaps-ac38ea48-ed98-4d0f-ae3b-3b62a682cf36": Phase="Pending", Reason="", readiness=false. Elapsed: 11.379944ms
Jul 22 21:09:39.361: INFO: Pod "pod-configmaps-ac38ea48-ed98-4d0f-ae3b-3b62a682cf36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023735037s
Jul 22 21:09:41.374: INFO: Pod "pod-configmaps-ac38ea48-ed98-4d0f-ae3b-3b62a682cf36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036465088s
STEP: Saw pod success
Jul 22 21:09:41.374: INFO: Pod "pod-configmaps-ac38ea48-ed98-4d0f-ae3b-3b62a682cf36" satisfied condition "Succeeded or Failed"
Jul 22 21:09:41.386: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-configmaps-ac38ea48-ed98-4d0f-ae3b-3b62a682cf36 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:09:41.460: INFO: Waiting for pod pod-configmaps-ac38ea48-ed98-4d0f-ae3b-3b62a682cf36 to disappear
Jul 22 21:09:41.472: INFO: Pod pod-configmaps-ac38ea48-ed98-4d0f-ae3b-3b62a682cf36 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:09:41.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2314" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3369,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:09:41.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 22 21:09:41.700: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-429 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Jul 22 21:09:41.850: INFO: stderr: ""
Jul 22 21:09:41.850: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Jul 22 21:09:41.862: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-429 delete pods e2e-test-httpd-pod'
Jul 22 21:09:52.752: INFO: stderr: ""
Jul 22 21:09:52.752: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:09:52.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-429" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":199,"skipped":3388,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:09:52.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-89
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-89
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 21:09:52.971: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 22 21:09:53.045: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:09:55.058: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:09:57.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:09:59.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:10:01.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:10:03.057: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:10:05.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:10:07.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:10:09.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:10:11.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:10:13.058: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 22 21:10:13.081: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 22 21:10:17.144: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 22 21:10:17.145: INFO: Breadth first check of 100.96.1.225 on host 10.250.0.4...
Jul 22 21:10:17.157: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.226:9080/dial?request=hostname&protocol=udp&host=100.96.1.225&port=8081&tries=1'] Namespace:pod-network-test-89 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:10:17.157: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:10:17.586: INFO: Waiting for responses: map[]
Jul 22 21:10:17.586: INFO: reached 100.96.1.225 after 0/1 tries
Jul 22 21:10:17.586: INFO: Breadth first check of 100.96.0.79 on host 10.250.0.5...
Jul 22 21:10:17.598: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.226:9080/dial?request=hostname&protocol=udp&host=100.96.0.79&port=8081&tries=1'] Namespace:pod-network-test-89 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:10:17.598: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:10:18.014: INFO: Waiting for responses: map[]
Jul 22 21:10:18.014: INFO: reached 100.96.0.79 after 0/1 tries
Jul 22 21:10:18.014: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:10:18.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-89" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":200,"skipped":3408,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:10:18.051: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2184
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:10:18.263: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-20734165-7a81-492d-b0c3-582d688ffba7" in namespace "security-context-test-2184" to be "Succeeded or Failed"
Jul 22 21:10:18.274: INFO: Pod "busybox-readonly-false-20734165-7a81-492d-b0c3-582d688ffba7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.154968ms
Jul 22 21:10:20.287: INFO: Pod "busybox-readonly-false-20734165-7a81-492d-b0c3-582d688ffba7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023573052s
Jul 22 21:10:22.299: INFO: Pod "busybox-readonly-false-20734165-7a81-492d-b0c3-582d688ffba7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035874188s
Jul 22 21:10:22.299: INFO: Pod "busybox-readonly-false-20734165-7a81-492d-b0c3-582d688ffba7" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:10:22.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2184" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":201,"skipped":3416,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:10:22.334: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:10:22.550: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 22 21:10:27.562: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 21:10:27.562: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jul 22 21:10:33.661: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9033  2956efef-5c89-46c3-82a2-89c46aeba80e 32822 1 2021-07-22 21:10:27 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-07-22 21:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 21:10:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0099dcec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-22 21:10:27 +0000 UTC,LastTransitionTime:2021-07-22 21:10:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-07-22 21:10:31 +0000 UTC,LastTransitionTime:2021-07-22 21:10:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 21:10:33.672: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-9033  5699dc81-69f7-4489-9a41-5481ded745e8 32815 1 2021-07-22 21:10:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 2956efef-5c89-46c3-82a2-89c46aeba80e 0xc0099dd257 0xc0099dd258}] []  [{kube-controller-manager Update apps/v1 2021-07-22 21:10:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2956efef-5c89-46c3-82a2-89c46aeba80e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0099dd2e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 22 21:10:33.684: INFO: Pod "test-cleanup-deployment-685c4f8568-4fn52" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-4fn52 test-cleanup-deployment-685c4f8568- deployment-9033  c4679e46-621f-4adb-8387-37be9e5e8764 32814 0 2021-07-22 21:10:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[cni.projectcalico.org/podIP:100.96.1.229/32 cni.projectcalico.org/podIPs:100.96.1.229/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 5699dc81-69f7-4489-9a41-5481ded745e8 0xc0099dd647 0xc0099dd648}] []  [{kube-controller-manager Update v1 2021-07-22 21:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5699dc81-69f7-4489-9a41-5481ded745e8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 21:10:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 21:10:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gs9xf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gs9xf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gs9xf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:10:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:10:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:10:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:10:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.1.229,StartTime:2021-07-22 21:10:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 21:10:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://1f758601b5acd281ad3fb0e47cf86959352b98ac582d6697d707a67785d3c556,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:10:33.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9033" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":202,"skipped":3422,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:10:33.730: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Jul 22 21:10:33.920: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9477 create -f -'
Jul 22 21:10:34.240: INFO: stderr: ""
Jul 22 21:10:34.240: INFO: stdout: "pod/pause created\n"
Jul 22 21:10:34.240: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 22 21:10:34.240: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9477" to be "running and ready"
Jul 22 21:10:34.251: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.512288ms
Jul 22 21:10:36.263: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023234936s
Jul 22 21:10:38.276: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.036050688s
Jul 22 21:10:38.276: INFO: Pod "pause" satisfied condition "running and ready"
Jul 22 21:10:38.276: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 22 21:10:38.276: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9477 label pods pause testing-label=testing-label-value'
Jul 22 21:10:38.416: INFO: stderr: ""
Jul 22 21:10:38.416: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 22 21:10:38.416: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9477 get pod pause -L testing-label'
Jul 22 21:10:38.528: INFO: stderr: ""
Jul 22 21:10:38.528: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 22 21:10:38.528: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9477 label pods pause testing-label-'
Jul 22 21:10:38.650: INFO: stderr: ""
Jul 22 21:10:38.650: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 22 21:10:38.650: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9477 get pod pause -L testing-label'
Jul 22 21:10:38.782: INFO: stderr: ""
Jul 22 21:10:38.782: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Jul 22 21:10:38.782: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9477 delete --grace-period=0 --force -f -'
Jul 22 21:10:38.928: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:10:38.928: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 22 21:10:38.928: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9477 get rc,svc -l name=pause --no-headers'
Jul 22 21:10:39.043: INFO: stderr: "No resources found in kubectl-9477 namespace.\n"
Jul 22 21:10:39.043: INFO: stdout: ""
Jul 22 21:10:39.043: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9477 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 21:10:39.419: INFO: stderr: ""
Jul 22 21:10:39.419: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:10:39.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9477" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":203,"skipped":3436,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:10:39.455: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jul 22 21:10:44.365: INFO: Successfully updated pod "labelsupdate8908bd5a-6912-4e16-a8bf-1444346da817"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:10:48.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6441" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":204,"skipped":3455,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:10:48.591: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-8903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:10:49.648: INFO: Checking APIGroup: apiregistration.k8s.io
Jul 22 21:10:49.663: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul 22 21:10:49.663: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.663: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul 22 21:10:49.663: INFO: Checking APIGroup: apps
Jul 22 21:10:49.673: INFO: PreferredVersion.GroupVersion: apps/v1
Jul 22 21:10:49.674: INFO: Versions found [{apps/v1 v1}]
Jul 22 21:10:49.674: INFO: apps/v1 matches apps/v1
Jul 22 21:10:49.674: INFO: Checking APIGroup: events.k8s.io
Jul 22 21:10:49.684: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul 22 21:10:49.684: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.684: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul 22 21:10:49.684: INFO: Checking APIGroup: authentication.k8s.io
Jul 22 21:10:49.694: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul 22 21:10:49.695: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.695: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul 22 21:10:49.695: INFO: Checking APIGroup: authorization.k8s.io
Jul 22 21:10:49.705: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul 22 21:10:49.705: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.705: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul 22 21:10:49.705: INFO: Checking APIGroup: autoscaling
Jul 22 21:10:49.715: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jul 22 21:10:49.715: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jul 22 21:10:49.715: INFO: autoscaling/v1 matches autoscaling/v1
Jul 22 21:10:49.715: INFO: Checking APIGroup: batch
Jul 22 21:10:49.725: INFO: PreferredVersion.GroupVersion: batch/v1
Jul 22 21:10:49.725: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jul 22 21:10:49.725: INFO: batch/v1 matches batch/v1
Jul 22 21:10:49.725: INFO: Checking APIGroup: certificates.k8s.io
Jul 22 21:10:49.744: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul 22 21:10:49.744: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.744: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul 22 21:10:49.744: INFO: Checking APIGroup: networking.k8s.io
Jul 22 21:10:49.755: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul 22 21:10:49.755: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.755: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul 22 21:10:49.755: INFO: Checking APIGroup: extensions
Jul 22 21:10:49.765: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jul 22 21:10:49.765: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jul 22 21:10:49.765: INFO: extensions/v1beta1 matches extensions/v1beta1
Jul 22 21:10:49.765: INFO: Checking APIGroup: policy
Jul 22 21:10:49.775: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jul 22 21:10:49.775: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jul 22 21:10:49.775: INFO: policy/v1beta1 matches policy/v1beta1
Jul 22 21:10:49.775: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul 22 21:10:49.786: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul 22 21:10:49.786: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.786: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul 22 21:10:49.786: INFO: Checking APIGroup: storage.k8s.io
Jul 22 21:10:49.796: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul 22 21:10:49.796: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.796: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul 22 21:10:49.796: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul 22 21:10:49.807: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul 22 21:10:49.807: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.807: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul 22 21:10:49.807: INFO: Checking APIGroup: apiextensions.k8s.io
Jul 22 21:10:49.817: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul 22 21:10:49.817: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.818: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul 22 21:10:49.818: INFO: Checking APIGroup: scheduling.k8s.io
Jul 22 21:10:49.827: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul 22 21:10:49.827: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.827: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul 22 21:10:49.827: INFO: Checking APIGroup: coordination.k8s.io
Jul 22 21:10:49.838: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul 22 21:10:49.838: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.838: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul 22 21:10:49.838: INFO: Checking APIGroup: node.k8s.io
Jul 22 21:10:49.848: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jul 22 21:10:49.848: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.848: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jul 22 21:10:49.848: INFO: Checking APIGroup: discovery.k8s.io
Jul 22 21:10:49.860: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jul 22 21:10:49.860: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.860: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jul 22 21:10:49.860: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jul 22 21:10:49.871: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Jul 22 21:10:49.872: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.872: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Jul 22 21:10:49.872: INFO: Checking APIGroup: autoscaling.k8s.io
Jul 22 21:10:49.882: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
Jul 22 21:10:49.882: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
Jul 22 21:10:49.882: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
Jul 22 21:10:49.882: INFO: Checking APIGroup: crd.projectcalico.org
Jul 22 21:10:49.892: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jul 22 21:10:49.892: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jul 22 21:10:49.892: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jul 22 21:10:49.892: INFO: Checking APIGroup: cert.gardener.cloud
Jul 22 21:10:49.903: INFO: PreferredVersion.GroupVersion: cert.gardener.cloud/v1alpha1
Jul 22 21:10:49.903: INFO: Versions found [{cert.gardener.cloud/v1alpha1 v1alpha1}]
Jul 22 21:10:49.903: INFO: cert.gardener.cloud/v1alpha1 matches cert.gardener.cloud/v1alpha1
Jul 22 21:10:49.903: INFO: Checking APIGroup: dns.gardener.cloud
Jul 22 21:10:49.914: INFO: PreferredVersion.GroupVersion: dns.gardener.cloud/v1alpha1
Jul 22 21:10:49.914: INFO: Versions found [{dns.gardener.cloud/v1alpha1 v1alpha1}]
Jul 22 21:10:49.914: INFO: dns.gardener.cloud/v1alpha1 matches dns.gardener.cloud/v1alpha1
Jul 22 21:10:49.914: INFO: Checking APIGroup: metrics.k8s.io
Jul 22 21:10:49.925: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jul 22 21:10:49.925: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jul 22 21:10:49.925: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:10:49.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8903" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":205,"skipped":3464,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:10:49.962: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7454
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7454
STEP: creating replication controller externalsvc in namespace services-7454
I0722 21:10:50.200362    5567 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7454, replica count: 2
I0722 21:10:53.250766    5567 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul 22 21:10:53.291: INFO: Creating new exec pod
Jul 22 21:10:57.332: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7454 exec execpodvdhb2 -- /bin/sh -x -c nslookup clusterip-service.services-7454.svc.cluster.local'
Jul 22 21:10:57.920: INFO: stderr: "+ nslookup clusterip-service.services-7454.svc.cluster.local\n"
Jul 22 21:10:57.920: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nclusterip-service.services-7454.svc.cluster.local\tcanonical name = externalsvc.services-7454.svc.cluster.local.\nName:\texternalsvc.services-7454.svc.cluster.local\nAddress: 100.64.214.250\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7454, will wait for the garbage collector to delete the pods
Jul 22 21:10:57.998: INFO: Deleting ReplicationController externalsvc took: 15.601648ms
Jul 22 21:10:58.098: INFO: Terminating ReplicationController externalsvc pods took: 100.203876ms
Jul 22 21:11:12.827: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:11:12.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7454" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":206,"skipped":3479,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:11:12.878: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Jul 22 21:11:13.067: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul 22 21:11:13.067: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 create -f -'
Jul 22 21:11:13.359: INFO: stderr: ""
Jul 22 21:11:13.359: INFO: stdout: "service/agnhost-replica created\n"
Jul 22 21:11:13.359: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul 22 21:11:13.359: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 create -f -'
Jul 22 21:11:13.622: INFO: stderr: ""
Jul 22 21:11:13.622: INFO: stdout: "service/agnhost-primary created\n"
Jul 22 21:11:13.623: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 22 21:11:13.623: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 create -f -'
Jul 22 21:11:13.882: INFO: stderr: ""
Jul 22 21:11:13.882: INFO: stdout: "service/frontend created\n"
Jul 22 21:11:13.883: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 22 21:11:13.883: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 create -f -'
Jul 22 21:11:14.172: INFO: stderr: ""
Jul 22 21:11:14.172: INFO: stdout: "deployment.apps/frontend created\n"
Jul 22 21:11:14.172: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 22 21:11:14.172: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 create -f -'
Jul 22 21:11:14.428: INFO: stderr: ""
Jul 22 21:11:14.428: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul 22 21:11:14.429: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 22 21:11:14.429: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 create -f -'
Jul 22 21:11:14.671: INFO: stderr: ""
Jul 22 21:11:14.671: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jul 22 21:11:14.671: INFO: Waiting for all frontend pods to be Running.
Jul 22 21:11:19.721: INFO: Waiting for frontend to serve content.
Jul 22 21:11:19.845: INFO: Trying to add a new entry to the guestbook.
Jul 22 21:11:20.902: INFO: Verifying that added entry can be retrieved.
Jul 22 21:11:21.041: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jul 22 21:11:26.133: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 delete --grace-period=0 --force -f -'
Jul 22 21:11:26.258: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:11:26.258: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 21:11:26.258: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 delete --grace-period=0 --force -f -'
Jul 22 21:11:26.397: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:11:26.397: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 21:11:26.397: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 delete --grace-period=0 --force -f -'
Jul 22 21:11:26.554: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:11:26.554: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 21:11:26.555: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 delete --grace-period=0 --force -f -'
Jul 22 21:11:26.737: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:11:26.737: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 21:11:26.738: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 delete --grace-period=0 --force -f -'
Jul 22 21:11:26.899: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:11:26.899: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 21:11:26.899: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9530 delete --grace-period=0 --force -f -'
Jul 22 21:11:27.067: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:11:27.067: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:11:27.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9530" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":207,"skipped":3481,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:11:27.101: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:11:27.322: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-073217de-e704-4b39-81cf-7dcebd1bfcf9" in namespace "security-context-test-6843" to be "Succeeded or Failed"
Jul 22 21:11:27.334: INFO: Pod "alpine-nnp-false-073217de-e704-4b39-81cf-7dcebd1bfcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.145981ms
Jul 22 21:11:29.349: INFO: Pod "alpine-nnp-false-073217de-e704-4b39-81cf-7dcebd1bfcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027500296s
Jul 22 21:11:31.363: INFO: Pod "alpine-nnp-false-073217de-e704-4b39-81cf-7dcebd1bfcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041462042s
Jul 22 21:11:33.377: INFO: Pod "alpine-nnp-false-073217de-e704-4b39-81cf-7dcebd1bfcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055282201s
Jul 22 21:11:35.390: INFO: Pod "alpine-nnp-false-073217de-e704-4b39-81cf-7dcebd1bfcf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068457372s
Jul 22 21:11:35.390: INFO: Pod "alpine-nnp-false-073217de-e704-4b39-81cf-7dcebd1bfcf9" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:11:35.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6843" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:11:35.474: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Jul 22 21:11:35.681: INFO: Waiting up to 5m0s for pod "var-expansion-e9047429-1d27-40b1-bc41-d5c40a18b918" in namespace "var-expansion-4650" to be "Succeeded or Failed"
Jul 22 21:11:35.694: INFO: Pod "var-expansion-e9047429-1d27-40b1-bc41-d5c40a18b918": Phase="Pending", Reason="", readiness=false. Elapsed: 12.721317ms
Jul 22 21:11:37.707: INFO: Pod "var-expansion-e9047429-1d27-40b1-bc41-d5c40a18b918": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025816457s
Jul 22 21:11:39.720: INFO: Pod "var-expansion-e9047429-1d27-40b1-bc41-d5c40a18b918": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038646137s
STEP: Saw pod success
Jul 22 21:11:39.720: INFO: Pod "var-expansion-e9047429-1d27-40b1-bc41-d5c40a18b918" satisfied condition "Succeeded or Failed"
Jul 22 21:11:39.731: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod var-expansion-e9047429-1d27-40b1-bc41-d5c40a18b918 container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:11:39.801: INFO: Waiting for pod var-expansion-e9047429-1d27-40b1-bc41-d5c40a18b918 to disappear
Jul 22 21:11:39.812: INFO: Pod var-expansion-e9047429-1d27-40b1-bc41-d5c40a18b918 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:11:39.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4650" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":209,"skipped":3549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:11:39.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:11:56.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6992" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":210,"skipped":3604,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:11:56.291: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4624
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-16a166fb-80dd-47c8-b824-7f2608a33210
STEP: Creating a pod to test consume configMaps
Jul 22 21:11:56.526: INFO: Waiting up to 5m0s for pod "pod-configmaps-027f7c33-0474-400a-9483-b757b6070aec" in namespace "configmap-4624" to be "Succeeded or Failed"
Jul 22 21:11:56.538: INFO: Pod "pod-configmaps-027f7c33-0474-400a-9483-b757b6070aec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.638981ms
Jul 22 21:11:58.555: INFO: Pod "pod-configmaps-027f7c33-0474-400a-9483-b757b6070aec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029064461s
Jul 22 21:12:00.567: INFO: Pod "pod-configmaps-027f7c33-0474-400a-9483-b757b6070aec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041256058s
STEP: Saw pod success
Jul 22 21:12:00.567: INFO: Pod "pod-configmaps-027f7c33-0474-400a-9483-b757b6070aec" satisfied condition "Succeeded or Failed"
Jul 22 21:12:00.579: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-configmaps-027f7c33-0474-400a-9483-b757b6070aec container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:12:00.765: INFO: Waiting for pod pod-configmaps-027f7c33-0474-400a-9483-b757b6070aec to disappear
Jul 22 21:12:00.776: INFO: Pod pod-configmaps-027f7c33-0474-400a-9483-b757b6070aec no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:12:00.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4624" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":211,"skipped":3605,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:12:00.811: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1667
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-6023d069-5572-4c9f-8652-54b0c04b6c63
STEP: Creating configMap with name cm-test-opt-upd-88267ec5-1a90-4490-a6fa-a2becc502d88
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6023d069-5572-4c9f-8652-54b0c04b6c63
STEP: Updating configmap cm-test-opt-upd-88267ec5-1a90-4490-a6fa-a2becc502d88
STEP: Creating configMap with name cm-test-opt-create-9640c246-2562-4602-9142-fef4df81873a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:13:23.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1667" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:13:23.508: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1461
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1461.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1461.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:13:28.391: INFO: DNS probes using dns-1461/dns-test-bc1b4298-bb02-41cc-a6f2-555e6b1294fb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:13:28.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1461" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":213,"skipped":3636,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:13:28.445: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-9f36a5e8-f8fe-4803-99db-2e953fe268d8
Jul 22 21:13:28.664: INFO: Pod name my-hostname-basic-9f36a5e8-f8fe-4803-99db-2e953fe268d8: Found 0 pods out of 1
Jul 22 21:13:33.676: INFO: Pod name my-hostname-basic-9f36a5e8-f8fe-4803-99db-2e953fe268d8: Found 1 pods out of 1
Jul 22 21:13:33.676: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9f36a5e8-f8fe-4803-99db-2e953fe268d8" are running
Jul 22 21:13:33.688: INFO: Pod "my-hostname-basic-9f36a5e8-f8fe-4803-99db-2e953fe268d8-mmgjj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 21:13:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 21:13:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 21:13:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 21:13:28 +0000 UTC Reason: Message:}])
Jul 22 21:13:33.688: INFO: Trying to dial the pod
Jul 22 21:13:38.785: INFO: Controller my-hostname-basic-9f36a5e8-f8fe-4803-99db-2e953fe268d8: Got expected result from replica 1 [my-hostname-basic-9f36a5e8-f8fe-4803-99db-2e953fe268d8-mmgjj]: "my-hostname-basic-9f36a5e8-f8fe-4803-99db-2e953fe268d8-mmgjj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:13:38.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-589" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":214,"skipped":3651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:13:38.821: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul 22 21:13:43.589: INFO: Successfully updated pod "adopt-release-fsd2z"
STEP: Checking that the Job readopts the Pod
Jul 22 21:13:43.589: INFO: Waiting up to 15m0s for pod "adopt-release-fsd2z" in namespace "job-306" to be "adopted"
Jul 22 21:13:43.600: INFO: Pod "adopt-release-fsd2z": Phase="Running", Reason="", readiness=true. Elapsed: 11.233594ms
Jul 22 21:13:43.601: INFO: Pod "adopt-release-fsd2z" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul 22 21:13:44.132: INFO: Successfully updated pod "adopt-release-fsd2z"
STEP: Checking that the Job releases the Pod
Jul 22 21:13:44.132: INFO: Waiting up to 15m0s for pod "adopt-release-fsd2z" in namespace "job-306" to be "released"
Jul 22 21:13:44.144: INFO: Pod "adopt-release-fsd2z": Phase="Running", Reason="", readiness=true. Elapsed: 11.417439ms
Jul 22 21:13:44.144: INFO: Pod "adopt-release-fsd2z" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:13:44.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-306" for this suite.
•{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":215,"skipped":3687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:13:44.178: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3149
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:13:44.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:13:46.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:13:48.839: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585224, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:13:51.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:13:52.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3149" for this suite.
STEP: Destroying namespace "webhook-3149-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":216,"skipped":3717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:13:52.873: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5362
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:13:53.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5362" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":217,"skipped":3748,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:13:53.248: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:13:54.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585234, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585234, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585234, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585234, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:13:56.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585234, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585234, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585234, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585234, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:13:59.146: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:13:59.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5917" for this suite.
STEP: Destroying namespace "webhook-5917-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":218,"skipped":3766,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:13:59.688: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7044
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 22 21:14:03.970: INFO: &Pod{ObjectMeta:{send-events-ddcabe73-5e1a-4c0a-8424-7e883042efd0  events-7044  c42d2f94-eb58-4ce3-9595-3e06be50a760 34450 0 2021-07-22 21:13:59 +0000 UTC <nil> <nil> map[name:foo time:904248368] map[cni.projectcalico.org/podIP:100.96.1.249/32 cni.projectcalico.org/podIPs:100.96.1.249/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-07-22 21:13:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 21:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 21:14:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-z99gf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-z99gf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-z99gf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:13:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:14:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:14:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 21:13:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.1.249,StartTime:2021-07-22 21:13:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 21:14:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://24d54e5feac13e61dadccb568137f31c8e25b7c634db0bb13df07fbecdd7bd28,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jul 22 21:14:05.983: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 22 21:14:07.997: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:14:08.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7044" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":219,"skipped":3782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:14:08.051: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jul 22 21:14:08.242: INFO: PodSpec: initContainers in spec.initContainers
Jul 22 21:14:58.895: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-460ade2c-0a2a-44b3-b3fa-c07ff051f6e0", GenerateName:"", Namespace:"init-container-1363", SelfLink:"", UID:"ebb6c8f3-dd15-437b-bea1-206e154ac061", ResourceVersion:"34756", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63762585248, loc:(*time.Location)(0x7980f00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"242439885"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.250/32", "cni.projectcalico.org/podIPs":"100.96.1.250/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001c180a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001c180c0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001c180e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001c18100)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001c18120), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001c18140)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-sjstm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0058ce040), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sjstm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sjstm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sjstm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0034100c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002aaa000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003410140)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003410160)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003410168), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00341016c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0022de020), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585248, loc:(*time.Location)(0x7980f00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585248, loc:(*time.Location)(0x7980f00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585248, loc:(*time.Location)(0x7980f00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585248, loc:(*time.Location)(0x7980f00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.4", PodIP:"100.96.1.250", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.96.1.250"}}, StartTime:(*v1.Time)(0xc001c18160), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002aaa0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002aaa150)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://5453dbd62c32664ded1a5d4afcce92c05393daef6d7d2355b2ffdc435a6c1c98", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001c181a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001c18180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0034101ef)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:14:58.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1363" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":220,"skipped":3815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:14:58.930: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2474
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Jul 22 21:14:59.168: INFO: Found 0 stateful pods, waiting for 3
Jul 22 21:15:09.181: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:15:09.181: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:15:09.181: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Jul 22 21:15:19.182: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:15:19.182: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:15:19.182: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:15:19.219: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2474 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:15:19.819: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:15:19.819: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:15:19.819: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 22 21:15:29.907: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 22 21:15:39.970: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2474 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:15:40.741: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 21:15:40.741: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:15:40.741: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:15:50.825: INFO: Waiting for StatefulSet statefulset-2474/ss2 to complete update
Jul 22 21:15:50.825: INFO: Waiting for Pod statefulset-2474/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 22 21:15:50.825: INFO: Waiting for Pod statefulset-2474/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 22 21:15:50.825: INFO: Waiting for Pod statefulset-2474/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 22 21:16:00.858: INFO: Waiting for StatefulSet statefulset-2474/ss2 to complete update
Jul 22 21:16:00.858: INFO: Waiting for Pod statefulset-2474/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 22 21:16:00.858: INFO: Waiting for Pod statefulset-2474/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 22 21:16:10.850: INFO: Waiting for StatefulSet statefulset-2474/ss2 to complete update
Jul 22 21:16:10.850: INFO: Waiting for Pod statefulset-2474/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 22 21:16:20.850: INFO: Waiting for StatefulSet statefulset-2474/ss2 to complete update
Jul 22 21:16:20.850: INFO: Waiting for Pod statefulset-2474/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 22 21:16:30.852: INFO: Waiting for StatefulSet statefulset-2474/ss2 to complete update
Jul 22 21:16:30.852: INFO: Waiting for Pod statefulset-2474/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jul 22 21:16:40.850: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2474 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:16:41.403: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:16:41.403: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:16:41.403: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 21:16:51.488: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 22 21:16:51.524: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2474 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:16:52.024: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 21:16:52.024: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:16:52.024: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:17:12.096: INFO: Waiting for StatefulSet statefulset-2474/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 22 21:17:22.133: INFO: Deleting all statefulset in ns statefulset-2474
Jul 22 21:17:22.144: INFO: Scaling statefulset ss2 to 0
Jul 22 21:17:52.196: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:17:52.208: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:17:52.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2474" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":221,"skipped":3839,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:17:52.286: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3738
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 22 21:17:52.493: INFO: Waiting up to 5m0s for pod "pod-911595c0-ce4d-435e-aa3b-9dbdce6c916d" in namespace "emptydir-3738" to be "Succeeded or Failed"
Jul 22 21:17:52.504: INFO: Pod "pod-911595c0-ce4d-435e-aa3b-9dbdce6c916d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.28046ms
Jul 22 21:17:54.516: INFO: Pod "pod-911595c0-ce4d-435e-aa3b-9dbdce6c916d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023193197s
Jul 22 21:17:56.529: INFO: Pod "pod-911595c0-ce4d-435e-aa3b-9dbdce6c916d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035646187s
STEP: Saw pod success
Jul 22 21:17:56.529: INFO: Pod "pod-911595c0-ce4d-435e-aa3b-9dbdce6c916d" satisfied condition "Succeeded or Failed"
Jul 22 21:17:56.542: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-911595c0-ce4d-435e-aa3b-9dbdce6c916d container test-container: <nil>
STEP: delete the pod
Jul 22 21:17:56.707: INFO: Waiting for pod pod-911595c0-ce4d-435e-aa3b-9dbdce6c916d to disappear
Jul 22 21:17:56.719: INFO: Pod pod-911595c0-ce4d-435e-aa3b-9dbdce6c916d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:17:56.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3738" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3854,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:17:56.757: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-qmvd
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 21:17:56.985: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qmvd" in namespace "subpath-1053" to be "Succeeded or Failed"
Jul 22 21:17:56.996: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.426811ms
Jul 22 21:17:59.010: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024911339s
Jul 22 21:18:01.052: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Running", Reason="", readiness=true. Elapsed: 4.06744807s
Jul 22 21:18:03.065: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Running", Reason="", readiness=true. Elapsed: 6.079558086s
Jul 22 21:18:05.077: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Running", Reason="", readiness=true. Elapsed: 8.09172823s
Jul 22 21:18:07.090: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Running", Reason="", readiness=true. Elapsed: 10.104937472s
Jul 22 21:18:09.102: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Running", Reason="", readiness=true. Elapsed: 12.117421446s
Jul 22 21:18:11.114: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Running", Reason="", readiness=true. Elapsed: 14.129170399s
Jul 22 21:18:13.126: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Running", Reason="", readiness=true. Elapsed: 16.141500065s
Jul 22 21:18:15.140: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Running", Reason="", readiness=true. Elapsed: 18.154741821s
Jul 22 21:18:17.153: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Running", Reason="", readiness=true. Elapsed: 20.1675706s
Jul 22 21:18:19.165: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Running", Reason="", readiness=true. Elapsed: 22.180404899s
Jul 22 21:18:21.177: INFO: Pod "pod-subpath-test-configmap-qmvd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.192522186s
STEP: Saw pod success
Jul 22 21:18:21.178: INFO: Pod "pod-subpath-test-configmap-qmvd" satisfied condition "Succeeded or Failed"
Jul 22 21:18:21.189: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-subpath-test-configmap-qmvd container test-container-subpath-configmap-qmvd: <nil>
STEP: delete the pod
Jul 22 21:18:21.254: INFO: Waiting for pod pod-subpath-test-configmap-qmvd to disappear
Jul 22 21:18:21.266: INFO: Pod pod-subpath-test-configmap-qmvd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qmvd
Jul 22 21:18:21.266: INFO: Deleting pod "pod-subpath-test-configmap-qmvd" in namespace "subpath-1053"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:18:21.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1053" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":223,"skipped":3854,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:18:21.322: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3291
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 22 21:18:21.534: INFO: Waiting up to 5m0s for pod "pod-889c2822-01f1-4d44-b272-deec0af3301d" in namespace "emptydir-3291" to be "Succeeded or Failed"
Jul 22 21:18:21.546: INFO: Pod "pod-889c2822-01f1-4d44-b272-deec0af3301d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.526788ms
Jul 22 21:18:23.559: INFO: Pod "pod-889c2822-01f1-4d44-b272-deec0af3301d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025082548s
Jul 22 21:18:25.574: INFO: Pod "pod-889c2822-01f1-4d44-b272-deec0af3301d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040235885s
STEP: Saw pod success
Jul 22 21:18:25.574: INFO: Pod "pod-889c2822-01f1-4d44-b272-deec0af3301d" satisfied condition "Succeeded or Failed"
Jul 22 21:18:25.586: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-889c2822-01f1-4d44-b272-deec0af3301d container test-container: <nil>
STEP: delete the pod
Jul 22 21:18:25.811: INFO: Waiting for pod pod-889c2822-01f1-4d44-b272-deec0af3301d to disappear
Jul 22 21:18:25.825: INFO: Pod pod-889c2822-01f1-4d44-b272-deec0af3301d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:18:25.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3291" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":224,"skipped":3873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:18:25.863: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:18:26.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585506, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585506, loc:(*time.Location)(0x7980f00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-7d6697c5b7\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585506, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585506, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:18:28.632: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585506, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585506, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585506, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585506, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:18:31.649: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:18:31.668: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:18:32.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1001" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":225,"skipped":3895,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:18:32.868: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-3456
STEP: creating replication controller nodeport-test in namespace services-3456
I0722 21:18:33.127646    5567 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-3456, replica count: 2
I0722 21:18:36.178021    5567 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:18:39.178: INFO: Creating new exec pod
I0722 21:18:39.178238    5567 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:18:44.257: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3456 exec execpodbs8ff -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jul 22 21:18:44.771: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 22 21:18:44.771: INFO: stdout: ""
Jul 22 21:18:44.772: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3456 exec execpodbs8ff -- /bin/sh -x -c nc -zv -t -w 2 100.64.204.253 80'
Jul 22 21:18:45.284: INFO: stderr: "+ nc -zv -t -w 2 100.64.204.253 80\nConnection to 100.64.204.253 80 port [tcp/http] succeeded!\n"
Jul 22 21:18:45.284: INFO: stdout: ""
Jul 22 21:18:45.284: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3456 exec execpodbs8ff -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.4 30410'
Jul 22 21:18:45.811: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.4 30410\nConnection to 10.250.0.4 30410 port [tcp/30410] succeeded!\n"
Jul 22 21:18:45.811: INFO: stdout: ""
Jul 22 21:18:45.811: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3456 exec execpodbs8ff -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.5 30410'
Jul 22 21:18:46.335: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.5 30410\nConnection to 10.250.0.5 30410 port [tcp/30410] succeeded!\n"
Jul 22 21:18:46.335: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:18:46.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3456" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":226,"skipped":3900,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:18:46.371: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-202
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Jul 22 21:18:46.592: INFO: Waiting up to 5m0s for pod "test-pod-231ae33e-209b-4a84-82d8-c1d7e5fc53db" in namespace "svcaccounts-202" to be "Succeeded or Failed"
Jul 22 21:18:46.604: INFO: Pod "test-pod-231ae33e-209b-4a84-82d8-c1d7e5fc53db": Phase="Pending", Reason="", readiness=false. Elapsed: 11.71617ms
Jul 22 21:18:48.617: INFO: Pod "test-pod-231ae33e-209b-4a84-82d8-c1d7e5fc53db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025297532s
Jul 22 21:18:50.630: INFO: Pod "test-pod-231ae33e-209b-4a84-82d8-c1d7e5fc53db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037645024s
STEP: Saw pod success
Jul 22 21:18:50.630: INFO: Pod "test-pod-231ae33e-209b-4a84-82d8-c1d7e5fc53db" satisfied condition "Succeeded or Failed"
Jul 22 21:18:50.643: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod test-pod-231ae33e-209b-4a84-82d8-c1d7e5fc53db container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:18:50.710: INFO: Waiting for pod test-pod-231ae33e-209b-4a84-82d8-c1d7e5fc53db to disappear
Jul 22 21:18:50.723: INFO: Pod test-pod-231ae33e-209b-4a84-82d8-c1d7e5fc53db no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:18:50.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-202" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":227,"skipped":3921,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:18:50.758: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:18:57.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5372" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":228,"skipped":3938,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:18:58.022: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9564
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 22 21:18:58.863: INFO: Pod name wrapped-volume-race-ff3dc68b-5ed0-44af-aa47-f8d4097862bd: Found 0 pods out of 5
Jul 22 21:19:03.911: INFO: Pod name wrapped-volume-race-ff3dc68b-5ed0-44af-aa47-f8d4097862bd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ff3dc68b-5ed0-44af-aa47-f8d4097862bd in namespace emptydir-wrapper-9564, will wait for the garbage collector to delete the pods
Jul 22 21:19:06.076: INFO: Deleting ReplicationController wrapped-volume-race-ff3dc68b-5ed0-44af-aa47-f8d4097862bd took: 17.435031ms
Jul 22 21:19:06.876: INFO: Terminating ReplicationController wrapped-volume-race-ff3dc68b-5ed0-44af-aa47-f8d4097862bd pods took: 800.207217ms
STEP: Creating RC which spawns configmap-volume pods
Jul 22 21:19:23.019: INFO: Pod name wrapped-volume-race-b154899c-3e49-46b3-aa7b-65dce7aad4eb: Found 0 pods out of 5
Jul 22 21:19:28.069: INFO: Pod name wrapped-volume-race-b154899c-3e49-46b3-aa7b-65dce7aad4eb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b154899c-3e49-46b3-aa7b-65dce7aad4eb in namespace emptydir-wrapper-9564, will wait for the garbage collector to delete the pods
Jul 22 21:19:32.266: INFO: Deleting ReplicationController wrapped-volume-race-b154899c-3e49-46b3-aa7b-65dce7aad4eb took: 15.315806ms
Jul 22 21:19:33.167: INFO: Terminating ReplicationController wrapped-volume-race-b154899c-3e49-46b3-aa7b-65dce7aad4eb pods took: 900.231443ms
STEP: Creating RC which spawns configmap-volume pods
Jul 22 21:19:42.912: INFO: Pod name wrapped-volume-race-546cc64d-0ee1-4172-a420-7863925ccfdb: Found 0 pods out of 5
Jul 22 21:19:47.957: INFO: Pod name wrapped-volume-race-546cc64d-0ee1-4172-a420-7863925ccfdb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-546cc64d-0ee1-4172-a420-7863925ccfdb in namespace emptydir-wrapper-9564, will wait for the garbage collector to delete the pods
Jul 22 21:19:50.129: INFO: Deleting ReplicationController wrapped-volume-race-546cc64d-0ee1-4172-a420-7863925ccfdb took: 14.539948ms
Jul 22 21:19:50.230: INFO: Terminating ReplicationController wrapped-volume-race-546cc64d-0ee1-4172-a420-7863925ccfdb pods took: 100.244277ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:20:03.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9564" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":229,"skipped":3945,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:20:03.663: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6639
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-3b932e8a-0482-46f2-8bc4-a8c8f5139bed
STEP: Creating a pod to test consume secrets
Jul 22 21:20:03.889: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b085ec6b-1198-440d-9383-4808f2e784b8" in namespace "projected-6639" to be "Succeeded or Failed"
Jul 22 21:20:03.900: INFO: Pod "pod-projected-secrets-b085ec6b-1198-440d-9383-4808f2e784b8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.309943ms
Jul 22 21:20:05.913: INFO: Pod "pod-projected-secrets-b085ec6b-1198-440d-9383-4808f2e784b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024549199s
Jul 22 21:20:07.926: INFO: Pod "pod-projected-secrets-b085ec6b-1198-440d-9383-4808f2e784b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037019435s
STEP: Saw pod success
Jul 22 21:20:07.926: INFO: Pod "pod-projected-secrets-b085ec6b-1198-440d-9383-4808f2e784b8" satisfied condition "Succeeded or Failed"
Jul 22 21:20:07.939: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-secrets-b085ec6b-1198-440d-9383-4808f2e784b8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:20:08.052: INFO: Waiting for pod pod-projected-secrets-b085ec6b-1198-440d-9383-4808f2e784b8 to disappear
Jul 22 21:20:08.065: INFO: Pod pod-projected-secrets-b085ec6b-1198-440d-9383-4808f2e784b8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:20:08.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6639" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":230,"skipped":3956,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:20:08.100: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6167
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-e1d4d612-8019-4169-9725-1558a7c1037d
STEP: Creating a pod to test consume secrets
Jul 22 21:20:08.327: INFO: Waiting up to 5m0s for pod "pod-secrets-87874b4e-686a-4b50-8703-b91a51548bd8" in namespace "secrets-6167" to be "Succeeded or Failed"
Jul 22 21:20:08.338: INFO: Pod "pod-secrets-87874b4e-686a-4b50-8703-b91a51548bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.378183ms
Jul 22 21:20:10.351: INFO: Pod "pod-secrets-87874b4e-686a-4b50-8703-b91a51548bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02424496s
Jul 22 21:20:12.370: INFO: Pod "pod-secrets-87874b4e-686a-4b50-8703-b91a51548bd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043213118s
STEP: Saw pod success
Jul 22 21:20:12.370: INFO: Pod "pod-secrets-87874b4e-686a-4b50-8703-b91a51548bd8" satisfied condition "Succeeded or Failed"
Jul 22 21:20:12.382: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-secrets-87874b4e-686a-4b50-8703-b91a51548bd8 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:20:12.612: INFO: Waiting for pod pod-secrets-87874b4e-686a-4b50-8703-b91a51548bd8 to disappear
Jul 22 21:20:12.627: INFO: Pod pod-secrets-87874b4e-686a-4b50-8703-b91a51548bd8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:20:12.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6167" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":231,"skipped":3957,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:20:12.661: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:20:23.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3584" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":232,"skipped":3979,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:20:23.981: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-f9df55fa-7970-4ce5-8d49-3b5127b4e782
STEP: Creating a pod to test consume secrets
Jul 22 21:20:24.318: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a47def7d-b6ef-4e6b-beaf-368f5b939a47" in namespace "projected-2823" to be "Succeeded or Failed"
Jul 22 21:20:24.330: INFO: Pod "pod-projected-secrets-a47def7d-b6ef-4e6b-beaf-368f5b939a47": Phase="Pending", Reason="", readiness=false. Elapsed: 11.660654ms
Jul 22 21:20:26.342: INFO: Pod "pod-projected-secrets-a47def7d-b6ef-4e6b-beaf-368f5b939a47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023948488s
Jul 22 21:20:28.355: INFO: Pod "pod-projected-secrets-a47def7d-b6ef-4e6b-beaf-368f5b939a47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036111114s
STEP: Saw pod success
Jul 22 21:20:28.355: INFO: Pod "pod-projected-secrets-a47def7d-b6ef-4e6b-beaf-368f5b939a47" satisfied condition "Succeeded or Failed"
Jul 22 21:20:28.366: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-secrets-a47def7d-b6ef-4e6b-beaf-368f5b939a47 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:20:28.434: INFO: Waiting for pod pod-projected-secrets-a47def7d-b6ef-4e6b-beaf-368f5b939a47 to disappear
Jul 22 21:20:28.445: INFO: Pod pod-projected-secrets-a47def7d-b6ef-4e6b-beaf-368f5b939a47 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:20:28.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2823" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":233,"skipped":3986,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:20:28.480: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jul 22 21:20:28.671: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 21:20:28.696: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 21:20:28.708: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 before test
Jul 22 21:20:28.732: INFO: apiserver-proxy-b49r5 from kube-system started at 2021-07-22 19:48:59 +0000 UTC (2 container statuses recorded)
Jul 22 21:20:28.732: INFO: 	Container proxy ready: true, restart count 0
Jul 22 21:20:28.732: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 21:20:28.732: INFO: blackbox-exporter-859b5d9c8c-nf6n7 from kube-system started at 2021-07-22 19:55:18 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.732: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul 22 21:20:28.732: INFO: calico-node-jxrzv from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.732: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 21:20:28.732: INFO: kube-proxy-nbdq9 from kube-system started at 2021-07-22 19:51:32 +0000 UTC (2 container statuses recorded)
Jul 22 21:20:28.732: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 21:20:28.732: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 21:20:28.732: INFO: node-exporter-bxzmq from kube-system started at 2021-07-22 19:48:59 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.732: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 21:20:28.732: INFO: node-problem-detector-vlbxm from kube-system started at 2021-07-22 20:15:32 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.732: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 21:20:28.732: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv before test
Jul 22 21:20:28.758: INFO: addons-nginx-ingress-controller-5f6b8d6b9b-z5p47 from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 22 21:20:28.758: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-799f5cb4df-cnbrj from kube-system started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul 22 21:20:28.758: INFO: apiserver-proxy-lgksb from kube-system started at 2021-07-22 19:48:53 +0000 UTC (2 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container proxy ready: true, restart count 0
Jul 22 21:20:28.758: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 21:20:28.758: INFO: calico-node-96j6z from kube-system started at 2021-07-22 19:50:28 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 21:20:28.758: INFO: calico-node-vertical-autoscaler-785b5f968-dl6lj from kube-system started at 2021-07-22 19:49:15 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 21:20:28.758: INFO: calico-typha-deploy-59966cd68c-kk766 from kube-system started at 2021-07-22 19:49:04 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container calico-typha ready: true, restart count 0
Jul 22 21:20:28.758: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-5dvs2 from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 21:20:28.758: INFO: calico-typha-vertical-autoscaler-5c9655cddd-9t9nc from kube-system started at 2021-07-22 19:49:15 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 21:20:28.758: INFO: coredns-7589655f7c-8gplg from kube-system started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container coredns ready: true, restart count 0
Jul 22 21:20:28.758: INFO: coredns-7589655f7c-cck5m from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container coredns ready: true, restart count 0
Jul 22 21:20:28.758: INFO: kube-proxy-r86fj from kube-system started at 2021-07-22 19:51:22 +0000 UTC (2 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 21:20:28.758: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 21:20:28.758: INFO: metrics-server-7fcbc9df99-qm94n from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 21:20:28.758: INFO: node-exporter-lkd2s from kube-system started at 2021-07-22 19:48:53 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 21:20:28.758: INFO: node-problem-detector-kj6cn from kube-system started at 2021-07-22 20:15:21 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 21:20:28.758: INFO: vpn-shoot-6c79f97679-hz7lk from kube-system started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul 22 21:20:28.758: INFO: dashboard-metrics-scraper-5fc7d79f9-tk49g from kubernetes-dashboard started at 2021-07-22 20:24:43 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 22 21:20:28.758: INFO: kubernetes-dashboard-775d7d55c5-q9hcp from kubernetes-dashboard started at 2021-07-22 19:49:43 +0000 UTC (1 container statuses recorded)
Jul 22 21:20:28.758: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
STEP: verifying the node has the label node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.045: INFO: Pod addons-nginx-ingress-controller-5f6b8d6b9b-z5p47 requesting resource cpu=100m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.045: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-799f5cb4df-cnbrj requesting resource cpu=0m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.045: INFO: Pod apiserver-proxy-b49r5 requesting resource cpu=40m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
Jul 22 21:20:29.045: INFO: Pod apiserver-proxy-lgksb requesting resource cpu=40m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.045: INFO: Pod blackbox-exporter-859b5d9c8c-nf6n7 requesting resource cpu=11m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
Jul 22 21:20:29.045: INFO: Pod calico-node-96j6z requesting resource cpu=250m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.045: INFO: Pod calico-node-jxrzv requesting resource cpu=250m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
Jul 22 21:20:29.045: INFO: Pod calico-node-vertical-autoscaler-785b5f968-dl6lj requesting resource cpu=10m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.045: INFO: Pod calico-typha-deploy-59966cd68c-kk766 requesting resource cpu=200m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.045: INFO: Pod calico-typha-horizontal-autoscaler-5b58bb446c-5dvs2 requesting resource cpu=10m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.046: INFO: Pod calico-typha-vertical-autoscaler-5c9655cddd-9t9nc requesting resource cpu=10m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.046: INFO: Pod coredns-7589655f7c-8gplg requesting resource cpu=50m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.046: INFO: Pod coredns-7589655f7c-cck5m requesting resource cpu=50m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.046: INFO: Pod kube-proxy-nbdq9 requesting resource cpu=22m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
Jul 22 21:20:29.046: INFO: Pod kube-proxy-r86fj requesting resource cpu=22m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.046: INFO: Pod metrics-server-7fcbc9df99-qm94n requesting resource cpu=50m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.046: INFO: Pod node-exporter-bxzmq requesting resource cpu=50m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
Jul 22 21:20:29.046: INFO: Pod node-exporter-lkd2s requesting resource cpu=50m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.046: INFO: Pod node-problem-detector-kj6cn requesting resource cpu=11m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.046: INFO: Pod node-problem-detector-vlbxm requesting resource cpu=11m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
Jul 22 21:20:29.046: INFO: Pod vpn-shoot-6c79f97679-hz7lk requesting resource cpu=11m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.046: INFO: Pod dashboard-metrics-scraper-5fc7d79f9-tk49g requesting resource cpu=0m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
Jul 22 21:20:29.046: INFO: Pod kubernetes-dashboard-775d7d55c5-q9hcp requesting resource cpu=50m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
STEP: Starting Pods to consume most of the cluster CPU.
Jul 22 21:20:29.046: INFO: Creating a pod which consumes cpu=1075m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
Jul 22 21:20:29.120: INFO: Creating a pod which consumes cpu=704m on Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26853351-2534-48be-87d3-4bd5a6c79e0a.169439b84e573ebf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-814/filler-pod-26853351-2534-48be-87d3-4bd5a6c79e0a to shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26853351-2534-48be-87d3-4bd5a6c79e0a.169439b8b7f2747a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26853351-2534-48be-87d3-4bd5a6c79e0a.169439b8d963fb79], Reason = [Created], Message = [Created container filler-pod-26853351-2534-48be-87d3-4bd5a6c79e0a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26853351-2534-48be-87d3-4bd5a6c79e0a.169439b8e2a64ffe], Reason = [Started], Message = [Started container filler-pod-26853351-2534-48be-87d3-4bd5a6c79e0a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bbce268d-23e2-4ea4-8967-ee4dbd9bcef4.169439b84da2aa7a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-814/filler-pod-bbce268d-23e2-4ea4-8967-ee4dbd9bcef4 to shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bbce268d-23e2-4ea4-8967-ee4dbd9bcef4.169439b8a3d1bdbd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bbce268d-23e2-4ea4-8967-ee4dbd9bcef4.169439b8c2f339db], Reason = [Created], Message = [Created container filler-pod-bbce268d-23e2-4ea4-8967-ee4dbd9bcef4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bbce268d-23e2-4ea4-8967-ee4dbd9bcef4.169439b8cb0e3ae9], Reason = [Started], Message = [Started container filler-pod-bbce268d-23e2-4ea4-8967-ee4dbd9bcef4]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.169439b940fea235], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.169439b9413f4ce5], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:20:34.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-814" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":234,"skipped":4007,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:20:34.351: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:21:34.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-698" for this suite.
•{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":235,"skipped":4019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:21:34.623: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:21:34.835: INFO: Waiting up to 5m0s for pod "downwardapi-volume-858c82ff-15a6-4880-aa40-a65fa64549ec" in namespace "downward-api-2587" to be "Succeeded or Failed"
Jul 22 21:21:34.852: INFO: Pod "downwardapi-volume-858c82ff-15a6-4880-aa40-a65fa64549ec": Phase="Pending", Reason="", readiness=false. Elapsed: 16.292063ms
Jul 22 21:21:36.864: INFO: Pod "downwardapi-volume-858c82ff-15a6-4880-aa40-a65fa64549ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028486935s
Jul 22 21:21:38.882: INFO: Pod "downwardapi-volume-858c82ff-15a6-4880-aa40-a65fa64549ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047070096s
STEP: Saw pod success
Jul 22 21:21:38.882: INFO: Pod "downwardapi-volume-858c82ff-15a6-4880-aa40-a65fa64549ec" satisfied condition "Succeeded or Failed"
Jul 22 21:21:38.894: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-858c82ff-15a6-4880-aa40-a65fa64549ec container client-container: <nil>
STEP: delete the pod
Jul 22 21:21:39.016: INFO: Waiting for pod downwardapi-volume-858c82ff-15a6-4880-aa40-a65fa64549ec to disappear
Jul 22 21:21:39.031: INFO: Pod downwardapi-volume-858c82ff-15a6-4880-aa40-a65fa64549ec no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:21:39.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2587" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":4042,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:21:39.065: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:21:39.291: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:21:41.303: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:21:43.303: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Running (Ready = false)
Jul 22 21:21:45.303: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Running (Ready = false)
Jul 22 21:21:47.303: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Running (Ready = false)
Jul 22 21:21:49.303: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Running (Ready = false)
Jul 22 21:21:51.303: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Running (Ready = false)
Jul 22 21:21:57.021: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Running (Ready = false)
Jul 22 21:21:57.304: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Running (Ready = false)
Jul 22 21:21:59.312: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Running (Ready = false)
Jul 22 21:22:01.303: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Running (Ready = false)
Jul 22 21:22:03.303: INFO: The status of Pod test-webserver-093894b0-dc9a-4816-afd0-4cd50ecf18e6 is Running (Ready = true)
Jul 22 21:22:03.315: INFO: Container started at 2021-07-22 21:21:41 +0000 UTC, pod became ready at 2021-07-22 21:22:02 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:22:03.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4728" for this suite.
•{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":4047,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:22:03.350: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7430
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jul 22 21:22:03.584: INFO: observed Pod pod-test in namespace pods-7430 in phase Pending conditions []
Jul 22 21:22:03.584: INFO: observed Pod pod-test in namespace pods-7430 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:22:03 +0000 UTC  }]
Jul 22 21:22:03.625: INFO: observed Pod pod-test in namespace pods-7430 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:22:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:22:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:22:03 +0000 UTC  }]
Jul 22 21:22:05.135: INFO: observed Pod pod-test in namespace pods-7430 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:22:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:22:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:22:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:22:03 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jul 22 21:22:06.237: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jul 22 21:22:06.308: INFO: observed event type ADDED
Jul 22 21:22:06.308: INFO: observed event type MODIFIED
Jul 22 21:22:06.308: INFO: observed event type MODIFIED
Jul 22 21:22:06.308: INFO: observed event type MODIFIED
Jul 22 21:22:06.308: INFO: observed event type MODIFIED
Jul 22 21:22:06.308: INFO: observed event type MODIFIED
Jul 22 21:22:06.308: INFO: observed event type MODIFIED
Jul 22 21:22:06.309: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:22:06.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7430" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":238,"skipped":4061,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:22:06.334: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 22 21:22:06.525: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4546 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jul 22 21:22:06.656: INFO: stderr: ""
Jul 22 21:22:06.656: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jul 22 21:22:06.656: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4546 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Jul 22 21:22:06.994: INFO: stderr: ""
Jul 22 21:22:06.994: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Jul 22 21:22:07.007: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4546 delete pods e2e-test-httpd-pod'
Jul 22 21:22:08.874: INFO: stderr: ""
Jul 22 21:22:08.874: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:22:08.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4546" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":239,"skipped":4070,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:22:08.909: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 22 21:22:17.285: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 21:22:17.306: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 21:22:19.307: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 21:22:19.330: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 21:22:21.306: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 21:22:21.319: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 21:22:23.306: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 21:22:23.319: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:22:23.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-489" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":240,"skipped":4078,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:22:23.354: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4064
STEP: creating service affinity-clusterip-transition in namespace services-4064
STEP: creating replication controller affinity-clusterip-transition in namespace services-4064
I0722 21:22:23.573748    5567 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-4064, replica count: 3
I0722 21:22:26.624122    5567 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 21:22:29.624358    5567 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:22:29.646: INFO: Creating new exec pod
Jul 22 21:22:34.757: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4064 exec execpod-affinityl7tj4 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jul 22 21:22:35.278: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul 22 21:22:35.278: INFO: stdout: ""
Jul 22 21:22:35.278: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4064 exec execpod-affinityl7tj4 -- /bin/sh -x -c nc -zv -t -w 2 100.64.41.17 80'
Jul 22 21:22:35.796: INFO: stderr: "+ nc -zv -t -w 2 100.64.41.17 80\nConnection to 100.64.41.17 80 port [tcp/http] succeeded!\n"
Jul 22 21:22:35.796: INFO: stdout: ""
Jul 22 21:22:35.825: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4064 exec execpod-affinityl7tj4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.41.17:80/ ; done'
Jul 22 21:22:36.561: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n"
Jul 22 21:22:36.561: INFO: stdout: "\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8"
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:22:36.561: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:06.562: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4064 exec execpod-affinityl7tj4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.41.17:80/ ; done'
Jul 22 21:23:07.327: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n"
Jul 22 21:23:07.327: INFO: stdout: "\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-fg9zl\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-fg9zl\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-fg9zl"
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-fg9zl
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-fg9zl
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.328: INFO: Received response from host: affinity-clusterip-transition-fg9zl
Jul 22 21:23:07.356: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4064 exec execpod-affinityl7tj4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.41.17:80/ ; done'
Jul 22 21:23:07.951: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n"
Jul 22 21:23:07.952: INFO: stdout: "\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-fg9zl\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-fg9zl\naffinity-clusterip-transition-w4vsp\naffinity-clusterip-transition-fg9zl\naffinity-clusterip-transition-fg9zl\naffinity-clusterip-transition-fg9zl\naffinity-clusterip-transition-w4vsp"
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-fg9zl
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-fg9zl
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-fg9zl
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-fg9zl
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-fg9zl
Jul 22 21:23:07.952: INFO: Received response from host: affinity-clusterip-transition-w4vsp
Jul 22 21:23:37.952: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4064 exec execpod-affinityl7tj4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.41.17:80/ ; done'
Jul 22 21:23:38.662: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.41.17:80/\n"
Jul 22 21:23:38.662: INFO: stdout: "\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8\naffinity-clusterip-transition-5d8k8"
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Received response from host: affinity-clusterip-transition-5d8k8
Jul 22 21:23:38.662: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4064, will wait for the garbage collector to delete the pods
Jul 22 21:23:38.760: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.958819ms
Jul 22 21:23:39.560: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 800.252498ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:23:45.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4064" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":241,"skipped":4084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:23:45.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 22 21:23:53.726: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:23:53.739: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:23:55.739: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:23:55.751: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 21:23:57.739: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 21:23:57.755: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:23:57.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9576" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:23:57.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Jul 22 21:23:58.021: INFO: created test-event-1
Jul 22 21:23:58.033: INFO: created test-event-2
Jul 22 21:23:58.046: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jul 22 21:23:58.057: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jul 22 21:23:58.086: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:23:58.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6650" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":243,"skipped":4240,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:23:58.129: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Jul 22 21:23:58.319: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 create -f -'
Jul 22 21:24:03.587: INFO: stderr: ""
Jul 22 21:24:03.587: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 21:24:03.587: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 21:24:03.696: INFO: stderr: ""
Jul 22 21:24:03.696: INFO: stdout: "update-demo-nautilus-czdl4 update-demo-nautilus-vdmfd "
Jul 22 21:24:03.696: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get pods update-demo-nautilus-czdl4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 21:24:03.800: INFO: stderr: ""
Jul 22 21:24:03.800: INFO: stdout: ""
Jul 22 21:24:03.800: INFO: update-demo-nautilus-czdl4 is created but not running
Jul 22 21:24:08.801: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 21:24:08.912: INFO: stderr: ""
Jul 22 21:24:08.912: INFO: stdout: "update-demo-nautilus-czdl4 update-demo-nautilus-vdmfd "
Jul 22 21:24:08.912: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get pods update-demo-nautilus-czdl4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 21:24:09.011: INFO: stderr: ""
Jul 22 21:24:09.011: INFO: stdout: "true"
Jul 22 21:24:09.011: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get pods update-demo-nautilus-czdl4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 21:24:09.109: INFO: stderr: ""
Jul 22 21:24:09.109: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:24:09.109: INFO: validating pod update-demo-nautilus-czdl4
Jul 22 21:24:09.237: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:24:09.237: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:24:09.237: INFO: update-demo-nautilus-czdl4 is verified up and running
Jul 22 21:24:09.237: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get pods update-demo-nautilus-vdmfd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 21:24:09.342: INFO: stderr: ""
Jul 22 21:24:09.342: INFO: stdout: "true"
Jul 22 21:24:09.342: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get pods update-demo-nautilus-vdmfd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 21:24:09.447: INFO: stderr: ""
Jul 22 21:24:09.447: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 21:24:09.447: INFO: validating pod update-demo-nautilus-vdmfd
Jul 22 21:24:09.569: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 21:24:09.569: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 21:24:09.569: INFO: update-demo-nautilus-vdmfd is verified up and running
STEP: using delete to clean up resources
Jul 22 21:24:09.569: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 delete --grace-period=0 --force -f -'
Jul 22 21:24:09.692: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:24:09.692: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 22 21:24:09.692: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get rc,svc -l name=update-demo --no-headers'
Jul 22 21:24:09.806: INFO: stderr: "No resources found in kubectl-4555 namespace.\n"
Jul 22 21:24:09.806: INFO: stdout: ""
Jul 22 21:24:09.806: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 21:24:09.912: INFO: stderr: ""
Jul 22 21:24:09.912: INFO: stdout: "update-demo-nautilus-czdl4\nupdate-demo-nautilus-vdmfd\n"
Jul 22 21:24:10.412: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get rc,svc -l name=update-demo --no-headers'
Jul 22 21:24:10.542: INFO: stderr: "No resources found in kubectl-4555 namespace.\n"
Jul 22 21:24:10.542: INFO: stdout: ""
Jul 22 21:24:10.542: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4555 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 21:24:10.644: INFO: stderr: ""
Jul 22 21:24:10.644: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:24:10.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4555" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":244,"skipped":4258,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:24:10.680: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3520
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Jul 22 21:24:10.904: INFO: Waiting up to 5m0s for pod "var-expansion-c0fbb6eb-1d00-4748-b5a4-9f39ec2240b1" in namespace "var-expansion-3520" to be "Succeeded or Failed"
Jul 22 21:24:10.917: INFO: Pod "var-expansion-c0fbb6eb-1d00-4748-b5a4-9f39ec2240b1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.663227ms
Jul 22 21:24:12.930: INFO: Pod "var-expansion-c0fbb6eb-1d00-4748-b5a4-9f39ec2240b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025097767s
Jul 22 21:24:14.942: INFO: Pod "var-expansion-c0fbb6eb-1d00-4748-b5a4-9f39ec2240b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037506032s
STEP: Saw pod success
Jul 22 21:24:14.942: INFO: Pod "var-expansion-c0fbb6eb-1d00-4748-b5a4-9f39ec2240b1" satisfied condition "Succeeded or Failed"
Jul 22 21:24:14.954: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod var-expansion-c0fbb6eb-1d00-4748-b5a4-9f39ec2240b1 container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:24:15.061: INFO: Waiting for pod var-expansion-c0fbb6eb-1d00-4748-b5a4-9f39ec2240b1 to disappear
Jul 22 21:24:15.072: INFO: Pod var-expansion-c0fbb6eb-1d00-4748-b5a4-9f39ec2240b1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:24:15.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3520" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4276,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:24:15.123: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5626
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Jul 22 21:24:15.321: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:24:36.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5626" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":246,"skipped":4282,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:24:36.998: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:24:37.266: INFO: Create a RollingUpdate DaemonSet
Jul 22 21:24:37.278: INFO: Check that daemon pods launch on every node of the cluster
Jul 22 21:24:37.303: INFO: Number of nodes with available pods: 0
Jul 22 21:24:37.303: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 21:24:38.338: INFO: Number of nodes with available pods: 0
Jul 22 21:24:38.338: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 21:24:39.338: INFO: Number of nodes with available pods: 0
Jul 22 21:24:39.338: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 21:24:40.338: INFO: Number of nodes with available pods: 2
Jul 22 21:24:40.338: INFO: Number of running nodes: 2, number of available pods: 2
Jul 22 21:24:40.338: INFO: Update the DaemonSet to trigger a rollout
Jul 22 21:24:40.363: INFO: Updating DaemonSet daemon-set
Jul 22 21:24:45.434: INFO: Roll back the DaemonSet before rollout is complete
Jul 22 21:24:45.458: INFO: Updating DaemonSet daemon-set
Jul 22 21:24:45.458: INFO: Make sure DaemonSet rollback is complete
Jul 22 21:24:45.479: INFO: Wrong image for pod: daemon-set-pvmcn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 22 21:24:45.479: INFO: Pod daemon-set-pvmcn is not available
Jul 22 21:24:46.504: INFO: Wrong image for pod: daemon-set-pvmcn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 22 21:24:46.504: INFO: Pod daemon-set-pvmcn is not available
Jul 22 21:24:47.504: INFO: Pod daemon-set-g9hfr is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6991, will wait for the garbage collector to delete the pods
Jul 22 21:24:47.628: INFO: Deleting DaemonSet.extensions daemon-set took: 13.531395ms
Jul 22 21:24:48.429: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.259001ms
Jul 22 21:25:02.841: INFO: Number of nodes with available pods: 0
Jul 22 21:25:02.841: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 21:25:02.852: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38835"},"items":null}

Jul 22 21:25:02.870: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38835"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:25:02.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6991" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":247,"skipped":4296,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:25:02.940: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8884
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-8884
STEP: creating service affinity-nodeport-transition in namespace services-8884
STEP: creating replication controller affinity-nodeport-transition in namespace services-8884
I0722 21:25:03.173143    5567 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-8884, replica count: 3
I0722 21:25:06.223516    5567 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:25:06.267: INFO: Creating new exec pod
Jul 22 21:25:11.340: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8884 exec execpod-affinity4hth5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jul 22 21:25:11.923: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul 22 21:25:11.923: INFO: stdout: ""
Jul 22 21:25:11.924: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8884 exec execpod-affinity4hth5 -- /bin/sh -x -c nc -zv -t -w 2 100.70.205.9 80'
Jul 22 21:25:12.553: INFO: stderr: "+ nc -zv -t -w 2 100.70.205.9 80\nConnection to 100.70.205.9 80 port [tcp/http] succeeded!\n"
Jul 22 21:25:12.553: INFO: stdout: ""
Jul 22 21:25:12.553: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8884 exec execpod-affinity4hth5 -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.4 32191'
Jul 22 21:25:13.098: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.4 32191\nConnection to 10.250.0.4 32191 port [tcp/32191] succeeded!\n"
Jul 22 21:25:13.098: INFO: stdout: ""
Jul 22 21:25:13.098: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8884 exec execpod-affinity4hth5 -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.5 32191'
Jul 22 21:25:13.653: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.5 32191\nConnection to 10.250.0.5 32191 port [tcp/32191] succeeded!\n"
Jul 22 21:25:13.653: INFO: stdout: ""
Jul 22 21:25:13.688: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8884 exec execpod-affinity4hth5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.4:32191/ ; done'
Jul 22 21:25:14.410: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n"
Jul 22 21:25:14.410: INFO: stdout: "\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb"
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:14.410: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:44.410: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8884 exec execpod-affinity4hth5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.4:32191/ ; done'
Jul 22 21:25:45.718: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n"
Jul 22 21:25:45.718: INFO: stdout: "\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-9hj5c\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-9hj5c\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-9hj5c\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-9hj5c"
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-9hj5c
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-9hj5c
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-9hj5c
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:45.718: INFO: Received response from host: affinity-nodeport-transition-9hj5c
Jul 22 21:25:45.745: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8884 exec execpod-affinity4hth5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.4:32191/ ; done'
Jul 22 21:25:46.345: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n"
Jul 22 21:25:46.345: INFO: stdout: "\naffinity-nodeport-transition-9hj5c\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-9hj5c\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-9hj5c\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-9hj5c\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-z4298\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-9hj5c\naffinity-nodeport-transition-8kzjb"
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-9hj5c
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-9hj5c
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-9hj5c
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-9hj5c
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-z4298
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-9hj5c
Jul 22 21:25:46.345: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:16.346: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8884 exec execpod-affinity4hth5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.4:32191/ ; done'
Jul 22 21:26:17.014: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.4:32191/\n"
Jul 22 21:26:17.014: INFO: stdout: "\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb\naffinity-nodeport-transition-8kzjb"
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Received response from host: affinity-nodeport-transition-8kzjb
Jul 22 21:26:17.014: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8884, will wait for the garbage collector to delete the pods
Jul 22 21:26:17.109: INFO: Deleting ReplicationController affinity-nodeport-transition took: 13.827763ms
Jul 22 21:26:17.910: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 800.233571ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:26:32.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8884" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":248,"skipped":4302,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:26:32.906: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-599
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 22 21:26:43.325: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 22 21:26:43.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-5g2lf" in namespace "gc-599"
W0722 21:26:43.324979    5567 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 21:26:43.325024    5567 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 21:26:43.325033    5567 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 22 21:26:43.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-9srp6" in namespace "gc-599"
Jul 22 21:26:43.370: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6tt6" in namespace "gc-599"
Jul 22 21:26:43.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-bp4nw" in namespace "gc-599"
Jul 22 21:26:43.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzgb8" in namespace "gc-599"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:26:43.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-599" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":249,"skipped":4306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:26:43.446: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:26:44.649: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:26:46.662: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:26:48.661: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586004, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:26:51.685: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:26:52.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8512" for this suite.
STEP: Destroying namespace "webhook-8512-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":250,"skipped":4348,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:26:52.366: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7086
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Jul 22 21:26:52.608: INFO: Found 0 stateful pods, waiting for 3
Jul 22 21:27:02.621: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:27:02.622: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:27:02.622: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Jul 22 21:27:12.621: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:27:12.621: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:27:12.621: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 22 21:27:12.692: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 22 21:27:12.755: INFO: Updating stateful set ss2
Jul 22 21:27:12.781: INFO: Waiting for Pod statefulset-7086/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jul 22 21:27:32.869: INFO: Found 2 stateful pods, waiting for 3
Jul 22 21:27:42.881: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:27:42.882: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:27:42.882: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 22 21:27:42.941: INFO: Updating stateful set ss2
Jul 22 21:27:42.966: INFO: Waiting for Pod statefulset-7086/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 22 21:28:23.028: INFO: Updating stateful set ss2
Jul 22 21:28:23.052: INFO: Waiting for StatefulSet statefulset-7086/ss2 to complete update
Jul 22 21:28:23.052: INFO: Waiting for Pod statefulset-7086/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 22 21:28:33.078: INFO: Waiting for StatefulSet statefulset-7086/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 22 21:28:43.079: INFO: Deleting all statefulset in ns statefulset-7086
Jul 22 21:28:43.091: INFO: Scaling statefulset ss2 to 0
Jul 22 21:29:13.149: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:29:13.160: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:29:13.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7086" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":251,"skipped":4360,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:29:13.242: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8016
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-6a3bcd41-4127-4448-902a-c1418f062e36
STEP: Creating a pod to test consume configMaps
Jul 22 21:29:13.488: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07a5ca7c-18cf-480d-aedf-49baa2b12d40" in namespace "projected-8016" to be "Succeeded or Failed"
Jul 22 21:29:13.500: INFO: Pod "pod-projected-configmaps-07a5ca7c-18cf-480d-aedf-49baa2b12d40": Phase="Pending", Reason="", readiness=false. Elapsed: 11.226955ms
Jul 22 21:29:15.512: INFO: Pod "pod-projected-configmaps-07a5ca7c-18cf-480d-aedf-49baa2b12d40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023767805s
Jul 22 21:29:17.525: INFO: Pod "pod-projected-configmaps-07a5ca7c-18cf-480d-aedf-49baa2b12d40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036483483s
STEP: Saw pod success
Jul 22 21:29:17.525: INFO: Pod "pod-projected-configmaps-07a5ca7c-18cf-480d-aedf-49baa2b12d40" satisfied condition "Succeeded or Failed"
Jul 22 21:29:17.537: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-configmaps-07a5ca7c-18cf-480d-aedf-49baa2b12d40 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:29:17.636: INFO: Waiting for pod pod-projected-configmaps-07a5ca7c-18cf-480d-aedf-49baa2b12d40 to disappear
Jul 22 21:29:17.648: INFO: Pod pod-projected-configmaps-07a5ca7c-18cf-480d-aedf-49baa2b12d40 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:29:17.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8016" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":252,"skipped":4370,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:29:17.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5378
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5378.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5378.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:29:22.132: INFO: DNS probes using dns-test-a813596b-669a-4ace-8fc4-2ba76137aa30 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5378.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5378.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:29:28.358: INFO: File wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local from pod  dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 21:29:28.407: INFO: File jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local from pod  dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 21:29:28.407: INFO: Lookups using dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 failed for: [wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local]

Jul 22 21:29:33.498: INFO: File wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local from pod  dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 21:29:33.552: INFO: File jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local from pod  dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 21:29:33.552: INFO: Lookups using dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 failed for: [wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local]

Jul 22 21:29:38.461: INFO: File wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local from pod  dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 21:29:38.513: INFO: File jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local from pod  dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 21:29:38.513: INFO: Lookups using dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 failed for: [wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local]

Jul 22 21:29:43.459: INFO: File wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local from pod  dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 21:29:43.550: INFO: File jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local from pod  dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 21:29:43.551: INFO: Lookups using dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 failed for: [wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local]

Jul 22 21:29:48.458: INFO: File wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local from pod  dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 21:29:48.508: INFO: File jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local from pod  dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 21:29:48.508: INFO: Lookups using dns-5378/dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 failed for: [wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local]

Jul 22 21:29:53.537: INFO: DNS probes using dns-test-4604b8e0-f93c-4887-9339-df5e27c92a08 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5378.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5378.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5378.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5378.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:29:59.892: INFO: DNS probes using dns-test-f1dabf6b-3715-410d-9153-adcf8e0671ee succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:29:59.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5378" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":253,"skipped":4375,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:29:59.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 21:30:00.293: INFO: Number of nodes with available pods: 0
Jul 22 21:30:00.293: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 21:30:01.329: INFO: Number of nodes with available pods: 0
Jul 22 21:30:01.329: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 21:30:02.330: INFO: Number of nodes with available pods: 0
Jul 22 21:30:02.331: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 21:30:03.334: INFO: Number of nodes with available pods: 1
Jul 22 21:30:03.334: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 is running more than one daemon pod
Jul 22 21:30:04.328: INFO: Number of nodes with available pods: 2
Jul 22 21:30:04.328: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 22 21:30:04.394: INFO: Number of nodes with available pods: 1
Jul 22 21:30:04.394: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 21:30:05.428: INFO: Number of nodes with available pods: 1
Jul 22 21:30:05.428: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 21:30:06.434: INFO: Number of nodes with available pods: 1
Jul 22 21:30:06.434: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 21:30:07.431: INFO: Number of nodes with available pods: 1
Jul 22 21:30:07.431: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 21:30:08.429: INFO: Number of nodes with available pods: 1
Jul 22 21:30:08.429: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 21:30:09.428: INFO: Number of nodes with available pods: 1
Jul 22 21:30:09.428: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 21:30:10.432: INFO: Number of nodes with available pods: 1
Jul 22 21:30:10.432: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 21:30:11.429: INFO: Number of nodes with available pods: 1
Jul 22 21:30:11.429: INFO: Node shoot--it--tmi8i-6ya-worker-1-6fc4f-9mscv is running more than one daemon pod
Jul 22 21:30:12.429: INFO: Number of nodes with available pods: 2
Jul 22 21:30:12.429: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1852, will wait for the garbage collector to delete the pods
Jul 22 21:30:12.518: INFO: Deleting DaemonSet.extensions daemon-set took: 13.877848ms
Jul 22 21:30:13.319: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.220278ms
Jul 22 21:30:16.830: INFO: Number of nodes with available pods: 0
Jul 22 21:30:16.831: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 21:30:16.842: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40950"},"items":null}

Jul 22 21:30:16.854: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40950"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:30:16.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1852" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":254,"skipped":4383,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:30:16.927: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-195
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-b2eee877-5301-40ff-b87f-53d1ac9c158b
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:30:17.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-195" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":255,"skipped":4402,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:30:17.157: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3294, will wait for the garbage collector to delete the pods
Jul 22 21:30:21.463: INFO: Deleting Job.batch foo took: 14.635217ms
Jul 22 21:30:21.563: INFO: Terminating Job.batch foo pods took: 100.21837ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:31:02.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3294" for this suite.
•{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":256,"skipped":4410,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:31:02.909: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3065
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5263
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7682
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:31:09.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3065" for this suite.
STEP: Destroying namespace "nsdeletetest-5263" for this suite.
Jul 22 21:31:09.564: INFO: Namespace nsdeletetest-5263 was already deleted
STEP: Destroying namespace "nsdeletetest-7682" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":257,"skipped":4418,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:31:09.590: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9457
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 22 21:31:09.798: INFO: Waiting up to 5m0s for pod "pod-c94e4d8a-6d49-44ef-ada7-43871502fd35" in namespace "emptydir-9457" to be "Succeeded or Failed"
Jul 22 21:31:09.809: INFO: Pod "pod-c94e4d8a-6d49-44ef-ada7-43871502fd35": Phase="Pending", Reason="", readiness=false. Elapsed: 11.250939ms
Jul 22 21:31:11.821: INFO: Pod "pod-c94e4d8a-6d49-44ef-ada7-43871502fd35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023313132s
Jul 22 21:31:13.833: INFO: Pod "pod-c94e4d8a-6d49-44ef-ada7-43871502fd35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034896732s
STEP: Saw pod success
Jul 22 21:31:13.833: INFO: Pod "pod-c94e4d8a-6d49-44ef-ada7-43871502fd35" satisfied condition "Succeeded or Failed"
Jul 22 21:31:13.844: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-c94e4d8a-6d49-44ef-ada7-43871502fd35 container test-container: <nil>
STEP: delete the pod
Jul 22 21:31:14.265: INFO: Waiting for pod pod-c94e4d8a-6d49-44ef-ada7-43871502fd35 to disappear
Jul 22 21:31:14.276: INFO: Pod pod-c94e4d8a-6d49-44ef-ada7-43871502fd35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:31:14.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9457" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":258,"skipped":4434,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:31:14.309: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:31:14.576: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1ae3652c-ceee-4c6e-9801-5cd5a39bcaef", Controller:(*bool)(0xc0077cccba), BlockOwnerDeletion:(*bool)(0xc0077cccbb)}}
Jul 22 21:31:14.611: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"796778d0-9799-4b55-9081-62f9509d1534", Controller:(*bool)(0xc003b43c3e), BlockOwnerDeletion:(*bool)(0xc003b43c3f)}}
Jul 22 21:31:14.624: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d704b29c-69e9-490c-b489-6fdec65da111", Controller:(*bool)(0xc0077ccea6), BlockOwnerDeletion:(*bool)(0xc0077ccea7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:31:19.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4085" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":259,"skipped":4435,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:31:19.688: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 22 21:31:19.914: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 21:32:20.024: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:32:20.039: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-2398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:32:20.262: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jul 22 21:32:20.273: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:32:20.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2398" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:32:20.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5413" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":260,"skipped":4441,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:32:20.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 22 21:32:20.682: INFO: Waiting up to 5m0s for pod "pod-db9cde9d-4577-4846-adb2-08f6bb447ffe" in namespace "emptydir-2689" to be "Succeeded or Failed"
Jul 22 21:32:20.694: INFO: Pod "pod-db9cde9d-4577-4846-adb2-08f6bb447ffe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.373266ms
Jul 22 21:32:22.707: INFO: Pod "pod-db9cde9d-4577-4846-adb2-08f6bb447ffe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024411861s
Jul 22 21:32:24.719: INFO: Pod "pod-db9cde9d-4577-4846-adb2-08f6bb447ffe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037087619s
STEP: Saw pod success
Jul 22 21:32:24.719: INFO: Pod "pod-db9cde9d-4577-4846-adb2-08f6bb447ffe" satisfied condition "Succeeded or Failed"
Jul 22 21:32:24.738: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-db9cde9d-4577-4846-adb2-08f6bb447ffe container test-container: <nil>
STEP: delete the pod
Jul 22 21:32:24.809: INFO: Waiting for pod pod-db9cde9d-4577-4846-adb2-08f6bb447ffe to disappear
Jul 22 21:32:24.820: INFO: Pod pod-db9cde9d-4577-4846-adb2-08f6bb447ffe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:32:24.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2689" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":261,"skipped":4455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:32:24.856: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6273
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-fb71c01b-015a-41ba-b95c-f300e4d349cb
STEP: Creating a pod to test consume configMaps
Jul 22 21:32:25.088: INFO: Waiting up to 5m0s for pod "pod-configmaps-1f46067f-4a88-4986-9184-26f8641b8060" in namespace "configmap-6273" to be "Succeeded or Failed"
Jul 22 21:32:25.099: INFO: Pod "pod-configmaps-1f46067f-4a88-4986-9184-26f8641b8060": Phase="Pending", Reason="", readiness=false. Elapsed: 11.311479ms
Jul 22 21:32:27.112: INFO: Pod "pod-configmaps-1f46067f-4a88-4986-9184-26f8641b8060": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024550591s
Jul 22 21:32:29.124: INFO: Pod "pod-configmaps-1f46067f-4a88-4986-9184-26f8641b8060": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036616504s
STEP: Saw pod success
Jul 22 21:32:29.124: INFO: Pod "pod-configmaps-1f46067f-4a88-4986-9184-26f8641b8060" satisfied condition "Succeeded or Failed"
Jul 22 21:32:29.136: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-configmaps-1f46067f-4a88-4986-9184-26f8641b8060 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:32:29.208: INFO: Waiting for pod pod-configmaps-1f46067f-4a88-4986-9184-26f8641b8060 to disappear
Jul 22 21:32:29.218: INFO: Pod pod-configmaps-1f46067f-4a88-4986-9184-26f8641b8060 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:32:29.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6273" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":262,"skipped":4571,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:32:29.250: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:32:29.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586349, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586349, loc:(*time.Location)(0x7980f00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586349, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586349, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:32:31.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586349, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586349, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586349, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586349, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:32:34.917: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:32:35.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9626" for this suite.
STEP: Destroying namespace "webhook-9626-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":263,"skipped":4576,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:32:35.539: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5158
Jul 22 21:32:39.779: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 22 21:32:40.342: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 22 21:32:40.342: INFO: stdout: "iptables"
Jul 22 21:32:40.342: INFO: proxyMode: iptables
Jul 22 21:32:40.359: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 22 21:32:40.371: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-5158
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5158
I0722 21:32:40.404514    5567 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5158, replica count: 3
I0722 21:32:43.454918    5567 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 21:32:46.455143    5567 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:32:46.478: INFO: Creating new exec pod
Jul 22 21:32:51.532: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jul 22 21:32:57.075: INFO: rc: 1
Jul 22 21:32:57.075: INFO: Service reachability failing with error: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-clusterip-timeout 80
nc: getaddrinfo: Try again
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 22 21:32:58.075: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jul 22 21:33:03.577: INFO: rc: 1
Jul 22 21:33:03.577: INFO: Service reachability failing with error: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-clusterip-timeout 80
nc: getaddrinfo: Try again
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 22 21:33:04.076: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jul 22 21:33:09.596: INFO: rc: 1
Jul 22 21:33:09.596: INFO: Service reachability failing with error: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-clusterip-timeout 80
nc: getaddrinfo: Try again
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 22 21:33:10.075: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jul 22 21:33:15.627: INFO: rc: 1
Jul 22 21:33:15.628: INFO: Service reachability failing with error: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-clusterip-timeout 80
nc: getaddrinfo: Try again
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 22 21:33:16.075: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jul 22 21:33:21.688: INFO: rc: 1
Jul 22 21:33:21.688: INFO: Service reachability failing with error: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-clusterip-timeout 80
nc: getaddrinfo: Try again
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 22 21:33:22.076: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jul 22 21:33:27.588: INFO: rc: 1
Jul 22 21:33:27.588: INFO: Service reachability failing with error: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-clusterip-timeout 80
nc: getaddrinfo: Try again
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 22 21:33:28.075: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jul 22 21:33:31.077: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jul 22 21:33:31.077: INFO: stdout: ""
Jul 22 21:33:31.078: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c nc -zv -t -w 2 100.65.62.233 80'
Jul 22 21:33:31.591: INFO: stderr: "+ nc -zv -t -w 2 100.65.62.233 80\nConnection to 100.65.62.233 80 port [tcp/http] succeeded!\n"
Jul 22 21:33:31.591: INFO: stdout: ""
Jul 22 21:33:31.591: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.65.62.233:80/ ; done'
Jul 22 21:33:32.215: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n"
Jul 22 21:33:32.215: INFO: stdout: "\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx\naffinity-clusterip-timeout-dj8hx"
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.215: INFO: Received response from host: affinity-clusterip-timeout-dj8hx
Jul 22 21:33:32.216: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.65.62.233:80/'
Jul 22 21:33:32.924: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n"
Jul 22 21:33:32.924: INFO: stdout: "affinity-clusterip-timeout-dj8hx"
Jul 22 21:33:52.924: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinitykcfkq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.65.62.233:80/'
Jul 22 21:33:53.430: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.65.62.233:80/\n"
Jul 22 21:33:53.430: INFO: stdout: "affinity-clusterip-timeout-tcmp2"
Jul 22 21:33:53.430: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5158, will wait for the garbage collector to delete the pods
Jul 22 21:33:53.523: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 13.634984ms
Jul 22 21:33:53.623: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.235338ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:34:02.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5158" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":264,"skipped":4592,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:34:02.886: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3465
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:34:03.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586443, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586443, loc:(*time.Location)(0x7980f00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586443, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586443, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:34:05.992: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586443, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586443, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586443, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586443, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:34:09.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:34:21.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3465" for this suite.
STEP: Destroying namespace "webhook-3465-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":265,"skipped":4604,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:34:21.850: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jul 22 21:34:26.876: INFO: Successfully updated pod "labelsupdatee0af3406-f735-4fb4-9151-d969e914a198"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:34:28.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1391" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":266,"skipped":4635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:34:29.021: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6124
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:34:29.661: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586469, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586469, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586469, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586469, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:34:31.673: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586469, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586469, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586469, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586469, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:34:34.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:34:35.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6124" for this suite.
STEP: Destroying namespace "webhook-6124-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":267,"skipped":4658,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:34:35.232: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-1dc3926b-7fca-4bc5-bfb9-49f568e19284
STEP: Creating a pod to test consume secrets
Jul 22 21:34:35.452: INFO: Waiting up to 5m0s for pod "pod-secrets-eaac9bab-8ee9-40a8-a4b4-6038efb5cf93" in namespace "secrets-301" to be "Succeeded or Failed"
Jul 22 21:34:35.471: INFO: Pod "pod-secrets-eaac9bab-8ee9-40a8-a4b4-6038efb5cf93": Phase="Pending", Reason="", readiness=false. Elapsed: 19.745241ms
Jul 22 21:34:37.483: INFO: Pod "pod-secrets-eaac9bab-8ee9-40a8-a4b4-6038efb5cf93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031679384s
Jul 22 21:34:39.495: INFO: Pod "pod-secrets-eaac9bab-8ee9-40a8-a4b4-6038efb5cf93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043646263s
Jul 22 21:34:41.514: INFO: Pod "pod-secrets-eaac9bab-8ee9-40a8-a4b4-6038efb5cf93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062003921s
STEP: Saw pod success
Jul 22 21:34:41.514: INFO: Pod "pod-secrets-eaac9bab-8ee9-40a8-a4b4-6038efb5cf93" satisfied condition "Succeeded or Failed"
Jul 22 21:34:41.525: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-secrets-eaac9bab-8ee9-40a8-a4b4-6038efb5cf93 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:34:41.617: INFO: Waiting for pod pod-secrets-eaac9bab-8ee9-40a8-a4b4-6038efb5cf93 to disappear
Jul 22 21:34:41.628: INFO: Pod pod-secrets-eaac9bab-8ee9-40a8-a4b4-6038efb5cf93 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:34:41.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-301" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":268,"skipped":4664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:34:41.661: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2746
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Jul 22 21:34:41.861: INFO: Waiting up to 5m0s for pod "client-containers-865dfcb7-d340-4a76-81cc-466d731b1dac" in namespace "containers-2746" to be "Succeeded or Failed"
Jul 22 21:34:41.872: INFO: Pod "client-containers-865dfcb7-d340-4a76-81cc-466d731b1dac": Phase="Pending", Reason="", readiness=false. Elapsed: 11.519806ms
Jul 22 21:34:43.884: INFO: Pod "client-containers-865dfcb7-d340-4a76-81cc-466d731b1dac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023789466s
Jul 22 21:34:45.897: INFO: Pod "client-containers-865dfcb7-d340-4a76-81cc-466d731b1dac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036052986s
STEP: Saw pod success
Jul 22 21:34:45.897: INFO: Pod "client-containers-865dfcb7-d340-4a76-81cc-466d731b1dac" satisfied condition "Succeeded or Failed"
Jul 22 21:34:45.908: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod client-containers-865dfcb7-d340-4a76-81cc-466d731b1dac container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:34:46.059: INFO: Waiting for pod client-containers-865dfcb7-d340-4a76-81cc-466d731b1dac to disappear
Jul 22 21:34:46.070: INFO: Pod client-containers-865dfcb7-d340-4a76-81cc-466d731b1dac no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:34:46.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2746" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4690,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:34:46.103: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:34:50.385: INFO: Waiting up to 5m0s for pod "client-envvars-192cb2d4-43c0-4c84-bf0d-b3479bd42a39" in namespace "pods-9582" to be "Succeeded or Failed"
Jul 22 21:34:50.396: INFO: Pod "client-envvars-192cb2d4-43c0-4c84-bf0d-b3479bd42a39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.555643ms
Jul 22 21:34:52.408: INFO: Pod "client-envvars-192cb2d4-43c0-4c84-bf0d-b3479bd42a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022785418s
Jul 22 21:34:54.420: INFO: Pod "client-envvars-192cb2d4-43c0-4c84-bf0d-b3479bd42a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034903224s
STEP: Saw pod success
Jul 22 21:34:54.420: INFO: Pod "client-envvars-192cb2d4-43c0-4c84-bf0d-b3479bd42a39" satisfied condition "Succeeded or Failed"
Jul 22 21:34:54.437: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod client-envvars-192cb2d4-43c0-4c84-bf0d-b3479bd42a39 container env3cont: <nil>
STEP: delete the pod
Jul 22 21:34:54.609: INFO: Waiting for pod client-envvars-192cb2d4-43c0-4c84-bf0d-b3479bd42a39 to disappear
Jul 22 21:34:54.619: INFO: Pod client-envvars-192cb2d4-43c0-4c84-bf0d-b3479bd42a39 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:34:54.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9582" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":270,"skipped":4698,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:34:54.652: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3731
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:34:54.890: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-12752ad0-81e4-492c-84f6-415494fd5440" in namespace "security-context-test-3731" to be "Succeeded or Failed"
Jul 22 21:34:54.902: INFO: Pod "busybox-privileged-false-12752ad0-81e4-492c-84f6-415494fd5440": Phase="Pending", Reason="", readiness=false. Elapsed: 11.73272ms
Jul 22 21:34:56.914: INFO: Pod "busybox-privileged-false-12752ad0-81e4-492c-84f6-415494fd5440": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023375916s
Jul 22 21:34:58.925: INFO: Pod "busybox-privileged-false-12752ad0-81e4-492c-84f6-415494fd5440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034781803s
Jul 22 21:34:58.925: INFO: Pod "busybox-privileged-false-12752ad0-81e4-492c-84f6-415494fd5440" satisfied condition "Succeeded or Failed"
Jul 22 21:34:58.974: INFO: Got logs for pod "busybox-privileged-false-12752ad0-81e4-492c-84f6-415494fd5440": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:34:58.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3731" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4705,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:34:59.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 22 21:34:59.269: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2931  56c5ce9e-4f6f-44a4-a571-1965f74f42a7 42856 0 2021-07-22 21:34:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-22 21:34:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:34:59.269: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2931  56c5ce9e-4f6f-44a4-a571-1965f74f42a7 42857 0 2021-07-22 21:34:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-22 21:34:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:34:59.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2931" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":272,"skipped":4708,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:34:59.301: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-5835
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 21:34:59.491: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 22 21:34:59.562: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:35:01.574: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:35:03.575: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:35:05.575: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:35:07.574: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:35:09.575: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:35:11.574: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:35:13.574: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:35:15.575: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:35:17.574: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:35:19.574: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 21:35:21.581: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 22 21:35:21.603: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 22 21:35:25.704: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 22 21:35:25.704: INFO: Going to poll 100.96.1.78 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jul 22 21:35:25.715: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5835 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:35:25.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:35:27.109: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 22 21:35:27.109: INFO: Going to poll 100.96.0.113 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jul 22 21:35:27.120: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.113 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5835 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:35:27.120: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:35:28.612: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:35:28.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5835" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4716,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:35:28.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 22 21:35:28.841: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4107 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jul 22 21:35:28.956: INFO: stderr: ""
Jul 22 21:35:28.956: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul 22 21:35:34.006: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4107 get pod e2e-test-httpd-pod -o json'
Jul 22 21:35:34.102: INFO: stderr: ""
Jul 22 21:35:34.102: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.80/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.96.1.80/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-07-22T21:35:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-22T21:35:28Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-22T21:35:30Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"100.96.1.80\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-22T21:35:31Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4107\",\n        \"resourceVersion\": \"43086\",\n        \"uid\": \"08719021-9c1a-45cd-8dc6-d2f06c069d40\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.tmi8i-6ya.it.internal.staging.k8s.ondemand.com\"\n                    }\n                ],\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-457q9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-457q9\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-457q9\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-22T21:35:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-22T21:35:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-22T21:35:31Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-22T21:35:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://dfe9a1d606230850fbffdd1e3b034657f5b16b7cfe73139a63485af17b599702\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-07-22T21:35:30Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.80\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.96.1.80\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-07-22T21:35:28Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 22 21:35:34.102: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4107 replace -f -'
Jul 22 21:35:34.398: INFO: stderr: ""
Jul 22 21:35:34.398: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Jul 22 21:35:34.410: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4107 delete pods e2e-test-httpd-pod'
Jul 22 21:35:39.640: INFO: stderr: ""
Jul 22 21:35:39.640: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:35:39.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4107" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":274,"skipped":4717,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:35:39.677: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7489
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 22 21:35:39.907: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7489  2b7beb2b-133c-4271-af57-5ae125ec70bd 43150 0 2021-07-22 21:35:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:35:39.907: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7489  2b7beb2b-133c-4271-af57-5ae125ec70bd 43150 0 2021-07-22 21:35:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 22 21:35:49.936: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7489  2b7beb2b-133c-4271-af57-5ae125ec70bd 43202 0 2021-07-22 21:35:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:35:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:35:49.936: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7489  2b7beb2b-133c-4271-af57-5ae125ec70bd 43202 0 2021-07-22 21:35:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:35:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 22 21:35:59.960: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7489  2b7beb2b-133c-4271-af57-5ae125ec70bd 43245 0 2021-07-22 21:35:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:35:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:35:59.960: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7489  2b7beb2b-133c-4271-af57-5ae125ec70bd 43245 0 2021-07-22 21:35:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:35:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 22 21:36:09.976: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7489  2b7beb2b-133c-4271-af57-5ae125ec70bd 43288 0 2021-07-22 21:35:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:35:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:36:09.976: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7489  2b7beb2b-133c-4271-af57-5ae125ec70bd 43288 0 2021-07-22 21:35:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:35:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 22 21:36:19.990: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7489  5eec0d20-2120-41c0-9a2f-f41557908f50 43349 0 2021-07-22 21:36:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-22 21:36:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:36:19.990: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7489  5eec0d20-2120-41c0-9a2f-f41557908f50 43349 0 2021-07-22 21:36:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-22 21:36:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 22 21:36:30.004: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7489  5eec0d20-2120-41c0-9a2f-f41557908f50 43392 0 2021-07-22 21:36:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-22 21:36:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:36:30.005: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7489  5eec0d20-2120-41c0-9a2f-f41557908f50 43392 0 2021-07-22 21:36:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-22 21:36:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:36:40.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7489" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":275,"skipped":4723,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:36:40.042: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:36:44.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6137" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4743,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:36:44.383: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:37:13.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1027" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":277,"skipped":4748,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:37:13.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9459
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:37:24.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9459" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":278,"skipped":4760,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:37:24.663: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:37:24.864: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 22 21:37:25.960: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:37:25.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1171" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":279,"skipped":4773,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:37:26.005: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4452
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul 22 21:37:26.187: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:37:29.849: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:37:44.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4452" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":280,"skipped":4773,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:37:44.745: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jul 22 21:37:49.622: INFO: Successfully updated pod "annotationupdate3b431179-341c-451e-a259-403595e54fa2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:37:51.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1570" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4777,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:37:51.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2984
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:37:51.941: INFO: Creating ReplicaSet my-hostname-basic-88f9f585-db5e-494b-ba6a-11c48c2dd5e1
Jul 22 21:37:51.964: INFO: Pod name my-hostname-basic-88f9f585-db5e-494b-ba6a-11c48c2dd5e1: Found 0 pods out of 1
Jul 22 21:37:57.012: INFO: Pod name my-hostname-basic-88f9f585-db5e-494b-ba6a-11c48c2dd5e1: Found 1 pods out of 1
Jul 22 21:37:57.012: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-88f9f585-db5e-494b-ba6a-11c48c2dd5e1" is running
Jul 22 21:37:57.023: INFO: Pod "my-hostname-basic-88f9f585-db5e-494b-ba6a-11c48c2dd5e1-tgtws" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 21:37:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 21:37:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 21:37:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 21:37:51 +0000 UTC Reason: Message:}])
Jul 22 21:37:57.023: INFO: Trying to dial the pod
Jul 22 21:38:02.162: INFO: Controller my-hostname-basic-88f9f585-db5e-494b-ba6a-11c48c2dd5e1: Got expected result from replica 1 [my-hostname-basic-88f9f585-db5e-494b-ba6a-11c48c2dd5e1-tgtws]: "my-hostname-basic-88f9f585-db5e-494b-ba6a-11c48c2dd5e1-tgtws", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:38:02.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2984" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":282,"skipped":4786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:38:02.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 22 21:38:10.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:38:10.524: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:38:12.525: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:38:12.537: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:38:14.525: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:38:14.537: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:38:16.525: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:38:16.537: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:38:18.525: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:38:18.537: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:38:20.525: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:38:20.539: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:38:22.525: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:38:22.538: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:38:24.525: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:38:24.537: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:38:24.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-304" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4817,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:38:24.622: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7209
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:38:24.817: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:38:27.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7209" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":284,"skipped":4827,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:38:27.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:38:32.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7944" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4828,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:38:32.418: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 22 21:38:32.630: INFO: Waiting up to 5m0s for pod "pod-337087f0-f676-4584-a7f2-912ce92928dc" in namespace "emptydir-4789" to be "Succeeded or Failed"
Jul 22 21:38:32.641: INFO: Pod "pod-337087f0-f676-4584-a7f2-912ce92928dc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.860905ms
Jul 22 21:38:34.653: INFO: Pod "pod-337087f0-f676-4584-a7f2-912ce92928dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023170262s
Jul 22 21:38:36.665: INFO: Pod "pod-337087f0-f676-4584-a7f2-912ce92928dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035119202s
STEP: Saw pod success
Jul 22 21:38:36.665: INFO: Pod "pod-337087f0-f676-4584-a7f2-912ce92928dc" satisfied condition "Succeeded or Failed"
Jul 22 21:38:36.681: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-337087f0-f676-4584-a7f2-912ce92928dc container test-container: <nil>
STEP: delete the pod
Jul 22 21:38:36.808: INFO: Waiting for pod pod-337087f0-f676-4584-a7f2-912ce92928dc to disappear
Jul 22 21:38:36.819: INFO: Pod pod-337087f0-f676-4584-a7f2-912ce92928dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:38:36.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4789" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":286,"skipped":4829,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:38:36.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5064
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-5064/secret-test-61985dd8-9b0d-4638-8fdd-86340a741748
STEP: Creating a pod to test consume secrets
Jul 22 21:38:37.070: INFO: Waiting up to 5m0s for pod "pod-configmaps-74326581-c7eb-4042-a07f-8e4e5cb885ea" in namespace "secrets-5064" to be "Succeeded or Failed"
Jul 22 21:38:37.084: INFO: Pod "pod-configmaps-74326581-c7eb-4042-a07f-8e4e5cb885ea": Phase="Pending", Reason="", readiness=false. Elapsed: 13.804418ms
Jul 22 21:38:39.096: INFO: Pod "pod-configmaps-74326581-c7eb-4042-a07f-8e4e5cb885ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025431261s
Jul 22 21:38:41.108: INFO: Pod "pod-configmaps-74326581-c7eb-4042-a07f-8e4e5cb885ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037954409s
STEP: Saw pod success
Jul 22 21:38:41.108: INFO: Pod "pod-configmaps-74326581-c7eb-4042-a07f-8e4e5cb885ea" satisfied condition "Succeeded or Failed"
Jul 22 21:38:41.119: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-configmaps-74326581-c7eb-4042-a07f-8e4e5cb885ea container env-test: <nil>
STEP: delete the pod
Jul 22 21:38:41.188: INFO: Waiting for pod pod-configmaps-74326581-c7eb-4042-a07f-8e4e5cb885ea to disappear
Jul 22 21:38:41.200: INFO: Pod pod-configmaps-74326581-c7eb-4042-a07f-8e4e5cb885ea no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:38:41.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5064" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":4887,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:38:41.238: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:38:41.444: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9062cff3-86b0-4931-a5f8-881ca0e60751" in namespace "projected-534" to be "Succeeded or Failed"
Jul 22 21:38:41.455: INFO: Pod "downwardapi-volume-9062cff3-86b0-4931-a5f8-881ca0e60751": Phase="Pending", Reason="", readiness=false. Elapsed: 10.809638ms
Jul 22 21:38:43.466: INFO: Pod "downwardapi-volume-9062cff3-86b0-4931-a5f8-881ca0e60751": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022245231s
Jul 22 21:38:45.478: INFO: Pod "downwardapi-volume-9062cff3-86b0-4931-a5f8-881ca0e60751": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034292781s
STEP: Saw pod success
Jul 22 21:38:45.478: INFO: Pod "downwardapi-volume-9062cff3-86b0-4931-a5f8-881ca0e60751" satisfied condition "Succeeded or Failed"
Jul 22 21:38:45.490: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-9062cff3-86b0-4931-a5f8-881ca0e60751 container client-container: <nil>
STEP: delete the pod
Jul 22 21:38:45.553: INFO: Waiting for pod downwardapi-volume-9062cff3-86b0-4931-a5f8-881ca0e60751 to disappear
Jul 22 21:38:45.564: INFO: Pod downwardapi-volume-9062cff3-86b0-4931-a5f8-881ca0e60751 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:38:45.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-534" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":288,"skipped":4888,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:38:45.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:38:46.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586726, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586726, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586726, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586726, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:38:48.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586726, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586726, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586726, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586726, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:38:51.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:38:51.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2368" for this suite.
STEP: Destroying namespace "webhook-2368-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":289,"skipped":4892,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:38:51.452: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:38:52.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:38:54.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:38:56.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586732, loc:(*time.Location)(0x7980f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:38:59.090: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul 22 21:38:59.269: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:38:59.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7444" for this suite.
STEP: Destroying namespace "webhook-7444-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":290,"skipped":4893,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:38:59.568: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jul 22 21:38:59.797: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 21:38:59.797: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 21:38:59.797: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 21:38:59.798: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 21:38:59.802: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 21:38:59.803: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 21:38:59.812: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 21:38:59.812: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 21:39:03.085: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 22 21:39:03.085: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 22 21:39:03.519: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jul 22 21:39:03.545: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jul 22 21:39:03.555: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0
Jul 22 21:39:03.555: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0
Jul 22 21:39:03.555: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0
Jul 22 21:39:03.555: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0
Jul 22 21:39:03.555: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0
Jul 22 21:39:03.555: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0
Jul 22 21:39:03.555: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0
Jul 22 21:39:03.555: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 0
Jul 22 21:39:03.555: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
Jul 22 21:39:03.555: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
Jul 22 21:39:03.556: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 2
Jul 22 21:39:03.556: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 2
Jul 22 21:39:03.562: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 2
Jul 22 21:39:03.562: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 2
Jul 22 21:39:03.562: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 2
Jul 22 21:39:03.562: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 2
Jul 22 21:39:03.562: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 2
Jul 22 21:39:03.562: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 2
Jul 22 21:39:03.562: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
STEP: listing Deployments
Jul 22 21:39:03.612: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jul 22 21:39:03.639: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jul 22 21:39:03.663: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1 and labels map[test-deployment:patched test-deployment-static:true]
Jul 22 21:39:03.663: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 21:39:03.663: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 21:39:03.663: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 21:39:03.710: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 21:39:03.719: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 21:39:03.726: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 21:39:03.797: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jul 22 21:39:05.968: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
Jul 22 21:39:05.969: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
Jul 22 21:39:05.969: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
Jul 22 21:39:05.969: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
Jul 22 21:39:05.969: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
Jul 22 21:39:05.969: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
Jul 22 21:39:05.969: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
Jul 22 21:39:05.969: INFO: observed Deployment test-deployment in namespace deployment-1595 with ReadyReplicas 1
STEP: deleting the Deployment
Jul 22 21:39:06.011: INFO: observed event type MODIFIED
Jul 22 21:39:06.011: INFO: observed event type MODIFIED
Jul 22 21:39:06.011: INFO: observed event type MODIFIED
Jul 22 21:39:06.012: INFO: observed event type MODIFIED
Jul 22 21:39:06.012: INFO: observed event type MODIFIED
Jul 22 21:39:06.012: INFO: observed event type MODIFIED
Jul 22 21:39:06.012: INFO: observed event type MODIFIED
Jul 22 21:39:06.012: INFO: observed event type MODIFIED
Jul 22 21:39:06.012: INFO: observed event type MODIFIED
Jul 22 21:39:06.012: INFO: observed event type MODIFIED
Jul 22 21:39:06.013: INFO: observed event type MODIFIED
Jul 22 21:39:06.013: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jul 22 21:39:06.052: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:06.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1595" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":291,"skipped":4900,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:06.112: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-5d825bc0-0162-4864-b7da-7ffbdfaabba5
STEP: Creating a pod to test consume configMaps
Jul 22 21:39:06.337: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09acc38c-6c95-42c9-b21e-1ccc99e04923" in namespace "projected-1345" to be "Succeeded or Failed"
Jul 22 21:39:06.348: INFO: Pod "pod-projected-configmaps-09acc38c-6c95-42c9-b21e-1ccc99e04923": Phase="Pending", Reason="", readiness=false. Elapsed: 10.873006ms
Jul 22 21:39:08.360: INFO: Pod "pod-projected-configmaps-09acc38c-6c95-42c9-b21e-1ccc99e04923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02286238s
Jul 22 21:39:10.371: INFO: Pod "pod-projected-configmaps-09acc38c-6c95-42c9-b21e-1ccc99e04923": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034719564s
Jul 22 21:39:12.385: INFO: Pod "pod-projected-configmaps-09acc38c-6c95-42c9-b21e-1ccc99e04923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048084119s
STEP: Saw pod success
Jul 22 21:39:12.385: INFO: Pod "pod-projected-configmaps-09acc38c-6c95-42c9-b21e-1ccc99e04923" satisfied condition "Succeeded or Failed"
Jul 22 21:39:12.396: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-configmaps-09acc38c-6c95-42c9-b21e-1ccc99e04923 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 21:39:12.462: INFO: Waiting for pod pod-projected-configmaps-09acc38c-6c95-42c9-b21e-1ccc99e04923 to disappear
Jul 22 21:39:12.475: INFO: Pod pod-projected-configmaps-09acc38c-6c95-42c9-b21e-1ccc99e04923 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:12.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1345" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":4912,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:12.508: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8379
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-bc93a5e3-2c82-4fb2-921f-4ce1a2cd47cc
STEP: Creating secret with name s-test-opt-upd-fb3091d9-80aa-45ed-8ced-d0f79592110d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-bc93a5e3-2c82-4fb2-921f-4ce1a2cd47cc
STEP: Updating secret s-test-opt-upd-fb3091d9-80aa-45ed-8ced-d0f79592110d
STEP: Creating secret with name s-test-opt-create-67c57a09-87b6-4007-9675-128a8adea797
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:21.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8379" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":4919,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:21.326: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jul 22 21:39:22.686: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0722 21:39:22.686813    5567 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 21:39:22.686849    5567 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 21:39:22.686857    5567 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:22.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2999" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":294,"skipped":4926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:22.713: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-97063334-163f-4107-9af1-562ecdaf3c45
STEP: Creating a pod to test consume secrets
Jul 22 21:39:22.925: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2075d760-8265-4ef2-b0e5-6717b5165f3b" in namespace "projected-3704" to be "Succeeded or Failed"
Jul 22 21:39:22.937: INFO: Pod "pod-projected-secrets-2075d760-8265-4ef2-b0e5-6717b5165f3b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.468529ms
Jul 22 21:39:24.949: INFO: Pod "pod-projected-secrets-2075d760-8265-4ef2-b0e5-6717b5165f3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024030401s
Jul 22 21:39:26.963: INFO: Pod "pod-projected-secrets-2075d760-8265-4ef2-b0e5-6717b5165f3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037483709s
STEP: Saw pod success
Jul 22 21:39:26.963: INFO: Pod "pod-projected-secrets-2075d760-8265-4ef2-b0e5-6717b5165f3b" satisfied condition "Succeeded or Failed"
Jul 22 21:39:26.974: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-projected-secrets-2075d760-8265-4ef2-b0e5-6717b5165f3b container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:39:27.253: INFO: Waiting for pod pod-projected-secrets-2075d760-8265-4ef2-b0e5-6717b5165f3b to disappear
Jul 22 21:39:27.264: INFO: Pod pod-projected-secrets-2075d760-8265-4ef2-b0e5-6717b5165f3b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:27.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3704" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":295,"skipped":4963,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:27.297: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 22 21:39:37.590: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1484 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:39:37.590: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:39:38.077: INFO: Exec stderr: ""
Jul 22 21:39:38.077: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1484 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:39:38.077: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:39:38.468: INFO: Exec stderr: ""
Jul 22 21:39:38.468: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1484 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:39:38.468: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:39:38.922: INFO: Exec stderr: ""
Jul 22 21:39:38.922: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1484 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:39:38.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:39:39.394: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 22 21:39:39.394: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1484 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:39:39.394: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:39:39.848: INFO: Exec stderr: ""
Jul 22 21:39:39.848: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1484 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:39:39.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:39:40.300: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 22 21:39:40.300: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1484 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:39:40.300: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:39:40.694: INFO: Exec stderr: ""
Jul 22 21:39:40.694: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1484 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:39:40.694: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:39:41.134: INFO: Exec stderr: ""
Jul 22 21:39:41.134: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1484 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:39:41.134: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:39:41.597: INFO: Exec stderr: ""
Jul 22 21:39:41.597: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1484 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 21:39:41.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:39:42.029: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:42.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1484" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":296,"skipped":4988,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:42.065: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2149
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:39:42.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d173db9-8f16-4538-a9bf-2ea972abf89b" in namespace "projected-2149" to be "Succeeded or Failed"
Jul 22 21:39:42.285: INFO: Pod "downwardapi-volume-9d173db9-8f16-4538-a9bf-2ea972abf89b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.750377ms
Jul 22 21:39:44.300: INFO: Pod "downwardapi-volume-9d173db9-8f16-4538-a9bf-2ea972abf89b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025676423s
Jul 22 21:39:46.313: INFO: Pod "downwardapi-volume-9d173db9-8f16-4538-a9bf-2ea972abf89b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038246502s
STEP: Saw pod success
Jul 22 21:39:46.313: INFO: Pod "downwardapi-volume-9d173db9-8f16-4538-a9bf-2ea972abf89b" satisfied condition "Succeeded or Failed"
Jul 22 21:39:46.324: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-9d173db9-8f16-4538-a9bf-2ea972abf89b container client-container: <nil>
STEP: delete the pod
Jul 22 21:39:46.396: INFO: Waiting for pod downwardapi-volume-9d173db9-8f16-4538-a9bf-2ea972abf89b to disappear
Jul 22 21:39:46.406: INFO: Pod downwardapi-volume-9d173db9-8f16-4538-a9bf-2ea972abf89b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:46.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2149" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":4993,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:46.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Jul 22 21:39:46.647: INFO: Waiting up to 5m0s for pod "var-expansion-c798b517-5d05-4d74-99e5-c1e82dd23bab" in namespace "var-expansion-3750" to be "Succeeded or Failed"
Jul 22 21:39:46.658: INFO: Pod "var-expansion-c798b517-5d05-4d74-99e5-c1e82dd23bab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.721088ms
Jul 22 21:39:48.670: INFO: Pod "var-expansion-c798b517-5d05-4d74-99e5-c1e82dd23bab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02303983s
Jul 22 21:39:50.681: INFO: Pod "var-expansion-c798b517-5d05-4d74-99e5-c1e82dd23bab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034615739s
STEP: Saw pod success
Jul 22 21:39:50.681: INFO: Pod "var-expansion-c798b517-5d05-4d74-99e5-c1e82dd23bab" satisfied condition "Succeeded or Failed"
Jul 22 21:39:50.692: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod var-expansion-c798b517-5d05-4d74-99e5-c1e82dd23bab container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:39:50.764: INFO: Waiting for pod var-expansion-c798b517-5d05-4d74-99e5-c1e82dd23bab to disappear
Jul 22 21:39:50.775: INFO: Pod var-expansion-c798b517-5d05-4d74-99e5-c1e82dd23bab no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:50.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3750" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":298,"skipped":4997,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:50.808: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Jul 22 21:39:50.999: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmi8i-6ya.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1189 api-versions'
Jul 22 21:39:51.116: INFO: stderr: ""
Jul 22 21:39:51.116: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:51.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1189" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":299,"skipped":5017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:51.144: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 22 21:39:51.352: INFO: Waiting up to 5m0s for pod "pod-dac1344d-0e28-41e2-a883-1b48bfb9833a" in namespace "emptydir-9985" to be "Succeeded or Failed"
Jul 22 21:39:51.363: INFO: Pod "pod-dac1344d-0e28-41e2-a883-1b48bfb9833a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.854897ms
Jul 22 21:39:53.375: INFO: Pod "pod-dac1344d-0e28-41e2-a883-1b48bfb9833a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02310517s
Jul 22 21:39:55.390: INFO: Pod "pod-dac1344d-0e28-41e2-a883-1b48bfb9833a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03781581s
STEP: Saw pod success
Jul 22 21:39:55.390: INFO: Pod "pod-dac1344d-0e28-41e2-a883-1b48bfb9833a" satisfied condition "Succeeded or Failed"
Jul 22 21:39:55.401: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-dac1344d-0e28-41e2-a883-1b48bfb9833a container test-container: <nil>
STEP: delete the pod
Jul 22 21:39:55.481: INFO: Waiting for pod pod-dac1344d-0e28-41e2-a883-1b48bfb9833a to disappear
Jul 22 21:39:55.495: INFO: Pod pod-dac1344d-0e28-41e2-a883-1b48bfb9833a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:55.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9985" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":300,"skipped":5050,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:55.529: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1776
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 22 21:39:59.804: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:39:59.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1776" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":301,"skipped":5055,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:39:59.878: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-855
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 22 21:40:00.084: INFO: Waiting up to 5m0s for pod "pod-4545a2cc-1200-4650-9f1c-da524c1772da" in namespace "emptydir-855" to be "Succeeded or Failed"
Jul 22 21:40:00.095: INFO: Pod "pod-4545a2cc-1200-4650-9f1c-da524c1772da": Phase="Pending", Reason="", readiness=false. Elapsed: 11.501966ms
Jul 22 21:40:02.107: INFO: Pod "pod-4545a2cc-1200-4650-9f1c-da524c1772da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023740353s
Jul 22 21:40:04.119: INFO: Pod "pod-4545a2cc-1200-4650-9f1c-da524c1772da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035655465s
STEP: Saw pod success
Jul 22 21:40:04.119: INFO: Pod "pod-4545a2cc-1200-4650-9f1c-da524c1772da" satisfied condition "Succeeded or Failed"
Jul 22 21:40:04.131: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-4545a2cc-1200-4650-9f1c-da524c1772da container test-container: <nil>
STEP: delete the pod
Jul 22 21:40:04.196: INFO: Waiting for pod pod-4545a2cc-1200-4650-9f1c-da524c1772da to disappear
Jul 22 21:40:04.208: INFO: Pod pod-4545a2cc-1200-4650-9f1c-da524c1772da no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:40:04.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-855" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":302,"skipped":5061,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:40:04.242: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Jul 22 21:40:08.492: INFO: Pod pod-hostip-14d9bead-2a36-43a9-80c3-cc7955f78eb8 has hostIP: 10.250.0.4
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:40:08.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1579" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:40:08.525: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-7984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jul 22 21:40:08.710: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 21:41:08.816: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jul 22 21:41:08.828: INFO: Starting informer...
STEP: Starting pod...
Jul 22 21:41:08.859: INFO: Pod is running on shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul 22 21:41:08.899: INFO: Pod wasn't evicted. Proceeding
Jul 22 21:41:08.899: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul 22 21:42:23.943: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:42:23.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7984" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":304,"skipped":5139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:42:23.976: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-c206d863-d846-42f2-b58d-47fbac363368
STEP: Creating a pod to test consume secrets
Jul 22 21:42:24.190: INFO: Waiting up to 5m0s for pod "pod-secrets-ff8cf486-4cb6-4267-8eb1-8765ca5d877a" in namespace "secrets-5991" to be "Succeeded or Failed"
Jul 22 21:42:24.200: INFO: Pod "pod-secrets-ff8cf486-4cb6-4267-8eb1-8765ca5d877a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.372208ms
Jul 22 21:42:26.212: INFO: Pod "pod-secrets-ff8cf486-4cb6-4267-8eb1-8765ca5d877a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021745657s
Jul 22 21:42:28.223: INFO: Pod "pod-secrets-ff8cf486-4cb6-4267-8eb1-8765ca5d877a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033406262s
STEP: Saw pod success
Jul 22 21:42:28.223: INFO: Pod "pod-secrets-ff8cf486-4cb6-4267-8eb1-8765ca5d877a" satisfied condition "Succeeded or Failed"
Jul 22 21:42:28.234: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-secrets-ff8cf486-4cb6-4267-8eb1-8765ca5d877a container secret-env-test: <nil>
STEP: delete the pod
Jul 22 21:42:28.298: INFO: Waiting for pod pod-secrets-ff8cf486-4cb6-4267-8eb1-8765ca5d877a to disappear
Jul 22 21:42:28.309: INFO: Pod pod-secrets-ff8cf486-4cb6-4267-8eb1-8765ca5d877a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:42:28.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5991" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5172,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:42:28.341: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:42:28.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6693" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":306,"skipped":5183,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:42:28.561: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:42:32.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4849" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":307,"skipped":5187,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:42:32.876: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-319
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 22 21:42:33.081: INFO: Waiting up to 5m0s for pod "pod-94d2c14f-b89e-4512-9917-c6bf0cce1228" in namespace "emptydir-319" to be "Succeeded or Failed"
Jul 22 21:42:33.091: INFO: Pod "pod-94d2c14f-b89e-4512-9917-c6bf0cce1228": Phase="Pending", Reason="", readiness=false. Elapsed: 10.305526ms
Jul 22 21:42:35.115: INFO: Pod "pod-94d2c14f-b89e-4512-9917-c6bf0cce1228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034132772s
Jul 22 21:42:37.128: INFO: Pod "pod-94d2c14f-b89e-4512-9917-c6bf0cce1228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046729274s
STEP: Saw pod success
Jul 22 21:42:37.128: INFO: Pod "pod-94d2c14f-b89e-4512-9917-c6bf0cce1228" satisfied condition "Succeeded or Failed"
Jul 22 21:42:37.138: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-94d2c14f-b89e-4512-9917-c6bf0cce1228 container test-container: <nil>
STEP: delete the pod
Jul 22 21:42:37.274: INFO: Waiting for pod pod-94d2c14f-b89e-4512-9917-c6bf0cce1228 to disappear
Jul 22 21:42:37.284: INFO: Pod pod-94d2c14f-b89e-4512-9917-c6bf0cce1228 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:42:37.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-319" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:42:37.316: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:42:37.520: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2a1ff76-7496-4bc8-b600-406863b7c0fd" in namespace "projected-1888" to be "Succeeded or Failed"
Jul 22 21:42:37.532: INFO: Pod "downwardapi-volume-a2a1ff76-7496-4bc8-b600-406863b7c0fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034307ms
Jul 22 21:42:39.545: INFO: Pod "downwardapi-volume-a2a1ff76-7496-4bc8-b600-406863b7c0fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024986505s
Jul 22 21:42:41.557: INFO: Pod "downwardapi-volume-a2a1ff76-7496-4bc8-b600-406863b7c0fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03642341s
STEP: Saw pod success
Jul 22 21:42:41.557: INFO: Pod "downwardapi-volume-a2a1ff76-7496-4bc8-b600-406863b7c0fd" satisfied condition "Succeeded or Failed"
Jul 22 21:42:41.567: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downwardapi-volume-a2a1ff76-7496-4bc8-b600-406863b7c0fd container client-container: <nil>
STEP: delete the pod
Jul 22 21:42:41.771: INFO: Waiting for pod downwardapi-volume-a2a1ff76-7496-4bc8-b600-406863b7c0fd to disappear
Jul 22 21:42:41.782: INFO: Pod downwardapi-volume-a2a1ff76-7496-4bc8-b600-406863b7c0fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:42:41.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1888" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:42:41.816: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jul 22 21:42:42.021: INFO: Waiting up to 5m0s for pod "downward-api-04f348fc-ea79-448e-8edd-e0bc3d1b4593" in namespace "downward-api-4771" to be "Succeeded or Failed"
Jul 22 21:42:42.033: INFO: Pod "downward-api-04f348fc-ea79-448e-8edd-e0bc3d1b4593": Phase="Pending", Reason="", readiness=false. Elapsed: 11.386683ms
Jul 22 21:42:44.044: INFO: Pod "downward-api-04f348fc-ea79-448e-8edd-e0bc3d1b4593": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02318016s
Jul 22 21:42:46.056: INFO: Pod "downward-api-04f348fc-ea79-448e-8edd-e0bc3d1b4593": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034819029s
STEP: Saw pod success
Jul 22 21:42:46.056: INFO: Pod "downward-api-04f348fc-ea79-448e-8edd-e0bc3d1b4593" satisfied condition "Succeeded or Failed"
Jul 22 21:42:46.067: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod downward-api-04f348fc-ea79-448e-8edd-e0bc3d1b4593 container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:42:46.128: INFO: Waiting for pod downward-api-04f348fc-ea79-448e-8edd-e0bc3d1b4593 to disappear
Jul 22 21:42:46.139: INFO: Pod downward-api-04f348fc-ea79-448e-8edd-e0bc3d1b4593 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:42:46.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4771" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":310,"skipped":5318,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 22 21:42:46.171: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-430cd8e3-14ce-4aba-8f43-eb59d00a411e
STEP: Creating a pod to test consume secrets
Jul 22 21:42:46.582: INFO: Waiting up to 5m0s for pod "pod-secrets-a66f8f20-a157-425f-a037-d8add9905744" in namespace "secrets-8340" to be "Succeeded or Failed"
Jul 22 21:42:46.592: INFO: Pod "pod-secrets-a66f8f20-a157-425f-a037-d8add9905744": Phase="Pending", Reason="", readiness=false. Elapsed: 10.378535ms
Jul 22 21:42:48.603: INFO: Pod "pod-secrets-a66f8f20-a157-425f-a037-d8add9905744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021511619s
Jul 22 21:42:50.615: INFO: Pod "pod-secrets-a66f8f20-a157-425f-a037-d8add9905744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03286456s
STEP: Saw pod success
Jul 22 21:42:50.615: INFO: Pod "pod-secrets-a66f8f20-a157-425f-a037-d8add9905744" satisfied condition "Succeeded or Failed"
Jul 22 21:42:50.626: INFO: Trying to get logs from node shoot--it--tmi8i-6ya-worker-1-6fc4f-6f4m9 pod pod-secrets-a66f8f20-a157-425f-a037-d8add9905744 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:42:50.737: INFO: Waiting for pod pod-secrets-a66f8f20-a157-425f-a037-d8add9905744 to disappear
Jul 22 21:42:50.747: INFO: Pod pod-secrets-a66f8f20-a157-425f-a037-d8add9905744 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 22 21:42:50.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8340" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":311,"skipped":5329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSJul 22 21:42:50.779: INFO: Running AfterSuite actions on all nodes
Jul 22 21:42:50.779: INFO: Running AfterSuite actions on node 1
Jul 22 21:42:50.779: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/e2e/artifacts/1626983694/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5357,"failed":0}

Ran 311 of 5668 Specs in 6473.743 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Flaked | 0 Pending | 5357 Skipped
PASS

Ginkgo ran 1 suite in 1h47m55.658016618s
Test Suite Passed
