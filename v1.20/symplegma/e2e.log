I1216 22:33:38.145243      24 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-864705134
I1216 22:33:38.145279      24 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1216 22:33:38.145396      24 e2e.go:129] Starting e2e run "da087946-2ccf-414e-aecc-270729779099" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1608158016 - Will randomize all specs
Will run 311 of 5667 specs

Dec 16 22:33:38.174: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 22:33:38.176: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 16 22:33:38.207: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 16 22:33:38.248: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 16 22:33:38.248: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Dec 16 22:33:38.248: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 16 22:33:38.256: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 16 22:33:38.256: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 16 22:33:38.256: INFO: e2e test version: v1.20.0
Dec 16 22:33:38.257: INFO: kube-apiserver version: v1.20.0
Dec 16 22:33:38.257: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 22:33:38.263: INFO: Cluster IP family: ipv4
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:33:38.263: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
Dec 16 22:33:38.310: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-cd33c69d-4f28-44a8-bb80-c04f1f9b8a91
STEP: Creating a pod to test consume secrets
Dec 16 22:33:38.326: INFO: Waiting up to 5m0s for pod "pod-secrets-09ac806d-3c1c-4dfe-9094-21db217850de" in namespace "secrets-3687" to be "Succeeded or Failed"
Dec 16 22:33:38.329: INFO: Pod "pod-secrets-09ac806d-3c1c-4dfe-9094-21db217850de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.909905ms
Dec 16 22:33:40.337: INFO: Pod "pod-secrets-09ac806d-3c1c-4dfe-9094-21db217850de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011340394s
STEP: Saw pod success
Dec 16 22:33:40.337: INFO: Pod "pod-secrets-09ac806d-3c1c-4dfe-9094-21db217850de" satisfied condition "Succeeded or Failed"
Dec 16 22:33:40.340: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-secrets-09ac806d-3c1c-4dfe-9094-21db217850de container secret-env-test: <nil>
STEP: delete the pod
Dec 16 22:33:40.386: INFO: Waiting for pod pod-secrets-09ac806d-3c1c-4dfe-9094-21db217850de to disappear
Dec 16 22:33:40.392: INFO: Pod pod-secrets-09ac806d-3c1c-4dfe-9094-21db217850de no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:33:40.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3687" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":1,"skipped":12,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:33:40.403: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 22:33:40.461: INFO: Waiting up to 5m0s for pod "downwardapi-volume-798166e8-2989-41e8-a155-c391d90eceb3" in namespace "downward-api-8265" to be "Succeeded or Failed"
Dec 16 22:33:40.473: INFO: Pod "downwardapi-volume-798166e8-2989-41e8-a155-c391d90eceb3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.047305ms
Dec 16 22:33:42.484: INFO: Pod "downwardapi-volume-798166e8-2989-41e8-a155-c391d90eceb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022456696s
STEP: Saw pod success
Dec 16 22:33:42.484: INFO: Pod "downwardapi-volume-798166e8-2989-41e8-a155-c391d90eceb3" satisfied condition "Succeeded or Failed"
Dec 16 22:33:42.487: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-798166e8-2989-41e8-a155-c391d90eceb3 container client-container: <nil>
STEP: delete the pod
Dec 16 22:33:42.512: INFO: Waiting for pod downwardapi-volume-798166e8-2989-41e8-a155-c391d90eceb3 to disappear
Dec 16 22:33:42.516: INFO: Pod downwardapi-volume-798166e8-2989-41e8-a155-c391d90eceb3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:33:42.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8265" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":2,"skipped":18,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:33:42.528: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 16 22:33:42.571: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Dec 16 22:33:43.058: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 16 22:33:45.174: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743754823, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743754823, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743754823, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743754823, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 22:33:48.419: INFO: Waited 1.231085608s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:33:49.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6270" for this suite.

• [SLOW TEST:6.701 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":3,"skipped":41,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:33:49.231: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Dec 16 22:33:49.318: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:33:53.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3933" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":4,"skipped":71,"failed":0}
SSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:33:53.100: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 16 22:33:53.171: INFO: starting watch
STEP: patching
STEP: updating
Dec 16 22:33:53.181: INFO: waiting for watch events with expected annotations
Dec 16 22:33:53.181: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:33:53.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1506" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":5,"skipped":77,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:33:53.258: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 16 22:33:57.408: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 16 22:33:57.411: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 16 22:33:59.412: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 16 22:33:59.426: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 16 22:34:01.412: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 16 22:34:01.420: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 16 22:34:03.412: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 16 22:34:03.419: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 16 22:34:05.412: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 16 22:34:05.419: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 16 22:34:07.412: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 16 22:34:07.420: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 16 22:34:09.412: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 16 22:34:09.416: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:34:09.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3116" for this suite.

• [SLOW TEST:16.178 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":90,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:34:09.437: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3044.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3044.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3044.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3044.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 16 22:34:11.517: INFO: DNS probes using dns-test-991f73a2-a2f1-4a2f-9386-9cffb83d4065 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3044.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3044.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3044.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3044.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 16 22:34:13.611: INFO: File jessie_udp@dns-test-service-3.dns-3044.svc.cluster.local from pod  dns-3044/dns-test-6b8f9f2e-f2fd-430f-ac9e-548d0d1ae4aa contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 16 22:34:13.611: INFO: Lookups using dns-3044/dns-test-6b8f9f2e-f2fd-430f-ac9e-548d0d1ae4aa failed for: [jessie_udp@dns-test-service-3.dns-3044.svc.cluster.local]

Dec 16 22:34:18.617: INFO: File wheezy_udp@dns-test-service-3.dns-3044.svc.cluster.local from pod  dns-3044/dns-test-6b8f9f2e-f2fd-430f-ac9e-548d0d1ae4aa contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 16 22:34:18.621: INFO: Lookups using dns-3044/dns-test-6b8f9f2e-f2fd-430f-ac9e-548d0d1ae4aa failed for: [wheezy_udp@dns-test-service-3.dns-3044.svc.cluster.local]

Dec 16 22:34:23.619: INFO: DNS probes using dns-test-6b8f9f2e-f2fd-430f-ac9e-548d0d1ae4aa succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3044.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3044.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3044.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3044.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 16 22:34:25.781: INFO: DNS probes using dns-test-e066b74b-30a5-47c5-8106-a99470a37cb3 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:34:25.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3044" for this suite.

• [SLOW TEST:16.448 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":7,"skipped":174,"failed":0}
SSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:34:25.887: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Dec 16 22:34:25.949: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:34:25.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5666" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":8,"skipped":182,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:34:25.980: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-8095
Dec 16 22:34:28.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-8095 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 16 22:34:28.381: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 16 22:34:28.381: INFO: stdout: "iptables"
Dec 16 22:34:28.381: INFO: proxyMode: iptables
Dec 16 22:34:28.396: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 16 22:34:28.403: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-8095
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8095
I1216 22:34:28.433622      24 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8095, replica count: 3
I1216 22:34:31.483940      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 22:34:31.497: INFO: Creating new exec pod
Dec 16 22:34:34.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-8095 exec execpod-affinityv84nl -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Dec 16 22:34:34.690: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Dec 16 22:34:34.690: INFO: stdout: ""
Dec 16 22:34:34.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-8095 exec execpod-affinityv84nl -- /bin/sh -x -c nc -zv -t -w 2 10.100.73.73 80'
Dec 16 22:34:34.859: INFO: stderr: "+ nc -zv -t -w 2 10.100.73.73 80\nConnection to 10.100.73.73 80 port [tcp/http] succeeded!\n"
Dec 16 22:34:34.859: INFO: stdout: ""
Dec 16 22:34:34.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-8095 exec execpod-affinityv84nl -- /bin/sh -x -c nc -zv -t -w 2 172.31.1.217 32754'
Dec 16 22:34:35.019: INFO: stderr: "+ nc -zv -t -w 2 172.31.1.217 32754\nConnection to 172.31.1.217 32754 port [tcp/32754] succeeded!\n"
Dec 16 22:34:35.019: INFO: stdout: ""
Dec 16 22:34:35.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-8095 exec execpod-affinityv84nl -- /bin/sh -x -c nc -zv -t -w 2 172.31.6.174 32754'
Dec 16 22:34:35.168: INFO: stderr: "+ nc -zv -t -w 2 172.31.6.174 32754\nConnection to 172.31.6.174 32754 port [tcp/32754] succeeded!\n"
Dec 16 22:34:35.168: INFO: stdout: ""
Dec 16 22:34:35.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-8095 exec execpod-affinityv84nl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.1.217:32754/ ; done'
Dec 16 22:34:35.377: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n"
Dec 16 22:34:35.377: INFO: stdout: "\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz\naffinity-nodeport-timeout-2dghz"
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Received response from host: affinity-nodeport-timeout-2dghz
Dec 16 22:34:35.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-8095 exec execpod-affinityv84nl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.1.217:32754/'
Dec 16 22:34:35.521: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n"
Dec 16 22:34:35.521: INFO: stdout: "affinity-nodeport-timeout-2dghz"
Dec 16 22:34:55.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-8095 exec execpod-affinityv84nl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.1.217:32754/'
Dec 16 22:34:55.711: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n"
Dec 16 22:34:55.712: INFO: stdout: "affinity-nodeport-timeout-2dghz"
Dec 16 22:35:15.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-8095 exec execpod-affinityv84nl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.1.217:32754/'
Dec 16 22:35:15.908: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.1.217:32754/\n"
Dec 16 22:35:15.908: INFO: stdout: "affinity-nodeport-timeout-gljc6"
Dec 16 22:35:15.908: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8095, will wait for the garbage collector to delete the pods
Dec 16 22:35:15.997: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 7.938034ms
Dec 16 22:35:16.597: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.186177ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:35:57.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8095" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:92.010 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":9,"skipped":185,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:35:57.993: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 22:35:58.068: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7544f282-3767-43ce-8cb3-0ffd9093efcd" in namespace "projected-2713" to be "Succeeded or Failed"
Dec 16 22:35:58.073: INFO: Pod "downwardapi-volume-7544f282-3767-43ce-8cb3-0ffd9093efcd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063762ms
Dec 16 22:36:00.083: INFO: Pod "downwardapi-volume-7544f282-3767-43ce-8cb3-0ffd9093efcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0140579s
STEP: Saw pod success
Dec 16 22:36:00.083: INFO: Pod "downwardapi-volume-7544f282-3767-43ce-8cb3-0ffd9093efcd" satisfied condition "Succeeded or Failed"
Dec 16 22:36:00.086: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-7544f282-3767-43ce-8cb3-0ffd9093efcd container client-container: <nil>
STEP: delete the pod
Dec 16 22:36:00.130: INFO: Waiting for pod downwardapi-volume-7544f282-3767-43ce-8cb3-0ffd9093efcd to disappear
Dec 16 22:36:00.134: INFO: Pod downwardapi-volume-7544f282-3767-43ce-8cb3-0ffd9093efcd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:36:00.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2713" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":10,"skipped":199,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:36:00.154: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:36:02.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-956" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":11,"skipped":205,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:36:02.293: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Dec 16 22:36:02.400: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 22:37:02.431: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:37:02.434: INFO: Starting informer...
STEP: Starting pods...
Dec 16 22:37:02.655: INFO: Pod1 is running on ip-172-31-1-217. Tainting Node
Dec 16 22:37:04.883: INFO: Pod2 is running on ip-172-31-1-217. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 16 22:37:17.819: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 16 22:37:37.823: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:37:37.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9373" for this suite.

• [SLOW TEST:95.582 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":12,"skipped":212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:37:37.880: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 16 22:37:37.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4959 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Dec 16 22:37:38.075: INFO: stderr: ""
Dec 16 22:37:38.075: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Dec 16 22:37:38.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4959 delete pods e2e-test-httpd-pod'
Dec 16 22:37:47.853: INFO: stderr: ""
Dec 16 22:37:47.853: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:37:47.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4959" for this suite.

• [SLOW TEST:10.014 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":13,"skipped":248,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:37:47.896: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Dec 16 22:37:48.011: INFO: Waiting up to 5m0s for pod "var-expansion-bc1552ee-c820-48fd-85ff-4e4bc6dceee1" in namespace "var-expansion-5508" to be "Succeeded or Failed"
Dec 16 22:37:48.030: INFO: Pod "var-expansion-bc1552ee-c820-48fd-85ff-4e4bc6dceee1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.738003ms
Dec 16 22:37:50.053: INFO: Pod "var-expansion-bc1552ee-c820-48fd-85ff-4e4bc6dceee1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04225513s
STEP: Saw pod success
Dec 16 22:37:50.053: INFO: Pod "var-expansion-bc1552ee-c820-48fd-85ff-4e4bc6dceee1" satisfied condition "Succeeded or Failed"
Dec 16 22:37:50.062: INFO: Trying to get logs from node ip-172-31-1-217 pod var-expansion-bc1552ee-c820-48fd-85ff-4e4bc6dceee1 container dapi-container: <nil>
STEP: delete the pod
Dec 16 22:37:50.147: INFO: Waiting for pod var-expansion-bc1552ee-c820-48fd-85ff-4e4bc6dceee1 to disappear
Dec 16 22:37:50.154: INFO: Pod var-expansion-bc1552ee-c820-48fd-85ff-4e4bc6dceee1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:37:50.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5508" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":259,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:37:50.179: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1216 22:37:56.316203      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 16 22:38:58.336: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:38:58.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6145" for this suite.

• [SLOW TEST:68.184 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":15,"skipped":298,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:38:58.363: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-bf4254e4-4ffb-4919-92d2-35e081649f79
STEP: Creating a pod to test consume secrets
Dec 16 22:38:58.421: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ac72edb8-1f15-4b25-bd76-24a6fbec1acc" in namespace "projected-2263" to be "Succeeded or Failed"
Dec 16 22:38:58.425: INFO: Pod "pod-projected-secrets-ac72edb8-1f15-4b25-bd76-24a6fbec1acc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.955592ms
Dec 16 22:39:00.430: INFO: Pod "pod-projected-secrets-ac72edb8-1f15-4b25-bd76-24a6fbec1acc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008844913s
STEP: Saw pod success
Dec 16 22:39:00.430: INFO: Pod "pod-projected-secrets-ac72edb8-1f15-4b25-bd76-24a6fbec1acc" satisfied condition "Succeeded or Failed"
Dec 16 22:39:00.433: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-secrets-ac72edb8-1f15-4b25-bd76-24a6fbec1acc container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 16 22:39:00.459: INFO: Waiting for pod pod-projected-secrets-ac72edb8-1f15-4b25-bd76-24a6fbec1acc to disappear
Dec 16 22:39:00.461: INFO: Pod pod-projected-secrets-ac72edb8-1f15-4b25-bd76-24a6fbec1acc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:39:00.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2263" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":308,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:39:00.478: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 22:39:00.923: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 16 22:39:02.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755140, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755140, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755140, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755140, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 22:39:05.970: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:39:06.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3802" for this suite.
STEP: Destroying namespace "webhook-3802-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.760 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":17,"skipped":316,"failed":0}
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:39:06.239: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:39:06.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4356" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":18,"skipped":316,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:39:06.384: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 16 22:39:06.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9431 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Dec 16 22:39:06.557: INFO: stderr: ""
Dec 16 22:39:06.558: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Dec 16 22:39:06.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9431 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Dec 16 22:39:06.882: INFO: stderr: ""
Dec 16 22:39:06.882: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Dec 16 22:39:06.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9431 delete pods e2e-test-httpd-pod'
Dec 16 22:40:07.825: INFO: stderr: ""
Dec 16 22:40:07.825: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:40:07.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9431" for this suite.

• [SLOW TEST:61.478 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":19,"skipped":326,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:40:07.862: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-2308cb26-e386-4231-9e9e-c716ddf20a33
STEP: Creating a pod to test consume secrets
Dec 16 22:40:07.986: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2ffc3178-8791-4c3a-91c6-1677c19bbc0b" in namespace "projected-6783" to be "Succeeded or Failed"
Dec 16 22:40:07.990: INFO: Pod "pod-projected-secrets-2ffc3178-8791-4c3a-91c6-1677c19bbc0b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.747005ms
Dec 16 22:40:09.997: INFO: Pod "pod-projected-secrets-2ffc3178-8791-4c3a-91c6-1677c19bbc0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010782261s
STEP: Saw pod success
Dec 16 22:40:09.997: INFO: Pod "pod-projected-secrets-2ffc3178-8791-4c3a-91c6-1677c19bbc0b" satisfied condition "Succeeded or Failed"
Dec 16 22:40:10.000: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-secrets-2ffc3178-8791-4c3a-91c6-1677c19bbc0b container secret-volume-test: <nil>
STEP: delete the pod
Dec 16 22:40:10.026: INFO: Waiting for pod pod-projected-secrets-2ffc3178-8791-4c3a-91c6-1677c19bbc0b to disappear
Dec 16 22:40:10.031: INFO: Pod pod-projected-secrets-2ffc3178-8791-4c3a-91c6-1677c19bbc0b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:40:10.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6783" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":20,"skipped":341,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:40:10.040: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-ed312f91-548a-47d6-8942-080386f547f3
STEP: Creating a pod to test consume secrets
Dec 16 22:40:10.098: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-074b4bec-ec8f-4a0d-b90e-bbb3f4e20dd0" in namespace "projected-6110" to be "Succeeded or Failed"
Dec 16 22:40:10.101: INFO: Pod "pod-projected-secrets-074b4bec-ec8f-4a0d-b90e-bbb3f4e20dd0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.374266ms
Dec 16 22:40:12.109: INFO: Pod "pod-projected-secrets-074b4bec-ec8f-4a0d-b90e-bbb3f4e20dd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011111245s
STEP: Saw pod success
Dec 16 22:40:12.109: INFO: Pod "pod-projected-secrets-074b4bec-ec8f-4a0d-b90e-bbb3f4e20dd0" satisfied condition "Succeeded or Failed"
Dec 16 22:40:12.112: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-secrets-074b4bec-ec8f-4a0d-b90e-bbb3f4e20dd0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 16 22:40:12.132: INFO: Waiting for pod pod-projected-secrets-074b4bec-ec8f-4a0d-b90e-bbb3f4e20dd0 to disappear
Dec 16 22:40:12.136: INFO: Pod pod-projected-secrets-074b4bec-ec8f-4a0d-b90e-bbb3f4e20dd0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:40:12.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6110" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":21,"skipped":350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:40:12.151: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 16 22:40:12.214: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec 16 22:40:12.218: INFO: starting watch
STEP: patching
STEP: updating
Dec 16 22:40:12.231: INFO: waiting for watch events with expected annotations
Dec 16 22:40:12.231: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:40:12.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8371" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":22,"skipped":386,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:40:12.313: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 22:40:12.374: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3889862f-b701-4456-8776-0baa3e6a57f8" in namespace "projected-5205" to be "Succeeded or Failed"
Dec 16 22:40:12.378: INFO: Pod "downwardapi-volume-3889862f-b701-4456-8776-0baa3e6a57f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.521095ms
Dec 16 22:40:14.385: INFO: Pod "downwardapi-volume-3889862f-b701-4456-8776-0baa3e6a57f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010369068s
STEP: Saw pod success
Dec 16 22:40:14.385: INFO: Pod "downwardapi-volume-3889862f-b701-4456-8776-0baa3e6a57f8" satisfied condition "Succeeded or Failed"
Dec 16 22:40:14.398: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-3889862f-b701-4456-8776-0baa3e6a57f8 container client-container: <nil>
STEP: delete the pod
Dec 16 22:40:14.421: INFO: Waiting for pod downwardapi-volume-3889862f-b701-4456-8776-0baa3e6a57f8 to disappear
Dec 16 22:40:14.424: INFO: Pod downwardapi-volume-3889862f-b701-4456-8776-0baa3e6a57f8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:40:14.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5205" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":394,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:40:14.433: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Dec 16 22:40:14.475: INFO: Waiting up to 5m0s for pod "client-containers-e9a2ce05-b785-4572-ad89-5f0e1fe1ad27" in namespace "containers-3780" to be "Succeeded or Failed"
Dec 16 22:40:14.478: INFO: Pod "client-containers-e9a2ce05-b785-4572-ad89-5f0e1fe1ad27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.106769ms
Dec 16 22:40:16.515: INFO: Pod "client-containers-e9a2ce05-b785-4572-ad89-5f0e1fe1ad27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039357658s
STEP: Saw pod success
Dec 16 22:40:16.515: INFO: Pod "client-containers-e9a2ce05-b785-4572-ad89-5f0e1fe1ad27" satisfied condition "Succeeded or Failed"
Dec 16 22:40:16.518: INFO: Trying to get logs from node ip-172-31-1-217 pod client-containers-e9a2ce05-b785-4572-ad89-5f0e1fe1ad27 container agnhost-container: <nil>
STEP: delete the pod
Dec 16 22:40:16.548: INFO: Waiting for pod client-containers-e9a2ce05-b785-4572-ad89-5f0e1fe1ad27 to disappear
Dec 16 22:40:16.551: INFO: Pod client-containers-e9a2ce05-b785-4572-ad89-5f0e1fe1ad27 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:40:16.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3780" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":24,"skipped":408,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:40:16.563: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 16 22:40:19.149: INFO: Successfully updated pod "pod-update-4d930bd8-8d88-4f3c-a47c-bc8df531063a"
STEP: verifying the updated pod is in kubernetes
Dec 16 22:40:19.158: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:40:19.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6708" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:40:19.176: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-8650
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8650 to expose endpoints map[]
Dec 16 22:40:19.263: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Dec 16 22:40:20.294: INFO: successfully validated that service multi-endpoint-test in namespace services-8650 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8650
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8650 to expose endpoints map[pod1:[100]]
Dec 16 22:40:22.316: INFO: successfully validated that service multi-endpoint-test in namespace services-8650 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8650
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8650 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 16 22:40:24.340: INFO: successfully validated that service multi-endpoint-test in namespace services-8650 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-8650
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8650 to expose endpoints map[pod2:[101]]
Dec 16 22:40:24.380: INFO: successfully validated that service multi-endpoint-test in namespace services-8650 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8650
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8650 to expose endpoints map[]
Dec 16 22:40:24.441: INFO: successfully validated that service multi-endpoint-test in namespace services-8650 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:40:24.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8650" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.312 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":26,"skipped":449,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:40:24.491: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec 16 22:40:24.535: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1957  7940d0f7-f96a-4ce5-9e12-1566035f302c 20988 0 2020-12-16 22:40:24 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-12-16 22:40:24 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2nlmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2nlmb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2nlmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:40:24.537: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec 16 22:40:26.559: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec 16 22:40:28.544: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Dec 16 22:40:28.544: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1957 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 22:40:28.544: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Verifying customized DNS server is configured on pod...
Dec 16 22:40:28.707: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1957 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 22:40:28.707: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 22:40:28.941: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:40:28.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1957" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":27,"skipped":463,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:40:28.971: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 22:40:29.535: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 16 22:40:31.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755229, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755229, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755229, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755229, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 22:40:34.576: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:40:34.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7369" for this suite.
STEP: Destroying namespace "webhook-7369-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.771 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":28,"skipped":477,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:40:34.743: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7930
Dec 16 22:40:36.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-7930 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 16 22:40:36.999: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 16 22:40:36.999: INFO: stdout: "iptables"
Dec 16 22:40:36.999: INFO: proxyMode: iptables
Dec 16 22:40:37.012: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 16 22:40:37.015: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7930
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7930
I1216 22:40:37.042168      24 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7930, replica count: 3
I1216 22:40:40.093667      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 22:40:40.105: INFO: Creating new exec pod
Dec 16 22:40:43.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-7930 exec execpod-affinityp4x7s -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Dec 16 22:40:43.311: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Dec 16 22:40:43.311: INFO: stdout: ""
Dec 16 22:40:43.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-7930 exec execpod-affinityp4x7s -- /bin/sh -x -c nc -zv -t -w 2 10.106.71.140 80'
Dec 16 22:40:43.495: INFO: stderr: "+ nc -zv -t -w 2 10.106.71.140 80\nConnection to 10.106.71.140 80 port [tcp/http] succeeded!\n"
Dec 16 22:40:43.495: INFO: stdout: ""
Dec 16 22:40:43.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-7930 exec execpod-affinityp4x7s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.71.140:80/ ; done'
Dec 16 22:40:43.715: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n"
Dec 16 22:40:43.715: INFO: stdout: "\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr\naffinity-clusterip-timeout-zjjnr"
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Received response from host: affinity-clusterip-timeout-zjjnr
Dec 16 22:40:43.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-7930 exec execpod-affinityp4x7s -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.71.140:80/'
Dec 16 22:40:43.884: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n"
Dec 16 22:40:43.884: INFO: stdout: "affinity-clusterip-timeout-zjjnr"
Dec 16 22:41:03.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-7930 exec execpod-affinityp4x7s -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.71.140:80/'
Dec 16 22:41:04.176: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n"
Dec 16 22:41:04.176: INFO: stdout: "affinity-clusterip-timeout-zjjnr"
Dec 16 22:41:24.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-7930 exec execpod-affinityp4x7s -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.71.140:80/'
Dec 16 22:41:24.380: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.71.140:80/\n"
Dec 16 22:41:24.380: INFO: stdout: "affinity-clusterip-timeout-qkrd5"
Dec 16 22:41:24.380: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7930, will wait for the garbage collector to delete the pods
Dec 16 22:41:24.479: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.811018ms
Dec 16 22:41:25.080: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.618941ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:42:07.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7930" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:93.234 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":29,"skipped":486,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:42:07.981: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 16 22:42:08.066: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4190  0da3528a-6b2d-422a-8cea-1398d670b2ab 21468 0 2020-12-16 22:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-16 22:42:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 22:42:08.066: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4190  0da3528a-6b2d-422a-8cea-1398d670b2ab 21469 0 2020-12-16 22:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-16 22:42:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 22:42:08.067: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4190  0da3528a-6b2d-422a-8cea-1398d670b2ab 21470 0 2020-12-16 22:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-16 22:42:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 16 22:42:18.123: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4190  0da3528a-6b2d-422a-8cea-1398d670b2ab 21527 0 2020-12-16 22:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-16 22:42:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 22:42:18.123: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4190  0da3528a-6b2d-422a-8cea-1398d670b2ab 21528 0 2020-12-16 22:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-16 22:42:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 22:42:18.123: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4190  0da3528a-6b2d-422a-8cea-1398d670b2ab 21529 0 2020-12-16 22:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-16 22:42:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:42:18.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4190" for this suite.

• [SLOW TEST:10.152 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":30,"skipped":506,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:42:18.135: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:42:18.179: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 16 22:42:20.217: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:42:21.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-176" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":31,"skipped":518,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:42:21.237: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:42:21.279: INFO: Creating deployment "webserver-deployment"
Dec 16 22:42:21.285: INFO: Waiting for observed generation 1
Dec 16 22:42:23.308: INFO: Waiting for all required pods to come up
Dec 16 22:42:23.313: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 16 22:42:25.336: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 16 22:42:25.341: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 16 22:42:25.351: INFO: Updating deployment webserver-deployment
Dec 16 22:42:25.351: INFO: Waiting for observed generation 2
Dec 16 22:42:27.365: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 16 22:42:27.368: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 16 22:42:27.370: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 16 22:42:27.379: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 16 22:42:27.379: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 16 22:42:27.381: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 16 22:42:27.389: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 16 22:42:27.389: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 16 22:42:27.398: INFO: Updating deployment webserver-deployment
Dec 16 22:42:27.398: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 16 22:42:27.407: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 16 22:42:27.425: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 16 22:42:27.460: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1769  8f010c3d-1bce-4c6d-a154-b4f47465d6fe 21916 3 2020-12-16 22:42:21 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-16 22:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dea198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-12-16 22:42:25 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-12-16 22:42:27 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 16 22:42:27.494: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1769  893e4a67-5bfe-4acd-b6fc-3bffb040a398 21907 3 2020-12-16 22:42:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8f010c3d-1bce-4c6d-a154-b4f47465d6fe 0xc003883957 0xc003883958}] []  [{kube-controller-manager Update apps/v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f010c3d-1bce-4c6d-a154-b4f47465d6fe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038839e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 16 22:42:27.494: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 16 22:42:27.494: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-1769  8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 21904 3 2020-12-16 22:42:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8f010c3d-1bce-4c6d-a154-b4f47465d6fe 0xc003883a47 0xc003883a48}] []  [{kube-controller-manager Update apps/v1 2020-12-16 22:42:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f010c3d-1bce-4c6d-a154-b4f47465d6fe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003883ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 16 22:42:27.531: INFO: Pod "webserver-deployment-795d758f88-4ltxm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4ltxm webserver-deployment-795d758f88- deployment-1769  ab9dac98-69a6-479b-bd17-f2ea10f0161a 21963 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003dea577 0xc003dea578}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.531: INFO: Pod "webserver-deployment-795d758f88-4zsl8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4zsl8 webserver-deployment-795d758f88- deployment-1769  00187617-2eda-4464-b20a-aeda33c75ef8 21935 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003dea6b0 0xc003dea6b1}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.531: INFO: Pod "webserver-deployment-795d758f88-8ssgb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8ssgb webserver-deployment-795d758f88- deployment-1769  a4e64037-7ffe-4f88-82ac-27574715da8c 21962 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003dea800 0xc003dea801}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.531: INFO: Pod "webserver-deployment-795d758f88-b9m6f" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-b9m6f webserver-deployment-795d758f88- deployment-1769  ddf6597a-ce15-4022-9cc3-968e72117b7c 21862 0 2020-12-16 22:42:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.140.48/32 cni.projectcalico.org/podIPs:192.168.140.48/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003dea940 0xc003dea941}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-16 22:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:,StartTime:2020-12-16 22:42:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.532: INFO: Pod "webserver-deployment-795d758f88-g4rdg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-g4rdg webserver-deployment-795d758f88- deployment-1769  ef8ed0f3-776d-4881-a4d3-a0485e933073 21848 0 2020-12-16 22:42:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.91.25/32 cni.projectcalico.org/podIPs:192.168.91.25/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003deab27 0xc003deab28}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-16 22:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.174,PodIP:,StartTime:2020-12-16 22:42:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.532: INFO: Pod "webserver-deployment-795d758f88-hpc8b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hpc8b webserver-deployment-795d758f88- deployment-1769  42d76514-b9b0-42f0-8863-412fbb16c3d1 21838 0 2020-12-16 22:42:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.91.24/32 cni.projectcalico.org/podIPs:192.168.91.24/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003dead17 0xc003dead18}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-16 22:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.174,PodIP:,StartTime:2020-12-16 22:42:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.532: INFO: Pod "webserver-deployment-795d758f88-hqs4s" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hqs4s webserver-deployment-795d758f88- deployment-1769  f07067d2-c0a3-4cbd-b14e-543067115198 21853 0 2020-12-16 22:42:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.140.47/32 cni.projectcalico.org/podIPs:192.168.140.47/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003deaed7 0xc003deaed8}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-16 22:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:,StartTime:2020-12-16 22:42:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.532: INFO: Pod "webserver-deployment-795d758f88-kndd2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kndd2 webserver-deployment-795d758f88- deployment-1769  eeccd252-24f9-4135-9fc6-76ee2f69a9b0 21936 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003deb097 0xc003deb098}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.532: INFO: Pod "webserver-deployment-795d758f88-nrj4l" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nrj4l webserver-deployment-795d758f88- deployment-1769  95fcc4f6-9744-4d82-a0cc-28479c9e32d5 21931 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003deb1d0 0xc003deb1d1}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.174,PodIP:,StartTime:2020-12-16 22:42:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.533: INFO: Pod "webserver-deployment-795d758f88-qzqh8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qzqh8 webserver-deployment-795d758f88- deployment-1769  4dc8631e-5b47-4050-9b87-99fa4961c85e 21961 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003deb387 0xc003deb388}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.533: INFO: Pod "webserver-deployment-795d758f88-t82hm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-t82hm webserver-deployment-795d758f88- deployment-1769  be1fcd98-647e-4d09-980e-d374b17970f0 21846 0 2020-12-16 22:42:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.140.46/32 cni.projectcalico.org/podIPs:192.168.140.46/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003deb4e0 0xc003deb4e1}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-16 22:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-16 22:42:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:,StartTime:2020-12-16 22:42:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.533: INFO: Pod "webserver-deployment-795d758f88-vnvkg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vnvkg webserver-deployment-795d758f88- deployment-1769  c23f8993-fbfa-45ee-a44b-3745f626bf3e 21960 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003deb6a7 0xc003deb6a8}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.533: INFO: Pod "webserver-deployment-795d758f88-wnw5k" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wnw5k webserver-deployment-795d758f88- deployment-1769  22f5aa79-00a4-4ef0-ad60-02bcd4627b61 21965 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 893e4a67-5bfe-4acd-b6fc-3bffb040a398 0xc003deb7e0 0xc003deb7e1}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893e4a67-5bfe-4acd-b6fc-3bffb040a398\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.534: INFO: Pod "webserver-deployment-dd94f59b7-4mr94" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4mr94 webserver-deployment-dd94f59b7- deployment-1769  7c9e81fc-5389-464b-b1fd-f6158d895d22 21959 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc003deb8f7 0xc003deb8f8}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.534: INFO: Pod "webserver-deployment-dd94f59b7-645bj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-645bj webserver-deployment-dd94f59b7- deployment-1769  dc9a0d5c-2fd7-4431-b248-47ff7962dbb4 21745 0 2020-12-16 22:42:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.140.41/32 cni.projectcalico.org/podIPs:192.168.140.41/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc003deba20 0xc003deba21}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-16 22:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-16 22:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.140.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:192.168.140.41,StartTime:2020-12-16 22:42:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 22:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://43b4a705f8ddaa873542df2412e26936ee168c6122295757d271ebe441738244,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.140.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.534: INFO: Pod "webserver-deployment-dd94f59b7-6j7ws" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6j7ws webserver-deployment-dd94f59b7- deployment-1769  a1e27dd5-4a02-4e97-a737-4b64d7abca03 21966 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc003debbe7 0xc003debbe8}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.174,PodIP:,StartTime:2020-12-16 22:42:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.534: INFO: Pod "webserver-deployment-dd94f59b7-7rg2k" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7rg2k webserver-deployment-dd94f59b7- deployment-1769  c4155d97-a8fb-413b-b554-03b7eb4e2ab6 21918 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc003debd77 0xc003debd78}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.534: INFO: Pod "webserver-deployment-dd94f59b7-7x4m4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7x4m4 webserver-deployment-dd94f59b7- deployment-1769  eac61339-3843-40d7-9e82-e7b6a6b22999 21958 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc003debea0 0xc003debea1}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.535: INFO: Pod "webserver-deployment-dd94f59b7-8bptz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8bptz webserver-deployment-dd94f59b7- deployment-1769  e7927bc8-4fd5-4058-ae61-952967557bde 21764 0 2020-12-16 22:42:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.140.44/32 cni.projectcalico.org/podIPs:192.168.140.44/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc003debfd0 0xc003debfd1}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-16 22:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-16 22:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.140.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:192.168.140.44,StartTime:2020-12-16 22:42:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 22:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://58d925fd5544231d9fb07c673d0033be15c37e2ce3cde369ad6ba61af44ccc57,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.140.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.535: INFO: Pod "webserver-deployment-dd94f59b7-bqkx9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bqkx9 webserver-deployment-dd94f59b7- deployment-1769  47f12aab-4f7c-466c-a583-2e57e5acc51a 21713 0 2020-12-16 22:42:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.140.40/32 cni.projectcalico.org/podIPs:192.168.140.40/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc0005826e7 0xc0005826e8}] []  [{calico Update v1 2020-12-16 22:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-12-16 22:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 22:42:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.140.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:192.168.140.40,StartTime:2020-12-16 22:42:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 22:42:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://9db5a2e170877dce53ce2a4df0f0ac65d089421134bc795d972a2bce5797e856,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.140.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.535: INFO: Pod "webserver-deployment-dd94f59b7-dhbrr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dhbrr webserver-deployment-dd94f59b7- deployment-1769  41c4ff96-135f-4b48-9dff-a345a2143c07 21750 0 2020-12-16 22:42:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.91.20/32 cni.projectcalico.org/podIPs:192.168.91.20/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc000582d27 0xc000582d28}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-16 22:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-16 22:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.91.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.174,PodIP:192.168.91.20,StartTime:2020-12-16 22:42:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 22:42:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://0d82083af860a9c164f4f140dbcc9d82e2f3732a99060ee22c9e5432513d3c1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.91.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.535: INFO: Pod "webserver-deployment-dd94f59b7-dng4s" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dng4s webserver-deployment-dd94f59b7- deployment-1769  213208c9-4241-4adb-9b14-0bc2780a3189 21748 0 2020-12-16 22:42:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.91.22/32 cni.projectcalico.org/podIPs:192.168.91.22/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc0005837c7 0xc0005837c8}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-16 22:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-16 22:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.91.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.174,PodIP:192.168.91.22,StartTime:2020-12-16 22:42:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 22:42:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6e9539921996d5c00e9f404f876ce0fb36749db4a5a3026f7ca18c3323b52edf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.91.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.536: INFO: Pod "webserver-deployment-dd94f59b7-fndnp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fndnp webserver-deployment-dd94f59b7- deployment-1769  05ec5bd2-d989-45e7-a7aa-fcf3416877e5 21957 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc000583ba7 0xc000583ba8}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.536: INFO: Pod "webserver-deployment-dd94f59b7-g2qdm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-g2qdm webserver-deployment-dd94f59b7- deployment-1769  fbf14b0c-e643-4a6b-b788-03a6974621ef 21914 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc000583e90 0xc000583e91}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:,StartTime:2020-12-16 22:42:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.536: INFO: Pod "webserver-deployment-dd94f59b7-ks9nr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ks9nr webserver-deployment-dd94f59b7- deployment-1769  47989c1c-303b-4ac5-9f05-f7bed15a4747 21953 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc0007a6a57 0xc0007a6a58}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.174,PodIP:,StartTime:2020-12-16 22:42:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.536: INFO: Pod "webserver-deployment-dd94f59b7-l64pd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-l64pd webserver-deployment-dd94f59b7- deployment-1769  6eb22093-bb45-4296-9081-bb6d96f121b3 21752 0 2020-12-16 22:42:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.140.43/32 cni.projectcalico.org/podIPs:192.168.140.43/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc0007a6c67 0xc0007a6c68}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-16 22:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-16 22:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.140.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:192.168.140.43,StartTime:2020-12-16 22:42:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 22:42:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://bf196d1a97f29d4b44f6663663c710adf4e84a59a10b1018df38e9e8ab076803,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.140.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.536: INFO: Pod "webserver-deployment-dd94f59b7-l6k4r" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-l6k4r webserver-deployment-dd94f59b7- deployment-1769  74ed00f1-9032-4d9e-bd9c-909139399426 21955 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc0007a6e67 0xc0007a6e68}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.536: INFO: Pod "webserver-deployment-dd94f59b7-mrwvq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mrwvq webserver-deployment-dd94f59b7- deployment-1769  6df65ad4-3d85-44a8-bcb8-6f0638da62ac 21942 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc0007a74d0 0xc0007a74d1}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:,StartTime:2020-12-16 22:42:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.537: INFO: Pod "webserver-deployment-dd94f59b7-n2npv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n2npv webserver-deployment-dd94f59b7- deployment-1769  7c86799d-955c-4ecd-b771-f6053a2ab054 21956 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc003280037 0xc003280038}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.537: INFO: Pod "webserver-deployment-dd94f59b7-pnmzc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pnmzc webserver-deployment-dd94f59b7- deployment-1769  ddb7b673-eebd-42b6-8870-96ca004559e5 21930 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc003280170 0xc003280171}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.537: INFO: Pod "webserver-deployment-dd94f59b7-pr46x" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pr46x webserver-deployment-dd94f59b7- deployment-1769  7083d7b1-85a5-422b-bdaa-e370e6eb492b 21757 0 2020-12-16 22:42:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.91.23/32 cni.projectcalico.org/podIPs:192.168.91.23/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc0032802d0 0xc0032802d1}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-16 22:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-16 22:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.91.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.174,PodIP:192.168.91.23,StartTime:2020-12-16 22:42:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 22:42:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f93a5da6f19969b2f12d2ce994d7212bcf01709cd3c162ba12d94a1ff653e820,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.91.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.537: INFO: Pod "webserver-deployment-dd94f59b7-sdfhh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sdfhh webserver-deployment-dd94f59b7- deployment-1769  116ef01b-add9-44bb-b62f-ad0057b060ca 21743 0 2020-12-16 22:42:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.91.21/32 cni.projectcalico.org/podIPs:192.168.91.21/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc0032804f7 0xc0032804f8}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-16 22:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-16 22:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.91.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.174,PodIP:192.168.91.21,StartTime:2020-12-16 22:42:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 22:42:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://82e419e7ce21c39a61b30a7fba57c95238ce165e36389cd78502bbfc35a3092f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.91.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 22:42:27.537: INFO: Pod "webserver-deployment-dd94f59b7-xt56g" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xt56g webserver-deployment-dd94f59b7- deployment-1769  b83bc466-2bce-4b2b-b378-331783061c84 21933 0 2020-12-16 22:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3 0xc003280717 0xc003280718}] []  [{kube-controller-manager Update v1 2020-12-16 22:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e2b9cfe-33c5-4e04-a7e8-dcf60dfffcf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mv2vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mv2vf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mv2vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 22:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:42:27.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1769" for this suite.

• [SLOW TEST:6.330 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":32,"skipped":520,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:42:27.567: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 22:42:27.657: INFO: Waiting up to 5m0s for pod "downwardapi-volume-460b8730-e066-46f2-af6c-a9d1e1329a3a" in namespace "downward-api-7855" to be "Succeeded or Failed"
Dec 16 22:42:27.667: INFO: Pod "downwardapi-volume-460b8730-e066-46f2-af6c-a9d1e1329a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.506723ms
Dec 16 22:42:29.675: INFO: Pod "downwardapi-volume-460b8730-e066-46f2-af6c-a9d1e1329a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017149361s
Dec 16 22:42:31.685: INFO: Pod "downwardapi-volume-460b8730-e066-46f2-af6c-a9d1e1329a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027691335s
Dec 16 22:42:33.695: INFO: Pod "downwardapi-volume-460b8730-e066-46f2-af6c-a9d1e1329a3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037514811s
STEP: Saw pod success
Dec 16 22:42:33.695: INFO: Pod "downwardapi-volume-460b8730-e066-46f2-af6c-a9d1e1329a3a" satisfied condition "Succeeded or Failed"
Dec 16 22:42:33.700: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-460b8730-e066-46f2-af6c-a9d1e1329a3a container client-container: <nil>
STEP: delete the pod
Dec 16 22:42:33.746: INFO: Waiting for pod downwardapi-volume-460b8730-e066-46f2-af6c-a9d1e1329a3a to disappear
Dec 16 22:42:33.754: INFO: Pod downwardapi-volume-460b8730-e066-46f2-af6c-a9d1e1329a3a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:42:33.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7855" for this suite.

• [SLOW TEST:6.215 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":33,"skipped":537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:42:33.800: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 22:42:34.649: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 16 22:42:36.662: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755354, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755354, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755354, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755354, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 22:42:39.684: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:42:39.690: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9149-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:42:40.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6131" for this suite.
STEP: Destroying namespace "webhook-6131-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.271 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":34,"skipped":565,"failed":0}
SSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:42:41.073: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Dec 16 22:42:41.140: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Dec 16 22:42:41.150: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 16 22:42:41.151: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Dec 16 22:42:41.161: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 16 22:42:41.161: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Dec 16 22:42:41.170: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 16 22:42:41.170: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Dec 16 22:42:48.223: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:42:48.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2837" for this suite.

• [SLOW TEST:7.176 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":35,"skipped":570,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:42:48.253: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:42:48.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5864" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":36,"skipped":591,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:42:48.324: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 22:42:48.751: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 16 22:42:50.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755368, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755368, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755368, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755368, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 22:42:53.793: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:42:53.798: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9080-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:42:55.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6194" for this suite.
STEP: Destroying namespace "webhook-6194-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.859 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":37,"skipped":593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:42:55.190: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 16 22:42:55.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-8836 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Dec 16 22:42:55.417: INFO: stderr: ""
Dec 16 22:42:55.417: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 16 22:43:00.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-8836 get pod e2e-test-httpd-pod -o json'
Dec 16 22:43:00.558: INFO: stderr: ""
Dec 16 22:43:00.558: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.140.2/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.140.2/32\"\n        },\n        \"creationTimestamp\": \"2020-12-16T22:42:55Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-16T22:42:55Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-16T22:42:56Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"192.168.140.2\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-16T22:42:56Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8836\",\n        \"resourceVersion\": \"22700\",\n        \"uid\": \"ecaf42b6-9d96-4196-ab26-393e1754daae\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5zxf4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-1-217\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5zxf4\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5zxf4\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-16T22:42:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-16T22:42:56Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-16T22:42:56Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-16T22:42:55Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://b09cc14680bbed39906be1835089e0fedb098e9633092fe1962e2534526480fe\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-12-16T22:42:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.1.217\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.140.2\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.140.2\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-12-16T22:42:55Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 16 22:43:00.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-8836 replace -f -'
Dec 16 22:43:01.034: INFO: stderr: ""
Dec 16 22:43:01.034: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Dec 16 22:43:01.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-8836 delete pods e2e-test-httpd-pod'
Dec 16 22:43:07.821: INFO: stderr: ""
Dec 16 22:43:07.821: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:43:07.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8836" for this suite.

• [SLOW TEST:12.673 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":38,"skipped":627,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:43:07.863: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-72066064-ea62-44f7-837c-092c0ee0656d
STEP: Creating a pod to test consume configMaps
Dec 16 22:43:07.927: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7c6af156-aa47-4673-b1be-b1ba1cddc2ec" in namespace "projected-6371" to be "Succeeded or Failed"
Dec 16 22:43:07.931: INFO: Pod "pod-projected-configmaps-7c6af156-aa47-4673-b1be-b1ba1cddc2ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013593ms
Dec 16 22:43:09.941: INFO: Pod "pod-projected-configmaps-7c6af156-aa47-4673-b1be-b1ba1cddc2ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013716497s
STEP: Saw pod success
Dec 16 22:43:09.941: INFO: Pod "pod-projected-configmaps-7c6af156-aa47-4673-b1be-b1ba1cddc2ec" satisfied condition "Succeeded or Failed"
Dec 16 22:43:09.944: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-configmaps-7c6af156-aa47-4673-b1be-b1ba1cddc2ec container agnhost-container: <nil>
STEP: delete the pod
Dec 16 22:43:09.965: INFO: Waiting for pod pod-projected-configmaps-7c6af156-aa47-4673-b1be-b1ba1cddc2ec to disappear
Dec 16 22:43:09.968: INFO: Pod pod-projected-configmaps-7c6af156-aa47-4673-b1be-b1ba1cddc2ec no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:43:09.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6371" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":39,"skipped":632,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:43:09.981: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-9296
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9296 to expose endpoints map[]
Dec 16 22:43:10.061: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 16 22:43:11.072: INFO: successfully validated that service endpoint-test2 in namespace services-9296 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9296
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9296 to expose endpoints map[pod1:[80]]
Dec 16 22:43:13.103: INFO: successfully validated that service endpoint-test2 in namespace services-9296 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-9296
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9296 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 16 22:43:15.136: INFO: successfully validated that service endpoint-test2 in namespace services-9296 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-9296
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9296 to expose endpoints map[pod2:[80]]
Dec 16 22:43:15.175: INFO: successfully validated that service endpoint-test2 in namespace services-9296 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-9296
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9296 to expose endpoints map[]
Dec 16 22:43:15.243: INFO: successfully validated that service endpoint-test2 in namespace services-9296 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:43:15.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9296" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.369 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":40,"skipped":637,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:43:15.351: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:43:18.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4926" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":41,"skipped":660,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:43:18.519: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 22:43:18.573: INFO: Waiting up to 5m0s for pod "downwardapi-volume-308a6b27-df0e-4fb6-b7d4-609ea910f8cd" in namespace "projected-4814" to be "Succeeded or Failed"
Dec 16 22:43:18.575: INFO: Pod "downwardapi-volume-308a6b27-df0e-4fb6-b7d4-609ea910f8cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.469826ms
Dec 16 22:43:20.584: INFO: Pod "downwardapi-volume-308a6b27-df0e-4fb6-b7d4-609ea910f8cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010997095s
STEP: Saw pod success
Dec 16 22:43:20.584: INFO: Pod "downwardapi-volume-308a6b27-df0e-4fb6-b7d4-609ea910f8cd" satisfied condition "Succeeded or Failed"
Dec 16 22:43:20.588: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-308a6b27-df0e-4fb6-b7d4-609ea910f8cd container client-container: <nil>
STEP: delete the pod
Dec 16 22:43:20.615: INFO: Waiting for pod downwardapi-volume-308a6b27-df0e-4fb6-b7d4-609ea910f8cd to disappear
Dec 16 22:43:20.623: INFO: Pod downwardapi-volume-308a6b27-df0e-4fb6-b7d4-609ea910f8cd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:43:20.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4814" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":42,"skipped":675,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:43:20.645: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:43:26.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8334" for this suite.
STEP: Destroying namespace "nsdeletetest-2364" for this suite.
Dec 16 22:43:26.935: INFO: Namespace nsdeletetest-2364 was already deleted
STEP: Destroying namespace "nsdeletetest-8663" for this suite.

• [SLOW TEST:6.297 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":43,"skipped":709,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:43:26.942: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8560
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8560
STEP: Creating statefulset with conflicting port in namespace statefulset-8560
STEP: Waiting until pod test-pod will start running in namespace statefulset-8560
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8560
Dec 16 22:43:31.034: INFO: Observed stateful pod in namespace: statefulset-8560, name: ss-0, uid: 7029aaea-cd8d-4bda-bd8a-c643abc1ebc1, status phase: Failed. Waiting for statefulset controller to delete.
Dec 16 22:43:31.037: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8560
STEP: Removing pod with conflicting port in namespace statefulset-8560
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8560 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 16 22:43:35.160: INFO: Deleting all statefulset in ns statefulset-8560
Dec 16 22:43:35.165: INFO: Scaling statefulset ss to 0
Dec 16 22:43:55.208: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 22:43:55.235: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:43:55.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8560" for this suite.

• [SLOW TEST:28.371 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":44,"skipped":722,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:43:55.313: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-928279f2-4547-4637-8666-01b644335d5b
STEP: Creating a pod to test consume configMaps
Dec 16 22:43:55.366: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d29dfe0-16ce-44ae-a996-0137b1786862" in namespace "projected-1694" to be "Succeeded or Failed"
Dec 16 22:43:55.369: INFO: Pod "pod-projected-configmaps-5d29dfe0-16ce-44ae-a996-0137b1786862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949648ms
Dec 16 22:43:57.382: INFO: Pod "pod-projected-configmaps-5d29dfe0-16ce-44ae-a996-0137b1786862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015907911s
STEP: Saw pod success
Dec 16 22:43:57.382: INFO: Pod "pod-projected-configmaps-5d29dfe0-16ce-44ae-a996-0137b1786862" satisfied condition "Succeeded or Failed"
Dec 16 22:43:57.385: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-configmaps-5d29dfe0-16ce-44ae-a996-0137b1786862 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 16 22:43:57.412: INFO: Waiting for pod pod-projected-configmaps-5d29dfe0-16ce-44ae-a996-0137b1786862 to disappear
Dec 16 22:43:57.415: INFO: Pod pod-projected-configmaps-5d29dfe0-16ce-44ae-a996-0137b1786862 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:43:57.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1694" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":741,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:43:57.430: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-b16a6994-f2dc-457e-bebb-cc9901bb6c5c
STEP: Creating a pod to test consume secrets
Dec 16 22:43:57.487: INFO: Waiting up to 5m0s for pod "pod-secrets-87367ffd-d954-440c-93f2-3d7128559c2f" in namespace "secrets-1160" to be "Succeeded or Failed"
Dec 16 22:43:57.492: INFO: Pod "pod-secrets-87367ffd-d954-440c-93f2-3d7128559c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.317612ms
Dec 16 22:43:59.499: INFO: Pod "pod-secrets-87367ffd-d954-440c-93f2-3d7128559c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012126645s
STEP: Saw pod success
Dec 16 22:43:59.499: INFO: Pod "pod-secrets-87367ffd-d954-440c-93f2-3d7128559c2f" satisfied condition "Succeeded or Failed"
Dec 16 22:43:59.501: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-secrets-87367ffd-d954-440c-93f2-3d7128559c2f container secret-volume-test: <nil>
STEP: delete the pod
Dec 16 22:43:59.520: INFO: Waiting for pod pod-secrets-87367ffd-d954-440c-93f2-3d7128559c2f to disappear
Dec 16 22:43:59.528: INFO: Pod pod-secrets-87367ffd-d954-440c-93f2-3d7128559c2f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:43:59.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1160" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":46,"skipped":752,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:43:59.545: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1216 22:44:00.699496      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 16 22:45:02.722: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:02.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9491" for this suite.

• [SLOW TEST:63.192 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":47,"skipped":752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:02.738: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 16 22:45:02.802: INFO: Waiting up to 5m0s for pod "pod-44186d86-b476-4177-bca3-5b636bbdeb97" in namespace "emptydir-1070" to be "Succeeded or Failed"
Dec 16 22:45:02.809: INFO: Pod "pod-44186d86-b476-4177-bca3-5b636bbdeb97": Phase="Pending", Reason="", readiness=false. Elapsed: 6.504068ms
Dec 16 22:45:04.815: INFO: Pod "pod-44186d86-b476-4177-bca3-5b636bbdeb97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012816015s
Dec 16 22:45:06.823: INFO: Pod "pod-44186d86-b476-4177-bca3-5b636bbdeb97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020528825s
STEP: Saw pod success
Dec 16 22:45:06.823: INFO: Pod "pod-44186d86-b476-4177-bca3-5b636bbdeb97" satisfied condition "Succeeded or Failed"
Dec 16 22:45:06.826: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-44186d86-b476-4177-bca3-5b636bbdeb97 container test-container: <nil>
STEP: delete the pod
Dec 16 22:45:06.861: INFO: Waiting for pod pod-44186d86-b476-4177-bca3-5b636bbdeb97 to disappear
Dec 16 22:45:06.869: INFO: Pod pod-44186d86-b476-4177-bca3-5b636bbdeb97 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:06.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1070" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":48,"skipped":809,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:06.879: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-9279
STEP: creating replication controller nodeport-test in namespace services-9279
I1216 22:45:07.023160      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9279, replica count: 2
I1216 22:45:10.073783      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 22:45:10.073: INFO: Creating new exec pod
Dec 16 22:45:13.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-9279 exec execpodp2f7r -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 16 22:45:13.449: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 16 22:45:13.449: INFO: stdout: ""
Dec 16 22:45:13.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-9279 exec execpodp2f7r -- /bin/sh -x -c nc -zv -t -w 2 10.101.51.90 80'
Dec 16 22:45:13.604: INFO: stderr: "+ nc -zv -t -w 2 10.101.51.90 80\nConnection to 10.101.51.90 80 port [tcp/http] succeeded!\n"
Dec 16 22:45:13.604: INFO: stdout: ""
Dec 16 22:45:13.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-9279 exec execpodp2f7r -- /bin/sh -x -c nc -zv -t -w 2 172.31.1.217 30141'
Dec 16 22:45:13.757: INFO: stderr: "+ nc -zv -t -w 2 172.31.1.217 30141\nConnection to 172.31.1.217 30141 port [tcp/30141] succeeded!\n"
Dec 16 22:45:13.757: INFO: stdout: ""
Dec 16 22:45:13.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-9279 exec execpodp2f7r -- /bin/sh -x -c nc -zv -t -w 2 172.31.6.174 30141'
Dec 16 22:45:13.899: INFO: stderr: "+ nc -zv -t -w 2 172.31.6.174 30141\nConnection to 172.31.6.174 30141 port [tcp/30141] succeeded!\n"
Dec 16 22:45:13.899: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:13.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9279" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.031 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":49,"skipped":825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:13.911: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-1b537d90-759d-4bc6-ac9b-780aca3bc414
STEP: Creating a pod to test consume secrets
Dec 16 22:45:13.984: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f464018-fde9-48bd-8105-aee686154af7" in namespace "projected-9546" to be "Succeeded or Failed"
Dec 16 22:45:13.989: INFO: Pod "pod-projected-secrets-1f464018-fde9-48bd-8105-aee686154af7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.004956ms
Dec 16 22:45:15.996: INFO: Pod "pod-projected-secrets-1f464018-fde9-48bd-8105-aee686154af7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011582292s
STEP: Saw pod success
Dec 16 22:45:15.996: INFO: Pod "pod-projected-secrets-1f464018-fde9-48bd-8105-aee686154af7" satisfied condition "Succeeded or Failed"
Dec 16 22:45:16.003: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-secrets-1f464018-fde9-48bd-8105-aee686154af7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 16 22:45:16.024: INFO: Waiting for pod pod-projected-secrets-1f464018-fde9-48bd-8105-aee686154af7 to disappear
Dec 16 22:45:16.030: INFO: Pod pod-projected-secrets-1f464018-fde9-48bd-8105-aee686154af7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:16.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9546" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":50,"skipped":848,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:16.043: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2971
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2971
I1216 22:45:16.134906      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2971, replica count: 2
I1216 22:45:19.191803      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 22:45:19.191: INFO: Creating new exec pod
Dec 16 22:45:22.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-2971 exec execpodggnd8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 16 22:45:22.382: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 16 22:45:22.382: INFO: stdout: ""
Dec 16 22:45:22.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-2971 exec execpodggnd8 -- /bin/sh -x -c nc -zv -t -w 2 10.99.192.230 80'
Dec 16 22:45:22.544: INFO: stderr: "+ nc -zv -t -w 2 10.99.192.230 80\nConnection to 10.99.192.230 80 port [tcp/http] succeeded!\n"
Dec 16 22:45:22.544: INFO: stdout: ""
Dec 16 22:45:22.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-2971 exec execpodggnd8 -- /bin/sh -x -c nc -zv -t -w 2 172.31.1.217 31437'
Dec 16 22:45:22.712: INFO: stderr: "+ nc -zv -t -w 2 172.31.1.217 31437\nConnection to 172.31.1.217 31437 port [tcp/31437] succeeded!\n"
Dec 16 22:45:22.712: INFO: stdout: ""
Dec 16 22:45:22.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-2971 exec execpodggnd8 -- /bin/sh -x -c nc -zv -t -w 2 172.31.6.174 31437'
Dec 16 22:45:22.914: INFO: stderr: "+ nc -zv -t -w 2 172.31.6.174 31437\nConnection to 172.31.6.174 31437 port [tcp/31437] succeeded!\n"
Dec 16 22:45:22.914: INFO: stdout: ""
Dec 16 22:45:22.914: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:22.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2971" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.919 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":51,"skipped":864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:22.963: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-63b42ef7-40d5-4c32-8ef3-95dc21170be1
STEP: Creating a pod to test consume configMaps
Dec 16 22:45:23.035: INFO: Waiting up to 5m0s for pod "pod-configmaps-0244a2d7-3b7b-40b5-abf6-f82b49fda7b6" in namespace "configmap-9252" to be "Succeeded or Failed"
Dec 16 22:45:23.039: INFO: Pod "pod-configmaps-0244a2d7-3b7b-40b5-abf6-f82b49fda7b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.120559ms
Dec 16 22:45:25.045: INFO: Pod "pod-configmaps-0244a2d7-3b7b-40b5-abf6-f82b49fda7b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009773887s
STEP: Saw pod success
Dec 16 22:45:25.045: INFO: Pod "pod-configmaps-0244a2d7-3b7b-40b5-abf6-f82b49fda7b6" satisfied condition "Succeeded or Failed"
Dec 16 22:45:25.048: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-configmaps-0244a2d7-3b7b-40b5-abf6-f82b49fda7b6 container agnhost-container: <nil>
STEP: delete the pod
Dec 16 22:45:25.074: INFO: Waiting for pod pod-configmaps-0244a2d7-3b7b-40b5-abf6-f82b49fda7b6 to disappear
Dec 16 22:45:25.077: INFO: Pod pod-configmaps-0244a2d7-3b7b-40b5-abf6-f82b49fda7b6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:25.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9252" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":52,"skipped":904,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:25.088: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:25.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-973" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":53,"skipped":918,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:25.159: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Dec 16 22:45:25.212: INFO: Waiting up to 5m0s for pod "var-expansion-862a6310-1b8b-4ad7-8e47-bf59f30b417e" in namespace "var-expansion-6809" to be "Succeeded or Failed"
Dec 16 22:45:25.216: INFO: Pod "var-expansion-862a6310-1b8b-4ad7-8e47-bf59f30b417e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.373369ms
Dec 16 22:45:27.223: INFO: Pod "var-expansion-862a6310-1b8b-4ad7-8e47-bf59f30b417e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010736005s
STEP: Saw pod success
Dec 16 22:45:27.223: INFO: Pod "var-expansion-862a6310-1b8b-4ad7-8e47-bf59f30b417e" satisfied condition "Succeeded or Failed"
Dec 16 22:45:27.226: INFO: Trying to get logs from node ip-172-31-1-217 pod var-expansion-862a6310-1b8b-4ad7-8e47-bf59f30b417e container dapi-container: <nil>
STEP: delete the pod
Dec 16 22:45:27.244: INFO: Waiting for pod var-expansion-862a6310-1b8b-4ad7-8e47-bf59f30b417e to disappear
Dec 16 22:45:27.246: INFO: Pod var-expansion-862a6310-1b8b-4ad7-8e47-bf59f30b417e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:27.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6809" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":940,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:27.260: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-q298
STEP: Creating a pod to test atomic-volume-subpath
Dec 16 22:45:27.312: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-q298" in namespace "subpath-6616" to be "Succeeded or Failed"
Dec 16 22:45:27.315: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692986ms
Dec 16 22:45:29.322: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Running", Reason="", readiness=true. Elapsed: 2.009925009s
Dec 16 22:45:31.348: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Running", Reason="", readiness=true. Elapsed: 4.036241701s
Dec 16 22:45:33.355: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Running", Reason="", readiness=true. Elapsed: 6.043041152s
Dec 16 22:45:35.362: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Running", Reason="", readiness=true. Elapsed: 8.05006731s
Dec 16 22:45:37.370: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Running", Reason="", readiness=true. Elapsed: 10.057766811s
Dec 16 22:45:39.378: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Running", Reason="", readiness=true. Elapsed: 12.065563129s
Dec 16 22:45:41.385: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Running", Reason="", readiness=true. Elapsed: 14.073164109s
Dec 16 22:45:43.393: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Running", Reason="", readiness=true. Elapsed: 16.080397684s
Dec 16 22:45:45.409: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Running", Reason="", readiness=true. Elapsed: 18.096941085s
Dec 16 22:45:47.417: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Running", Reason="", readiness=true. Elapsed: 20.104986405s
Dec 16 22:45:49.425: INFO: Pod "pod-subpath-test-configmap-q298": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.112718384s
STEP: Saw pod success
Dec 16 22:45:49.425: INFO: Pod "pod-subpath-test-configmap-q298" satisfied condition "Succeeded or Failed"
Dec 16 22:45:49.429: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-subpath-test-configmap-q298 container test-container-subpath-configmap-q298: <nil>
STEP: delete the pod
Dec 16 22:45:49.453: INFO: Waiting for pod pod-subpath-test-configmap-q298 to disappear
Dec 16 22:45:49.458: INFO: Pod pod-subpath-test-configmap-q298 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-q298
Dec 16 22:45:49.458: INFO: Deleting pod "pod-subpath-test-configmap-q298" in namespace "subpath-6616"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:49.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6616" for this suite.

• [SLOW TEST:22.211 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":55,"skipped":949,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:49.473: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:45:49.523: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5ed029da-0b54-4e5d-8692-311f6f63c995" in namespace "security-context-test-6862" to be "Succeeded or Failed"
Dec 16 22:45:49.526: INFO: Pod "busybox-readonly-false-5ed029da-0b54-4e5d-8692-311f6f63c995": Phase="Pending", Reason="", readiness=false. Elapsed: 2.965384ms
Dec 16 22:45:51.535: INFO: Pod "busybox-readonly-false-5ed029da-0b54-4e5d-8692-311f6f63c995": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011737519s
Dec 16 22:45:51.535: INFO: Pod "busybox-readonly-false-5ed029da-0b54-4e5d-8692-311f6f63c995" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:51.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6862" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":956,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:51.553: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Dec 16 22:45:51.605: INFO: Waiting up to 5m0s for pod "downward-api-6b696ab5-31fd-4c97-ad9c-67fb9bd65f50" in namespace "downward-api-6148" to be "Succeeded or Failed"
Dec 16 22:45:51.608: INFO: Pod "downward-api-6b696ab5-31fd-4c97-ad9c-67fb9bd65f50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.990362ms
Dec 16 22:45:53.624: INFO: Pod "downward-api-6b696ab5-31fd-4c97-ad9c-67fb9bd65f50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018646296s
STEP: Saw pod success
Dec 16 22:45:53.624: INFO: Pod "downward-api-6b696ab5-31fd-4c97-ad9c-67fb9bd65f50" satisfied condition "Succeeded or Failed"
Dec 16 22:45:53.627: INFO: Trying to get logs from node ip-172-31-1-217 pod downward-api-6b696ab5-31fd-4c97-ad9c-67fb9bd65f50 container dapi-container: <nil>
STEP: delete the pod
Dec 16 22:45:53.655: INFO: Waiting for pod downward-api-6b696ab5-31fd-4c97-ad9c-67fb9bd65f50 to disappear
Dec 16 22:45:53.658: INFO: Pod downward-api-6b696ab5-31fd-4c97-ad9c-67fb9bd65f50 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:45:53.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6148" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":57,"skipped":967,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:45:53.694: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:46:06.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6048" for this suite.

• [SLOW TEST:13.146 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":58,"skipped":1024,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:46:06.842: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:46:06.887: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 16 22:46:10.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-2120 --namespace=crd-publish-openapi-2120 create -f -'
Dec 16 22:46:10.845: INFO: stderr: ""
Dec 16 22:46:10.845: INFO: stdout: "e2e-test-crd-publish-openapi-3759-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 16 22:46:10.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-2120 --namespace=crd-publish-openapi-2120 delete e2e-test-crd-publish-openapi-3759-crds test-cr'
Dec 16 22:46:10.956: INFO: stderr: ""
Dec 16 22:46:10.956: INFO: stdout: "e2e-test-crd-publish-openapi-3759-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 16 22:46:10.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-2120 --namespace=crd-publish-openapi-2120 apply -f -'
Dec 16 22:46:11.217: INFO: stderr: ""
Dec 16 22:46:11.217: INFO: stdout: "e2e-test-crd-publish-openapi-3759-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 16 22:46:11.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-2120 --namespace=crd-publish-openapi-2120 delete e2e-test-crd-publish-openapi-3759-crds test-cr'
Dec 16 22:46:11.295: INFO: stderr: ""
Dec 16 22:46:11.295: INFO: stdout: "e2e-test-crd-publish-openapi-3759-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 16 22:46:11.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-2120 explain e2e-test-crd-publish-openapi-3759-crds'
Dec 16 22:46:11.540: INFO: stderr: ""
Dec 16 22:46:11.540: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3759-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:46:15.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2120" for this suite.

• [SLOW TEST:8.191 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":59,"skipped":1036,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:46:15.033: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-635
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 16 22:46:15.074: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 16 22:46:15.104: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 22:46:17.116: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 22:46:19.113: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 22:46:21.111: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 22:46:23.110: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 22:46:25.111: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 22:46:27.111: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 16 22:46:27.117: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 16 22:46:29.137: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 16 22:46:31.165: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 16 22:46:31.165: INFO: Breadth first check of 192.168.140.15 on host 172.31.1.217...
Dec 16 22:46:31.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.140.22:9080/dial?request=hostname&protocol=udp&host=192.168.140.15&port=8081&tries=1'] Namespace:pod-network-test-635 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 22:46:31.168: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 22:46:31.245: INFO: Waiting for responses: map[]
Dec 16 22:46:31.245: INFO: reached 192.168.140.15 after 0/1 tries
Dec 16 22:46:31.245: INFO: Breadth first check of 192.168.91.42 on host 172.31.6.174...
Dec 16 22:46:31.249: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.140.22:9080/dial?request=hostname&protocol=udp&host=192.168.91.42&port=8081&tries=1'] Namespace:pod-network-test-635 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 22:46:31.249: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 22:46:31.342: INFO: Waiting for responses: map[]
Dec 16 22:46:31.342: INFO: reached 192.168.91.42 after 0/1 tries
Dec 16 22:46:31.342: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:46:31.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-635" for this suite.

• [SLOW TEST:16.322 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":1046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:46:31.357: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3561.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3561.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 16 22:46:33.458: INFO: DNS probes using dns-3561/dns-test-3a197023-02a2-4e8d-9273-1e4b8bf77927 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:46:33.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3561" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":61,"skipped":1082,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:46:33.499: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:46:33.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7699" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":62,"skipped":1092,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:46:33.558: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9d08b421-3add-4593-84e5-4bdf74b9ec66
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-9d08b421-3add-4593-84e5-4bdf74b9ec66
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:46:39.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8279" for this suite.

• [SLOW TEST:6.111 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":1102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:46:39.673: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 22:46:40.188: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 16 22:46:42.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755600, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755600, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755600, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743755600, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 22:46:45.237: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:46:45.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3264" for this suite.
STEP: Destroying namespace "webhook-3264-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.077 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":64,"skipped":1138,"failed":0}
SSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:46:45.753: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Dec 16 22:46:45.789: INFO: Major version: 1
STEP: Confirm minor version
Dec 16 22:46:45.789: INFO: cleanMinorVersion: 20
Dec 16 22:46:45.789: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:46:45.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7300" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":65,"skipped":1142,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:46:45.806: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Dec 16 22:46:45.853: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:46:50.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6418" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":66,"skipped":1148,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:46:50.211: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:46:50.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-573 create -f -'
Dec 16 22:46:50.532: INFO: stderr: ""
Dec 16 22:46:50.532: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec 16 22:46:50.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-573 create -f -'
Dec 16 22:46:50.847: INFO: stderr: ""
Dec 16 22:46:50.847: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 16 22:46:51.853: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 22:46:51.853: INFO: Found 0 / 1
Dec 16 22:46:52.852: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 22:46:52.852: INFO: Found 1 / 1
Dec 16 22:46:52.852: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 16 22:46:52.855: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 22:46:52.855: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 16 22:46:52.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-573 describe pod agnhost-primary-ctxsp'
Dec 16 22:46:52.948: INFO: stderr: ""
Dec 16 22:46:52.948: INFO: stdout: "Name:         agnhost-primary-ctxsp\nNamespace:    kubectl-573\nPriority:     0\nNode:         ip-172-31-1-217/172.31.1.217\nStart Time:   Wed, 16 Dec 2020 22:46:50 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 192.168.140.27/32\n              cni.projectcalico.org/podIPs: 192.168.140.27/32\nStatus:       Running\nIP:           192.168.140.27\nIPs:\n  IP:           192.168.140.27\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://d078d019f3b6e17f8a749f269b0b5cb650a7a65734b9778d3188399b2d3c5bb2\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 16 Dec 2020 22:46:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5l5cb (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-5l5cb:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-5l5cb\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-573/agnhost-primary-ctxsp to ip-172-31-1-217\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Dec 16 22:46:52.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-573 describe rc agnhost-primary'
Dec 16 22:46:53.071: INFO: stderr: ""
Dec 16 22:46:53.071: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-573\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-ctxsp\n"
Dec 16 22:46:53.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-573 describe service agnhost-primary'
Dec 16 22:46:53.163: INFO: stderr: ""
Dec 16 22:46:53.163: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-573\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.101.1.121\nIPs:               10.101.1.121\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.140.27:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 16 22:46:53.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-573 describe node ip-172-31-1-201'
Dec 16 22:46:53.307: INFO: stderr: ""
Dec 16 22:46:53.307: INFO: stdout: "Name:               ip-172-31-1-201\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-1-201\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.1.201/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.252.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 16 Dec 2020 21:14:05 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-1-201\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 16 Dec 2020 22:46:48 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 16 Dec 2020 21:15:10 +0000   Wed, 16 Dec 2020 21:15:10 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 16 Dec 2020 22:45:18 +0000   Wed, 16 Dec 2020 21:14:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 16 Dec 2020 22:45:18 +0000   Wed, 16 Dec 2020 21:14:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 16 Dec 2020 22:45:18 +0000   Wed, 16 Dec 2020 21:14:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 16 Dec 2020 22:45:18 +0000   Wed, 16 Dec 2020 21:15:10 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.1.201\n  Hostname:    ip-172-31-1-201\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30428560Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3969528Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  28042960850\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3867128Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ec288e20503d801a35474454c6452760\n  System UUID:                ec288e20-503d-801a-3547-4454c6452760\n  Boot ID:                    8e305035-5f3a-4b43-9359-463220546b29\n  Kernel Version:             5.4.0-1015-aws\n  OS Image:                   Ubuntu 20.04 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.3\n  Kubelet Version:            v1.20.0\n  Kube-Proxy Version:         v1.20.0\nPodCIDR:                      192.168.1.0/24\nPodCIDRs:                     192.168.1.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-8cbsh                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         92m\n  kube-system                 etcd-ip-172-31-1-201                                       100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         92m\n  kube-system                 kube-apiserver-ip-172-31-1-201                             250m (12%)    0 (0%)      0 (0%)           0 (0%)         92m\n  kube-system                 kube-controller-manager-ip-172-31-1-201                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         92m\n  kube-system                 kube-proxy-fxwpn                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\n  kube-system                 kube-scheduler-ip-172-31-1-201                             100m (5%)     0 (0%)      0 (0%)           0 (0%)         92m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-v99z4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                900m (45%)  0 (0%)\n  memory             100Mi (2%)  0 (0%)\n  ephemeral-storage  100Mi (0%)  0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Dec 16 22:46:53.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-573 describe namespace kubectl-573'
Dec 16 22:46:53.385: INFO: stderr: ""
Dec 16 22:46:53.385: INFO: stdout: "Name:         kubectl-573\nLabels:       e2e-framework=kubectl\n              e2e-run=da087946-2ccf-414e-aecc-270729779099\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:46:53.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-573" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":67,"skipped":1153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:46:53.407: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8448 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8448;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8448 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8448;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8448.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8448.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8448.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8448.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8448.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8448.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8448.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8448.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8448.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8448.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8448.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 7.138.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.138.7_udp@PTR;check="$$(dig +tcp +noall +answer +search 7.138.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.138.7_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8448 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8448;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8448 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8448;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8448.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8448.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8448.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8448.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8448.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8448.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8448.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8448.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8448.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8448.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8448.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8448.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 7.138.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.138.7_udp@PTR;check="$$(dig +tcp +noall +answer +search 7.138.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.138.7_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 16 22:46:55.512: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.516: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.519: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.523: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.526: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.531: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.535: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.538: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.542: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.546: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.556: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.563: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.568: INFO: Unable to read 10.106.138.7_udp@PTR from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.573: INFO: Unable to read 10.106.138.7_tcp@PTR from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.578: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.586: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.589: INFO: Unable to read jessie_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.592: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.595: INFO: Unable to read jessie_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.598: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.602: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.605: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.607: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.610: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.613: INFO: Unable to read jessie_udp@PodARecord from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.616: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.618: INFO: Unable to read 10.106.138.7_udp@PTR from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.621: INFO: Unable to read 10.106.138.7_tcp@PTR from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:46:55.621: INFO: Lookups using dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8448 wheezy_tcp@dns-test-service.dns-8448 wheezy_udp@dns-test-service.dns-8448.svc wheezy_tcp@dns-test-service.dns-8448.svc wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc wheezy_udp@_http._tcp.test-service-2.dns-8448.svc wheezy_tcp@_http._tcp.test-service-2.dns-8448.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.106.138.7_udp@PTR 10.106.138.7_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8448 jessie_tcp@dns-test-service.dns-8448 jessie_udp@dns-test-service.dns-8448.svc jessie_tcp@dns-test-service.dns-8448.svc jessie_udp@_http._tcp.dns-test-service.dns-8448.svc jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc jessie_udp@_http._tcp.test-service-2.dns-8448.svc jessie_tcp@_http._tcp.test-service-2.dns-8448.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.106.138.7_udp@PTR 10.106.138.7_tcp@PTR]

Dec 16 22:47:00.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.629: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.632: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.635: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.640: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.643: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.648: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.652: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.697: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.701: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.708: INFO: Unable to read jessie_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.712: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.716: INFO: Unable to read jessie_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.724: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.731: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:00.765: INFO: Lookups using dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8448 wheezy_tcp@dns-test-service.dns-8448 wheezy_udp@dns-test-service.dns-8448.svc wheezy_tcp@dns-test-service.dns-8448.svc wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8448 jessie_tcp@dns-test-service.dns-8448 jessie_udp@dns-test-service.dns-8448.svc jessie_tcp@dns-test-service.dns-8448.svc jessie_udp@_http._tcp.dns-test-service.dns-8448.svc jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc]

Dec 16 22:47:05.626: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.630: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.637: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.640: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.643: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.645: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.649: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.667: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.670: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.673: INFO: Unable to read jessie_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.675: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.679: INFO: Unable to read jessie_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.682: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.684: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.686: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:05.703: INFO: Lookups using dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8448 wheezy_tcp@dns-test-service.dns-8448 wheezy_udp@dns-test-service.dns-8448.svc wheezy_tcp@dns-test-service.dns-8448.svc wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8448 jessie_tcp@dns-test-service.dns-8448 jessie_udp@dns-test-service.dns-8448.svc jessie_tcp@dns-test-service.dns-8448.svc jessie_udp@_http._tcp.dns-test-service.dns-8448.svc jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc]

Dec 16 22:47:10.645: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.670: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.679: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.713: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.729: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.762: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.775: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.778: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.855: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.859: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.862: INFO: Unable to read jessie_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.865: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.867: INFO: Unable to read jessie_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.871: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.874: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.876: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:10.893: INFO: Lookups using dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8448 wheezy_tcp@dns-test-service.dns-8448 wheezy_udp@dns-test-service.dns-8448.svc wheezy_tcp@dns-test-service.dns-8448.svc wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8448 jessie_tcp@dns-test-service.dns-8448 jessie_udp@dns-test-service.dns-8448.svc jessie_tcp@dns-test-service.dns-8448.svc jessie_udp@_http._tcp.dns-test-service.dns-8448.svc jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc]

Dec 16 22:47:15.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.630: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.637: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.645: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.650: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.659: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.664: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.687: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.690: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.694: INFO: Unable to read jessie_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.698: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.701: INFO: Unable to read jessie_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.704: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.708: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.711: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:15.732: INFO: Lookups using dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8448 wheezy_tcp@dns-test-service.dns-8448 wheezy_udp@dns-test-service.dns-8448.svc wheezy_tcp@dns-test-service.dns-8448.svc wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8448 jessie_tcp@dns-test-service.dns-8448 jessie_udp@dns-test-service.dns-8448.svc jessie_tcp@dns-test-service.dns-8448.svc jessie_udp@_http._tcp.dns-test-service.dns-8448.svc jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc]

Dec 16 22:47:20.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.628: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.631: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.634: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.638: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.641: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.644: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.648: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.668: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.675: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.680: INFO: Unable to read jessie_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.684: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.687: INFO: Unable to read jessie_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.691: INFO: Unable to read jessie_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.694: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.698: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:20.719: INFO: Lookups using dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8448 wheezy_tcp@dns-test-service.dns-8448 wheezy_udp@dns-test-service.dns-8448.svc wheezy_tcp@dns-test-service.dns-8448.svc wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8448 jessie_tcp@dns-test-service.dns-8448 jessie_udp@dns-test-service.dns-8448.svc jessie_tcp@dns-test-service.dns-8448.svc jessie_udp@_http._tcp.dns-test-service.dns-8448.svc jessie_tcp@_http._tcp.dns-test-service.dns-8448.svc]

Dec 16 22:47:25.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:25.628: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:25.631: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:25.633: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448 from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:25.636: INFO: Unable to read wheezy_udp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:25.639: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:25.642: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:25.645: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc from pod dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980: the server could not find the requested resource (get pods dns-test-56677602-6325-42bd-82c8-c078fd45b980)
Dec 16 22:47:25.709: INFO: Lookups using dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8448 wheezy_tcp@dns-test-service.dns-8448 wheezy_udp@dns-test-service.dns-8448.svc wheezy_tcp@dns-test-service.dns-8448.svc wheezy_udp@_http._tcp.dns-test-service.dns-8448.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8448.svc]

Dec 16 22:47:30.715: INFO: DNS probes using dns-8448/dns-test-56677602-6325-42bd-82c8-c078fd45b980 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:47:30.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8448" for this suite.

• [SLOW TEST:37.461 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":68,"skipped":1183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:47:30.871: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 16 22:47:31.417: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 22:47:34.443: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:47:34.448: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:47:35.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8905" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":69,"skipped":1205,"failed":0}
S
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:47:35.673: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:47:35.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6296" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":70,"skipped":1206,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:47:35.768: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Dec 16 22:47:35.816: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2623 proxy --unix-socket=/tmp/kubectl-proxy-unix592033873/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:47:35.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2623" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":71,"skipped":1211,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:47:35.924: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-3404f1fd-3908-4aad-a057-68406b91b2fe
STEP: Creating a pod to test consume secrets
Dec 16 22:47:36.053: INFO: Waiting up to 5m0s for pod "pod-secrets-7d5506dd-3997-417d-b3bd-baf75544df34" in namespace "secrets-9388" to be "Succeeded or Failed"
Dec 16 22:47:36.062: INFO: Pod "pod-secrets-7d5506dd-3997-417d-b3bd-baf75544df34": Phase="Pending", Reason="", readiness=false. Elapsed: 8.79357ms
Dec 16 22:47:38.081: INFO: Pod "pod-secrets-7d5506dd-3997-417d-b3bd-baf75544df34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028285693s
Dec 16 22:47:40.092: INFO: Pod "pod-secrets-7d5506dd-3997-417d-b3bd-baf75544df34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038934237s
STEP: Saw pod success
Dec 16 22:47:40.092: INFO: Pod "pod-secrets-7d5506dd-3997-417d-b3bd-baf75544df34" satisfied condition "Succeeded or Failed"
Dec 16 22:47:40.095: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-secrets-7d5506dd-3997-417d-b3bd-baf75544df34 container secret-volume-test: <nil>
STEP: delete the pod
Dec 16 22:47:40.116: INFO: Waiting for pod pod-secrets-7d5506dd-3997-417d-b3bd-baf75544df34 to disappear
Dec 16 22:47:40.120: INFO: Pod pod-secrets-7d5506dd-3997-417d-b3bd-baf75544df34 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:47:40.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9388" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":72,"skipped":1222,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:47:40.131: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:47:40.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4783" for this suite.
STEP: Destroying namespace "nspatchtest-0ec2b093-47e8-44ad-af62-7373155bd855-912" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":73,"skipped":1230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:47:40.243: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:47:42.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7531" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":74,"skipped":1283,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:47:42.882: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:47:42.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-73" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":75,"skipped":1299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:47:42.963: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-3842/configmap-test-1f1f1037-dc4b-419d-8407-a81c3a0c8106
STEP: Creating a pod to test consume configMaps
Dec 16 22:47:43.022: INFO: Waiting up to 5m0s for pod "pod-configmaps-fbad3ccc-5e32-46ba-8e77-bd07a9768c9c" in namespace "configmap-3842" to be "Succeeded or Failed"
Dec 16 22:47:43.031: INFO: Pod "pod-configmaps-fbad3ccc-5e32-46ba-8e77-bd07a9768c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.003301ms
Dec 16 22:47:45.045: INFO: Pod "pod-configmaps-fbad3ccc-5e32-46ba-8e77-bd07a9768c9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022950962s
STEP: Saw pod success
Dec 16 22:47:45.045: INFO: Pod "pod-configmaps-fbad3ccc-5e32-46ba-8e77-bd07a9768c9c" satisfied condition "Succeeded or Failed"
Dec 16 22:47:45.052: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-configmaps-fbad3ccc-5e32-46ba-8e77-bd07a9768c9c container env-test: <nil>
STEP: delete the pod
Dec 16 22:47:45.091: INFO: Waiting for pod pod-configmaps-fbad3ccc-5e32-46ba-8e77-bd07a9768c9c to disappear
Dec 16 22:47:45.098: INFO: Pod pod-configmaps-fbad3ccc-5e32-46ba-8e77-bd07a9768c9c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:47:45.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3842" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:47:45.125: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 16 22:47:45.226: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 22:48:45.271: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Dec 16 22:48:45.295: INFO: Created pod: pod0-sched-preemption-low-priority
Dec 16 22:48:45.309: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:49:01.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5804" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.597 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":77,"skipped":1385,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:49:01.722: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4887
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Dec 16 22:49:01.831: INFO: Found 0 stateful pods, waiting for 3
Dec 16 22:49:11.848: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 22:49:11.848: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 22:49:11.848: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 16 22:49:11.878: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 16 22:49:21.930: INFO: Updating stateful set ss2
Dec 16 22:49:21.947: INFO: Waiting for Pod statefulset-4887/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 16 22:49:32.134: INFO: Found 1 stateful pods, waiting for 3
Dec 16 22:49:42.155: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 22:49:42.155: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 22:49:42.155: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 16 22:49:42.181: INFO: Updating stateful set ss2
Dec 16 22:49:42.195: INFO: Waiting for Pod statefulset-4887/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 16 22:49:52.216: INFO: Waiting for Pod statefulset-4887/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 16 22:50:02.226: INFO: Waiting for Pod statefulset-4887/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 16 22:50:12.251: INFO: Updating stateful set ss2
Dec 16 22:50:12.265: INFO: Waiting for StatefulSet statefulset-4887/ss2 to complete update
Dec 16 22:50:12.265: INFO: Waiting for Pod statefulset-4887/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 16 22:50:22.284: INFO: Waiting for StatefulSet statefulset-4887/ss2 to complete update
Dec 16 22:50:22.284: INFO: Waiting for Pod statefulset-4887/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 16 22:50:32.285: INFO: Waiting for StatefulSet statefulset-4887/ss2 to complete update
Dec 16 22:50:32.285: INFO: Waiting for Pod statefulset-4887/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 16 22:50:42.284: INFO: Waiting for StatefulSet statefulset-4887/ss2 to complete update
Dec 16 22:50:42.284: INFO: Waiting for Pod statefulset-4887/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 16 22:50:52.286: INFO: Waiting for StatefulSet statefulset-4887/ss2 to complete update
Dec 16 22:50:52.286: INFO: Waiting for Pod statefulset-4887/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 16 22:51:02.286: INFO: Waiting for StatefulSet statefulset-4887/ss2 to complete update
Dec 16 22:51:02.286: INFO: Waiting for Pod statefulset-4887/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 16 22:51:12.284: INFO: Deleting all statefulset in ns statefulset-4887
Dec 16 22:51:12.298: INFO: Scaling statefulset ss2 to 0
Dec 16 22:52:32.336: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 22:52:32.339: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:52:32.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4887" for this suite.

• [SLOW TEST:210.655 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":78,"skipped":1386,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:52:32.378: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Dec 16 22:52:32.421: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec 16 22:52:32.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 create -f -'
Dec 16 22:52:32.762: INFO: stderr: ""
Dec 16 22:52:32.762: INFO: stdout: "service/agnhost-replica created\n"
Dec 16 22:52:32.762: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec 16 22:52:32.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 create -f -'
Dec 16 22:52:33.219: INFO: stderr: ""
Dec 16 22:52:33.219: INFO: stdout: "service/agnhost-primary created\n"
Dec 16 22:52:33.220: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 16 22:52:33.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 create -f -'
Dec 16 22:52:33.497: INFO: stderr: ""
Dec 16 22:52:33.497: INFO: stdout: "service/frontend created\n"
Dec 16 22:52:33.497: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 16 22:52:33.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 create -f -'
Dec 16 22:52:33.700: INFO: stderr: ""
Dec 16 22:52:33.700: INFO: stdout: "deployment.apps/frontend created\n"
Dec 16 22:52:33.700: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 16 22:52:33.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 create -f -'
Dec 16 22:52:33.950: INFO: stderr: ""
Dec 16 22:52:33.950: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec 16 22:52:33.950: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 16 22:52:33.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 create -f -'
Dec 16 22:52:34.237: INFO: stderr: ""
Dec 16 22:52:34.237: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Dec 16 22:52:34.237: INFO: Waiting for all frontend pods to be Running.
Dec 16 22:52:39.289: INFO: Waiting for frontend to serve content.
Dec 16 22:52:39.303: INFO: Trying to add a new entry to the guestbook.
Dec 16 22:52:39.312: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 16 22:52:39.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 delete --grace-period=0 --force -f -'
Dec 16 22:52:39.457: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 22:52:39.457: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Dec 16 22:52:39.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 delete --grace-period=0 --force -f -'
Dec 16 22:52:39.611: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 22:52:39.611: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 16 22:52:39.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 delete --grace-period=0 --force -f -'
Dec 16 22:52:39.738: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 22:52:39.738: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 16 22:52:39.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 delete --grace-period=0 --force -f -'
Dec 16 22:52:39.853: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 22:52:39.854: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 16 22:52:39.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 delete --grace-period=0 --force -f -'
Dec 16 22:52:39.956: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 22:52:39.956: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 16 22:52:39.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6045 delete --grace-period=0 --force -f -'
Dec 16 22:52:40.144: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 22:52:40.144: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:52:40.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6045" for this suite.

• [SLOW TEST:7.779 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":79,"skipped":1395,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:52:40.157: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Dec 16 22:52:40.211: INFO: Waiting up to 5m0s for pod "downward-api-8f3d5b26-03dc-4b8b-bfe6-8a0652e0c6e3" in namespace "downward-api-3113" to be "Succeeded or Failed"
Dec 16 22:52:40.213: INFO: Pod "downward-api-8f3d5b26-03dc-4b8b-bfe6-8a0652e0c6e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355958ms
Dec 16 22:52:42.220: INFO: Pod "downward-api-8f3d5b26-03dc-4b8b-bfe6-8a0652e0c6e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008407999s
STEP: Saw pod success
Dec 16 22:52:42.220: INFO: Pod "downward-api-8f3d5b26-03dc-4b8b-bfe6-8a0652e0c6e3" satisfied condition "Succeeded or Failed"
Dec 16 22:52:42.222: INFO: Trying to get logs from node ip-172-31-1-217 pod downward-api-8f3d5b26-03dc-4b8b-bfe6-8a0652e0c6e3 container dapi-container: <nil>
STEP: delete the pod
Dec 16 22:52:42.251: INFO: Waiting for pod downward-api-8f3d5b26-03dc-4b8b-bfe6-8a0652e0c6e3 to disappear
Dec 16 22:52:42.253: INFO: Pod downward-api-8f3d5b26-03dc-4b8b-bfe6-8a0652e0c6e3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:52:42.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3113" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":80,"skipped":1398,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:52:42.266: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:52:44.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5447" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":81,"skipped":1398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:52:44.361: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:52:44.445: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 16 22:52:44.456: INFO: Number of nodes with available pods: 0
Dec 16 22:52:44.456: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 16 22:52:44.478: INFO: Number of nodes with available pods: 0
Dec 16 22:52:44.478: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:45.484: INFO: Number of nodes with available pods: 0
Dec 16 22:52:45.484: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:46.483: INFO: Number of nodes with available pods: 1
Dec 16 22:52:46.483: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 16 22:52:46.501: INFO: Number of nodes with available pods: 1
Dec 16 22:52:46.501: INFO: Number of running nodes: 0, number of available pods: 1
Dec 16 22:52:47.509: INFO: Number of nodes with available pods: 0
Dec 16 22:52:47.509: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 16 22:52:47.527: INFO: Number of nodes with available pods: 0
Dec 16 22:52:47.528: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:48.534: INFO: Number of nodes with available pods: 0
Dec 16 22:52:48.534: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:49.534: INFO: Number of nodes with available pods: 0
Dec 16 22:52:49.534: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:50.534: INFO: Number of nodes with available pods: 0
Dec 16 22:52:50.534: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:51.533: INFO: Number of nodes with available pods: 0
Dec 16 22:52:51.533: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:52.533: INFO: Number of nodes with available pods: 0
Dec 16 22:52:52.533: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:53.538: INFO: Number of nodes with available pods: 0
Dec 16 22:52:53.538: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:54.535: INFO: Number of nodes with available pods: 0
Dec 16 22:52:54.535: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:55.534: INFO: Number of nodes with available pods: 0
Dec 16 22:52:55.534: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:56.535: INFO: Number of nodes with available pods: 0
Dec 16 22:52:56.535: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:57.533: INFO: Number of nodes with available pods: 0
Dec 16 22:52:57.534: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:58.533: INFO: Number of nodes with available pods: 0
Dec 16 22:52:58.533: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:52:59.533: INFO: Number of nodes with available pods: 1
Dec 16 22:52:59.533: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7575, will wait for the garbage collector to delete the pods
Dec 16 22:52:59.600: INFO: Deleting DaemonSet.extensions daemon-set took: 8.091284ms
Dec 16 22:52:59.700: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.274025ms
Dec 16 22:53:07.909: INFO: Number of nodes with available pods: 0
Dec 16 22:53:07.909: INFO: Number of running nodes: 0, number of available pods: 0
Dec 16 22:53:07.912: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26559"},"items":null}

Dec 16 22:53:07.915: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26559"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:53:07.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7575" for this suite.

• [SLOW TEST:23.586 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":82,"skipped":1429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:53:07.948: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-1714c2d9-9ac2-4c64-a7ad-b5b9dc58a754
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:53:07.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3836" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":83,"skipped":1462,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:53:08.014: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Dec 16 22:53:08.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 create -f -'
Dec 16 22:53:08.266: INFO: stderr: ""
Dec 16 22:53:08.266: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 16 22:53:08.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 22:53:08.357: INFO: stderr: ""
Dec 16 22:53:08.357: INFO: stdout: "update-demo-nautilus-6xjjr update-demo-nautilus-wqhqd "
Dec 16 22:53:08.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get pods update-demo-nautilus-6xjjr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 22:53:08.439: INFO: stderr: ""
Dec 16 22:53:08.439: INFO: stdout: ""
Dec 16 22:53:08.439: INFO: update-demo-nautilus-6xjjr is created but not running
Dec 16 22:53:13.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 22:53:13.525: INFO: stderr: ""
Dec 16 22:53:13.525: INFO: stdout: "update-demo-nautilus-6xjjr update-demo-nautilus-wqhqd "
Dec 16 22:53:13.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get pods update-demo-nautilus-6xjjr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 22:53:13.601: INFO: stderr: ""
Dec 16 22:53:13.601: INFO: stdout: "true"
Dec 16 22:53:13.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get pods update-demo-nautilus-6xjjr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 22:53:13.695: INFO: stderr: ""
Dec 16 22:53:13.695: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 16 22:53:13.695: INFO: validating pod update-demo-nautilus-6xjjr
Dec 16 22:53:13.699: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 22:53:13.699: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 22:53:13.699: INFO: update-demo-nautilus-6xjjr is verified up and running
Dec 16 22:53:13.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get pods update-demo-nautilus-wqhqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 22:53:13.787: INFO: stderr: ""
Dec 16 22:53:13.787: INFO: stdout: "true"
Dec 16 22:53:13.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get pods update-demo-nautilus-wqhqd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 22:53:13.878: INFO: stderr: ""
Dec 16 22:53:13.878: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 16 22:53:13.878: INFO: validating pod update-demo-nautilus-wqhqd
Dec 16 22:53:13.885: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 22:53:13.886: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 22:53:13.886: INFO: update-demo-nautilus-wqhqd is verified up and running
STEP: using delete to clean up resources
Dec 16 22:53:13.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 delete --grace-period=0 --force -f -'
Dec 16 22:53:14.036: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 22:53:14.036: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 16 22:53:14.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get rc,svc -l name=update-demo --no-headers'
Dec 16 22:53:14.118: INFO: stderr: "No resources found in kubectl-3511 namespace.\n"
Dec 16 22:53:14.118: INFO: stdout: ""
Dec 16 22:53:14.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 16 22:53:14.197: INFO: stderr: ""
Dec 16 22:53:14.197: INFO: stdout: "update-demo-nautilus-6xjjr\nupdate-demo-nautilus-wqhqd\n"
Dec 16 22:53:14.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get rc,svc -l name=update-demo --no-headers'
Dec 16 22:53:14.778: INFO: stderr: "No resources found in kubectl-3511 namespace.\n"
Dec 16 22:53:14.778: INFO: stdout: ""
Dec 16 22:53:14.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3511 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 16 22:53:14.853: INFO: stderr: ""
Dec 16 22:53:14.853: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:53:14.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3511" for this suite.

• [SLOW TEST:6.851 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":84,"skipped":1467,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:53:14.866: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:53:18.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9115" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1469,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:53:18.946: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 16 22:53:18.986: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6140  6927f234-e40b-4b85-aa1a-9cf94da5ea85 26699 0 2020-12-16 22:53:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 22:53:18.986: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6140  6927f234-e40b-4b85-aa1a-9cf94da5ea85 26699 0 2020-12-16 22:53:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 16 22:53:29.010: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6140  6927f234-e40b-4b85-aa1a-9cf94da5ea85 26757 0 2020-12-16 22:53:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 22:53:29.010: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6140  6927f234-e40b-4b85-aa1a-9cf94da5ea85 26757 0 2020-12-16 22:53:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 16 22:53:39.048: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6140  6927f234-e40b-4b85-aa1a-9cf94da5ea85 26776 0 2020-12-16 22:53:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 22:53:39.048: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6140  6927f234-e40b-4b85-aa1a-9cf94da5ea85 26776 0 2020-12-16 22:53:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 16 22:53:49.064: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6140  6927f234-e40b-4b85-aa1a-9cf94da5ea85 26795 0 2020-12-16 22:53:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 22:53:49.065: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6140  6927f234-e40b-4b85-aa1a-9cf94da5ea85 26795 0 2020-12-16 22:53:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 16 22:53:59.079: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6140  c4d81f61-0f05-4133-9ef2-76a789881e9c 26815 0 2020-12-16 22:53:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 22:53:59.079: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6140  c4d81f61-0f05-4133-9ef2-76a789881e9c 26815 0 2020-12-16 22:53:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 16 22:54:09.096: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6140  c4d81f61-0f05-4133-9ef2-76a789881e9c 26836 0 2020-12-16 22:53:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 22:54:09.096: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6140  c4d81f61-0f05-4133-9ef2-76a789881e9c 26836 0 2020-12-16 22:53:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-16 22:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:54:19.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6140" for this suite.

• [SLOW TEST:60.170 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":86,"skipped":1482,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:54:19.116: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 22:54:19.194: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fb17d10-0c37-4839-96b4-9777cfc926a0" in namespace "downward-api-7504" to be "Succeeded or Failed"
Dec 16 22:54:19.197: INFO: Pod "downwardapi-volume-7fb17d10-0c37-4839-96b4-9777cfc926a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.772557ms
Dec 16 22:54:21.203: INFO: Pod "downwardapi-volume-7fb17d10-0c37-4839-96b4-9777cfc926a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009430313s
STEP: Saw pod success
Dec 16 22:54:21.204: INFO: Pod "downwardapi-volume-7fb17d10-0c37-4839-96b4-9777cfc926a0" satisfied condition "Succeeded or Failed"
Dec 16 22:54:21.206: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-7fb17d10-0c37-4839-96b4-9777cfc926a0 container client-container: <nil>
STEP: delete the pod
Dec 16 22:54:21.238: INFO: Waiting for pod downwardapi-volume-7fb17d10-0c37-4839-96b4-9777cfc926a0 to disappear
Dec 16 22:54:21.241: INFO: Pod downwardapi-volume-7fb17d10-0c37-4839-96b4-9777cfc926a0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:54:21.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7504" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":87,"skipped":1499,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:54:21.273: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:54:21.336: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-facdc889-1c6f-4737-8bee-4e0e70e937f2" in namespace "security-context-test-4439" to be "Succeeded or Failed"
Dec 16 22:54:21.341: INFO: Pod "alpine-nnp-false-facdc889-1c6f-4737-8bee-4e0e70e937f2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.104118ms
Dec 16 22:54:23.349: INFO: Pod "alpine-nnp-false-facdc889-1c6f-4737-8bee-4e0e70e937f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012792561s
Dec 16 22:54:25.354: INFO: Pod "alpine-nnp-false-facdc889-1c6f-4737-8bee-4e0e70e937f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018050052s
Dec 16 22:54:25.354: INFO: Pod "alpine-nnp-false-facdc889-1c6f-4737-8bee-4e0e70e937f2" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:54:25.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4439" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":88,"skipped":1506,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:54:25.370: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 16 22:54:25.691: INFO: Pod name wrapped-volume-race-593e4217-19d3-45ed-a888-fab69ea1a959: Found 4 pods out of 5
Dec 16 22:54:30.713: INFO: Pod name wrapped-volume-race-593e4217-19d3-45ed-a888-fab69ea1a959: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-593e4217-19d3-45ed-a888-fab69ea1a959 in namespace emptydir-wrapper-521, will wait for the garbage collector to delete the pods
Dec 16 22:54:42.818: INFO: Deleting ReplicationController wrapped-volume-race-593e4217-19d3-45ed-a888-fab69ea1a959 took: 21.986092ms
Dec 16 22:54:43.421: INFO: Terminating ReplicationController wrapped-volume-race-593e4217-19d3-45ed-a888-fab69ea1a959 pods took: 602.833644ms
STEP: Creating RC which spawns configmap-volume pods
Dec 16 22:55:18.155: INFO: Pod name wrapped-volume-race-8570d0a3-e743-49e0-be78-6756bdab7170: Found 0 pods out of 5
Dec 16 22:55:23.168: INFO: Pod name wrapped-volume-race-8570d0a3-e743-49e0-be78-6756bdab7170: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8570d0a3-e743-49e0-be78-6756bdab7170 in namespace emptydir-wrapper-521, will wait for the garbage collector to delete the pods
Dec 16 22:55:35.281: INFO: Deleting ReplicationController wrapped-volume-race-8570d0a3-e743-49e0-be78-6756bdab7170 took: 10.17081ms
Dec 16 22:55:35.881: INFO: Terminating ReplicationController wrapped-volume-race-8570d0a3-e743-49e0-be78-6756bdab7170 pods took: 600.327077ms
STEP: Creating RC which spawns configmap-volume pods
Dec 16 22:56:17.937: INFO: Pod name wrapped-volume-race-e1eeff4a-1bd0-4b3e-a2f5-f1ca39c15f23: Found 0 pods out of 5
Dec 16 22:56:22.952: INFO: Pod name wrapped-volume-race-e1eeff4a-1bd0-4b3e-a2f5-f1ca39c15f23: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e1eeff4a-1bd0-4b3e-a2f5-f1ca39c15f23 in namespace emptydir-wrapper-521, will wait for the garbage collector to delete the pods
Dec 16 22:56:33.041: INFO: Deleting ReplicationController wrapped-volume-race-e1eeff4a-1bd0-4b3e-a2f5-f1ca39c15f23 took: 10.576165ms
Dec 16 22:56:33.642: INFO: Terminating ReplicationController wrapped-volume-race-e1eeff4a-1bd0-4b3e-a2f5-f1ca39c15f23 pods took: 600.518106ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:57:18.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-521" for this suite.

• [SLOW TEST:173.137 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":89,"skipped":1508,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:57:18.508: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 22:57:18.582: INFO: Create a RollingUpdate DaemonSet
Dec 16 22:57:18.587: INFO: Check that daemon pods launch on every node of the cluster
Dec 16 22:57:18.592: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:18.592: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:18.592: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:18.599: INFO: Number of nodes with available pods: 0
Dec 16 22:57:18.599: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:57:19.604: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:19.604: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:19.604: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:19.607: INFO: Number of nodes with available pods: 0
Dec 16 22:57:19.607: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 22:57:20.610: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:20.610: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:20.610: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:20.615: INFO: Number of nodes with available pods: 2
Dec 16 22:57:20.615: INFO: Number of running nodes: 2, number of available pods: 2
Dec 16 22:57:20.615: INFO: Update the DaemonSet to trigger a rollout
Dec 16 22:57:20.636: INFO: Updating DaemonSet daemon-set
Dec 16 22:57:28.662: INFO: Roll back the DaemonSet before rollout is complete
Dec 16 22:57:28.685: INFO: Updating DaemonSet daemon-set
Dec 16 22:57:28.685: INFO: Make sure DaemonSet rollback is complete
Dec 16 22:57:28.692: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:28.692: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:28.707: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:28.707: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:28.707: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:29.717: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:29.717: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:29.721: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:29.721: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:29.721: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:30.721: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:30.721: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:30.725: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:30.725: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:30.725: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:31.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:31.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:31.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:31.717: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:31.717: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:32.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:32.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:32.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:32.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:32.719: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:33.712: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:33.712: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:33.715: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:33.716: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:33.716: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:34.716: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:34.716: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:34.719: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:34.719: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:34.719: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:35.712: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:35.712: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:35.716: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:35.716: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:35.716: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:36.715: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:36.715: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:36.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:36.719: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:36.719: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:37.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:37.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:37.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:37.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:37.718: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:38.717: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:38.717: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:38.722: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:38.722: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:38.722: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:39.732: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:39.732: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:39.753: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:39.753: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:39.753: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:40.713: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:40.713: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:40.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:40.717: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:40.717: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:41.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:41.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:41.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:41.719: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:41.719: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:42.724: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:42.724: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:42.736: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:42.736: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:42.736: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:43.713: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:43.713: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:43.716: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:43.717: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:43.717: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:44.713: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:44.713: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:44.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:44.717: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:44.717: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:45.712: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:45.712: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:45.716: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:45.716: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:45.716: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:46.713: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:46.713: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:46.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:46.717: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:46.717: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:47.724: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:47.724: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:47.728: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:47.728: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:47.728: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:48.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:48.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:48.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:48.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:48.718: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:49.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:49.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:49.720: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:49.720: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:49.720: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:50.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:50.715: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:50.719: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:50.719: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:50.719: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:51.713: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:51.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:51.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:51.717: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:51.717: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:52.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:52.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:52.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:52.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:52.718: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:53.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:53.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:53.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:53.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:53.718: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:54.719: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:54.719: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:54.723: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:54.723: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:54.723: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:55.712: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:55.713: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:55.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:55.717: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:55.717: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:56.713: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:56.713: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:56.722: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:56.722: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:56.722: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:57.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:57.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:57.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:57.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:57.718: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:58.716: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:58.716: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:58.719: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:58.719: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:58.719: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:59.716: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:57:59.716: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:57:59.720: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:59.720: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:57:59.720: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:00.715: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:00.717: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:00.721: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:00.721: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:00.721: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:01.724: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:01.724: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:01.732: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:01.732: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:01.732: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:02.715: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:02.715: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:02.719: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:02.719: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:02.719: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:03.712: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:03.712: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:03.716: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:03.716: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:03.716: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:04.713: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:04.713: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:04.716: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:04.716: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:04.716: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:05.712: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:05.712: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:05.716: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:05.716: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:05.716: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:06.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:06.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:06.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:06.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:06.718: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:07.713: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:07.713: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:07.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:07.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:07.718: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:08.713: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:08.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:08.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:08.717: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:08.717: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:09.726: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:09.727: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:09.731: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:09.731: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:09.731: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:10.719: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:10.719: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:10.725: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:10.725: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:10.725: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:11.713: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:11.713: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:11.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:11.717: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:11.717: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:12.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:12.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:12.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:12.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:12.718: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:13.715: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:13.715: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:13.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:13.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:13.718: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:14.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:14.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:14.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:14.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:14.718: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:15.712: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:15.712: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:15.717: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:15.717: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:15.717: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:16.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:16.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:16.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:16.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:16.719: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:17.714: INFO: Wrong image for pod: daemon-set-5lkvk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 16 22:58:17.714: INFO: Pod daemon-set-5lkvk is not available
Dec 16 22:58:17.719: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:17.719: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:17.719: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:18.714: INFO: Pod daemon-set-qbtl4 is not available
Dec 16 22:58:18.721: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:18.722: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 22:58:18.722: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9586, will wait for the garbage collector to delete the pods
Dec 16 22:58:18.793: INFO: Deleting DaemonSet.extensions daemon-set took: 11.258684ms
Dec 16 22:58:19.394: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.23787ms
Dec 16 22:59:17.906: INFO: Number of nodes with available pods: 0
Dec 16 22:59:17.906: INFO: Number of running nodes: 0, number of available pods: 0
Dec 16 22:59:17.908: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28550"},"items":null}

Dec 16 22:59:17.912: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28550"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:59:17.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9586" for this suite.

• [SLOW TEST:119.427 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":90,"skipped":1510,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:59:17.937: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-8551
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8551
STEP: Deleting pre-stop pod
Dec 16 22:59:27.047: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 22:59:27.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8551" for this suite.

• [SLOW TEST:9.160 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":91,"skipped":1522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 22:59:27.102: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:00:19.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5220" for this suite.
STEP: Destroying namespace "nsdeletetest-8265" for this suite.
Dec 16 23:00:19.272: INFO: Namespace nsdeletetest-8265 was already deleted
STEP: Destroying namespace "nsdeletetest-8535" for this suite.

• [SLOW TEST:52.179 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":92,"skipped":1569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:00:19.282: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-82080a9f-9781-4d7a-8488-043c74dd1407 in namespace container-probe-1686
Dec 16 23:00:21.344: INFO: Started pod liveness-82080a9f-9781-4d7a-8488-043c74dd1407 in namespace container-probe-1686
STEP: checking the pod's current state and verifying that restartCount is present
Dec 16 23:00:21.349: INFO: Initial restart count of pod liveness-82080a9f-9781-4d7a-8488-043c74dd1407 is 0
Dec 16 23:00:43.434: INFO: Restart count of pod container-probe-1686/liveness-82080a9f-9781-4d7a-8488-043c74dd1407 is now 1 (22.084575137s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:00:43.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1686" for this suite.

• [SLOW TEST:24.189 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":93,"skipped":1591,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:00:43.471: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 23:00:43.526: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05178cb2-7133-477f-a698-7a55c731a795" in namespace "projected-8606" to be "Succeeded or Failed"
Dec 16 23:00:43.529: INFO: Pod "downwardapi-volume-05178cb2-7133-477f-a698-7a55c731a795": Phase="Pending", Reason="", readiness=false. Elapsed: 2.581034ms
Dec 16 23:00:45.542: INFO: Pod "downwardapi-volume-05178cb2-7133-477f-a698-7a55c731a795": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015556725s
STEP: Saw pod success
Dec 16 23:00:45.542: INFO: Pod "downwardapi-volume-05178cb2-7133-477f-a698-7a55c731a795" satisfied condition "Succeeded or Failed"
Dec 16 23:00:45.550: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-05178cb2-7133-477f-a698-7a55c731a795 container client-container: <nil>
STEP: delete the pod
Dec 16 23:00:45.583: INFO: Waiting for pod downwardapi-volume-05178cb2-7133-477f-a698-7a55c731a795 to disappear
Dec 16 23:00:45.586: INFO: Pod downwardapi-volume-05178cb2-7133-477f-a698-7a55c731a795 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:00:45.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8606" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":94,"skipped":1596,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:00:45.597: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 16 23:00:45.652: INFO: Waiting up to 5m0s for pod "pod-83286699-f9e2-401a-b3e7-acc4eabfbfb0" in namespace "emptydir-6097" to be "Succeeded or Failed"
Dec 16 23:00:45.655: INFO: Pod "pod-83286699-f9e2-401a-b3e7-acc4eabfbfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592124ms
Dec 16 23:00:47.662: INFO: Pod "pod-83286699-f9e2-401a-b3e7-acc4eabfbfb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009965807s
STEP: Saw pod success
Dec 16 23:00:47.662: INFO: Pod "pod-83286699-f9e2-401a-b3e7-acc4eabfbfb0" satisfied condition "Succeeded or Failed"
Dec 16 23:00:47.665: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-83286699-f9e2-401a-b3e7-acc4eabfbfb0 container test-container: <nil>
STEP: delete the pod
Dec 16 23:00:47.708: INFO: Waiting for pod pod-83286699-f9e2-401a-b3e7-acc4eabfbfb0 to disappear
Dec 16 23:00:47.715: INFO: Pod pod-83286699-f9e2-401a-b3e7-acc4eabfbfb0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:00:47.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6097" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1596,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:00:47.739: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:00:47.783: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 16 23:00:51.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 --namespace=crd-publish-openapi-9656 create -f -'
Dec 16 23:00:51.699: INFO: stderr: ""
Dec 16 23:00:51.699: INFO: stdout: "e2e-test-crd-publish-openapi-3714-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 16 23:00:51.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 --namespace=crd-publish-openapi-9656 delete e2e-test-crd-publish-openapi-3714-crds test-foo'
Dec 16 23:00:51.816: INFO: stderr: ""
Dec 16 23:00:51.816: INFO: stdout: "e2e-test-crd-publish-openapi-3714-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 16 23:00:51.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 --namespace=crd-publish-openapi-9656 apply -f -'
Dec 16 23:00:52.104: INFO: stderr: ""
Dec 16 23:00:52.104: INFO: stdout: "e2e-test-crd-publish-openapi-3714-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 16 23:00:52.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 --namespace=crd-publish-openapi-9656 delete e2e-test-crd-publish-openapi-3714-crds test-foo'
Dec 16 23:00:52.184: INFO: stderr: ""
Dec 16 23:00:52.184: INFO: stdout: "e2e-test-crd-publish-openapi-3714-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 16 23:00:52.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 --namespace=crd-publish-openapi-9656 create -f -'
Dec 16 23:00:52.423: INFO: rc: 1
Dec 16 23:00:52.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 --namespace=crd-publish-openapi-9656 apply -f -'
Dec 16 23:00:52.619: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 16 23:00:52.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 --namespace=crd-publish-openapi-9656 create -f -'
Dec 16 23:00:52.805: INFO: rc: 1
Dec 16 23:00:52.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 --namespace=crd-publish-openapi-9656 apply -f -'
Dec 16 23:00:53.000: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 16 23:00:53.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 explain e2e-test-crd-publish-openapi-3714-crds'
Dec 16 23:00:53.247: INFO: stderr: ""
Dec 16 23:00:53.247: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3714-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 16 23:00:53.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 explain e2e-test-crd-publish-openapi-3714-crds.metadata'
Dec 16 23:00:53.453: INFO: stderr: ""
Dec 16 23:00:53.453: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3714-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 16 23:00:53.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 explain e2e-test-crd-publish-openapi-3714-crds.spec'
Dec 16 23:00:53.694: INFO: stderr: ""
Dec 16 23:00:53.694: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3714-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 16 23:00:53.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 explain e2e-test-crd-publish-openapi-3714-crds.spec.bars'
Dec 16 23:00:53.886: INFO: stderr: ""
Dec 16 23:00:53.886: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3714-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 16 23:00:53.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-9656 explain e2e-test-crd-publish-openapi-3714-crds.spec.bars2'
Dec 16 23:00:54.072: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:00:57.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9656" for this suite.

• [SLOW TEST:9.834 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":96,"skipped":1603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:00:57.574: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:00:57.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7971" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":97,"skipped":1688,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:00:57.707: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Dec 16 23:00:57.754: INFO: Waiting up to 5m0s for pod "var-expansion-259aaef8-cabe-4e0f-b6d6-f95bdf391403" in namespace "var-expansion-3319" to be "Succeeded or Failed"
Dec 16 23:00:57.758: INFO: Pod "var-expansion-259aaef8-cabe-4e0f-b6d6-f95bdf391403": Phase="Pending", Reason="", readiness=false. Elapsed: 3.729123ms
Dec 16 23:00:59.767: INFO: Pod "var-expansion-259aaef8-cabe-4e0f-b6d6-f95bdf391403": Phase="Running", Reason="", readiness=true. Elapsed: 2.012806662s
Dec 16 23:01:01.776: INFO: Pod "var-expansion-259aaef8-cabe-4e0f-b6d6-f95bdf391403": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02213145s
STEP: Saw pod success
Dec 16 23:01:01.777: INFO: Pod "var-expansion-259aaef8-cabe-4e0f-b6d6-f95bdf391403" satisfied condition "Succeeded or Failed"
Dec 16 23:01:01.780: INFO: Trying to get logs from node ip-172-31-1-217 pod var-expansion-259aaef8-cabe-4e0f-b6d6-f95bdf391403 container dapi-container: <nil>
STEP: delete the pod
Dec 16 23:01:01.867: INFO: Waiting for pod var-expansion-259aaef8-cabe-4e0f-b6d6-f95bdf391403 to disappear
Dec 16 23:01:01.889: INFO: Pod var-expansion-259aaef8-cabe-4e0f-b6d6-f95bdf391403 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:01:01.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3319" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":98,"skipped":1709,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:01:01.948: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Dec 16 23:01:02.073: INFO: PodSpec: initContainers in spec.initContainers
Dec 16 23:01:50.888: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-47696a80-cbfa-4370-af32-9d3b261ef07d", GenerateName:"", Namespace:"init-container-8972", SelfLink:"", UID:"24e8d1fd-477d-4864-9554-0331ddf07343", ResourceVersion:"29203", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63743756462, loc:(*time.Location)(0x7962e20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"73165251"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.140.8/32", "cni.projectcalico.org/podIPs":"192.168.140.8/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004afff00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004afff20)}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004afff40), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004afff60)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004afff80), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004afffa0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-hwhb6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc007d88900), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hwhb6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hwhb6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hwhb6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0069917c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-1-217", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002efe0e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006991850)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006991870)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006991878), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00699187c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005926890), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743756462, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743756462, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743756462, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743756462, loc:(*time.Location)(0x7962e20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.1.217", PodIP:"192.168.140.8", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.140.8"}}, StartTime:(*v1.Time)(0xc004afffc0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002efe1c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002efe230)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://4d8448b9335248ea6a2faeb9938a55e9ef02c576cdab1efc161e25ed78a660fe", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b78000), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004afffe0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0069918ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:01:50.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8972" for this suite.

• [SLOW TEST:48.992 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":99,"skipped":1732,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:01:50.941: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:01:51.422: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 16 23:01:53.435: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743756511, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743756511, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743756511, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743756511, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:01:56.457: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:01:56.460: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2510-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:01:57.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-837" for this suite.
STEP: Destroying namespace "webhook-837-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.731 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":100,"skipped":1741,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:01:57.673: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 16 23:01:57.741: INFO: Waiting up to 5m0s for pod "pod-e9ac9a72-7de7-4979-bfe0-e17d0422a9ad" in namespace "emptydir-9019" to be "Succeeded or Failed"
Dec 16 23:01:57.744: INFO: Pod "pod-e9ac9a72-7de7-4979-bfe0-e17d0422a9ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.000994ms
Dec 16 23:01:59.754: INFO: Pod "pod-e9ac9a72-7de7-4979-bfe0-e17d0422a9ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012829827s
Dec 16 23:02:01.766: INFO: Pod "pod-e9ac9a72-7de7-4979-bfe0-e17d0422a9ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025019022s
STEP: Saw pod success
Dec 16 23:02:01.766: INFO: Pod "pod-e9ac9a72-7de7-4979-bfe0-e17d0422a9ad" satisfied condition "Succeeded or Failed"
Dec 16 23:02:01.770: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-e9ac9a72-7de7-4979-bfe0-e17d0422a9ad container test-container: <nil>
STEP: delete the pod
Dec 16 23:02:01.933: INFO: Waiting for pod pod-e9ac9a72-7de7-4979-bfe0-e17d0422a9ad to disappear
Dec 16 23:02:01.949: INFO: Pod pod-e9ac9a72-7de7-4979-bfe0-e17d0422a9ad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:02:01.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9019" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":101,"skipped":1762,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:02:02.005: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 16 23:02:02.224: INFO: Waiting up to 5m0s for pod "pod-2f66f387-b08d-40fa-ac95-79fa6ab79156" in namespace "emptydir-4690" to be "Succeeded or Failed"
Dec 16 23:02:02.227: INFO: Pod "pod-2f66f387-b08d-40fa-ac95-79fa6ab79156": Phase="Pending", Reason="", readiness=false. Elapsed: 3.008054ms
Dec 16 23:02:04.234: INFO: Pod "pod-2f66f387-b08d-40fa-ac95-79fa6ab79156": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01055882s
STEP: Saw pod success
Dec 16 23:02:04.234: INFO: Pod "pod-2f66f387-b08d-40fa-ac95-79fa6ab79156" satisfied condition "Succeeded or Failed"
Dec 16 23:02:04.237: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-2f66f387-b08d-40fa-ac95-79fa6ab79156 container test-container: <nil>
STEP: delete the pod
Dec 16 23:02:04.257: INFO: Waiting for pod pod-2f66f387-b08d-40fa-ac95-79fa6ab79156 to disappear
Dec 16 23:02:04.260: INFO: Pod pod-2f66f387-b08d-40fa-ac95-79fa6ab79156 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:02:04.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4690" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":1784,"failed":0}

------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:02:04.271: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 16 23:02:06.862: INFO: Successfully updated pod "adopt-release-pgtnl"
STEP: Checking that the Job readopts the Pod
Dec 16 23:02:06.863: INFO: Waiting up to 15m0s for pod "adopt-release-pgtnl" in namespace "job-7247" to be "adopted"
Dec 16 23:02:06.872: INFO: Pod "adopt-release-pgtnl": Phase="Running", Reason="", readiness=true. Elapsed: 9.817138ms
Dec 16 23:02:08.879: INFO: Pod "adopt-release-pgtnl": Phase="Running", Reason="", readiness=true. Elapsed: 2.016413714s
Dec 16 23:02:08.879: INFO: Pod "adopt-release-pgtnl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 16 23:02:09.390: INFO: Successfully updated pod "adopt-release-pgtnl"
STEP: Checking that the Job releases the Pod
Dec 16 23:02:09.390: INFO: Waiting up to 15m0s for pod "adopt-release-pgtnl" in namespace "job-7247" to be "released"
Dec 16 23:02:09.401: INFO: Pod "adopt-release-pgtnl": Phase="Running", Reason="", readiness=true. Elapsed: 10.562497ms
Dec 16 23:02:09.401: INFO: Pod "adopt-release-pgtnl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:02:09.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7247" for this suite.

• [SLOW TEST:5.157 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":103,"skipped":1784,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:02:09.429: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8498.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8498.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 16 23:02:13.547: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:13.550: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:13.553: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:13.556: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:13.566: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:13.568: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:13.571: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:13.576: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:13.599: INFO: Lookups using dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local]

Dec 16 23:02:18.606: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:18.610: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:18.613: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:18.616: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:18.624: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:18.627: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:18.629: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:18.632: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:18.639: INFO: Lookups using dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local]

Dec 16 23:02:23.603: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:23.609: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:23.617: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:23.622: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:23.633: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:23.636: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:23.640: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:23.643: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:23.649: INFO: Lookups using dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local]

Dec 16 23:02:28.606: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:28.611: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:28.617: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:28.621: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:28.634: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:28.637: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:28.642: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:28.645: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:28.654: INFO: Lookups using dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local]

Dec 16 23:02:33.614: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:33.622: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:33.632: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:33.647: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:33.688: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:33.701: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:33.711: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:33.723: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:33.747: INFO: Lookups using dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local]

Dec 16 23:02:38.607: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:38.612: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:38.617: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:38.623: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:38.657: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:38.663: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:38.670: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:38.674: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local from pod dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438: the server could not find the requested resource (get pods dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438)
Dec 16 23:02:38.682: INFO: Lookups using dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8498.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8498.svc.cluster.local jessie_udp@dns-test-service-2.dns-8498.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8498.svc.cluster.local]

Dec 16 23:02:43.649: INFO: DNS probes using dns-8498/dns-test-2e7fdcae-ea4b-4776-b5b5-f320f780f438 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:02:43.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8498" for this suite.

• [SLOW TEST:34.365 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":104,"skipped":1790,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:02:43.793: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Dec 16 23:04:44.435: INFO: Successfully updated pod "var-expansion-101ddc85-69e3-4621-8ce2-7b8d45ac4230"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Dec 16 23:04:46.454: INFO: Deleting pod "var-expansion-101ddc85-69e3-4621-8ce2-7b8d45ac4230" in namespace "var-expansion-4639"
Dec 16 23:04:46.464: INFO: Wait up to 5m0s for pod "var-expansion-101ddc85-69e3-4621-8ce2-7b8d45ac4230" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:05:28.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4639" for this suite.

• [SLOW TEST:164.697 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":105,"skipped":1800,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:05:28.497: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Dec 16 23:05:28.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-479 create -f -'
Dec 16 23:05:28.922: INFO: stderr: ""
Dec 16 23:05:28.923: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Dec 16 23:05:28.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-479 diff -f -'
Dec 16 23:05:29.363: INFO: rc: 1
Dec 16 23:05:29.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-479 delete -f -'
Dec 16 23:05:29.456: INFO: stderr: ""
Dec 16 23:05:29.456: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:05:29.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-479" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":106,"skipped":1833,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:05:29.473: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-873243cd-91d1-45ba-b451-8628066ffd09
STEP: Creating secret with name s-test-opt-upd-f7335a7e-17e4-4830-9fa1-451eba0ac28e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-873243cd-91d1-45ba-b451-8628066ffd09
STEP: Updating secret s-test-opt-upd-f7335a7e-17e4-4830-9fa1-451eba0ac28e
STEP: Creating secret with name s-test-opt-create-890ba861-2b12-4b3c-ac73-90dd2cbe0ad0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:05:33.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4689" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":107,"skipped":1837,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:05:33.639: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 16 23:05:36.214: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9145 pod-service-account-f9175bd2-6027-40f4-ad87-4711837eefeb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 16 23:05:36.414: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9145 pod-service-account-f9175bd2-6027-40f4-ad87-4711837eefeb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 16 23:05:36.610: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9145 pod-service-account-f9175bd2-6027-40f4-ad87-4711837eefeb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:05:36.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9145" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":108,"skipped":1845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:05:36.813: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-32a76432-38c2-4c3c-bddc-d20ea7490b04
STEP: Creating a pod to test consume configMaps
Dec 16 23:05:36.867: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6030a87f-13b9-4142-ab86-5fef3c32a45a" in namespace "projected-2924" to be "Succeeded or Failed"
Dec 16 23:05:36.871: INFO: Pod "pod-projected-configmaps-6030a87f-13b9-4142-ab86-5fef3c32a45a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.791734ms
Dec 16 23:05:38.878: INFO: Pod "pod-projected-configmaps-6030a87f-13b9-4142-ab86-5fef3c32a45a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010626121s
STEP: Saw pod success
Dec 16 23:05:38.878: INFO: Pod "pod-projected-configmaps-6030a87f-13b9-4142-ab86-5fef3c32a45a" satisfied condition "Succeeded or Failed"
Dec 16 23:05:38.891: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-configmaps-6030a87f-13b9-4142-ab86-5fef3c32a45a container agnhost-container: <nil>
STEP: delete the pod
Dec 16 23:05:38.942: INFO: Waiting for pod pod-projected-configmaps-6030a87f-13b9-4142-ab86-5fef3c32a45a to disappear
Dec 16 23:05:38.955: INFO: Pod pod-projected-configmaps-6030a87f-13b9-4142-ab86-5fef3c32a45a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:05:38.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2924" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":109,"skipped":1879,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:05:38.978: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Dec 16 23:05:41.566: INFO: Successfully updated pod "annotationupdate37f78c65-02ec-4eec-a92e-7daf5a34d8a6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:05:45.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8407" for this suite.

• [SLOW TEST:6.637 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":1892,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:05:45.615: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2511.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2511.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2511.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2511.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2511.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2511.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 16 23:05:47.703: INFO: DNS probes using dns-2511/dns-test-9349bc7b-4500-4919-826e-c68620819d15 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:05:47.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2511" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":111,"skipped":1895,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:05:47.768: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-c1dc5a29-25b4-4778-a50f-2b977dfd6042
STEP: Creating a pod to test consume configMaps
Dec 16 23:05:47.841: INFO: Waiting up to 5m0s for pod "pod-configmaps-95068995-3ae9-4029-b2ba-fbd6ae5983d9" in namespace "configmap-7804" to be "Succeeded or Failed"
Dec 16 23:05:47.844: INFO: Pod "pod-configmaps-95068995-3ae9-4029-b2ba-fbd6ae5983d9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.091156ms
Dec 16 23:05:49.851: INFO: Pod "pod-configmaps-95068995-3ae9-4029-b2ba-fbd6ae5983d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009920647s
STEP: Saw pod success
Dec 16 23:05:49.851: INFO: Pod "pod-configmaps-95068995-3ae9-4029-b2ba-fbd6ae5983d9" satisfied condition "Succeeded or Failed"
Dec 16 23:05:49.853: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-configmaps-95068995-3ae9-4029-b2ba-fbd6ae5983d9 container agnhost-container: <nil>
STEP: delete the pod
Dec 16 23:05:49.872: INFO: Waiting for pod pod-configmaps-95068995-3ae9-4029-b2ba-fbd6ae5983d9 to disappear
Dec 16 23:05:49.874: INFO: Pod pod-configmaps-95068995-3ae9-4029-b2ba-fbd6ae5983d9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:05:49.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7804" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":112,"skipped":1903,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:05:49.884: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-92fb2870-1440-4a66-893f-affe6ca38cae
STEP: Creating a pod to test consume secrets
Dec 16 23:05:49.977: INFO: Waiting up to 5m0s for pod "pod-secrets-d93e8dd9-6903-46b3-a861-12608d6cf92d" in namespace "secrets-4870" to be "Succeeded or Failed"
Dec 16 23:05:49.980: INFO: Pod "pod-secrets-d93e8dd9-6903-46b3-a861-12608d6cf92d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.749859ms
Dec 16 23:05:51.991: INFO: Pod "pod-secrets-d93e8dd9-6903-46b3-a861-12608d6cf92d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013580191s
STEP: Saw pod success
Dec 16 23:05:51.991: INFO: Pod "pod-secrets-d93e8dd9-6903-46b3-a861-12608d6cf92d" satisfied condition "Succeeded or Failed"
Dec 16 23:05:52.004: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-secrets-d93e8dd9-6903-46b3-a861-12608d6cf92d container secret-volume-test: <nil>
STEP: delete the pod
Dec 16 23:05:52.059: INFO: Waiting for pod pod-secrets-d93e8dd9-6903-46b3-a861-12608d6cf92d to disappear
Dec 16 23:05:52.069: INFO: Pod pod-secrets-d93e8dd9-6903-46b3-a861-12608d6cf92d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:05:52.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4870" for this suite.
STEP: Destroying namespace "secret-namespace-5740" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":1908,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:05:52.125: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 16 23:05:52.172: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:05:55.647: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:06:08.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-271" for this suite.

• [SLOW TEST:16.822 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":114,"skipped":1923,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:06:08.947: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2088
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-2088
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2088
Dec 16 23:06:09.065: INFO: Found 0 stateful pods, waiting for 1
Dec 16 23:06:19.072: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 16 23:06:19.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 23:06:19.340: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 23:06:19.340: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 23:06:19.340: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 23:06:19.344: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 16 23:06:29.351: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 23:06:29.351: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 23:06:29.409: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:29.409: INFO: ss-0  ip-172-31-1-217  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  }]
Dec 16 23:06:29.409: INFO: 
Dec 16 23:06:29.409: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 16 23:06:30.414: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.966091324s
Dec 16 23:06:31.421: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.960903717s
Dec 16 23:06:32.426: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.954141906s
Dec 16 23:06:33.433: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.94817911s
Dec 16 23:06:34.439: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.941537544s
Dec 16 23:06:35.445: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.936152713s
Dec 16 23:06:36.456: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.929821336s
Dec 16 23:06:37.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.91823989s
Dec 16 23:06:38.469: INFO: Verifying statefulset ss doesn't scale past 3 for another 911.336811ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2088
Dec 16 23:06:39.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:06:39.649: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 23:06:39.649: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 23:06:39.649: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 23:06:39.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:06:39.944: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 16 23:06:39.944: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 23:06:39.944: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 23:06:39.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:06:40.115: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 16 23:06:40.115: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 23:06:40.115: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 23:06:40.121: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 23:06:40.121: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 23:06:40.121: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 16 23:06:40.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 23:06:40.269: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 23:06:40.269: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 23:06:40.269: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 23:06:40.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 23:06:40.437: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 23:06:40.437: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 23:06:40.437: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 23:06:40.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 23:06:40.576: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 23:06:40.576: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 23:06:40.576: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 23:06:40.576: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 23:06:40.581: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 16 23:06:50.595: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 23:06:50.595: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 23:06:50.595: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 23:06:50.617: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:50.617: INFO: ss-0  ip-172-31-1-217  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  }]
Dec 16 23:06:50.617: INFO: ss-1  ip-172-31-1-217  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:50.617: INFO: ss-2  ip-172-31-1-217  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:50.617: INFO: 
Dec 16 23:06:50.617: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 16 23:06:51.626: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:51.626: INFO: ss-0  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  }]
Dec 16 23:06:51.626: INFO: ss-1  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:51.626: INFO: ss-2  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:51.626: INFO: 
Dec 16 23:06:51.626: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 16 23:06:52.633: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:52.633: INFO: ss-0  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  }]
Dec 16 23:06:52.633: INFO: ss-1  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:52.633: INFO: ss-2  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:52.633: INFO: 
Dec 16 23:06:52.633: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 16 23:06:53.641: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:53.641: INFO: ss-0  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  }]
Dec 16 23:06:53.641: INFO: ss-1  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:53.641: INFO: ss-2  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:53.641: INFO: 
Dec 16 23:06:53.641: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 16 23:06:54.647: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:54.647: INFO: ss-0  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  }]
Dec 16 23:06:54.647: INFO: ss-1  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:54.647: INFO: ss-2  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:54.647: INFO: 
Dec 16 23:06:54.647: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 16 23:06:55.654: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:55.654: INFO: ss-0  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  }]
Dec 16 23:06:55.654: INFO: ss-1  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:55.654: INFO: ss-2  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:55.654: INFO: 
Dec 16 23:06:55.654: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 16 23:06:56.662: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:56.663: INFO: ss-0  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  }]
Dec 16 23:06:56.663: INFO: ss-1  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:56.663: INFO: ss-2  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:56.663: INFO: 
Dec 16 23:06:56.664: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 16 23:06:57.678: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:57.679: INFO: ss-0  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:09 +0000 UTC  }]
Dec 16 23:06:57.679: INFO: ss-1  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:57.679: INFO: ss-2  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:57.679: INFO: 
Dec 16 23:06:57.679: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 16 23:06:58.685: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:58.685: INFO: ss-1  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:58.685: INFO: 
Dec 16 23:06:58.685: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 16 23:06:59.691: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec 16 23:06:59.691: INFO: ss-1  ip-172-31-1-217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:06:29 +0000 UTC  }]
Dec 16 23:06:59.691: INFO: 
Dec 16 23:06:59.692: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2088
Dec 16 23:07:00.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:07:00.821: INFO: rc: 1
Dec 16 23:07:00.821: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 16 23:07:10.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:07:10.927: INFO: rc: 1
Dec 16 23:07:10.927: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 16 23:07:20.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:07:21.128: INFO: rc: 1
Dec 16 23:07:21.128: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:07:31.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:07:31.220: INFO: rc: 1
Dec 16 23:07:31.220: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:07:41.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:07:41.373: INFO: rc: 1
Dec 16 23:07:41.373: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:07:51.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:07:51.456: INFO: rc: 1
Dec 16 23:07:51.456: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:08:01.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:08:01.551: INFO: rc: 1
Dec 16 23:08:01.551: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:08:11.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:08:11.667: INFO: rc: 1
Dec 16 23:08:11.667: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:08:21.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:08:21.763: INFO: rc: 1
Dec 16 23:08:21.763: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:08:31.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:08:31.875: INFO: rc: 1
Dec 16 23:08:31.875: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:08:41.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:08:42.003: INFO: rc: 1
Dec 16 23:08:42.003: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:08:52.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:08:52.104: INFO: rc: 1
Dec 16 23:08:52.104: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:09:02.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:09:02.281: INFO: rc: 1
Dec 16 23:09:02.281: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:09:12.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:09:12.393: INFO: rc: 1
Dec 16 23:09:12.393: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:09:22.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:09:22.488: INFO: rc: 1
Dec 16 23:09:22.489: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:09:32.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:09:32.736: INFO: rc: 1
Dec 16 23:09:32.736: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:09:42.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:09:42.863: INFO: rc: 1
Dec 16 23:09:42.863: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:09:52.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:09:52.954: INFO: rc: 1
Dec 16 23:09:52.954: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:10:02.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:10:03.046: INFO: rc: 1
Dec 16 23:10:03.046: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:10:13.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:10:13.158: INFO: rc: 1
Dec 16 23:10:13.158: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:10:23.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:10:23.312: INFO: rc: 1
Dec 16 23:10:23.312: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:10:33.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:10:33.453: INFO: rc: 1
Dec 16 23:10:33.453: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:10:43.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:10:43.548: INFO: rc: 1
Dec 16 23:10:43.548: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:10:53.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:10:53.864: INFO: rc: 1
Dec 16 23:10:53.864: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:11:03.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:11:03.977: INFO: rc: 1
Dec 16 23:11:03.977: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:11:13.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:11:14.076: INFO: rc: 1
Dec 16 23:11:14.076: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:11:24.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:11:24.210: INFO: rc: 1
Dec 16 23:11:24.210: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:11:34.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:11:34.294: INFO: rc: 1
Dec 16 23:11:34.294: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:11:44.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:11:44.398: INFO: rc: 1
Dec 16 23:11:44.398: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:11:54.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:11:54.486: INFO: rc: 1
Dec 16 23:11:54.486: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 16 23:12:04.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-2088 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:12:04.638: INFO: rc: 1
Dec 16 23:12:04.638: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Dec 16 23:12:04.638: INFO: Scaling statefulset ss to 0
Dec 16 23:12:04.649: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 16 23:12:04.652: INFO: Deleting all statefulset in ns statefulset-2088
Dec 16 23:12:04.655: INFO: Scaling statefulset ss to 0
Dec 16 23:12:04.663: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 23:12:04.665: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:12:04.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2088" for this suite.

• [SLOW TEST:355.750 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":115,"skipped":1930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:12:04.698: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:12:04.758: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 23:12:06.767: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = false)
Dec 16 23:12:08.766: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = false)
Dec 16 23:12:10.765: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = false)
Dec 16 23:12:12.772: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = false)
Dec 16 23:12:14.766: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = false)
Dec 16 23:12:16.772: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = false)
Dec 16 23:12:18.765: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = false)
Dec 16 23:12:20.766: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = false)
Dec 16 23:12:22.770: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = false)
Dec 16 23:12:24.766: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = false)
Dec 16 23:12:26.766: INFO: The status of Pod test-webserver-8de2de2a-46cb-475e-bc9a-8a3f48535f34 is Running (Ready = true)
Dec 16 23:12:26.770: INFO: Container started at 2020-12-16 23:12:05 +0000 UTC, pod became ready at 2020-12-16 23:12:25 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:12:26.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3963" for this suite.

• [SLOW TEST:22.083 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":1965,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:12:26.783: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5893
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5893
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5893
Dec 16 23:12:26.862: INFO: Found 0 stateful pods, waiting for 1
Dec 16 23:12:36.880: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 16 23:12:36.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-5893 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 23:12:37.061: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 23:12:37.061: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 23:12:37.061: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 23:12:37.064: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 16 23:12:47.082: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 23:12:47.082: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 23:12:47.102: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999744s
Dec 16 23:12:48.108: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992444471s
Dec 16 23:12:49.114: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98698927s
Dec 16 23:12:50.119: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981014791s
Dec 16 23:12:51.125: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.975625784s
Dec 16 23:12:52.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.969093547s
Dec 16 23:12:53.138: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.962462168s
Dec 16 23:12:54.144: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.956750735s
Dec 16 23:12:55.150: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.950444563s
Dec 16 23:12:56.156: INFO: Verifying statefulset ss doesn't scale past 1 for another 944.835879ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5893
Dec 16 23:12:57.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-5893 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:12:57.333: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 23:12:57.333: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 23:12:57.333: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 23:12:57.336: INFO: Found 1 stateful pods, waiting for 3
Dec 16 23:13:07.353: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 23:13:07.353: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 23:13:07.353: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 16 23:13:07.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-5893 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 23:13:07.631: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 23:13:07.631: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 23:13:07.631: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 23:13:07.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-5893 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 23:13:07.800: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 23:13:07.800: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 23:13:07.800: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 23:13:07.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-5893 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 23:13:07.970: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 23:13:07.970: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 23:13:07.970: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 23:13:07.970: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 23:13:07.980: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 16 23:13:18.001: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 23:13:18.001: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 23:13:18.001: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 23:13:18.041: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999382s
Dec 16 23:13:19.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981743119s
Dec 16 23:13:20.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976063945s
Dec 16 23:13:21.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.966387044s
Dec 16 23:13:22.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.960665957s
Dec 16 23:13:23.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.955196803s
Dec 16 23:13:24.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.949537277s
Dec 16 23:13:25.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.944048588s
Dec 16 23:13:26.100: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.938473648s
Dec 16 23:13:27.106: INFO: Verifying statefulset ss doesn't scale past 3 for another 922.013993ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5893
Dec 16 23:13:28.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-5893 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:13:28.338: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 23:13:28.338: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 23:13:28.338: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 23:13:28.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-5893 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:13:28.511: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 23:13:28.511: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 23:13:28.511: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 23:13:28.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-5893 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:13:28.682: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 23:13:28.683: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 23:13:28.683: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 23:13:28.683: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 16 23:15:18.718: INFO: Deleting all statefulset in ns statefulset-5893
Dec 16 23:15:18.722: INFO: Scaling statefulset ss to 0
Dec 16 23:15:18.732: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 23:15:18.735: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:15:18.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5893" for this suite.

• [SLOW TEST:171.982 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":117,"skipped":1979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:15:18.766: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:15:35.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6002" for this suite.

• [SLOW TEST:16.303 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":118,"skipped":2011,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:15:35.069: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:15:35.504: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:15:38.535: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:15:38.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9106" for this suite.
STEP: Destroying namespace "webhook-9106-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":119,"skipped":2020,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:15:38.795: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 16 23:15:38.850: INFO: Waiting up to 5m0s for pod "pod-35fc595d-6f1c-43ae-9aa9-5300b6282bae" in namespace "emptydir-4294" to be "Succeeded or Failed"
Dec 16 23:15:38.853: INFO: Pod "pod-35fc595d-6f1c-43ae-9aa9-5300b6282bae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.062784ms
Dec 16 23:15:40.858: INFO: Pod "pod-35fc595d-6f1c-43ae-9aa9-5300b6282bae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00798952s
STEP: Saw pod success
Dec 16 23:15:40.858: INFO: Pod "pod-35fc595d-6f1c-43ae-9aa9-5300b6282bae" satisfied condition "Succeeded or Failed"
Dec 16 23:15:40.861: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-35fc595d-6f1c-43ae-9aa9-5300b6282bae container test-container: <nil>
STEP: delete the pod
Dec 16 23:15:40.889: INFO: Waiting for pod pod-35fc595d-6f1c-43ae-9aa9-5300b6282bae to disappear
Dec 16 23:15:40.891: INFO: Pod pod-35fc595d-6f1c-43ae-9aa9-5300b6282bae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:15:40.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4294" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":2024,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:15:40.904: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:15:41.475: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 16 23:15:43.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743757341, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743757341, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743757341, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743757341, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:15:46.516: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:15:46.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8918" for this suite.
STEP: Destroying namespace "webhook-8918-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.723 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":121,"skipped":2026,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:15:46.628: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 16 23:15:46.689: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 16 23:15:51.699: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:15:51.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-468" for this suite.

• [SLOW TEST:5.119 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":122,"skipped":2072,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:15:51.748: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-09338a0d-ee07-4714-83f9-3cd2621cafd9
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:15:53.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2629" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":123,"skipped":2077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:15:53.902: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 23:15:53.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b22e0b18-a9f0-4f2c-a6fa-c2ca780e96e4" in namespace "projected-7856" to be "Succeeded or Failed"
Dec 16 23:15:53.984: INFO: Pod "downwardapi-volume-b22e0b18-a9f0-4f2c-a6fa-c2ca780e96e4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.050711ms
Dec 16 23:15:56.006: INFO: Pod "downwardapi-volume-b22e0b18-a9f0-4f2c-a6fa-c2ca780e96e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037361598s
STEP: Saw pod success
Dec 16 23:15:56.007: INFO: Pod "downwardapi-volume-b22e0b18-a9f0-4f2c-a6fa-c2ca780e96e4" satisfied condition "Succeeded or Failed"
Dec 16 23:15:56.011: INFO: Trying to get logs from node ip-172-31-6-174 pod downwardapi-volume-b22e0b18-a9f0-4f2c-a6fa-c2ca780e96e4 container client-container: <nil>
STEP: delete the pod
Dec 16 23:15:56.062: INFO: Waiting for pod downwardapi-volume-b22e0b18-a9f0-4f2c-a6fa-c2ca780e96e4 to disappear
Dec 16 23:15:56.065: INFO: Pod downwardapi-volume-b22e0b18-a9f0-4f2c-a6fa-c2ca780e96e4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:15:56.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7856" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":124,"skipped":2129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:15:56.076: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:16:03.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7831" for this suite.

• [SLOW TEST:7.076 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":125,"skipped":2155,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:16:03.152: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 16 23:16:03.198: INFO: Waiting up to 5m0s for pod "pod-f727da55-0703-467c-b39b-bc857ab4397e" in namespace "emptydir-7489" to be "Succeeded or Failed"
Dec 16 23:16:03.202: INFO: Pod "pod-f727da55-0703-467c-b39b-bc857ab4397e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033865ms
Dec 16 23:16:05.208: INFO: Pod "pod-f727da55-0703-467c-b39b-bc857ab4397e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009861596s
Dec 16 23:16:07.216: INFO: Pod "pod-f727da55-0703-467c-b39b-bc857ab4397e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017030896s
STEP: Saw pod success
Dec 16 23:16:07.216: INFO: Pod "pod-f727da55-0703-467c-b39b-bc857ab4397e" satisfied condition "Succeeded or Failed"
Dec 16 23:16:07.218: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-f727da55-0703-467c-b39b-bc857ab4397e container test-container: <nil>
STEP: delete the pod
Dec 16 23:16:07.239: INFO: Waiting for pod pod-f727da55-0703-467c-b39b-bc857ab4397e to disappear
Dec 16 23:16:07.242: INFO: Pod pod-f727da55-0703-467c-b39b-bc857ab4397e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:16:07.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7489" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":126,"skipped":2164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:16:07.258: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:16:07.315: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 16 23:16:07.325: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:07.325: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:07.325: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:07.331: INFO: Number of nodes with available pods: 0
Dec 16 23:16:07.331: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:16:08.343: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:08.343: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:08.343: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:08.354: INFO: Number of nodes with available pods: 0
Dec 16 23:16:08.354: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:16:09.337: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:09.337: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:09.337: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:09.339: INFO: Number of nodes with available pods: 2
Dec 16 23:16:09.340: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 16 23:16:09.363: INFO: Wrong image for pod: daemon-set-899cn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:09.363: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:09.366: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:09.366: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:09.366: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:10.371: INFO: Wrong image for pod: daemon-set-899cn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:10.371: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:10.374: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:10.374: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:10.374: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:11.371: INFO: Wrong image for pod: daemon-set-899cn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:11.371: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:11.375: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:11.375: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:11.375: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:12.372: INFO: Wrong image for pod: daemon-set-899cn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:12.372: INFO: Pod daemon-set-899cn is not available
Dec 16 23:16:12.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:12.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:12.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:12.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:13.380: INFO: Wrong image for pod: daemon-set-899cn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:13.380: INFO: Pod daemon-set-899cn is not available
Dec 16 23:16:13.380: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:13.383: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:13.383: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:13.383: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:14.373: INFO: Wrong image for pod: daemon-set-899cn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:14.373: INFO: Pod daemon-set-899cn is not available
Dec 16 23:16:14.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:14.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:14.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:14.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:15.372: INFO: Wrong image for pod: daemon-set-899cn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:15.372: INFO: Pod daemon-set-899cn is not available
Dec 16 23:16:15.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:15.375: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:15.375: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:15.375: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:16.380: INFO: Wrong image for pod: daemon-set-899cn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:16.380: INFO: Pod daemon-set-899cn is not available
Dec 16 23:16:16.380: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:16.390: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:16.390: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:16.390: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:17.376: INFO: Wrong image for pod: daemon-set-899cn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:17.376: INFO: Pod daemon-set-899cn is not available
Dec 16 23:16:17.376: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:17.390: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:17.390: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:17.390: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:18.376: INFO: Pod daemon-set-2zmfv is not available
Dec 16 23:16:18.376: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:18.379: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:18.380: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:18.380: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:19.372: INFO: Pod daemon-set-2zmfv is not available
Dec 16 23:16:19.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:19.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:19.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:19.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:20.379: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:20.386: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:20.386: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:20.386: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:21.376: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:21.376: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:21.379: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:21.379: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:21.379: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:22.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:22.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:22.375: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:22.375: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:22.375: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:23.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:23.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:23.385: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:23.385: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:23.385: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:24.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:24.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:24.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:24.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:24.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:25.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:25.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:25.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:25.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:25.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:26.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:26.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:26.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:26.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:26.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:27.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:27.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:27.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:27.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:27.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:28.374: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:28.374: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:28.379: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:28.379: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:28.379: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:29.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:29.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:29.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:29.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:29.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:30.374: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:30.374: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:30.379: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:30.379: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:30.379: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:31.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:31.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:31.375: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:31.375: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:31.375: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:32.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:32.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:32.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:32.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:32.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:33.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:33.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:33.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:33.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:33.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:34.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:34.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:34.378: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:34.379: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:34.379: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:35.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:35.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:35.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:35.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:35.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:36.381: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:36.381: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:36.387: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:36.387: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:36.387: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:37.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:37.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:37.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:37.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:37.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:38.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:38.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:38.375: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:38.375: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:38.375: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:39.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:39.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:39.380: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:39.380: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:39.380: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:40.375: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:40.375: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:40.378: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:40.378: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:40.378: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:41.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:41.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:41.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:41.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:41.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:42.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:42.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:42.375: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:42.375: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:42.375: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:43.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:43.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:43.375: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:43.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:43.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:44.375: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:44.375: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:44.379: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:44.379: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:44.379: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:45.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:45.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:45.375: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:45.375: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:45.375: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:46.375: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:46.375: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:46.391: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:46.391: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:46.391: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:47.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:47.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:47.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:47.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:47.378: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:48.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:48.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:48.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:48.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:48.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:49.374: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:49.374: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:49.378: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:49.378: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:49.378: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:50.370: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:50.370: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:50.373: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:50.373: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:50.373: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:51.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:51.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:51.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:51.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:51.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:52.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:52.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:52.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:52.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:52.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:53.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:53.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:53.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:53.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:53.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:54.381: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:54.381: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:54.397: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:54.397: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:54.397: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:55.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:55.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:55.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:55.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:55.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:56.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:56.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:56.378: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:56.378: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:56.378: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:57.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:57.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:57.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:57.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:57.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:58.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:58.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:58.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:58.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:58.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:59.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:16:59.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:16:59.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:59.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:16:59.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:00.380: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:00.380: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:00.393: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:00.393: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:00.394: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:01.379: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:01.379: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:01.393: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:01.394: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:01.394: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:02.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:02.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:02.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:02.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:02.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:03.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:03.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:03.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:03.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:03.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:04.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:04.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:04.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:04.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:04.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:05.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:05.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:05.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:05.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:05.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:06.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:06.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:06.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:06.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:06.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:07.373: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:07.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:07.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:07.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:07.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:08.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:08.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:08.375: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:08.375: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:08.375: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:09.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:09.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:09.377: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:09.377: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:09.377: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:10.432: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:10.432: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:10.438: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:10.438: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:10.438: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:11.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:11.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:11.383: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:11.383: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:11.383: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:12.376: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:12.376: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:12.379: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:12.379: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:12.379: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:13.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:13.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:13.384: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:13.384: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:13.384: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:14.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:14.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:14.376: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:14.376: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:14.376: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:15.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:15.373: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:15.378: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:15.378: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:15.378: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:16.374: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:16.374: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:16.378: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:16.378: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:16.378: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:17.372: INFO: Wrong image for pod: daemon-set-vh29k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Dec 16 23:17:17.372: INFO: Pod daemon-set-vh29k is not available
Dec 16 23:17:17.375: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:17.375: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:17.375: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:18.373: INFO: Pod daemon-set-d9v62 is not available
Dec 16 23:17:18.378: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:18.378: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:18.378: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 16 23:17:18.386: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:18.386: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:18.386: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:18.391: INFO: Number of nodes with available pods: 1
Dec 16 23:17:18.391: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:17:19.399: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:19.399: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:19.399: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:17:19.402: INFO: Number of nodes with available pods: 2
Dec 16 23:17:19.403: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-886, will wait for the garbage collector to delete the pods
Dec 16 23:17:19.477: INFO: Deleting DaemonSet.extensions daemon-set took: 7.535366ms
Dec 16 23:17:20.077: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.161545ms
Dec 16 23:18:17.892: INFO: Number of nodes with available pods: 0
Dec 16 23:18:17.892: INFO: Number of running nodes: 0, number of available pods: 0
Dec 16 23:18:17.897: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32822"},"items":null}

Dec 16 23:18:17.904: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32822"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:18:17.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-886" for this suite.

• [SLOW TEST:130.674 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":127,"skipped":2196,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:18:17.932: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:18:18.503: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 16 23:18:20.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743757498, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743757498, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743757498, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743757498, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:18:23.552: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:18:33.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6539" for this suite.
STEP: Destroying namespace "webhook-6539-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:16.006 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":128,"skipped":2236,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:18:33.939: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 16 23:18:34.039: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 23:19:34.100: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:19:34.104: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:19:34.162: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Dec 16 23:19:34.168: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:19:34.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-753" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:19:34.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2797" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.318 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":129,"skipped":2249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:19:34.257: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:19:34.725: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:19:37.760: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:19:37.765: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:19:39.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3588" for this suite.
STEP: Destroying namespace "webhook-3588-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.167 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":130,"skipped":2272,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:19:39.424: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Dec 16 23:19:39.479: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 23:20:39.520: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:20:39.523: INFO: Starting informer...
STEP: Starting pod...
Dec 16 23:20:39.742: INFO: Pod is running on ip-172-31-1-217. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 16 23:20:39.760: INFO: Pod wasn't evicted. Proceeding
Dec 16 23:20:39.760: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 16 23:21:54.787: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:21:54.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6043" for this suite.

• [SLOW TEST:135.385 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":131,"skipped":2274,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:21:54.811: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:22:11.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8717" for this suite.

• [SLOW TEST:16.293 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":132,"skipped":2281,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:22:11.104: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:22:11.162: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:22:18.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2827" for this suite.

• [SLOW TEST:7.349 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":133,"skipped":2283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:22:18.455: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-lvbw
STEP: Creating a pod to test atomic-volume-subpath
Dec 16 23:22:18.542: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lvbw" in namespace "subpath-3123" to be "Succeeded or Failed"
Dec 16 23:22:18.557: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Pending", Reason="", readiness=false. Elapsed: 15.223851ms
Dec 16 23:22:20.565: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Running", Reason="", readiness=true. Elapsed: 2.022496951s
Dec 16 23:22:22.569: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Running", Reason="", readiness=true. Elapsed: 4.027124419s
Dec 16 23:22:24.577: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Running", Reason="", readiness=true. Elapsed: 6.03432829s
Dec 16 23:22:26.586: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Running", Reason="", readiness=true. Elapsed: 8.043734831s
Dec 16 23:22:28.596: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Running", Reason="", readiness=true. Elapsed: 10.053351472s
Dec 16 23:22:30.611: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Running", Reason="", readiness=true. Elapsed: 12.069042778s
Dec 16 23:22:32.617: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Running", Reason="", readiness=true. Elapsed: 14.074444548s
Dec 16 23:22:34.635: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Running", Reason="", readiness=true. Elapsed: 16.09235998s
Dec 16 23:22:36.645: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Running", Reason="", readiness=true. Elapsed: 18.102853568s
Dec 16 23:22:38.660: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Running", Reason="", readiness=true. Elapsed: 20.117914877s
Dec 16 23:22:40.670: INFO: Pod "pod-subpath-test-secret-lvbw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.128023743s
STEP: Saw pod success
Dec 16 23:22:40.670: INFO: Pod "pod-subpath-test-secret-lvbw" satisfied condition "Succeeded or Failed"
Dec 16 23:22:40.673: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-subpath-test-secret-lvbw container test-container-subpath-secret-lvbw: <nil>
STEP: delete the pod
Dec 16 23:22:40.715: INFO: Waiting for pod pod-subpath-test-secret-lvbw to disappear
Dec 16 23:22:40.718: INFO: Pod pod-subpath-test-secret-lvbw no longer exists
STEP: Deleting pod pod-subpath-test-secret-lvbw
Dec 16 23:22:40.718: INFO: Deleting pod "pod-subpath-test-secret-lvbw" in namespace "subpath-3123"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:22:40.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3123" for this suite.

• [SLOW TEST:22.275 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":134,"skipped":2328,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:22:40.734: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8841
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Dec 16 23:22:40.788: INFO: Found 0 stateful pods, waiting for 3
Dec 16 23:22:50.837: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 23:22:50.837: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 23:22:50.837: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 23:22:50.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-8841 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 23:22:51.302: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 23:22:51.302: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 23:22:51.302: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 16 23:23:01.390: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 16 23:23:11.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-8841 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:23:11.611: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 23:23:11.611: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 23:23:11.611: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 23:23:31.643: INFO: Waiting for StatefulSet statefulset-8841/ss2 to complete update
Dec 16 23:23:31.643: INFO: Waiting for Pod statefulset-8841/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Dec 16 23:23:41.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-8841 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 23:23:41.853: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 23:23:41.853: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 23:23:41.853: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 23:23:51.902: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 16 23:24:02.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=statefulset-8841 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 23:24:02.287: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 23:24:02.287: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 23:24:02.287: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 23:24:12.472: INFO: Waiting for StatefulSet statefulset-8841/ss2 to complete update
Dec 16 23:24:12.472: INFO: Waiting for Pod statefulset-8841/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 16 23:24:12.472: INFO: Waiting for Pod statefulset-8841/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 16 23:24:12.474: INFO: Waiting for Pod statefulset-8841/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 16 23:24:22.518: INFO: Waiting for StatefulSet statefulset-8841/ss2 to complete update
Dec 16 23:24:22.518: INFO: Waiting for Pod statefulset-8841/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 16 23:24:22.518: INFO: Waiting for Pod statefulset-8841/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 16 23:24:32.495: INFO: Waiting for StatefulSet statefulset-8841/ss2 to complete update
Dec 16 23:24:32.495: INFO: Waiting for Pod statefulset-8841/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 16 23:24:42.495: INFO: Deleting all statefulset in ns statefulset-8841
Dec 16 23:24:42.498: INFO: Scaling statefulset ss2 to 0
Dec 16 23:26:22.548: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 23:26:22.552: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:26:22.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8841" for this suite.

• [SLOW TEST:221.861 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":135,"skipped":2338,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:26:22.594: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 16 23:26:22.663: INFO: Waiting up to 5m0s for pod "pod-373bc2b3-96dd-44fd-87a2-17edf7936558" in namespace "emptydir-8159" to be "Succeeded or Failed"
Dec 16 23:26:22.671: INFO: Pod "pod-373bc2b3-96dd-44fd-87a2-17edf7936558": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030483ms
Dec 16 23:26:24.678: INFO: Pod "pod-373bc2b3-96dd-44fd-87a2-17edf7936558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015374504s
STEP: Saw pod success
Dec 16 23:26:24.678: INFO: Pod "pod-373bc2b3-96dd-44fd-87a2-17edf7936558" satisfied condition "Succeeded or Failed"
Dec 16 23:26:24.681: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-373bc2b3-96dd-44fd-87a2-17edf7936558 container test-container: <nil>
STEP: delete the pod
Dec 16 23:26:24.716: INFO: Waiting for pod pod-373bc2b3-96dd-44fd-87a2-17edf7936558 to disappear
Dec 16 23:26:24.720: INFO: Pod pod-373bc2b3-96dd-44fd-87a2-17edf7936558 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:26:24.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8159" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:26:24.733: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:26:24.776: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5831
I1216 23:26:24.790800      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5831, replica count: 1
I1216 23:26:25.848334      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1216 23:26:26.848622      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 23:26:26.965: INFO: Created: latency-svc-xq9cj
Dec 16 23:26:26.975: INFO: Got endpoints: latency-svc-xq9cj [26.17448ms]
Dec 16 23:26:27.027: INFO: Created: latency-svc-r5q6h
Dec 16 23:26:27.038: INFO: Got endpoints: latency-svc-r5q6h [63.254925ms]
Dec 16 23:26:27.042: INFO: Created: latency-svc-j5fv9
Dec 16 23:26:27.054: INFO: Got endpoints: latency-svc-j5fv9 [79.317616ms]
Dec 16 23:26:27.060: INFO: Created: latency-svc-tvk65
Dec 16 23:26:27.070: INFO: Got endpoints: latency-svc-tvk65 [94.18214ms]
Dec 16 23:26:27.085: INFO: Created: latency-svc-hhnqw
Dec 16 23:26:27.099: INFO: Got endpoints: latency-svc-hhnqw [123.042989ms]
Dec 16 23:26:27.106: INFO: Created: latency-svc-qvrdv
Dec 16 23:26:27.112: INFO: Got endpoints: latency-svc-qvrdv [135.598205ms]
Dec 16 23:26:27.129: INFO: Created: latency-svc-pdstq
Dec 16 23:26:27.137: INFO: Got endpoints: latency-svc-pdstq [161.173542ms]
Dec 16 23:26:27.148: INFO: Created: latency-svc-6xt4q
Dec 16 23:26:27.157: INFO: Got endpoints: latency-svc-6xt4q [180.752763ms]
Dec 16 23:26:27.179: INFO: Created: latency-svc-slzlx
Dec 16 23:26:27.185: INFO: Got endpoints: latency-svc-slzlx [208.650357ms]
Dec 16 23:26:27.196: INFO: Created: latency-svc-6crpx
Dec 16 23:26:27.207: INFO: Got endpoints: latency-svc-6crpx [230.868901ms]
Dec 16 23:26:27.215: INFO: Created: latency-svc-jh42r
Dec 16 23:26:27.222: INFO: Got endpoints: latency-svc-jh42r [245.836262ms]
Dec 16 23:26:27.235: INFO: Created: latency-svc-cpvxk
Dec 16 23:26:27.261: INFO: Got endpoints: latency-svc-cpvxk [285.482843ms]
Dec 16 23:26:27.269: INFO: Created: latency-svc-p9zdn
Dec 16 23:26:27.272: INFO: Got endpoints: latency-svc-p9zdn [296.781219ms]
Dec 16 23:26:27.282: INFO: Created: latency-svc-sscm6
Dec 16 23:26:27.292: INFO: Got endpoints: latency-svc-sscm6 [315.664285ms]
Dec 16 23:26:27.297: INFO: Created: latency-svc-nnwl9
Dec 16 23:26:27.323: INFO: Got endpoints: latency-svc-nnwl9 [346.758634ms]
Dec 16 23:26:27.326: INFO: Created: latency-svc-pljf8
Dec 16 23:26:27.339: INFO: Got endpoints: latency-svc-pljf8 [363.493042ms]
Dec 16 23:26:27.345: INFO: Created: latency-svc-4llhr
Dec 16 23:26:27.357: INFO: Got endpoints: latency-svc-4llhr [318.339545ms]
Dec 16 23:26:27.363: INFO: Created: latency-svc-wql46
Dec 16 23:26:27.374: INFO: Got endpoints: latency-svc-wql46 [319.133027ms]
Dec 16 23:26:27.376: INFO: Created: latency-svc-cqlvw
Dec 16 23:26:27.386: INFO: Got endpoints: latency-svc-cqlvw [315.942229ms]
Dec 16 23:26:27.391: INFO: Created: latency-svc-2w899
Dec 16 23:26:27.401: INFO: Got endpoints: latency-svc-2w899 [301.674517ms]
Dec 16 23:26:27.407: INFO: Created: latency-svc-gbpmf
Dec 16 23:26:27.425: INFO: Created: latency-svc-zgk9c
Dec 16 23:26:27.427: INFO: Got endpoints: latency-svc-gbpmf [315.401714ms]
Dec 16 23:26:27.436: INFO: Got endpoints: latency-svc-zgk9c [298.54416ms]
Dec 16 23:26:27.438: INFO: Created: latency-svc-mdfr4
Dec 16 23:26:27.448: INFO: Got endpoints: latency-svc-mdfr4 [291.110919ms]
Dec 16 23:26:27.453: INFO: Created: latency-svc-z9nmt
Dec 16 23:26:27.461: INFO: Got endpoints: latency-svc-z9nmt [275.72584ms]
Dec 16 23:26:27.467: INFO: Created: latency-svc-cl8wm
Dec 16 23:26:27.479: INFO: Got endpoints: latency-svc-cl8wm [271.245602ms]
Dec 16 23:26:27.484: INFO: Created: latency-svc-xbxzw
Dec 16 23:26:27.492: INFO: Got endpoints: latency-svc-xbxzw [269.602717ms]
Dec 16 23:26:27.502: INFO: Created: latency-svc-56jd6
Dec 16 23:26:27.516: INFO: Got endpoints: latency-svc-56jd6 [254.283721ms]
Dec 16 23:26:27.518: INFO: Created: latency-svc-zdx4k
Dec 16 23:26:27.530: INFO: Got endpoints: latency-svc-zdx4k [257.092978ms]
Dec 16 23:26:27.536: INFO: Created: latency-svc-944sd
Dec 16 23:26:27.544: INFO: Got endpoints: latency-svc-944sd [252.168138ms]
Dec 16 23:26:27.558: INFO: Created: latency-svc-g5v4w
Dec 16 23:26:27.565: INFO: Got endpoints: latency-svc-g5v4w [242.026369ms]
Dec 16 23:26:27.572: INFO: Created: latency-svc-5cdxc
Dec 16 23:26:27.592: INFO: Got endpoints: latency-svc-5cdxc [252.215895ms]
Dec 16 23:26:27.607: INFO: Created: latency-svc-x6sgg
Dec 16 23:26:27.629: INFO: Got endpoints: latency-svc-x6sgg [270.589516ms]
Dec 16 23:26:27.635: INFO: Created: latency-svc-vwh5t
Dec 16 23:26:27.641: INFO: Got endpoints: latency-svc-vwh5t [266.480786ms]
Dec 16 23:26:27.650: INFO: Created: latency-svc-jvn9t
Dec 16 23:26:27.664: INFO: Got endpoints: latency-svc-jvn9t [277.952363ms]
Dec 16 23:26:27.671: INFO: Created: latency-svc-bfnrm
Dec 16 23:26:27.682: INFO: Got endpoints: latency-svc-bfnrm [280.965426ms]
Dec 16 23:26:27.686: INFO: Created: latency-svc-42gld
Dec 16 23:26:27.693: INFO: Got endpoints: latency-svc-42gld [266.201821ms]
Dec 16 23:26:27.713: INFO: Created: latency-svc-7wm2t
Dec 16 23:26:27.721: INFO: Got endpoints: latency-svc-7wm2t [282.11825ms]
Dec 16 23:26:27.730: INFO: Created: latency-svc-97b8k
Dec 16 23:26:27.741: INFO: Got endpoints: latency-svc-97b8k [292.610915ms]
Dec 16 23:26:27.746: INFO: Created: latency-svc-crqjn
Dec 16 23:26:27.755: INFO: Got endpoints: latency-svc-crqjn [293.828648ms]
Dec 16 23:26:27.768: INFO: Created: latency-svc-l4d7z
Dec 16 23:26:27.773: INFO: Got endpoints: latency-svc-l4d7z [293.877191ms]
Dec 16 23:26:27.784: INFO: Created: latency-svc-d5q5c
Dec 16 23:26:27.798: INFO: Got endpoints: latency-svc-d5q5c [306.087422ms]
Dec 16 23:26:27.806: INFO: Created: latency-svc-w7tbr
Dec 16 23:26:27.824: INFO: Got endpoints: latency-svc-w7tbr [307.899533ms]
Dec 16 23:26:27.826: INFO: Created: latency-svc-tkgb4
Dec 16 23:26:27.838: INFO: Got endpoints: latency-svc-tkgb4 [308.329699ms]
Dec 16 23:26:27.845: INFO: Created: latency-svc-drgl4
Dec 16 23:26:27.859: INFO: Created: latency-svc-7lslc
Dec 16 23:26:27.860: INFO: Got endpoints: latency-svc-drgl4 [315.59115ms]
Dec 16 23:26:27.868: INFO: Got endpoints: latency-svc-7lslc [303.038399ms]
Dec 16 23:26:27.888: INFO: Created: latency-svc-qvz2p
Dec 16 23:26:27.914: INFO: Got endpoints: latency-svc-qvz2p [321.912054ms]
Dec 16 23:26:27.927: INFO: Created: latency-svc-vxqf8
Dec 16 23:26:27.932: INFO: Got endpoints: latency-svc-vxqf8 [302.660954ms]
Dec 16 23:26:27.938: INFO: Created: latency-svc-74wv9
Dec 16 23:26:27.949: INFO: Got endpoints: latency-svc-74wv9 [307.688436ms]
Dec 16 23:26:27.953: INFO: Created: latency-svc-288jp
Dec 16 23:26:27.971: INFO: Got endpoints: latency-svc-288jp [307.235082ms]
Dec 16 23:26:27.975: INFO: Created: latency-svc-ffwn9
Dec 16 23:26:27.999: INFO: Got endpoints: latency-svc-ffwn9 [316.3721ms]
Dec 16 23:26:28.008: INFO: Created: latency-svc-55xkc
Dec 16 23:26:28.023: INFO: Created: latency-svc-kq84f
Dec 16 23:26:28.045: INFO: Got endpoints: latency-svc-55xkc [351.643698ms]
Dec 16 23:26:28.046: INFO: Created: latency-svc-5dxd6
Dec 16 23:26:28.099: INFO: Got endpoints: latency-svc-kq84f [378.009169ms]
Dec 16 23:26:28.102: INFO: Created: latency-svc-6mgvr
Dec 16 23:26:28.151: INFO: Got endpoints: latency-svc-5dxd6 [409.379602ms]
Dec 16 23:26:28.155: INFO: Created: latency-svc-xsmx9
Dec 16 23:26:28.186: INFO: Created: latency-svc-nh84f
Dec 16 23:26:28.190: INFO: Got endpoints: latency-svc-6mgvr [435.286263ms]
Dec 16 23:26:28.215: INFO: Created: latency-svc-cgs44
Dec 16 23:26:28.233: INFO: Created: latency-svc-96ll4
Dec 16 23:26:28.240: INFO: Got endpoints: latency-svc-xsmx9 [467.747068ms]
Dec 16 23:26:28.246: INFO: Created: latency-svc-rt4zp
Dec 16 23:26:28.259: INFO: Created: latency-svc-k4w64
Dec 16 23:26:28.289: INFO: Created: latency-svc-qs2fl
Dec 16 23:26:28.303: INFO: Got endpoints: latency-svc-nh84f [505.063017ms]
Dec 16 23:26:28.308: INFO: Created: latency-svc-tc7lt
Dec 16 23:26:28.326: INFO: Created: latency-svc-lk6mh
Dec 16 23:26:28.368: INFO: Got endpoints: latency-svc-cgs44 [544.74982ms]
Dec 16 23:26:28.374: INFO: Created: latency-svc-hxhdk
Dec 16 23:26:28.386: INFO: Got endpoints: latency-svc-96ll4 [547.833182ms]
Dec 16 23:26:28.389: INFO: Created: latency-svc-29xgk
Dec 16 23:26:28.402: INFO: Created: latency-svc-cpkk6
Dec 16 23:26:28.411: INFO: Created: latency-svc-fjtj8
Dec 16 23:26:28.423: INFO: Created: latency-svc-2rpbq
Dec 16 23:26:28.436: INFO: Got endpoints: latency-svc-rt4zp [576.384731ms]
Dec 16 23:26:28.441: INFO: Created: latency-svc-hjs4r
Dec 16 23:26:28.456: INFO: Created: latency-svc-l8hl6
Dec 16 23:26:28.469: INFO: Created: latency-svc-sjhmc
Dec 16 23:26:28.487: INFO: Got endpoints: latency-svc-k4w64 [618.120528ms]
Dec 16 23:26:28.492: INFO: Created: latency-svc-gs4cr
Dec 16 23:26:28.506: INFO: Created: latency-svc-pqnzv
Dec 16 23:26:28.522: INFO: Created: latency-svc-pq8q9
Dec 16 23:26:28.537: INFO: Created: latency-svc-m9bbx
Dec 16 23:26:28.539: INFO: Got endpoints: latency-svc-qs2fl [624.371729ms]
Dec 16 23:26:28.555: INFO: Created: latency-svc-z4rjj
Dec 16 23:26:28.589: INFO: Got endpoints: latency-svc-tc7lt [656.967766ms]
Dec 16 23:26:28.611: INFO: Created: latency-svc-9c5c9
Dec 16 23:26:28.634: INFO: Got endpoints: latency-svc-lk6mh [685.562898ms]
Dec 16 23:26:28.653: INFO: Created: latency-svc-jd6wc
Dec 16 23:26:28.685: INFO: Got endpoints: latency-svc-hxhdk [713.262556ms]
Dec 16 23:26:28.703: INFO: Created: latency-svc-9pjr2
Dec 16 23:26:28.733: INFO: Got endpoints: latency-svc-29xgk [733.749811ms]
Dec 16 23:26:28.751: INFO: Created: latency-svc-rmwln
Dec 16 23:26:28.785: INFO: Got endpoints: latency-svc-cpkk6 [739.623407ms]
Dec 16 23:26:28.800: INFO: Created: latency-svc-x2zrs
Dec 16 23:26:28.836: INFO: Got endpoints: latency-svc-fjtj8 [737.02492ms]
Dec 16 23:26:28.853: INFO: Created: latency-svc-lhzb4
Dec 16 23:26:28.885: INFO: Got endpoints: latency-svc-2rpbq [734.478183ms]
Dec 16 23:26:28.901: INFO: Created: latency-svc-ghfd8
Dec 16 23:26:28.934: INFO: Got endpoints: latency-svc-hjs4r [744.122243ms]
Dec 16 23:26:28.951: INFO: Created: latency-svc-fphns
Dec 16 23:26:28.987: INFO: Got endpoints: latency-svc-l8hl6 [746.857352ms]
Dec 16 23:26:29.007: INFO: Created: latency-svc-82dj7
Dec 16 23:26:29.038: INFO: Got endpoints: latency-svc-sjhmc [735.124729ms]
Dec 16 23:26:29.056: INFO: Created: latency-svc-ttpm6
Dec 16 23:26:29.084: INFO: Got endpoints: latency-svc-gs4cr [715.016536ms]
Dec 16 23:26:29.101: INFO: Created: latency-svc-p5f9k
Dec 16 23:26:29.136: INFO: Got endpoints: latency-svc-pqnzv [749.387353ms]
Dec 16 23:26:29.164: INFO: Created: latency-svc-zbhkp
Dec 16 23:26:29.186: INFO: Got endpoints: latency-svc-pq8q9 [749.941953ms]
Dec 16 23:26:29.201: INFO: Created: latency-svc-f7mzx
Dec 16 23:26:29.234: INFO: Got endpoints: latency-svc-m9bbx [747.283187ms]
Dec 16 23:26:29.261: INFO: Created: latency-svc-r8pfj
Dec 16 23:26:29.285: INFO: Got endpoints: latency-svc-z4rjj [746.185627ms]
Dec 16 23:26:29.306: INFO: Created: latency-svc-b9hjx
Dec 16 23:26:29.335: INFO: Got endpoints: latency-svc-9c5c9 [746.342416ms]
Dec 16 23:26:29.351: INFO: Created: latency-svc-b6ssr
Dec 16 23:26:29.386: INFO: Got endpoints: latency-svc-jd6wc [751.213712ms]
Dec 16 23:26:29.400: INFO: Created: latency-svc-nh98w
Dec 16 23:26:29.434: INFO: Got endpoints: latency-svc-9pjr2 [748.977835ms]
Dec 16 23:26:29.449: INFO: Created: latency-svc-vc7fd
Dec 16 23:26:29.489: INFO: Got endpoints: latency-svc-rmwln [755.146535ms]
Dec 16 23:26:29.501: INFO: Created: latency-svc-f5gvk
Dec 16 23:26:29.537: INFO: Got endpoints: latency-svc-x2zrs [752.153981ms]
Dec 16 23:26:29.552: INFO: Created: latency-svc-gr5cc
Dec 16 23:26:29.585: INFO: Got endpoints: latency-svc-lhzb4 [749.348223ms]
Dec 16 23:26:29.601: INFO: Created: latency-svc-5w5hv
Dec 16 23:26:29.686: INFO: Got endpoints: latency-svc-ghfd8 [800.675231ms]
Dec 16 23:26:29.702: INFO: Created: latency-svc-g85d9
Dec 16 23:26:29.773: INFO: Got endpoints: latency-svc-fphns [838.461857ms]
Dec 16 23:26:29.861: INFO: Got endpoints: latency-svc-ttpm6 [823.112567ms]
Dec 16 23:26:29.862: INFO: Got endpoints: latency-svc-82dj7 [874.105232ms]
Dec 16 23:26:29.867: INFO: Created: latency-svc-2lrk8
Dec 16 23:26:29.880: INFO: Created: latency-svc-pxhq5
Dec 16 23:26:29.895: INFO: Got endpoints: latency-svc-p5f9k [811.019049ms]
Dec 16 23:26:29.896: INFO: Created: latency-svc-sfh9k
Dec 16 23:26:29.911: INFO: Created: latency-svc-vqmjj
Dec 16 23:26:29.939: INFO: Got endpoints: latency-svc-zbhkp [803.039775ms]
Dec 16 23:26:29.993: INFO: Got endpoints: latency-svc-f7mzx [806.495806ms]
Dec 16 23:26:29.997: INFO: Created: latency-svc-6mkgn
Dec 16 23:26:30.027: INFO: Created: latency-svc-qxppw
Dec 16 23:26:30.043: INFO: Got endpoints: latency-svc-r8pfj [808.485018ms]
Dec 16 23:26:30.085: INFO: Created: latency-svc-2stqt
Dec 16 23:26:30.090: INFO: Got endpoints: latency-svc-b9hjx [804.34429ms]
Dec 16 23:26:30.106: INFO: Created: latency-svc-gwzqx
Dec 16 23:26:30.135: INFO: Got endpoints: latency-svc-b6ssr [799.105335ms]
Dec 16 23:26:30.150: INFO: Created: latency-svc-qbmw9
Dec 16 23:26:30.187: INFO: Got endpoints: latency-svc-nh98w [801.169578ms]
Dec 16 23:26:30.206: INFO: Created: latency-svc-q4z8p
Dec 16 23:26:30.236: INFO: Got endpoints: latency-svc-vc7fd [801.73992ms]
Dec 16 23:26:30.252: INFO: Created: latency-svc-npm5l
Dec 16 23:26:30.285: INFO: Got endpoints: latency-svc-f5gvk [796.699207ms]
Dec 16 23:26:30.300: INFO: Created: latency-svc-mgvpk
Dec 16 23:26:30.336: INFO: Got endpoints: latency-svc-gr5cc [797.852861ms]
Dec 16 23:26:30.395: INFO: Got endpoints: latency-svc-5w5hv [809.480404ms]
Dec 16 23:26:30.399: INFO: Created: latency-svc-r5f8l
Dec 16 23:26:30.412: INFO: Created: latency-svc-8p8x7
Dec 16 23:26:30.434: INFO: Got endpoints: latency-svc-g85d9 [747.850049ms]
Dec 16 23:26:30.449: INFO: Created: latency-svc-bwkqb
Dec 16 23:26:30.484: INFO: Got endpoints: latency-svc-2lrk8 [710.797032ms]
Dec 16 23:26:30.497: INFO: Created: latency-svc-vv5zm
Dec 16 23:26:30.536: INFO: Got endpoints: latency-svc-pxhq5 [673.730734ms]
Dec 16 23:26:30.551: INFO: Created: latency-svc-x7flr
Dec 16 23:26:30.584: INFO: Got endpoints: latency-svc-sfh9k [722.652327ms]
Dec 16 23:26:30.602: INFO: Created: latency-svc-zp4sh
Dec 16 23:26:30.636: INFO: Got endpoints: latency-svc-vqmjj [741.054476ms]
Dec 16 23:26:30.656: INFO: Created: latency-svc-w7fxd
Dec 16 23:26:30.689: INFO: Got endpoints: latency-svc-6mkgn [750.295476ms]
Dec 16 23:26:30.713: INFO: Created: latency-svc-nfmhw
Dec 16 23:26:30.735: INFO: Got endpoints: latency-svc-qxppw [741.669635ms]
Dec 16 23:26:30.750: INFO: Created: latency-svc-hqcdv
Dec 16 23:26:30.784: INFO: Got endpoints: latency-svc-2stqt [740.48038ms]
Dec 16 23:26:30.806: INFO: Created: latency-svc-znx5s
Dec 16 23:26:30.837: INFO: Got endpoints: latency-svc-gwzqx [746.377862ms]
Dec 16 23:26:30.852: INFO: Created: latency-svc-h6gg8
Dec 16 23:26:30.887: INFO: Got endpoints: latency-svc-qbmw9 [750.22839ms]
Dec 16 23:26:30.917: INFO: Created: latency-svc-r2w7r
Dec 16 23:26:30.936: INFO: Got endpoints: latency-svc-q4z8p [748.126792ms]
Dec 16 23:26:30.953: INFO: Created: latency-svc-wd4dd
Dec 16 23:26:30.984: INFO: Got endpoints: latency-svc-npm5l [747.482939ms]
Dec 16 23:26:31.010: INFO: Created: latency-svc-4kxfw
Dec 16 23:26:31.037: INFO: Got endpoints: latency-svc-mgvpk [751.601139ms]
Dec 16 23:26:31.059: INFO: Created: latency-svc-jf2xb
Dec 16 23:26:31.086: INFO: Got endpoints: latency-svc-r5f8l [750.261ms]
Dec 16 23:26:31.101: INFO: Created: latency-svc-tktjk
Dec 16 23:26:31.135: INFO: Got endpoints: latency-svc-8p8x7 [739.345451ms]
Dec 16 23:26:31.153: INFO: Created: latency-svc-tgnp9
Dec 16 23:26:31.184: INFO: Got endpoints: latency-svc-bwkqb [749.648628ms]
Dec 16 23:26:31.205: INFO: Created: latency-svc-wp5tr
Dec 16 23:26:31.235: INFO: Got endpoints: latency-svc-vv5zm [750.928012ms]
Dec 16 23:26:31.250: INFO: Created: latency-svc-5qtpj
Dec 16 23:26:31.286: INFO: Got endpoints: latency-svc-x7flr [749.872351ms]
Dec 16 23:26:31.300: INFO: Created: latency-svc-m6vzj
Dec 16 23:26:31.333: INFO: Got endpoints: latency-svc-zp4sh [749.012401ms]
Dec 16 23:26:31.349: INFO: Created: latency-svc-x5xpz
Dec 16 23:26:31.385: INFO: Got endpoints: latency-svc-w7fxd [748.810559ms]
Dec 16 23:26:31.401: INFO: Created: latency-svc-l69mm
Dec 16 23:26:31.438: INFO: Got endpoints: latency-svc-nfmhw [748.810521ms]
Dec 16 23:26:31.459: INFO: Created: latency-svc-rcj9g
Dec 16 23:26:31.488: INFO: Got endpoints: latency-svc-hqcdv [753.146144ms]
Dec 16 23:26:31.518: INFO: Created: latency-svc-nnnpf
Dec 16 23:26:31.534: INFO: Got endpoints: latency-svc-znx5s [750.642086ms]
Dec 16 23:26:31.550: INFO: Created: latency-svc-twnnp
Dec 16 23:26:31.584: INFO: Got endpoints: latency-svc-h6gg8 [747.577484ms]
Dec 16 23:26:31.600: INFO: Created: latency-svc-trx9h
Dec 16 23:26:31.638: INFO: Got endpoints: latency-svc-r2w7r [750.388826ms]
Dec 16 23:26:31.653: INFO: Created: latency-svc-78cq9
Dec 16 23:26:31.686: INFO: Got endpoints: latency-svc-wd4dd [749.945217ms]
Dec 16 23:26:31.706: INFO: Created: latency-svc-twn65
Dec 16 23:26:31.742: INFO: Got endpoints: latency-svc-4kxfw [757.908362ms]
Dec 16 23:26:31.758: INFO: Created: latency-svc-wtdcf
Dec 16 23:26:31.784: INFO: Got endpoints: latency-svc-jf2xb [742.361437ms]
Dec 16 23:26:31.801: INFO: Created: latency-svc-klp9k
Dec 16 23:26:31.862: INFO: Got endpoints: latency-svc-tktjk [775.535374ms]
Dec 16 23:26:31.960: INFO: Got endpoints: latency-svc-tgnp9 [825.064927ms]
Dec 16 23:26:31.972: INFO: Got endpoints: latency-svc-wp5tr [787.374449ms]
Dec 16 23:26:32.020: INFO: Got endpoints: latency-svc-5qtpj [785.497117ms]
Dec 16 23:26:32.022: INFO: Created: latency-svc-xt6px
Dec 16 23:26:32.038: INFO: Got endpoints: latency-svc-m6vzj [751.979877ms]
Dec 16 23:26:32.041: INFO: Created: latency-svc-nq759
Dec 16 23:26:32.054: INFO: Created: latency-svc-nzsr9
Dec 16 23:26:32.061: INFO: Created: latency-svc-qggtc
Dec 16 23:26:32.071: INFO: Created: latency-svc-bdt65
Dec 16 23:26:32.086: INFO: Got endpoints: latency-svc-x5xpz [752.080996ms]
Dec 16 23:26:32.104: INFO: Created: latency-svc-lzv58
Dec 16 23:26:32.136: INFO: Got endpoints: latency-svc-l69mm [750.41631ms]
Dec 16 23:26:32.151: INFO: Created: latency-svc-2xlh9
Dec 16 23:26:32.184: INFO: Got endpoints: latency-svc-rcj9g [745.519972ms]
Dec 16 23:26:32.200: INFO: Created: latency-svc-whhzk
Dec 16 23:26:32.242: INFO: Got endpoints: latency-svc-nnnpf [753.2955ms]
Dec 16 23:26:32.265: INFO: Created: latency-svc-7hql9
Dec 16 23:26:32.285: INFO: Got endpoints: latency-svc-twnnp [751.014913ms]
Dec 16 23:26:32.300: INFO: Created: latency-svc-lljhz
Dec 16 23:26:32.335: INFO: Got endpoints: latency-svc-trx9h [750.195719ms]
Dec 16 23:26:32.354: INFO: Created: latency-svc-7db2m
Dec 16 23:26:32.385: INFO: Got endpoints: latency-svc-78cq9 [746.697031ms]
Dec 16 23:26:32.399: INFO: Created: latency-svc-hkm5s
Dec 16 23:26:32.436: INFO: Got endpoints: latency-svc-twn65 [750.244586ms]
Dec 16 23:26:32.457: INFO: Created: latency-svc-q8bd2
Dec 16 23:26:32.486: INFO: Got endpoints: latency-svc-wtdcf [743.226908ms]
Dec 16 23:26:32.502: INFO: Created: latency-svc-qnrbp
Dec 16 23:26:32.535: INFO: Got endpoints: latency-svc-klp9k [750.487662ms]
Dec 16 23:26:32.550: INFO: Created: latency-svc-c9t4z
Dec 16 23:26:32.587: INFO: Got endpoints: latency-svc-xt6px [725.286225ms]
Dec 16 23:26:32.607: INFO: Created: latency-svc-d7d6t
Dec 16 23:26:32.635: INFO: Got endpoints: latency-svc-nq759 [675.543602ms]
Dec 16 23:26:32.653: INFO: Created: latency-svc-8kjdq
Dec 16 23:26:32.695: INFO: Got endpoints: latency-svc-nzsr9 [723.480932ms]
Dec 16 23:26:32.711: INFO: Created: latency-svc-qwrzw
Dec 16 23:26:32.735: INFO: Got endpoints: latency-svc-qggtc [715.032868ms]
Dec 16 23:26:32.748: INFO: Created: latency-svc-c6kfg
Dec 16 23:26:32.785: INFO: Got endpoints: latency-svc-bdt65 [747.004865ms]
Dec 16 23:26:32.800: INFO: Created: latency-svc-pxpc5
Dec 16 23:26:32.834: INFO: Got endpoints: latency-svc-lzv58 [748.544706ms]
Dec 16 23:26:32.849: INFO: Created: latency-svc-jkpdv
Dec 16 23:26:32.884: INFO: Got endpoints: latency-svc-2xlh9 [747.851747ms]
Dec 16 23:26:32.919: INFO: Created: latency-svc-rqds5
Dec 16 23:26:32.935: INFO: Got endpoints: latency-svc-whhzk [751.0518ms]
Dec 16 23:26:32.949: INFO: Created: latency-svc-b59b2
Dec 16 23:26:32.985: INFO: Got endpoints: latency-svc-7hql9 [743.563097ms]
Dec 16 23:26:33.001: INFO: Created: latency-svc-2wpvw
Dec 16 23:26:33.041: INFO: Got endpoints: latency-svc-lljhz [755.828085ms]
Dec 16 23:26:33.066: INFO: Created: latency-svc-ccrrt
Dec 16 23:26:33.107: INFO: Got endpoints: latency-svc-7db2m [772.470097ms]
Dec 16 23:26:33.126: INFO: Created: latency-svc-qrsj4
Dec 16 23:26:33.137: INFO: Got endpoints: latency-svc-hkm5s [752.001077ms]
Dec 16 23:26:33.156: INFO: Created: latency-svc-9b47j
Dec 16 23:26:33.186: INFO: Got endpoints: latency-svc-q8bd2 [749.58769ms]
Dec 16 23:26:33.204: INFO: Created: latency-svc-9whtw
Dec 16 23:26:33.236: INFO: Got endpoints: latency-svc-qnrbp [749.927185ms]
Dec 16 23:26:33.249: INFO: Created: latency-svc-74b8l
Dec 16 23:26:33.289: INFO: Got endpoints: latency-svc-c9t4z [754.387486ms]
Dec 16 23:26:33.328: INFO: Created: latency-svc-t958b
Dec 16 23:26:33.334: INFO: Got endpoints: latency-svc-d7d6t [746.797567ms]
Dec 16 23:26:33.349: INFO: Created: latency-svc-4bxrj
Dec 16 23:26:33.391: INFO: Got endpoints: latency-svc-8kjdq [755.457595ms]
Dec 16 23:26:33.408: INFO: Created: latency-svc-mvswn
Dec 16 23:26:33.436: INFO: Got endpoints: latency-svc-qwrzw [740.828782ms]
Dec 16 23:26:33.450: INFO: Created: latency-svc-jzf2v
Dec 16 23:26:33.484: INFO: Got endpoints: latency-svc-c6kfg [748.807743ms]
Dec 16 23:26:33.502: INFO: Created: latency-svc-z5j29
Dec 16 23:26:33.535: INFO: Got endpoints: latency-svc-pxpc5 [750.015901ms]
Dec 16 23:26:33.549: INFO: Created: latency-svc-9gx5h
Dec 16 23:26:33.585: INFO: Got endpoints: latency-svc-jkpdv [750.121301ms]
Dec 16 23:26:33.599: INFO: Created: latency-svc-qqjjp
Dec 16 23:26:33.637: INFO: Got endpoints: latency-svc-rqds5 [752.270813ms]
Dec 16 23:26:33.651: INFO: Created: latency-svc-hg9pk
Dec 16 23:26:33.685: INFO: Got endpoints: latency-svc-b59b2 [749.869698ms]
Dec 16 23:26:33.702: INFO: Created: latency-svc-dfhdz
Dec 16 23:26:33.733: INFO: Got endpoints: latency-svc-2wpvw [747.730895ms]
Dec 16 23:26:33.748: INFO: Created: latency-svc-kxjvk
Dec 16 23:26:33.784: INFO: Got endpoints: latency-svc-ccrrt [742.712522ms]
Dec 16 23:26:33.798: INFO: Created: latency-svc-l84zg
Dec 16 23:26:33.837: INFO: Got endpoints: latency-svc-qrsj4 [728.986496ms]
Dec 16 23:26:33.854: INFO: Created: latency-svc-zz5sm
Dec 16 23:26:33.889: INFO: Got endpoints: latency-svc-9b47j [751.235781ms]
Dec 16 23:26:33.917: INFO: Created: latency-svc-z7b9d
Dec 16 23:26:33.939: INFO: Got endpoints: latency-svc-9whtw [753.873322ms]
Dec 16 23:26:33.970: INFO: Created: latency-svc-j9dsp
Dec 16 23:26:33.984: INFO: Got endpoints: latency-svc-74b8l [748.41131ms]
Dec 16 23:26:34.000: INFO: Created: latency-svc-ct4ts
Dec 16 23:26:34.039: INFO: Got endpoints: latency-svc-t958b [749.512773ms]
Dec 16 23:26:34.057: INFO: Created: latency-svc-dqps7
Dec 16 23:26:34.085: INFO: Got endpoints: latency-svc-4bxrj [751.083696ms]
Dec 16 23:26:34.099: INFO: Created: latency-svc-g5whz
Dec 16 23:26:34.136: INFO: Got endpoints: latency-svc-mvswn [744.750735ms]
Dec 16 23:26:34.154: INFO: Created: latency-svc-ft88f
Dec 16 23:26:34.186: INFO: Got endpoints: latency-svc-jzf2v [749.918187ms]
Dec 16 23:26:34.201: INFO: Created: latency-svc-bl5bz
Dec 16 23:26:34.233: INFO: Got endpoints: latency-svc-z5j29 [749.024968ms]
Dec 16 23:26:34.255: INFO: Created: latency-svc-nbtxz
Dec 16 23:26:34.287: INFO: Got endpoints: latency-svc-9gx5h [751.229047ms]
Dec 16 23:26:34.303: INFO: Created: latency-svc-4zzt8
Dec 16 23:26:34.336: INFO: Got endpoints: latency-svc-qqjjp [750.900268ms]
Dec 16 23:26:34.350: INFO: Created: latency-svc-l9k4z
Dec 16 23:26:34.384: INFO: Got endpoints: latency-svc-hg9pk [747.396828ms]
Dec 16 23:26:34.400: INFO: Created: latency-svc-d5fts
Dec 16 23:26:34.434: INFO: Got endpoints: latency-svc-dfhdz [748.740042ms]
Dec 16 23:26:34.449: INFO: Created: latency-svc-rbpgb
Dec 16 23:26:34.486: INFO: Got endpoints: latency-svc-kxjvk [752.306386ms]
Dec 16 23:26:34.500: INFO: Created: latency-svc-w5h4x
Dec 16 23:26:34.536: INFO: Got endpoints: latency-svc-l84zg [751.591994ms]
Dec 16 23:26:34.550: INFO: Created: latency-svc-65m84
Dec 16 23:26:34.584: INFO: Got endpoints: latency-svc-zz5sm [747.240247ms]
Dec 16 23:26:34.601: INFO: Created: latency-svc-mkgdj
Dec 16 23:26:34.634: INFO: Got endpoints: latency-svc-z7b9d [745.264204ms]
Dec 16 23:26:34.649: INFO: Created: latency-svc-grrmq
Dec 16 23:26:34.686: INFO: Got endpoints: latency-svc-j9dsp [746.487503ms]
Dec 16 23:26:34.701: INFO: Created: latency-svc-54fj2
Dec 16 23:26:34.744: INFO: Got endpoints: latency-svc-ct4ts [758.970786ms]
Dec 16 23:26:34.758: INFO: Created: latency-svc-qbsqh
Dec 16 23:26:34.784: INFO: Got endpoints: latency-svc-dqps7 [744.815706ms]
Dec 16 23:26:34.799: INFO: Created: latency-svc-sx4vh
Dec 16 23:26:34.836: INFO: Got endpoints: latency-svc-g5whz [750.530775ms]
Dec 16 23:26:34.853: INFO: Created: latency-svc-lc2zc
Dec 16 23:26:34.888: INFO: Got endpoints: latency-svc-ft88f [751.662266ms]
Dec 16 23:26:34.935: INFO: Got endpoints: latency-svc-bl5bz [748.950562ms]
Dec 16 23:26:34.986: INFO: Got endpoints: latency-svc-nbtxz [751.958843ms]
Dec 16 23:26:35.041: INFO: Got endpoints: latency-svc-4zzt8 [753.628281ms]
Dec 16 23:26:35.086: INFO: Got endpoints: latency-svc-l9k4z [749.601438ms]
Dec 16 23:26:35.137: INFO: Got endpoints: latency-svc-d5fts [752.442752ms]
Dec 16 23:26:35.186: INFO: Got endpoints: latency-svc-rbpgb [751.429499ms]
Dec 16 23:26:35.236: INFO: Got endpoints: latency-svc-w5h4x [749.730223ms]
Dec 16 23:26:35.285: INFO: Got endpoints: latency-svc-65m84 [748.819879ms]
Dec 16 23:26:35.336: INFO: Got endpoints: latency-svc-mkgdj [751.678136ms]
Dec 16 23:26:35.385: INFO: Got endpoints: latency-svc-grrmq [750.194384ms]
Dec 16 23:26:35.434: INFO: Got endpoints: latency-svc-54fj2 [747.744218ms]
Dec 16 23:26:35.485: INFO: Got endpoints: latency-svc-qbsqh [740.517882ms]
Dec 16 23:26:35.536: INFO: Got endpoints: latency-svc-sx4vh [752.435562ms]
Dec 16 23:26:35.583: INFO: Got endpoints: latency-svc-lc2zc [745.966377ms]
Dec 16 23:26:35.583: INFO: Latencies: [63.254925ms 79.317616ms 94.18214ms 123.042989ms 135.598205ms 161.173542ms 180.752763ms 208.650357ms 230.868901ms 242.026369ms 245.836262ms 252.168138ms 252.215895ms 254.283721ms 257.092978ms 266.201821ms 266.480786ms 269.602717ms 270.589516ms 271.245602ms 275.72584ms 277.952363ms 280.965426ms 282.11825ms 285.482843ms 291.110919ms 292.610915ms 293.828648ms 293.877191ms 296.781219ms 298.54416ms 301.674517ms 302.660954ms 303.038399ms 306.087422ms 307.235082ms 307.688436ms 307.899533ms 308.329699ms 315.401714ms 315.59115ms 315.664285ms 315.942229ms 316.3721ms 318.339545ms 319.133027ms 321.912054ms 346.758634ms 351.643698ms 363.493042ms 378.009169ms 409.379602ms 435.286263ms 467.747068ms 505.063017ms 544.74982ms 547.833182ms 576.384731ms 618.120528ms 624.371729ms 656.967766ms 673.730734ms 675.543602ms 685.562898ms 710.797032ms 713.262556ms 715.016536ms 715.032868ms 722.652327ms 723.480932ms 725.286225ms 728.986496ms 733.749811ms 734.478183ms 735.124729ms 737.02492ms 739.345451ms 739.623407ms 740.48038ms 740.517882ms 740.828782ms 741.054476ms 741.669635ms 742.361437ms 742.712522ms 743.226908ms 743.563097ms 744.122243ms 744.750735ms 744.815706ms 745.264204ms 745.519972ms 745.966377ms 746.185627ms 746.342416ms 746.377862ms 746.487503ms 746.697031ms 746.797567ms 746.857352ms 747.004865ms 747.240247ms 747.283187ms 747.396828ms 747.482939ms 747.577484ms 747.730895ms 747.744218ms 747.850049ms 747.851747ms 748.126792ms 748.41131ms 748.544706ms 748.740042ms 748.807743ms 748.810521ms 748.810559ms 748.819879ms 748.950562ms 748.977835ms 749.012401ms 749.024968ms 749.348223ms 749.387353ms 749.512773ms 749.58769ms 749.601438ms 749.648628ms 749.730223ms 749.869698ms 749.872351ms 749.918187ms 749.927185ms 749.941953ms 749.945217ms 750.015901ms 750.121301ms 750.194384ms 750.195719ms 750.22839ms 750.244586ms 750.261ms 750.295476ms 750.388826ms 750.41631ms 750.487662ms 750.530775ms 750.642086ms 750.900268ms 750.928012ms 751.014913ms 751.0518ms 751.083696ms 751.213712ms 751.229047ms 751.235781ms 751.429499ms 751.591994ms 751.601139ms 751.662266ms 751.678136ms 751.958843ms 751.979877ms 752.001077ms 752.080996ms 752.153981ms 752.270813ms 752.306386ms 752.435562ms 752.442752ms 753.146144ms 753.2955ms 753.628281ms 753.873322ms 754.387486ms 755.146535ms 755.457595ms 755.828085ms 757.908362ms 758.970786ms 772.470097ms 775.535374ms 785.497117ms 787.374449ms 796.699207ms 797.852861ms 799.105335ms 800.675231ms 801.169578ms 801.73992ms 803.039775ms 804.34429ms 806.495806ms 808.485018ms 809.480404ms 811.019049ms 823.112567ms 825.064927ms 838.461857ms 874.105232ms]
Dec 16 23:26:35.583: INFO: 50 %ile: 747.004865ms
Dec 16 23:26:35.583: INFO: 90 %ile: 772.470097ms
Dec 16 23:26:35.583: INFO: 99 %ile: 838.461857ms
Dec 16 23:26:35.583: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:26:35.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5831" for this suite.

• [SLOW TEST:10.865 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":137,"skipped":2379,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:26:35.599: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 16 23:26:35.665: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:35.665: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:35.665: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:35.669: INFO: Number of nodes with available pods: 0
Dec 16 23:26:35.669: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:36.679: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:36.679: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:36.679: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:36.682: INFO: Number of nodes with available pods: 0
Dec 16 23:26:36.682: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:37.675: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:37.675: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:37.675: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:37.678: INFO: Number of nodes with available pods: 1
Dec 16 23:26:37.678: INFO: Node ip-172-31-6-174 is running more than one daemon pod
Dec 16 23:26:38.675: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:38.675: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:38.675: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:38.679: INFO: Number of nodes with available pods: 2
Dec 16 23:26:38.679: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 16 23:26:38.697: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:38.697: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:38.697: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:38.700: INFO: Number of nodes with available pods: 1
Dec 16 23:26:38.700: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:39.725: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:39.725: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:39.725: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:39.743: INFO: Number of nodes with available pods: 1
Dec 16 23:26:39.743: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:40.719: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:40.720: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:40.720: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:40.725: INFO: Number of nodes with available pods: 1
Dec 16 23:26:40.726: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:41.708: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:41.709: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:41.709: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:41.712: INFO: Number of nodes with available pods: 1
Dec 16 23:26:41.712: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:42.707: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:42.708: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:42.708: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:42.713: INFO: Number of nodes with available pods: 1
Dec 16 23:26:42.713: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:43.709: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:43.709: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:43.709: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:43.715: INFO: Number of nodes with available pods: 1
Dec 16 23:26:43.715: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:44.709: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:44.709: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:44.710: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:44.715: INFO: Number of nodes with available pods: 1
Dec 16 23:26:44.715: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:45.713: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:45.713: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:45.713: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:45.725: INFO: Number of nodes with available pods: 1
Dec 16 23:26:45.725: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:46.715: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:46.715: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:46.715: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:46.720: INFO: Number of nodes with available pods: 1
Dec 16 23:26:46.720: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:47.718: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:47.718: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:47.721: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:47.725: INFO: Number of nodes with available pods: 1
Dec 16 23:26:47.725: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:48.707: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:48.707: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:48.707: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:48.710: INFO: Number of nodes with available pods: 1
Dec 16 23:26:48.710: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 16 23:26:49.709: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:49.709: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:49.709: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 16 23:26:49.714: INFO: Number of nodes with available pods: 2
Dec 16 23:26:49.714: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7293, will wait for the garbage collector to delete the pods
Dec 16 23:26:49.780: INFO: Deleting DaemonSet.extensions daemon-set took: 8.086083ms
Dec 16 23:26:49.880: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.113168ms
Dec 16 23:27:17.890: INFO: Number of nodes with available pods: 0
Dec 16 23:27:17.890: INFO: Number of running nodes: 0, number of available pods: 0
Dec 16 23:27:17.895: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36510"},"items":null}

Dec 16 23:27:17.900: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36510"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:27:17.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7293" for this suite.

• [SLOW TEST:42.321 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":138,"skipped":2397,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:27:17.928: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 16 23:27:18.019: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8350  4d4a7dfe-031e-423d-bef9-646bf4d432c0 36519 0 2020-12-16 23:27:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-12-16 23:27:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 23:27:18.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8350  4d4a7dfe-031e-423d-bef9-646bf4d432c0 36520 0 2020-12-16 23:27:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-12-16 23:27:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:27:18.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8350" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":139,"skipped":2408,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:27:18.029: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-8c505654-a8c2-4214-9e32-112217c8b8ac
Dec 16 23:27:18.074: INFO: Pod name my-hostname-basic-8c505654-a8c2-4214-9e32-112217c8b8ac: Found 0 pods out of 1
Dec 16 23:27:23.088: INFO: Pod name my-hostname-basic-8c505654-a8c2-4214-9e32-112217c8b8ac: Found 1 pods out of 1
Dec 16 23:27:23.088: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8c505654-a8c2-4214-9e32-112217c8b8ac" are running
Dec 16 23:27:23.092: INFO: Pod "my-hostname-basic-8c505654-a8c2-4214-9e32-112217c8b8ac-xzk88" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-16 23:27:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-16 23:27:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-16 23:27:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-16 23:27:18 +0000 UTC Reason: Message:}])
Dec 16 23:27:23.092: INFO: Trying to dial the pod
Dec 16 23:27:28.113: INFO: Controller my-hostname-basic-8c505654-a8c2-4214-9e32-112217c8b8ac: Got expected result from replica 1 [my-hostname-basic-8c505654-a8c2-4214-9e32-112217c8b8ac-xzk88]: "my-hostname-basic-8c505654-a8c2-4214-9e32-112217c8b8ac-xzk88", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:27:28.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6535" for this suite.

• [SLOW TEST:10.102 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":140,"skipped":2420,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:27:28.132: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Dec 16 23:27:28.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9367 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 16 23:27:28.301: INFO: stderr: ""
Dec 16 23:27:28.301: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Dec 16 23:27:28.301: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 16 23:27:28.301: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9367" to be "running and ready, or succeeded"
Dec 16 23:27:28.304: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.031457ms
Dec 16 23:27:30.313: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012224063s
Dec 16 23:27:32.322: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.020704194s
Dec 16 23:27:32.322: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 16 23:27:32.322: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 16 23:27:32.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9367 logs logs-generator logs-generator'
Dec 16 23:27:32.445: INFO: stderr: ""
Dec 16 23:27:32.445: INFO: stdout: "I1216 23:27:29.322271       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/xhvk 471\nI1216 23:27:29.522352       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/85dv 500\nI1216 23:27:29.722336       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/wcg 554\nI1216 23:27:29.922339       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/zcvx 412\nI1216 23:27:30.122348       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/swnm 554\nI1216 23:27:30.323710       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/twr 525\nI1216 23:27:30.522350       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/kjk 305\nI1216 23:27:30.722347       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5sr 213\nI1216 23:27:30.922341       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/sl47 499\nI1216 23:27:31.122728       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/f5h 477\nI1216 23:27:31.322344       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/2cw 359\nI1216 23:27:31.522354       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/p82 541\nI1216 23:27:31.722338       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/x6g 259\nI1216 23:27:31.922344       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/xq8k 434\nI1216 23:27:32.122336       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/mkcp 566\nI1216 23:27:32.322370       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/6xk 255\n"
STEP: limiting log lines
Dec 16 23:27:32.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9367 logs logs-generator logs-generator --tail=1'
Dec 16 23:27:32.536: INFO: stderr: ""
Dec 16 23:27:32.536: INFO: stdout: "I1216 23:27:32.522331       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/vcz 229\n"
Dec 16 23:27:32.536: INFO: got output "I1216 23:27:32.522331       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/vcz 229\n"
STEP: limiting log bytes
Dec 16 23:27:32.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9367 logs logs-generator logs-generator --limit-bytes=1'
Dec 16 23:27:32.630: INFO: stderr: ""
Dec 16 23:27:32.630: INFO: stdout: "I"
Dec 16 23:27:32.630: INFO: got output "I"
STEP: exposing timestamps
Dec 16 23:27:32.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9367 logs logs-generator logs-generator --tail=1 --timestamps'
Dec 16 23:27:32.733: INFO: stderr: ""
Dec 16 23:27:32.733: INFO: stdout: "2020-12-16T23:27:32.722435192Z I1216 23:27:32.722318       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/6pl 521\n"
Dec 16 23:27:32.733: INFO: got output "2020-12-16T23:27:32.722435192Z I1216 23:27:32.722318       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/6pl 521\n"
STEP: restricting to a time range
Dec 16 23:27:35.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9367 logs logs-generator logs-generator --since=1s'
Dec 16 23:27:35.447: INFO: stderr: ""
Dec 16 23:27:35.447: INFO: stdout: "I1216 23:27:34.522357       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/pslw 591\nI1216 23:27:34.722433       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/6vcn 427\nI1216 23:27:34.922346       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/g42 591\nI1216 23:27:35.122341       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/2r5 545\nI1216 23:27:35.322348       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/grh 415\n"
Dec 16 23:27:35.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9367 logs logs-generator logs-generator --since=24h'
Dec 16 23:27:35.553: INFO: stderr: ""
Dec 16 23:27:35.553: INFO: stdout: "I1216 23:27:29.322271       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/xhvk 471\nI1216 23:27:29.522352       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/85dv 500\nI1216 23:27:29.722336       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/wcg 554\nI1216 23:27:29.922339       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/zcvx 412\nI1216 23:27:30.122348       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/swnm 554\nI1216 23:27:30.323710       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/twr 525\nI1216 23:27:30.522350       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/kjk 305\nI1216 23:27:30.722347       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5sr 213\nI1216 23:27:30.922341       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/sl47 499\nI1216 23:27:31.122728       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/f5h 477\nI1216 23:27:31.322344       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/2cw 359\nI1216 23:27:31.522354       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/p82 541\nI1216 23:27:31.722338       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/x6g 259\nI1216 23:27:31.922344       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/xq8k 434\nI1216 23:27:32.122336       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/mkcp 566\nI1216 23:27:32.322370       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/6xk 255\nI1216 23:27:32.522331       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/vcz 229\nI1216 23:27:32.722318       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/6pl 521\nI1216 23:27:32.922427       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/nw8 279\nI1216 23:27:33.122342       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/9wr9 573\nI1216 23:27:33.322619       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/k68 278\nI1216 23:27:33.522352       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/zwq9 592\nI1216 23:27:33.722341       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/s9tv 584\nI1216 23:27:33.922379       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/2s7d 381\nI1216 23:27:34.122352       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/gzt4 409\nI1216 23:27:34.322350       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/6v4 262\nI1216 23:27:34.522357       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/pslw 591\nI1216 23:27:34.722433       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/6vcn 427\nI1216 23:27:34.922346       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/g42 591\nI1216 23:27:35.122341       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/2r5 545\nI1216 23:27:35.322348       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/grh 415\nI1216 23:27:35.522348       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/ns/pods/tvn 347\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Dec 16 23:27:35.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-9367 delete pod logs-generator'
Dec 16 23:27:47.832: INFO: stderr: ""
Dec 16 23:27:47.832: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:27:47.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9367" for this suite.

• [SLOW TEST:19.735 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":141,"skipped":2430,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:27:47.867: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4249.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4249.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4249.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4249.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4249.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4249.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 16 23:27:49.982: INFO: DNS probes using dns-4249/dns-test-090d778d-2ab9-4f1a-9b39-562dfb407a14 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:27:50.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4249" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":142,"skipped":2431,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:27:50.061: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:27:52.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6011" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":143,"skipped":2450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:27:52.188: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-e44ec2ca-2cae-44a6-ac5a-ebe5d565d396
STEP: Creating a pod to test consume configMaps
Dec 16 23:27:52.242: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e664657-c9e8-4395-90cc-5fcd73405599" in namespace "configmap-4205" to be "Succeeded or Failed"
Dec 16 23:27:52.244: INFO: Pod "pod-configmaps-5e664657-c9e8-4395-90cc-5fcd73405599": Phase="Pending", Reason="", readiness=false. Elapsed: 2.554605ms
Dec 16 23:27:54.249: INFO: Pod "pod-configmaps-5e664657-c9e8-4395-90cc-5fcd73405599": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007757018s
STEP: Saw pod success
Dec 16 23:27:54.250: INFO: Pod "pod-configmaps-5e664657-c9e8-4395-90cc-5fcd73405599" satisfied condition "Succeeded or Failed"
Dec 16 23:27:54.252: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-configmaps-5e664657-c9e8-4395-90cc-5fcd73405599 container agnhost-container: <nil>
STEP: delete the pod
Dec 16 23:27:54.280: INFO: Waiting for pod pod-configmaps-5e664657-c9e8-4395-90cc-5fcd73405599 to disappear
Dec 16 23:27:54.283: INFO: Pod pod-configmaps-5e664657-c9e8-4395-90cc-5fcd73405599 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:27:54.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4205" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":144,"skipped":2478,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:27:54.299: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 16 23:27:56.367: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:27:56.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1768" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2527,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:27:56.399: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 16 23:27:56.453: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 16 23:28:09.743: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:13.164: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:28:25.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2968" for this suite.

• [SLOW TEST:29.477 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":146,"skipped":2530,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:28:25.878: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:28:43.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8620" for this suite.

• [SLOW TEST:17.143 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":147,"skipped":2544,"failed":0}
S
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:28:43.023: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:28:43.072: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-202aaf2e-4afb-443b-b851-25e9ba9819e8" in namespace "security-context-test-6824" to be "Succeeded or Failed"
Dec 16 23:28:43.076: INFO: Pod "busybox-privileged-false-202aaf2e-4afb-443b-b851-25e9ba9819e8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.443826ms
Dec 16 23:28:45.082: INFO: Pod "busybox-privileged-false-202aaf2e-4afb-443b-b851-25e9ba9819e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009702255s
Dec 16 23:28:45.082: INFO: Pod "busybox-privileged-false-202aaf2e-4afb-443b-b851-25e9ba9819e8" satisfied condition "Succeeded or Failed"
Dec 16 23:28:45.087: INFO: Got logs for pod "busybox-privileged-false-202aaf2e-4afb-443b-b851-25e9ba9819e8": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:28:45.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6824" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":148,"skipped":2545,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:28:45.098: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-94eb17f4-e077-438a-a153-c6e18bb13890
STEP: Creating a pod to test consume secrets
Dec 16 23:28:45.147: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6fc69b72-0cb3-45f7-9891-8fc7b2d742ef" in namespace "projected-210" to be "Succeeded or Failed"
Dec 16 23:28:45.161: INFO: Pod "pod-projected-secrets-6fc69b72-0cb3-45f7-9891-8fc7b2d742ef": Phase="Pending", Reason="", readiness=false. Elapsed: 13.573036ms
Dec 16 23:28:47.168: INFO: Pod "pod-projected-secrets-6fc69b72-0cb3-45f7-9891-8fc7b2d742ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020614644s
STEP: Saw pod success
Dec 16 23:28:47.168: INFO: Pod "pod-projected-secrets-6fc69b72-0cb3-45f7-9891-8fc7b2d742ef" satisfied condition "Succeeded or Failed"
Dec 16 23:28:47.171: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-secrets-6fc69b72-0cb3-45f7-9891-8fc7b2d742ef container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 16 23:28:47.187: INFO: Waiting for pod pod-projected-secrets-6fc69b72-0cb3-45f7-9891-8fc7b2d742ef to disappear
Dec 16 23:28:47.191: INFO: Pod pod-projected-secrets-6fc69b72-0cb3-45f7-9891-8fc7b2d742ef no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:28:47.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-210" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":149,"skipped":2546,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:28:47.202: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:28:47.251: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:28:49.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-635" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2555,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:28:49.355: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 23:28:49.404: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66205af2-2bd3-46f0-bbfd-e758b41edf51" in namespace "downward-api-2354" to be "Succeeded or Failed"
Dec 16 23:28:49.406: INFO: Pod "downwardapi-volume-66205af2-2bd3-46f0-bbfd-e758b41edf51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.244915ms
Dec 16 23:28:51.414: INFO: Pod "downwardapi-volume-66205af2-2bd3-46f0-bbfd-e758b41edf51": Phase="Running", Reason="", readiness=true. Elapsed: 2.010234904s
Dec 16 23:28:53.421: INFO: Pod "downwardapi-volume-66205af2-2bd3-46f0-bbfd-e758b41edf51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017145204s
STEP: Saw pod success
Dec 16 23:28:53.421: INFO: Pod "downwardapi-volume-66205af2-2bd3-46f0-bbfd-e758b41edf51" satisfied condition "Succeeded or Failed"
Dec 16 23:28:53.424: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-66205af2-2bd3-46f0-bbfd-e758b41edf51 container client-container: <nil>
STEP: delete the pod
Dec 16 23:28:53.445: INFO: Waiting for pod downwardapi-volume-66205af2-2bd3-46f0-bbfd-e758b41edf51 to disappear
Dec 16 23:28:53.447: INFO: Pod downwardapi-volume-66205af2-2bd3-46f0-bbfd-e758b41edf51 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:28:53.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2354" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":151,"skipped":2561,"failed":0}
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:28:53.456: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 16 23:28:57.540: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1956 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:28:57.540: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:57.626: INFO: Exec stderr: ""
Dec 16 23:28:57.626: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1956 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:28:57.626: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:57.689: INFO: Exec stderr: ""
Dec 16 23:28:57.690: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1956 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:28:57.690: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:57.758: INFO: Exec stderr: ""
Dec 16 23:28:57.758: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1956 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:28:57.758: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:57.837: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 16 23:28:57.837: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1956 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:28:57.837: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:57.905: INFO: Exec stderr: ""
Dec 16 23:28:57.905: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1956 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:28:57.905: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:57.977: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 16 23:28:57.977: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1956 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:28:57.977: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:58.070: INFO: Exec stderr: ""
Dec 16 23:28:58.071: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1956 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:28:58.071: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:58.138: INFO: Exec stderr: ""
Dec 16 23:28:58.138: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1956 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:28:58.138: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:58.199: INFO: Exec stderr: ""
Dec 16 23:28:58.199: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1956 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:28:58.199: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:28:58.272: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:28:58.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1956" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2562,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:28:58.288: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 16 23:28:58.406: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5367  6b30523e-5068-45b3-a6e4-5567c7ad594d 37201 0 2020-12-16 23:28:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-16 23:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 23:28:58.407: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5367  6b30523e-5068-45b3-a6e4-5567c7ad594d 37202 0 2020-12-16 23:28:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-16 23:28:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 16 23:28:58.424: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5367  6b30523e-5068-45b3-a6e4-5567c7ad594d 37203 0 2020-12-16 23:28:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-16 23:28:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 23:28:58.424: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5367  6b30523e-5068-45b3-a6e4-5567c7ad594d 37204 0 2020-12-16 23:28:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-16 23:28:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:28:58.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5367" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":153,"skipped":2576,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:28:58.435: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-267025dd-a33d-4d6b-95f5-c20bf86b4e14 in namespace container-probe-829
Dec 16 23:29:02.498: INFO: Started pod busybox-267025dd-a33d-4d6b-95f5-c20bf86b4e14 in namespace container-probe-829
STEP: checking the pod's current state and verifying that restartCount is present
Dec 16 23:29:02.501: INFO: Initial restart count of pod busybox-267025dd-a33d-4d6b-95f5-c20bf86b4e14 is 0
Dec 16 23:29:54.705: INFO: Restart count of pod container-probe-829/busybox-267025dd-a33d-4d6b-95f5-c20bf86b4e14 is now 1 (52.203937839s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:29:54.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-829" for this suite.

• [SLOW TEST:56.302 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2579,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:29:54.738: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 16 23:29:54.779: INFO: Waiting up to 5m0s for pod "pod-14ae7ad2-0abc-48ef-9d67-b6381040b497" in namespace "emptydir-2651" to be "Succeeded or Failed"
Dec 16 23:29:54.782: INFO: Pod "pod-14ae7ad2-0abc-48ef-9d67-b6381040b497": Phase="Pending", Reason="", readiness=false. Elapsed: 2.996951ms
Dec 16 23:29:56.841: INFO: Pod "pod-14ae7ad2-0abc-48ef-9d67-b6381040b497": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.061857369s
STEP: Saw pod success
Dec 16 23:29:56.842: INFO: Pod "pod-14ae7ad2-0abc-48ef-9d67-b6381040b497" satisfied condition "Succeeded or Failed"
Dec 16 23:29:56.859: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-14ae7ad2-0abc-48ef-9d67-b6381040b497 container test-container: <nil>
STEP: delete the pod
Dec 16 23:29:56.938: INFO: Waiting for pod pod-14ae7ad2-0abc-48ef-9d67-b6381040b497 to disappear
Dec 16 23:29:56.958: INFO: Pod pod-14ae7ad2-0abc-48ef-9d67-b6381040b497 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:29:56.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2651" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2594,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:29:56.996: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:29:57.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4549" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":156,"skipped":2605,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:29:57.192: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Dec 16 23:29:57.239: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 16 23:29:57.245: INFO: Waiting for terminating namespaces to be deleted...
Dec 16 23:29:57.248: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-1-217 before test
Dec 16 23:29:57.253: INFO: calico-node-ldh6v from kube-system started at 2020-12-16 21:14:55 +0000 UTC (1 container statuses recorded)
Dec 16 23:29:57.253: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 23:29:57.253: INFO: kube-proxy-v4mt9 from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 16 23:29:57.253: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 23:29:57.253: INFO: nginx-proxy-ip-172-31-1-217 from kube-system started at 2020-12-16 21:14:55 +0000 UTC (1 container statuses recorded)
Dec 16 23:29:57.253: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 16 23:29:57.253: INFO: sonobuoy from sonobuoy started at 2020-12-16 22:33:28 +0000 UTC (1 container statuses recorded)
Dec 16 23:29:57.253: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 16 23:29:57.253: INFO: sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-jkfz6 from sonobuoy started at 2020-12-16 22:33:30 +0000 UTC (2 container statuses recorded)
Dec 16 23:29:57.254: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 23:29:57.254: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 23:29:57.254: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-174 before test
Dec 16 23:29:57.259: INFO: calico-node-84s5l from kube-system started at 2020-12-16 21:14:54 +0000 UTC (1 container statuses recorded)
Dec 16 23:29:57.259: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 23:29:57.260: INFO: coredns-74ff55c5b-96bcf from kube-system started at 2020-12-16 21:15:19 +0000 UTC (1 container statuses recorded)
Dec 16 23:29:57.260: INFO: 	Container coredns ready: true, restart count 0
Dec 16 23:29:57.260: INFO: coredns-74ff55c5b-t4nvx from kube-system started at 2020-12-16 22:37:05 +0000 UTC (1 container statuses recorded)
Dec 16 23:29:57.260: INFO: 	Container coredns ready: true, restart count 0
Dec 16 23:29:57.260: INFO: kube-proxy-zpdbw from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 16 23:29:57.260: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 23:29:57.260: INFO: nginx-proxy-ip-172-31-6-174 from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 16 23:29:57.260: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 16 23:29:57.260: INFO: sonobuoy-e2e-job-1783c268d2664b69 from sonobuoy started at 2020-12-16 22:33:29 +0000 UTC (2 container statuses recorded)
Dec 16 23:29:57.260: INFO: 	Container e2e ready: true, restart count 0
Dec 16 23:29:57.260: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 23:29:57.260: INFO: sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-g7kjv from sonobuoy started at 2020-12-16 22:33:30 +0000 UTC (2 container statuses recorded)
Dec 16 23:29:57.260: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 23:29:57.260: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-457d9438-6c6f-4684-abc8-1ed6f4505cbe 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 172.31.1.217 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 172.31.1.217 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Dec 16 23:30:09.436: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.1.217 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:09.436: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.217, port: 54321
Dec 16 23:30:09.500: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.1.217:54321/hostname] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:09.500: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.217, port: 54321 UDP
Dec 16 23:30:09.576: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.1.217 54321] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:09.577: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Dec 16 23:30:14.649: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.1.217 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:14.649: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.217, port: 54321
Dec 16 23:30:14.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.1.217:54321/hostname] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:14.743: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.217, port: 54321 UDP
Dec 16 23:30:14.844: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.1.217 54321] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:14.844: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Dec 16 23:30:19.966: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.1.217 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:19.966: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.217, port: 54321
Dec 16 23:30:20.040: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.1.217:54321/hostname] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:20.040: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.217, port: 54321 UDP
Dec 16 23:30:20.125: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.1.217 54321] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:20.125: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Dec 16 23:30:25.197: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.1.217 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:25.197: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.217, port: 54321
Dec 16 23:30:25.275: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.1.217:54321/hostname] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:25.276: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.217, port: 54321 UDP
Dec 16 23:30:25.346: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.1.217 54321] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:25.346: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Dec 16 23:30:30.403: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.1.217 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:30.403: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.217, port: 54321
Dec 16 23:30:30.465: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.1.217:54321/hostname] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:30.466: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.217, port: 54321 UDP
Dec 16 23:30:30.545: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.1.217 54321] Namespace:sched-pred-7667 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:30:30.545: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: removing the label kubernetes.io/e2e-457d9438-6c6f-4684-abc8-1ed6f4505cbe off the node ip-172-31-1-217
STEP: verifying the node doesn't have the label kubernetes.io/e2e-457d9438-6c6f-4684-abc8-1ed6f4505cbe
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:30:35.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7667" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:38.468 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":157,"skipped":2606,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:30:35.660: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1216 23:30:45.821993      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 16 23:31:47.860: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:31:47.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7684" for this suite.

• [SLOW TEST:72.236 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":158,"skipped":2611,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:31:47.897: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9268
STEP: creating service affinity-nodeport-transition in namespace services-9268
STEP: creating replication controller affinity-nodeport-transition in namespace services-9268
I1216 23:31:47.989594      24 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-9268, replica count: 3
I1216 23:31:51.040302      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 23:31:51.055: INFO: Creating new exec pod
Dec 16 23:31:54.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-9268 exec execpod-affinityjbvcd -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Dec 16 23:31:54.266: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec 16 23:31:54.266: INFO: stdout: ""
Dec 16 23:31:54.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-9268 exec execpod-affinityjbvcd -- /bin/sh -x -c nc -zv -t -w 2 10.102.23.235 80'
Dec 16 23:31:54.427: INFO: stderr: "+ nc -zv -t -w 2 10.102.23.235 80\nConnection to 10.102.23.235 80 port [tcp/http] succeeded!\n"
Dec 16 23:31:54.427: INFO: stdout: ""
Dec 16 23:31:54.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-9268 exec execpod-affinityjbvcd -- /bin/sh -x -c nc -zv -t -w 2 172.31.1.217 31586'
Dec 16 23:31:54.591: INFO: stderr: "+ nc -zv -t -w 2 172.31.1.217 31586\nConnection to 172.31.1.217 31586 port [tcp/31586] succeeded!\n"
Dec 16 23:31:54.591: INFO: stdout: ""
Dec 16 23:31:54.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-9268 exec execpod-affinityjbvcd -- /bin/sh -x -c nc -zv -t -w 2 172.31.6.174 31586'
Dec 16 23:31:54.746: INFO: stderr: "+ nc -zv -t -w 2 172.31.6.174 31586\nConnection to 172.31.6.174 31586 port [tcp/31586] succeeded!\n"
Dec 16 23:31:54.746: INFO: stdout: ""
Dec 16 23:31:54.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-9268 exec execpod-affinityjbvcd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.1.217:31586/ ; done'
Dec 16 23:31:55.010: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n"
Dec 16 23:31:55.010: INFO: stdout: "\naffinity-nodeport-transition-s6sp7\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-s6sp7\naffinity-nodeport-transition-99clt\naffinity-nodeport-transition-s6sp7\naffinity-nodeport-transition-99clt\naffinity-nodeport-transition-s6sp7\naffinity-nodeport-transition-s6sp7\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-s6sp7\naffinity-nodeport-transition-99clt\naffinity-nodeport-transition-99clt\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l"
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-s6sp7
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-s6sp7
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-99clt
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-s6sp7
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-99clt
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-s6sp7
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-s6sp7
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-s6sp7
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-99clt
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-99clt
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.010: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-9268 exec execpod-affinityjbvcd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.1.217:31586/ ; done'
Dec 16 23:31:55.280: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:31586/\n"
Dec 16 23:31:55.280: INFO: stdout: "\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l\naffinity-nodeport-transition-8q22l"
Dec 16 23:31:55.280: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Received response from host: affinity-nodeport-transition-8q22l
Dec 16 23:31:55.281: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9268, will wait for the garbage collector to delete the pods
Dec 16 23:31:55.356: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.955078ms
Dec 16 23:31:55.956: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 600.148762ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:32:08.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9268" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:20.141 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":159,"skipped":2630,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:32:08.040: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-27a8d7ee-fa9d-4e34-953a-9030d0a9f8f8 in namespace container-probe-6756
Dec 16 23:32:10.110: INFO: Started pod liveness-27a8d7ee-fa9d-4e34-953a-9030d0a9f8f8 in namespace container-probe-6756
STEP: checking the pod's current state and verifying that restartCount is present
Dec 16 23:32:10.115: INFO: Initial restart count of pod liveness-27a8d7ee-fa9d-4e34-953a-9030d0a9f8f8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:36:11.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6756" for this suite.

• [SLOW TEST:243.149 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":2643,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:36:11.189: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-05a38131-c53f-497d-8944-da43b1e38607
STEP: Creating a pod to test consume configMaps
Dec 16 23:36:11.260: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef43354b-0da7-47cc-8273-7beb74f65745" in namespace "configmap-1271" to be "Succeeded or Failed"
Dec 16 23:36:11.263: INFO: Pod "pod-configmaps-ef43354b-0da7-47cc-8273-7beb74f65745": Phase="Pending", Reason="", readiness=false. Elapsed: 2.666586ms
Dec 16 23:36:13.271: INFO: Pod "pod-configmaps-ef43354b-0da7-47cc-8273-7beb74f65745": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011099985s
STEP: Saw pod success
Dec 16 23:36:13.271: INFO: Pod "pod-configmaps-ef43354b-0da7-47cc-8273-7beb74f65745" satisfied condition "Succeeded or Failed"
Dec 16 23:36:13.274: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-configmaps-ef43354b-0da7-47cc-8273-7beb74f65745 container agnhost-container: <nil>
STEP: delete the pod
Dec 16 23:36:13.299: INFO: Waiting for pod pod-configmaps-ef43354b-0da7-47cc-8273-7beb74f65745 to disappear
Dec 16 23:36:13.303: INFO: Pod pod-configmaps-ef43354b-0da7-47cc-8273-7beb74f65745 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:36:13.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1271" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":2657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:36:13.319: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 23:36:13.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c20bbe66-e3ea-403e-965d-1216c80fde98" in namespace "projected-9517" to be "Succeeded or Failed"
Dec 16 23:36:13.384: INFO: Pod "downwardapi-volume-c20bbe66-e3ea-403e-965d-1216c80fde98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18545ms
Dec 16 23:36:15.391: INFO: Pod "downwardapi-volume-c20bbe66-e3ea-403e-965d-1216c80fde98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009471955s
STEP: Saw pod success
Dec 16 23:36:15.391: INFO: Pod "downwardapi-volume-c20bbe66-e3ea-403e-965d-1216c80fde98" satisfied condition "Succeeded or Failed"
Dec 16 23:36:15.394: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-c20bbe66-e3ea-403e-965d-1216c80fde98 container client-container: <nil>
STEP: delete the pod
Dec 16 23:36:15.412: INFO: Waiting for pod downwardapi-volume-c20bbe66-e3ea-403e-965d-1216c80fde98 to disappear
Dec 16 23:36:15.416: INFO: Pod downwardapi-volume-c20bbe66-e3ea-403e-965d-1216c80fde98 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:36:15.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9517" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":2693,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:36:15.427: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:36:15.963: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 16 23:36:17.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758575, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758575, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758576, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758575, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:36:20.998: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:36:21.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2718" for this suite.
STEP: Destroying namespace "webhook-2718-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.778 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":163,"skipped":2695,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:36:21.205: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Dec 16 23:36:21.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-6255 api-versions'
Dec 16 23:36:21.465: INFO: stderr: ""
Dec 16 23:36:21.465: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:36:21.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6255" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":164,"skipped":2707,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:36:21.481: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:36:22.299: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 16 23:36:24.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758582, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758582, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758582, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758582, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:36:27.361: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:36:27.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9542" for this suite.
STEP: Destroying namespace "webhook-9542-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.151 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":165,"skipped":2746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:36:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:36:38.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4037" for this suite.

• [SLOW TEST:11.116 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":166,"skipped":2785,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:36:38.750: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:36:49.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4121" for this suite.

• [SLOW TEST:11.150 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":167,"skipped":2786,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:36:49.900: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 16 23:36:51.964: INFO: &Pod{ObjectMeta:{send-events-823901cb-3dc9-4516-8a61-eea91b8af202  events-1503  1de49f55-106b-4025-aca2-0031f9a3a586 39168 0 2020-12-16 23:36:49 +0000 UTC <nil> <nil> map[name:foo time:936278141] map[cni.projectcalico.org/podIP:192.168.140.22/32 cni.projectcalico.org/podIPs:192.168.140.22/32] [] []  [{e2e.test Update v1 2020-12-16 23:36:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-16 23:36:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-16 23:36:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.140.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n8t4b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n8t4b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n8t4b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:36:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:36:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:36:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:36:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:192.168.140.22,StartTime:2020-12-16 23:36:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 23:36:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://937c91f930f091c986993cb848e8940da758637bb24dbe8a9d46b591aed8bd5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.140.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 16 23:36:53.972: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 16 23:36:55.977: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:36:55.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1503" for this suite.

• [SLOW TEST:6.108 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":168,"skipped":2806,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:36:56.011: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:37:01.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3110" for this suite.

• [SLOW TEST:5.208 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":169,"skipped":2808,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:37:01.222: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-8b29417c-2ea3-4411-a7c5-d749fe97a326
STEP: Creating a pod to test consume secrets
Dec 16 23:37:01.309: INFO: Waiting up to 5m0s for pod "pod-secrets-a28b313a-6fea-4ff6-9126-f6f395968973" in namespace "secrets-4346" to be "Succeeded or Failed"
Dec 16 23:37:01.315: INFO: Pod "pod-secrets-a28b313a-6fea-4ff6-9126-f6f395968973": Phase="Pending", Reason="", readiness=false. Elapsed: 6.513603ms
Dec 16 23:37:03.323: INFO: Pod "pod-secrets-a28b313a-6fea-4ff6-9126-f6f395968973": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014323965s
Dec 16 23:37:05.330: INFO: Pod "pod-secrets-a28b313a-6fea-4ff6-9126-f6f395968973": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021513013s
STEP: Saw pod success
Dec 16 23:37:05.330: INFO: Pod "pod-secrets-a28b313a-6fea-4ff6-9126-f6f395968973" satisfied condition "Succeeded or Failed"
Dec 16 23:37:05.334: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-secrets-a28b313a-6fea-4ff6-9126-f6f395968973 container secret-volume-test: <nil>
STEP: delete the pod
Dec 16 23:37:05.355: INFO: Waiting for pod pod-secrets-a28b313a-6fea-4ff6-9126-f6f395968973 to disappear
Dec 16 23:37:05.358: INFO: Pod pod-secrets-a28b313a-6fea-4ff6-9126-f6f395968973 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:37:05.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4346" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":170,"skipped":2819,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:37:05.379: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 16 23:37:05.435: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:37:17.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8191" for this suite.

• [SLOW TEST:12.453 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":2828,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:37:17.832: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2214
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2214
I1216 23:37:17.921233      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2214, replica count: 2
I1216 23:37:20.971904      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 23:37:20.971: INFO: Creating new exec pod
Dec 16 23:37:23.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-2214 exec execpodcl6f7 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 16 23:37:24.396: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 16 23:37:24.396: INFO: stdout: ""
Dec 16 23:37:24.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-2214 exec execpodcl6f7 -- /bin/sh -x -c nc -zv -t -w 2 10.108.253.249 80'
Dec 16 23:37:24.554: INFO: stderr: "+ nc -zv -t -w 2 10.108.253.249 80\nConnection to 10.108.253.249 80 port [tcp/http] succeeded!\n"
Dec 16 23:37:24.554: INFO: stdout: ""
Dec 16 23:37:24.554: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:37:24.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2214" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.793 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":172,"skipped":2836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:37:24.626: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 23:37:24.716: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f4d66f4-c830-422b-b47b-db95f5f64e75" in namespace "projected-4643" to be "Succeeded or Failed"
Dec 16 23:37:24.719: INFO: Pod "downwardapi-volume-3f4d66f4-c830-422b-b47b-db95f5f64e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.948902ms
Dec 16 23:37:26.725: INFO: Pod "downwardapi-volume-3f4d66f4-c830-422b-b47b-db95f5f64e75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009205576s
STEP: Saw pod success
Dec 16 23:37:26.725: INFO: Pod "downwardapi-volume-3f4d66f4-c830-422b-b47b-db95f5f64e75" satisfied condition "Succeeded or Failed"
Dec 16 23:37:26.728: INFO: Trying to get logs from node ip-172-31-6-174 pod downwardapi-volume-3f4d66f4-c830-422b-b47b-db95f5f64e75 container client-container: <nil>
STEP: delete the pod
Dec 16 23:37:26.757: INFO: Waiting for pod downwardapi-volume-3f4d66f4-c830-422b-b47b-db95f5f64e75 to disappear
Dec 16 23:37:26.760: INFO: Pod downwardapi-volume-3f4d66f4-c830-422b-b47b-db95f5f64e75 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:37:26.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4643" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":173,"skipped":2861,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:37:26.771: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-czb4
STEP: Creating a pod to test atomic-volume-subpath
Dec 16 23:37:26.829: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-czb4" in namespace "subpath-8669" to be "Succeeded or Failed"
Dec 16 23:37:26.833: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079859ms
Dec 16 23:37:28.841: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011389424s
Dec 16 23:37:30.848: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Running", Reason="", readiness=true. Elapsed: 4.018096101s
Dec 16 23:37:32.854: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Running", Reason="", readiness=true. Elapsed: 6.024842952s
Dec 16 23:37:34.862: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Running", Reason="", readiness=true. Elapsed: 8.032364887s
Dec 16 23:37:36.869: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Running", Reason="", readiness=true. Elapsed: 10.039638264s
Dec 16 23:37:38.891: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Running", Reason="", readiness=true. Elapsed: 12.061375584s
Dec 16 23:37:40.900: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Running", Reason="", readiness=true. Elapsed: 14.070173734s
Dec 16 23:37:42.911: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Running", Reason="", readiness=true. Elapsed: 16.08109514s
Dec 16 23:37:44.918: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Running", Reason="", readiness=true. Elapsed: 18.088447912s
Dec 16 23:37:46.923: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Running", Reason="", readiness=true. Elapsed: 20.093740747s
Dec 16 23:37:48.934: INFO: Pod "pod-subpath-test-downwardapi-czb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.104622726s
STEP: Saw pod success
Dec 16 23:37:48.934: INFO: Pod "pod-subpath-test-downwardapi-czb4" satisfied condition "Succeeded or Failed"
Dec 16 23:37:48.938: INFO: Trying to get logs from node ip-172-31-6-174 pod pod-subpath-test-downwardapi-czb4 container test-container-subpath-downwardapi-czb4: <nil>
STEP: delete the pod
Dec 16 23:37:48.965: INFO: Waiting for pod pod-subpath-test-downwardapi-czb4 to disappear
Dec 16 23:37:48.970: INFO: Pod pod-subpath-test-downwardapi-czb4 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-czb4
Dec 16 23:37:48.971: INFO: Deleting pod "pod-subpath-test-downwardapi-czb4" in namespace "subpath-8669"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:37:48.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8669" for this suite.

• [SLOW TEST:22.214 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":174,"skipped":2882,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:37:48.985: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Dec 16 23:37:49.045: INFO: Waiting up to 5m0s for pod "client-containers-e5aa7e19-edeb-475d-a4e6-91991f682733" in namespace "containers-5072" to be "Succeeded or Failed"
Dec 16 23:37:49.047: INFO: Pod "client-containers-e5aa7e19-edeb-475d-a4e6-91991f682733": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08737ms
Dec 16 23:37:51.055: INFO: Pod "client-containers-e5aa7e19-edeb-475d-a4e6-91991f682733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009264582s
STEP: Saw pod success
Dec 16 23:37:51.055: INFO: Pod "client-containers-e5aa7e19-edeb-475d-a4e6-91991f682733" satisfied condition "Succeeded or Failed"
Dec 16 23:37:51.058: INFO: Trying to get logs from node ip-172-31-1-217 pod client-containers-e5aa7e19-edeb-475d-a4e6-91991f682733 container agnhost-container: <nil>
STEP: delete the pod
Dec 16 23:37:51.082: INFO: Waiting for pod client-containers-e5aa7e19-edeb-475d-a4e6-91991f682733 to disappear
Dec 16 23:37:51.084: INFO: Pod client-containers-e5aa7e19-edeb-475d-a4e6-91991f682733 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:37:51.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5072" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":175,"skipped":2897,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:37:51.101: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 23:37:51.168: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a06310a-9fc5-439f-b559-4f7c7bbd9644" in namespace "downward-api-5001" to be "Succeeded or Failed"
Dec 16 23:37:51.171: INFO: Pod "downwardapi-volume-2a06310a-9fc5-439f-b559-4f7c7bbd9644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.791068ms
Dec 16 23:37:53.177: INFO: Pod "downwardapi-volume-2a06310a-9fc5-439f-b559-4f7c7bbd9644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008860865s
STEP: Saw pod success
Dec 16 23:37:53.177: INFO: Pod "downwardapi-volume-2a06310a-9fc5-439f-b559-4f7c7bbd9644" satisfied condition "Succeeded or Failed"
Dec 16 23:37:53.181: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-2a06310a-9fc5-439f-b559-4f7c7bbd9644 container client-container: <nil>
STEP: delete the pod
Dec 16 23:37:53.197: INFO: Waiting for pod downwardapi-volume-2a06310a-9fc5-439f-b559-4f7c7bbd9644 to disappear
Dec 16 23:37:53.203: INFO: Pod downwardapi-volume-2a06310a-9fc5-439f-b559-4f7c7bbd9644 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:37:53.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5001" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":2918,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:37:53.217: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-93e98a68-1dc6-46cc-9290-c3ad626697a2
STEP: Creating a pod to test consume secrets
Dec 16 23:37:53.270: INFO: Waiting up to 5m0s for pod "pod-secrets-74f1d4c2-6308-4e73-896a-28e08eab504f" in namespace "secrets-5626" to be "Succeeded or Failed"
Dec 16 23:37:53.273: INFO: Pod "pod-secrets-74f1d4c2-6308-4e73-896a-28e08eab504f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.379189ms
Dec 16 23:37:55.280: INFO: Pod "pod-secrets-74f1d4c2-6308-4e73-896a-28e08eab504f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009535041s
STEP: Saw pod success
Dec 16 23:37:55.280: INFO: Pod "pod-secrets-74f1d4c2-6308-4e73-896a-28e08eab504f" satisfied condition "Succeeded or Failed"
Dec 16 23:37:55.283: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-secrets-74f1d4c2-6308-4e73-896a-28e08eab504f container secret-volume-test: <nil>
STEP: delete the pod
Dec 16 23:37:55.301: INFO: Waiting for pod pod-secrets-74f1d4c2-6308-4e73-896a-28e08eab504f to disappear
Dec 16 23:37:55.306: INFO: Pod pod-secrets-74f1d4c2-6308-4e73-896a-28e08eab504f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:37:55.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5626" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":177,"skipped":2932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:37:55.317: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:37:55.708: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 16 23:37:57.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758675, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758675, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758675, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758675, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:38:00.744: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 16 23:38:02.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=webhook-4255 attach --namespace=webhook-4255 to-be-attached-pod -i -c=container1'
Dec 16 23:38:02.928: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:38:02.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4255" for this suite.
STEP: Destroying namespace "webhook-4255-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.730 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":178,"skipped":3016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:38:03.047: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 16 23:38:03.119: INFO: Waiting up to 5m0s for pod "pod-90d65ca3-fa78-4fd7-9dc5-ecb5698728ea" in namespace "emptydir-904" to be "Succeeded or Failed"
Dec 16 23:38:03.122: INFO: Pod "pod-90d65ca3-fa78-4fd7-9dc5-ecb5698728ea": Phase="Pending", Reason="", readiness=false. Elapsed: 3.127138ms
Dec 16 23:38:05.129: INFO: Pod "pod-90d65ca3-fa78-4fd7-9dc5-ecb5698728ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009669199s
STEP: Saw pod success
Dec 16 23:38:05.129: INFO: Pod "pod-90d65ca3-fa78-4fd7-9dc5-ecb5698728ea" satisfied condition "Succeeded or Failed"
Dec 16 23:38:05.132: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-90d65ca3-fa78-4fd7-9dc5-ecb5698728ea container test-container: <nil>
STEP: delete the pod
Dec 16 23:38:05.150: INFO: Waiting for pod pod-90d65ca3-fa78-4fd7-9dc5-ecb5698728ea to disappear
Dec 16 23:38:05.152: INFO: Pod pod-90d65ca3-fa78-4fd7-9dc5-ecb5698728ea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:38:05.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-904" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":3051,"failed":0}

------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:38:05.162: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:38:05.208: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 16 23:38:10.220: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 16 23:38:10.220: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 16 23:38:12.256: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4298  e7d78ecd-be71-42d3-84aa-5898ecc6ccaf 39992 1 2020-12-16 23:38:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2020-12-16 23:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-16 23:38:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00420dde8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-16 23:38:10 +0000 UTC,LastTransitionTime:2020-12-16 23:38:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2020-12-16 23:38:11 +0000 UTC,LastTransitionTime:2020-12-16 23:38:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 16 23:38:12.259: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-4298  b93e47a0-38ed-4e32-9f80-b49dfbe30dd2 39981 1 2020-12-16 23:38:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e7d78ecd-be71-42d3-84aa-5898ecc6ccaf 0xc001253b47 0xc001253b48}] []  [{kube-controller-manager Update apps/v1 2020-12-16 23:38:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7d78ecd-be71-42d3-84aa-5898ecc6ccaf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001253c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 16 23:38:12.261: INFO: Pod "test-cleanup-deployment-685c4f8568-rgcjt" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-rgcjt test-cleanup-deployment-685c4f8568- deployment-4298  24d1eaf6-d6c0-4928-ae3f-20cec7891add 39980 0 2020-12-16 23:38:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[cni.projectcalico.org/podIP:192.168.140.34/32 cni.projectcalico.org/podIPs:192.168.140.34/32] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 b93e47a0-38ed-4e32-9f80-b49dfbe30dd2 0xc006bde147 0xc006bde148}] []  [{calico Update v1 2020-12-16 23:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-12-16 23:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b93e47a0-38ed-4e32-9f80-b49dfbe30dd2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 23:38:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.140.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-58k6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-58k6b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-58k6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:38:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:38:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:192.168.140.34,StartTime:2020-12-16 23:38:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 23:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://f6a44e9e58ae79a4e7eec7a8ab42e16b53cc1d75d78fd2904d4f306049b6d745,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.140.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:38:12.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4298" for this suite.

• [SLOW TEST:7.109 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":180,"skipped":3051,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:38:12.272: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:38:12.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5657" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":181,"skipped":3061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:38:12.391: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Dec 16 23:38:14.451: INFO: Pod pod-hostip-492ae5a9-1256-42dc-a325-f2387c3a3f2e has hostIP: 172.31.1.217
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:38:14.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1021" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":182,"skipped":3084,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:38:14.461: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 23:38:14.507: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9863fded-f6b2-4395-89ad-20746c071763" in namespace "projected-9426" to be "Succeeded or Failed"
Dec 16 23:38:14.510: INFO: Pod "downwardapi-volume-9863fded-f6b2-4395-89ad-20746c071763": Phase="Pending", Reason="", readiness=false. Elapsed: 2.775304ms
Dec 16 23:38:16.515: INFO: Pod "downwardapi-volume-9863fded-f6b2-4395-89ad-20746c071763": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008155143s
STEP: Saw pod success
Dec 16 23:38:16.516: INFO: Pod "downwardapi-volume-9863fded-f6b2-4395-89ad-20746c071763" satisfied condition "Succeeded or Failed"
Dec 16 23:38:16.519: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-9863fded-f6b2-4395-89ad-20746c071763 container client-container: <nil>
STEP: delete the pod
Dec 16 23:38:16.545: INFO: Waiting for pod downwardapi-volume-9863fded-f6b2-4395-89ad-20746c071763 to disappear
Dec 16 23:38:16.553: INFO: Pod downwardapi-volume-9863fded-f6b2-4395-89ad-20746c071763 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:38:16.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9426" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":183,"skipped":3085,"failed":0}
S
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:38:16.564: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2469, will wait for the garbage collector to delete the pods
Dec 16 23:38:18.681: INFO: Deleting Job.batch foo took: 7.721223ms
Dec 16 23:38:19.281: INFO: Terminating Job.batch foo pods took: 600.146758ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:38:57.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2469" for this suite.

• [SLOW TEST:41.346 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":184,"skipped":3086,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:38:57.911: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:38:58.325: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:39:01.382: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:39:01.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3855" for this suite.
STEP: Destroying namespace "webhook-3855-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":185,"skipped":3114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:39:01.819: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 16 23:39:03.923: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:39:03.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5023" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":186,"skipped":3205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:39:03.954: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:39:04.006: INFO: Waiting up to 5m0s for pod "busybox-user-65534-dc95dd80-9f24-4605-bbcb-ccf59a3940fd" in namespace "security-context-test-1937" to be "Succeeded or Failed"
Dec 16 23:39:04.009: INFO: Pod "busybox-user-65534-dc95dd80-9f24-4605-bbcb-ccf59a3940fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.756149ms
Dec 16 23:39:06.020: INFO: Pod "busybox-user-65534-dc95dd80-9f24-4605-bbcb-ccf59a3940fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013820844s
Dec 16 23:39:06.020: INFO: Pod "busybox-user-65534-dc95dd80-9f24-4605-bbcb-ccf59a3940fd" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:39:06.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1937" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3228,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:39:06.031: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Dec 16 23:39:06.075: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:39:25.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2951" for this suite.

• [SLOW TEST:19.204 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":188,"skipped":3244,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:39:25.235: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-2455ecb1-5d8d-4638-ac30-afe6295c80a4
STEP: Creating secret with name secret-projected-all-test-volume-48448fc4-3079-493f-a467-244be071d1b0
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 16 23:39:25.294: INFO: Waiting up to 5m0s for pod "projected-volume-289771c9-0ca9-4a92-a754-3e6c81338046" in namespace "projected-6691" to be "Succeeded or Failed"
Dec 16 23:39:25.297: INFO: Pod "projected-volume-289771c9-0ca9-4a92-a754-3e6c81338046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.929529ms
Dec 16 23:39:27.312: INFO: Pod "projected-volume-289771c9-0ca9-4a92-a754-3e6c81338046": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017771818s
STEP: Saw pod success
Dec 16 23:39:27.312: INFO: Pod "projected-volume-289771c9-0ca9-4a92-a754-3e6c81338046" satisfied condition "Succeeded or Failed"
Dec 16 23:39:27.315: INFO: Trying to get logs from node ip-172-31-1-217 pod projected-volume-289771c9-0ca9-4a92-a754-3e6c81338046 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 16 23:39:27.337: INFO: Waiting for pod projected-volume-289771c9-0ca9-4a92-a754-3e6c81338046 to disappear
Dec 16 23:39:27.339: INFO: Pod projected-volume-289771c9-0ca9-4a92-a754-3e6c81338046 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:39:27.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6691" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:39:27.353: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:39:27.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9134" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":190,"skipped":3274,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:39:27.483: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Dec 16 23:39:27.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2295 create -f -'
Dec 16 23:39:27.920: INFO: stderr: ""
Dec 16 23:39:27.920: INFO: stdout: "pod/pause created\n"
Dec 16 23:39:27.920: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 16 23:39:27.920: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2295" to be "running and ready"
Dec 16 23:39:27.924: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.774162ms
Dec 16 23:39:29.931: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011522589s
Dec 16 23:39:29.931: INFO: Pod "pause" satisfied condition "running and ready"
Dec 16 23:39:29.931: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 16 23:39:29.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2295 label pods pause testing-label=testing-label-value'
Dec 16 23:39:30.021: INFO: stderr: ""
Dec 16 23:39:30.021: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 16 23:39:30.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2295 get pod pause -L testing-label'
Dec 16 23:39:30.110: INFO: stderr: ""
Dec 16 23:39:30.110: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 16 23:39:30.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2295 label pods pause testing-label-'
Dec 16 23:39:30.211: INFO: stderr: ""
Dec 16 23:39:30.211: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 16 23:39:30.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2295 get pod pause -L testing-label'
Dec 16 23:39:30.300: INFO: stderr: ""
Dec 16 23:39:30.300: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Dec 16 23:39:30.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2295 delete --grace-period=0 --force -f -'
Dec 16 23:39:30.401: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 23:39:30.401: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 16 23:39:30.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2295 get rc,svc -l name=pause --no-headers'
Dec 16 23:39:30.486: INFO: stderr: "No resources found in kubectl-2295 namespace.\n"
Dec 16 23:39:30.486: INFO: stdout: ""
Dec 16 23:39:30.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2295 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 16 23:39:30.563: INFO: stderr: ""
Dec 16 23:39:30.563: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:39:30.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2295" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":191,"skipped":3291,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:39:30.575: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Dec 16 23:39:30.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2643 cluster-info'
Dec 16 23:39:30.699: INFO: stderr: ""
Dec 16 23:39:30.699: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:39:30.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2643" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":192,"skipped":3300,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:39:30.717: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:39:30.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6988" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":193,"skipped":3319,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:39:30.813: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3777
STEP: creating service affinity-clusterip in namespace services-3777
STEP: creating replication controller affinity-clusterip in namespace services-3777
I1216 23:39:30.873946      24 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-3777, replica count: 3
I1216 23:39:33.924984      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 23:39:33.945: INFO: Creating new exec pod
Dec 16 23:39:36.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-3777 exec execpod-affinitybtmsl -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Dec 16 23:39:37.179: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec 16 23:39:37.179: INFO: stdout: ""
Dec 16 23:39:37.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-3777 exec execpod-affinitybtmsl -- /bin/sh -x -c nc -zv -t -w 2 10.107.71.249 80'
Dec 16 23:39:37.336: INFO: stderr: "+ nc -zv -t -w 2 10.107.71.249 80\nConnection to 10.107.71.249 80 port [tcp/http] succeeded!\n"
Dec 16 23:39:37.336: INFO: stdout: ""
Dec 16 23:39:37.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-3777 exec execpod-affinitybtmsl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.71.249:80/ ; done'
Dec 16 23:39:37.609: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.71.249:80/\n"
Dec 16 23:39:37.609: INFO: stdout: "\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg\naffinity-clusterip-xkqdg"
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Received response from host: affinity-clusterip-xkqdg
Dec 16 23:39:37.609: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3777, will wait for the garbage collector to delete the pods
Dec 16 23:39:37.686: INFO: Deleting ReplicationController affinity-clusterip took: 8.281036ms
Dec 16 23:39:38.286: INFO: Terminating ReplicationController affinity-clusterip pods took: 600.275332ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:40:27.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3777" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:57.146 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":194,"skipped":3350,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:40:27.961: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:40:28.341: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 16 23:40:30.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758828, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758828, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758828, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743758828, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:40:33.407: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:40:33.413: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:40:34.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2957" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.771 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":195,"skipped":3358,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:40:34.737: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 23:40:34.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5702543f-05f2-45fd-a74c-e54baf875434" in namespace "downward-api-4877" to be "Succeeded or Failed"
Dec 16 23:40:34.817: INFO: Pod "downwardapi-volume-5702543f-05f2-45fd-a74c-e54baf875434": Phase="Pending", Reason="", readiness=false. Elapsed: 14.239556ms
Dec 16 23:40:36.826: INFO: Pod "downwardapi-volume-5702543f-05f2-45fd-a74c-e54baf875434": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022566604s
STEP: Saw pod success
Dec 16 23:40:36.826: INFO: Pod "downwardapi-volume-5702543f-05f2-45fd-a74c-e54baf875434" satisfied condition "Succeeded or Failed"
Dec 16 23:40:36.829: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-5702543f-05f2-45fd-a74c-e54baf875434 container client-container: <nil>
STEP: delete the pod
Dec 16 23:40:36.851: INFO: Waiting for pod downwardapi-volume-5702543f-05f2-45fd-a74c-e54baf875434 to disappear
Dec 16 23:40:36.855: INFO: Pod downwardapi-volume-5702543f-05f2-45fd-a74c-e54baf875434 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:40:36.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4877" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3365,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:40:36.864: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:40:36.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9510" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":197,"skipped":3395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:40:37.023: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 16 23:40:37.105: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 23:41:37.142: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:41:37.145: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Dec 16 23:41:39.261: INFO: found a healthy node: ip-172-31-1-217
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:41:55.377: INFO: pods created so far: [1 1 1]
Dec 16 23:41:55.377: INFO: length of pods created so far: 3
Dec 16 23:42:13.391: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:42:20.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3249" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:42:20.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8639" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:103.471 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":198,"skipped":3442,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:42:20.498: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:42:20.606: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3eb9e750-36e7-4cf7-af48-56e453ae8b57", Controller:(*bool)(0xc0049b47fe), BlockOwnerDeletion:(*bool)(0xc0049b47ff)}}
Dec 16 23:42:20.633: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2dfe463f-2735-4292-8e11-999c2c5128ef", Controller:(*bool)(0xc0049e6436), BlockOwnerDeletion:(*bool)(0xc0049e6437)}}
Dec 16 23:42:20.663: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a46d555c-78c0-4579-9890-9e832c06dfcb", Controller:(*bool)(0xc0049e663a), BlockOwnerDeletion:(*bool)(0xc0049e663b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:42:25.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3258" for this suite.

• [SLOW TEST:5.205 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":199,"skipped":3446,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:42:25.703: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1216 23:42:26.408605      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 16 23:43:28.427: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:43:28.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3024" for this suite.

• [SLOW TEST:62.738 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":200,"skipped":3460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:43:28.444: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Dec 16 23:43:28.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-939 create -f -'
Dec 16 23:43:28.797: INFO: stderr: ""
Dec 16 23:43:28.798: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 16 23:43:29.805: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 23:43:29.805: INFO: Found 0 / 1
Dec 16 23:43:30.817: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 23:43:30.817: INFO: Found 1 / 1
Dec 16 23:43:30.817: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 16 23:43:30.826: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 23:43:30.826: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 16 23:43:30.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-939 patch pod agnhost-primary-xjk5g -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 16 23:43:30.981: INFO: stderr: ""
Dec 16 23:43:30.981: INFO: stdout: "pod/agnhost-primary-xjk5g patched\n"
STEP: checking annotations
Dec 16 23:43:30.991: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 23:43:30.991: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:43:30.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-939" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":201,"skipped":3485,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:43:31.019: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-7236
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 16 23:43:31.121: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 16 23:43:31.147: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 23:43:33.154: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:43:35.154: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:43:37.159: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:43:39.154: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:43:41.156: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:43:43.155: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:43:45.155: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:43:47.156: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:43:49.155: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:43:51.161: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 16 23:43:51.171: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 16 23:43:53.215: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 16 23:43:53.215: INFO: Going to poll 192.168.140.57 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Dec 16 23:43:53.217: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.140.57 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7236 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:43:53.217: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:43:54.282: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 16 23:43:54.282: INFO: Going to poll 192.168.91.8 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Dec 16 23:43:54.287: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.91.8 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7236 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:43:54.287: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:43:55.353: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:43:55.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7236" for this suite.

• [SLOW TEST:24.351 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":202,"skipped":3530,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:43:55.380: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:43:55.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-7315 version'
Dec 16 23:43:55.530: INFO: stderr: ""
Dec 16 23:43:55.530: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.0\", GitCommit:\"af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38\", GitTreeState:\"clean\", BuildDate:\"2020-12-08T17:59:43Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.0\", GitCommit:\"af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38\", GitTreeState:\"clean\", BuildDate:\"2020-12-08T17:51:19Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:43:55.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7315" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":203,"skipped":3547,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:43:55.543: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Dec 16 23:43:55.587: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-3449 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:43:55.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3449" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":204,"skipped":3548,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:43:55.692: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-z768r in namespace proxy-6335
I1216 23:43:55.772628      24 runners.go:190] Created replication controller with name: proxy-service-z768r, namespace: proxy-6335, replica count: 1
I1216 23:43:56.822910      24 runners.go:190] proxy-service-z768r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1216 23:43:57.823069      24 runners.go:190] proxy-service-z768r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1216 23:43:58.823287      24 runners.go:190] proxy-service-z768r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1216 23:43:59.823487      24 runners.go:190] proxy-service-z768r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1216 23:44:00.823664      24 runners.go:190] proxy-service-z768r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1216 23:44:01.823815      24 runners.go:190] proxy-service-z768r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1216 23:44:02.824066      24 runners.go:190] proxy-service-z768r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1216 23:44:03.824259      24 runners.go:190] proxy-service-z768r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1216 23:44:04.824458      24 runners.go:190] proxy-service-z768r Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 23:44:04.840: INFO: setup took 9.112460954s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 16 23:44:04.847: INFO: (0) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 7.010931ms)
Dec 16 23:44:04.850: INFO: (0) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.520159ms)
Dec 16 23:44:04.856: INFO: (0) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 15.876041ms)
Dec 16 23:44:04.857: INFO: (0) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 16.407508ms)
Dec 16 23:44:04.857: INFO: (0) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 16.800854ms)
Dec 16 23:44:04.861: INFO: (0) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 20.955058ms)
Dec 16 23:44:04.863: INFO: (0) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 23.076927ms)
Dec 16 23:44:04.864: INFO: (0) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 23.686148ms)
Dec 16 23:44:04.869: INFO: (0) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 28.274879ms)
Dec 16 23:44:04.869: INFO: (0) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 28.877454ms)
Dec 16 23:44:04.870: INFO: (0) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 28.960784ms)
Dec 16 23:44:04.870: INFO: (0) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 29.388068ms)
Dec 16 23:44:04.870: INFO: (0) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 29.540713ms)
Dec 16 23:44:04.870: INFO: (0) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 29.318508ms)
Dec 16 23:44:04.871: INFO: (0) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 30.880389ms)
Dec 16 23:44:04.874: INFO: (0) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 33.407598ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 10.139643ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 10.733547ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 11.019782ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 10.977746ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 11.035581ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 10.629215ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 10.808747ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 10.891441ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 11.050942ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 11.321914ms)
Dec 16 23:44:04.885: INFO: (1) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 10.711982ms)
Dec 16 23:44:04.887: INFO: (1) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 12.530612ms)
Dec 16 23:44:04.888: INFO: (1) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 13.529235ms)
Dec 16 23:44:04.889: INFO: (1) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 14.615033ms)
Dec 16 23:44:04.889: INFO: (1) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 14.547802ms)
Dec 16 23:44:04.889: INFO: (1) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 15.306603ms)
Dec 16 23:44:04.894: INFO: (2) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 4.698654ms)
Dec 16 23:44:04.894: INFO: (2) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 5.242792ms)
Dec 16 23:44:04.900: INFO: (2) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.429481ms)
Dec 16 23:44:04.900: INFO: (2) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.295364ms)
Dec 16 23:44:04.900: INFO: (2) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 10.666863ms)
Dec 16 23:44:04.900: INFO: (2) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 10.387129ms)
Dec 16 23:44:04.901: INFO: (2) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 11.09656ms)
Dec 16 23:44:04.901: INFO: (2) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 11.179354ms)
Dec 16 23:44:04.901: INFO: (2) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 11.24063ms)
Dec 16 23:44:04.901: INFO: (2) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 11.293793ms)
Dec 16 23:44:04.904: INFO: (2) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 14.096519ms)
Dec 16 23:44:04.904: INFO: (2) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 13.808664ms)
Dec 16 23:44:04.904: INFO: (2) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 14.47249ms)
Dec 16 23:44:04.906: INFO: (2) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 16.091898ms)
Dec 16 23:44:04.906: INFO: (2) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 16.450578ms)
Dec 16 23:44:04.906: INFO: (2) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 16.444362ms)
Dec 16 23:44:04.912: INFO: (3) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 5.688762ms)
Dec 16 23:44:04.912: INFO: (3) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 6.001065ms)
Dec 16 23:44:04.913: INFO: (3) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 6.969234ms)
Dec 16 23:44:04.919: INFO: (3) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 11.412903ms)
Dec 16 23:44:04.919: INFO: (3) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 11.561924ms)
Dec 16 23:44:04.919: INFO: (3) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 12.586757ms)
Dec 16 23:44:04.920: INFO: (3) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 12.826044ms)
Dec 16 23:44:04.920: INFO: (3) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 12.65909ms)
Dec 16 23:44:04.920: INFO: (3) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 12.876443ms)
Dec 16 23:44:04.920: INFO: (3) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 13.415134ms)
Dec 16 23:44:04.923: INFO: (3) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 16.907804ms)
Dec 16 23:44:04.925: INFO: (3) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 17.813463ms)
Dec 16 23:44:04.925: INFO: (3) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 18.022604ms)
Dec 16 23:44:04.925: INFO: (3) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 18.214255ms)
Dec 16 23:44:04.925: INFO: (3) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 17.77225ms)
Dec 16 23:44:04.925: INFO: (3) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 18.292728ms)
Dec 16 23:44:04.932: INFO: (4) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 7.221737ms)
Dec 16 23:44:04.933: INFO: (4) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 7.457541ms)
Dec 16 23:44:04.933: INFO: (4) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 7.466442ms)
Dec 16 23:44:04.933: INFO: (4) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 7.664431ms)
Dec 16 23:44:04.940: INFO: (4) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 14.127808ms)
Dec 16 23:44:04.941: INFO: (4) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 15.256952ms)
Dec 16 23:44:04.941: INFO: (4) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 15.246597ms)
Dec 16 23:44:04.941: INFO: (4) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 15.246615ms)
Dec 16 23:44:04.941: INFO: (4) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 15.204992ms)
Dec 16 23:44:04.941: INFO: (4) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 15.324207ms)
Dec 16 23:44:04.941: INFO: (4) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 16.052178ms)
Dec 16 23:44:04.942: INFO: (4) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 15.972369ms)
Dec 16 23:44:04.942: INFO: (4) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 15.947138ms)
Dec 16 23:44:04.942: INFO: (4) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 16.231162ms)
Dec 16 23:44:04.943: INFO: (4) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 16.673123ms)
Dec 16 23:44:04.943: INFO: (4) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 16.720121ms)
Dec 16 23:44:04.952: INFO: (5) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 8.582263ms)
Dec 16 23:44:04.953: INFO: (5) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 9.872724ms)
Dec 16 23:44:04.953: INFO: (5) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.315152ms)
Dec 16 23:44:04.954: INFO: (5) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 10.49587ms)
Dec 16 23:44:04.954: INFO: (5) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.647564ms)
Dec 16 23:44:04.954: INFO: (5) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 11.252213ms)
Dec 16 23:44:04.955: INFO: (5) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 11.33302ms)
Dec 16 23:44:04.955: INFO: (5) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 11.009114ms)
Dec 16 23:44:04.955: INFO: (5) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 11.69773ms)
Dec 16 23:44:04.955: INFO: (5) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 11.400903ms)
Dec 16 23:44:04.955: INFO: (5) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 11.813419ms)
Dec 16 23:44:04.955: INFO: (5) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 11.798877ms)
Dec 16 23:44:04.956: INFO: (5) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 12.680745ms)
Dec 16 23:44:04.956: INFO: (5) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 13.064556ms)
Dec 16 23:44:04.956: INFO: (5) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 12.951363ms)
Dec 16 23:44:04.957: INFO: (5) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 13.029185ms)
Dec 16 23:44:04.967: INFO: (6) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 10.383908ms)
Dec 16 23:44:04.968: INFO: (6) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 10.757246ms)
Dec 16 23:44:04.968: INFO: (6) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.635668ms)
Dec 16 23:44:04.968: INFO: (6) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 11.109189ms)
Dec 16 23:44:04.968: INFO: (6) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 11.034187ms)
Dec 16 23:44:04.968: INFO: (6) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 11.579978ms)
Dec 16 23:44:04.968: INFO: (6) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 11.464632ms)
Dec 16 23:44:04.968: INFO: (6) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 11.340416ms)
Dec 16 23:44:04.968: INFO: (6) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 11.408302ms)
Dec 16 23:44:04.969: INFO: (6) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 11.736202ms)
Dec 16 23:44:04.971: INFO: (6) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 14.09412ms)
Dec 16 23:44:04.973: INFO: (6) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 15.838665ms)
Dec 16 23:44:04.974: INFO: (6) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 17.278609ms)
Dec 16 23:44:04.975: INFO: (6) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 17.68433ms)
Dec 16 23:44:04.975: INFO: (6) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 18.236902ms)
Dec 16 23:44:04.975: INFO: (6) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 18.67794ms)
Dec 16 23:44:04.981: INFO: (7) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 5.415527ms)
Dec 16 23:44:04.982: INFO: (7) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 5.968808ms)
Dec 16 23:44:04.982: INFO: (7) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 6.155991ms)
Dec 16 23:44:04.987: INFO: (7) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 10.670092ms)
Dec 16 23:44:04.987: INFO: (7) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 10.966744ms)
Dec 16 23:44:04.987: INFO: (7) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 10.967673ms)
Dec 16 23:44:04.987: INFO: (7) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 11.690957ms)
Dec 16 23:44:04.987: INFO: (7) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 11.378693ms)
Dec 16 23:44:04.988: INFO: (7) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 11.69335ms)
Dec 16 23:44:04.988: INFO: (7) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 11.309974ms)
Dec 16 23:44:04.988: INFO: (7) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 11.601336ms)
Dec 16 23:44:04.989: INFO: (7) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 12.471414ms)
Dec 16 23:44:04.989: INFO: (7) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 12.415042ms)
Dec 16 23:44:04.989: INFO: (7) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 12.904785ms)
Dec 16 23:44:04.989: INFO: (7) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 13.317705ms)
Dec 16 23:44:04.991: INFO: (7) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 15.203182ms)
Dec 16 23:44:04.998: INFO: (8) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 6.202668ms)
Dec 16 23:44:04.998: INFO: (8) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 6.525688ms)
Dec 16 23:44:05.003: INFO: (8) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 11.890047ms)
Dec 16 23:44:05.005: INFO: (8) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 13.182803ms)
Dec 16 23:44:05.005: INFO: (8) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 14.238296ms)
Dec 16 23:44:05.006: INFO: (8) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 13.596335ms)
Dec 16 23:44:05.006: INFO: (8) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 13.872185ms)
Dec 16 23:44:05.006: INFO: (8) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 14.207886ms)
Dec 16 23:44:05.006: INFO: (8) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 14.648259ms)
Dec 16 23:44:05.006: INFO: (8) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 14.981035ms)
Dec 16 23:44:05.007: INFO: (8) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 14.844857ms)
Dec 16 23:44:05.007: INFO: (8) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 14.999731ms)
Dec 16 23:44:05.015: INFO: (8) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 22.853118ms)
Dec 16 23:44:05.018: INFO: (8) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 26.235039ms)
Dec 16 23:44:05.018: INFO: (8) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 26.401576ms)
Dec 16 23:44:05.018: INFO: (8) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 26.529912ms)
Dec 16 23:44:05.034: INFO: (9) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 14.963398ms)
Dec 16 23:44:05.037: INFO: (9) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 18.332254ms)
Dec 16 23:44:05.037: INFO: (9) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 17.864426ms)
Dec 16 23:44:05.037: INFO: (9) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 17.977057ms)
Dec 16 23:44:05.037: INFO: (9) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 17.983124ms)
Dec 16 23:44:05.042: INFO: (9) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 23.006783ms)
Dec 16 23:44:05.042: INFO: (9) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 23.739311ms)
Dec 16 23:44:05.043: INFO: (9) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 23.699038ms)
Dec 16 23:44:05.043: INFO: (9) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 23.949241ms)
Dec 16 23:44:05.043: INFO: (9) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 24.091775ms)
Dec 16 23:44:05.043: INFO: (9) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 24.717615ms)
Dec 16 23:44:05.043: INFO: (9) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 24.420686ms)
Dec 16 23:44:05.044: INFO: (9) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 25.054841ms)
Dec 16 23:44:05.044: INFO: (9) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 25.374269ms)
Dec 16 23:44:05.044: INFO: (9) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 25.326665ms)
Dec 16 23:44:05.044: INFO: (9) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 25.633521ms)
Dec 16 23:44:05.051: INFO: (10) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 6.155392ms)
Dec 16 23:44:05.051: INFO: (10) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 6.124725ms)
Dec 16 23:44:05.056: INFO: (10) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 11.18341ms)
Dec 16 23:44:05.056: INFO: (10) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 10.97616ms)
Dec 16 23:44:05.057: INFO: (10) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 11.678198ms)
Dec 16 23:44:05.057: INFO: (10) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 11.699755ms)
Dec 16 23:44:05.057: INFO: (10) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 11.449642ms)
Dec 16 23:44:05.057: INFO: (10) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 11.794786ms)
Dec 16 23:44:05.057: INFO: (10) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 12.248443ms)
Dec 16 23:44:05.060: INFO: (10) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 15.050176ms)
Dec 16 23:44:05.060: INFO: (10) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 15.196ms)
Dec 16 23:44:05.061: INFO: (10) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 16.461389ms)
Dec 16 23:44:05.063: INFO: (10) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 17.57273ms)
Dec 16 23:44:05.063: INFO: (10) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 17.919806ms)
Dec 16 23:44:05.063: INFO: (10) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 18.218617ms)
Dec 16 23:44:05.064: INFO: (10) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 18.377085ms)
Dec 16 23:44:05.073: INFO: (11) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 8.895372ms)
Dec 16 23:44:05.073: INFO: (11) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 9.341687ms)
Dec 16 23:44:05.073: INFO: (11) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 9.26441ms)
Dec 16 23:44:05.074: INFO: (11) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 9.315036ms)
Dec 16 23:44:05.074: INFO: (11) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 10.565791ms)
Dec 16 23:44:05.074: INFO: (11) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 9.862919ms)
Dec 16 23:44:05.075: INFO: (11) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 10.732765ms)
Dec 16 23:44:05.080: INFO: (11) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 15.675567ms)
Dec 16 23:44:05.080: INFO: (11) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 15.991388ms)
Dec 16 23:44:05.080: INFO: (11) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 16.337892ms)
Dec 16 23:44:05.080: INFO: (11) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 15.721164ms)
Dec 16 23:44:05.083: INFO: (11) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 18.734388ms)
Dec 16 23:44:05.083: INFO: (11) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 18.865397ms)
Dec 16 23:44:05.084: INFO: (11) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 19.874287ms)
Dec 16 23:44:05.084: INFO: (11) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 19.443094ms)
Dec 16 23:44:05.084: INFO: (11) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 19.772077ms)
Dec 16 23:44:05.093: INFO: (12) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 8.821479ms)
Dec 16 23:44:05.094: INFO: (12) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 9.00083ms)
Dec 16 23:44:05.094: INFO: (12) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 9.314767ms)
Dec 16 23:44:05.094: INFO: (12) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 9.507948ms)
Dec 16 23:44:05.095: INFO: (12) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 10.04506ms)
Dec 16 23:44:05.095: INFO: (12) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.04122ms)
Dec 16 23:44:05.095: INFO: (12) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 10.441329ms)
Dec 16 23:44:05.095: INFO: (12) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 10.26336ms)
Dec 16 23:44:05.095: INFO: (12) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 10.246197ms)
Dec 16 23:44:05.095: INFO: (12) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 10.434205ms)
Dec 16 23:44:05.098: INFO: (12) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 12.942372ms)
Dec 16 23:44:05.099: INFO: (12) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 14.892713ms)
Dec 16 23:44:05.100: INFO: (12) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 15.507377ms)
Dec 16 23:44:05.101: INFO: (12) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 16.421537ms)
Dec 16 23:44:05.101: INFO: (12) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 16.273151ms)
Dec 16 23:44:05.101: INFO: (12) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 16.177214ms)
Dec 16 23:44:05.109: INFO: (13) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 7.367112ms)
Dec 16 23:44:05.113: INFO: (13) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 11.970612ms)
Dec 16 23:44:05.116: INFO: (13) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 14.407836ms)
Dec 16 23:44:05.116: INFO: (13) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 14.638635ms)
Dec 16 23:44:05.116: INFO: (13) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 14.539085ms)
Dec 16 23:44:05.117: INFO: (13) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 14.986928ms)
Dec 16 23:44:05.116: INFO: (13) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 14.519203ms)
Dec 16 23:44:05.117: INFO: (13) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 15.47211ms)
Dec 16 23:44:05.117: INFO: (13) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 15.277194ms)
Dec 16 23:44:05.117: INFO: (13) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 15.969894ms)
Dec 16 23:44:05.118: INFO: (13) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 16.16419ms)
Dec 16 23:44:05.118: INFO: (13) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 15.914016ms)
Dec 16 23:44:05.118: INFO: (13) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 16.484987ms)
Dec 16 23:44:05.118: INFO: (13) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 16.172897ms)
Dec 16 23:44:05.118: INFO: (13) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 15.899956ms)
Dec 16 23:44:05.118: INFO: (13) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 16.524258ms)
Dec 16 23:44:05.129: INFO: (14) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.703215ms)
Dec 16 23:44:05.130: INFO: (14) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 11.557116ms)
Dec 16 23:44:05.131: INFO: (14) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 12.2255ms)
Dec 16 23:44:05.131: INFO: (14) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 11.992435ms)
Dec 16 23:44:05.131: INFO: (14) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 12.26216ms)
Dec 16 23:44:05.131: INFO: (14) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 12.247061ms)
Dec 16 23:44:05.131: INFO: (14) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 12.436684ms)
Dec 16 23:44:05.132: INFO: (14) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 13.102484ms)
Dec 16 23:44:05.132: INFO: (14) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 13.055867ms)
Dec 16 23:44:05.132: INFO: (14) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 13.015197ms)
Dec 16 23:44:05.133: INFO: (14) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 14.214584ms)
Dec 16 23:44:05.133: INFO: (14) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 14.141132ms)
Dec 16 23:44:05.135: INFO: (14) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 16.462434ms)
Dec 16 23:44:05.136: INFO: (14) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 16.957277ms)
Dec 16 23:44:05.136: INFO: (14) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 16.892025ms)
Dec 16 23:44:05.136: INFO: (14) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 16.803515ms)
Dec 16 23:44:05.147: INFO: (15) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 10.588502ms)
Dec 16 23:44:05.147: INFO: (15) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 10.120039ms)
Dec 16 23:44:05.151: INFO: (15) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 14.740218ms)
Dec 16 23:44:05.151: INFO: (15) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 15.096239ms)
Dec 16 23:44:05.151: INFO: (15) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 15.059171ms)
Dec 16 23:44:05.152: INFO: (15) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 15.711206ms)
Dec 16 23:44:05.152: INFO: (15) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 15.987562ms)
Dec 16 23:44:05.152: INFO: (15) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 15.493264ms)
Dec 16 23:44:05.152: INFO: (15) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 15.55018ms)
Dec 16 23:44:05.152: INFO: (15) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 16.037981ms)
Dec 16 23:44:05.152: INFO: (15) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 16.48894ms)
Dec 16 23:44:05.152: INFO: (15) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 16.384302ms)
Dec 16 23:44:05.153: INFO: (15) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 16.316561ms)
Dec 16 23:44:05.153: INFO: (15) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 16.883979ms)
Dec 16 23:44:05.153: INFO: (15) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 16.532874ms)
Dec 16 23:44:05.153: INFO: (15) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 17.222526ms)
Dec 16 23:44:05.159: INFO: (16) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 5.79079ms)
Dec 16 23:44:05.164: INFO: (16) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 9.932004ms)
Dec 16 23:44:05.170: INFO: (16) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 15.996551ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 16.847609ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 16.628756ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 16.811857ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 16.936664ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 16.558007ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 16.784606ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 16.480001ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 17.063995ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 17.113929ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 16.771854ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 17.188884ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 16.577991ms)
Dec 16 23:44:05.171: INFO: (16) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 17.512419ms)
Dec 16 23:44:05.179: INFO: (17) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 6.716602ms)
Dec 16 23:44:05.183: INFO: (17) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.147882ms)
Dec 16 23:44:05.183: INFO: (17) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 10.374401ms)
Dec 16 23:44:05.184: INFO: (17) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 11.26497ms)
Dec 16 23:44:05.185: INFO: (17) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 12.758091ms)
Dec 16 23:44:05.186: INFO: (17) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 13.175437ms)
Dec 16 23:44:05.186: INFO: (17) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 13.504831ms)
Dec 16 23:44:05.186: INFO: (17) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 14.306648ms)
Dec 16 23:44:05.186: INFO: (17) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 14.386876ms)
Dec 16 23:44:05.186: INFO: (17) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 13.911246ms)
Dec 16 23:44:05.187: INFO: (17) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 15.110788ms)
Dec 16 23:44:05.187: INFO: (17) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 15.314803ms)
Dec 16 23:44:05.188: INFO: (17) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 15.735625ms)
Dec 16 23:44:05.188: INFO: (17) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 15.775088ms)
Dec 16 23:44:05.189: INFO: (17) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 15.894243ms)
Dec 16 23:44:05.191: INFO: (17) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 18.240328ms)
Dec 16 23:44:05.202: INFO: (18) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 10.414147ms)
Dec 16 23:44:05.202: INFO: (18) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.385251ms)
Dec 16 23:44:05.202: INFO: (18) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 10.781766ms)
Dec 16 23:44:05.202: INFO: (18) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 10.738101ms)
Dec 16 23:44:05.202: INFO: (18) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 10.626394ms)
Dec 16 23:44:05.202: INFO: (18) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 10.67528ms)
Dec 16 23:44:05.202: INFO: (18) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 11.59267ms)
Dec 16 23:44:05.203: INFO: (18) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 11.596348ms)
Dec 16 23:44:05.203: INFO: (18) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 11.923983ms)
Dec 16 23:44:05.204: INFO: (18) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 12.245982ms)
Dec 16 23:44:05.204: INFO: (18) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 13.26853ms)
Dec 16 23:44:05.205: INFO: (18) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 14.079707ms)
Dec 16 23:44:05.207: INFO: (18) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 15.209786ms)
Dec 16 23:44:05.207: INFO: (18) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 15.890889ms)
Dec 16 23:44:05.207: INFO: (18) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 16.067755ms)
Dec 16 23:44:05.207: INFO: (18) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 15.625713ms)
Dec 16 23:44:05.213: INFO: (19) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:443/proxy/tlsrewritem... (200; 5.626786ms)
Dec 16 23:44:05.214: INFO: (19) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:162/proxy/: bar (200; 6.841138ms)
Dec 16 23:44:05.220: INFO: (19) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:162/proxy/: bar (200; 11.699893ms)
Dec 16 23:44:05.220: INFO: (19) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:1080/proxy/rewriteme">test<... (200; 11.581799ms)
Dec 16 23:44:05.220: INFO: (19) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:462/proxy/: tls qux (200; 12.224854ms)
Dec 16 23:44:05.220: INFO: (19) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:1080/proxy/rewriteme">... (200; 11.505359ms)
Dec 16 23:44:05.220: INFO: (19) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname2/proxy/: bar (200; 12.603914ms)
Dec 16 23:44:05.220: INFO: (19) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/: <a href="/api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb/proxy/rewriteme">test</a> (200; 12.471957ms)
Dec 16 23:44:05.220: INFO: (19) /api/v1/namespaces/proxy-6335/services/proxy-service-z768r:portname1/proxy/: foo (200; 12.55315ms)
Dec 16 23:44:05.220: INFO: (19) /api/v1/namespaces/proxy-6335/pods/proxy-service-z768r-dxpcb:160/proxy/: foo (200; 12.618778ms)
Dec 16 23:44:05.221: INFO: (19) /api/v1/namespaces/proxy-6335/pods/http:proxy-service-z768r-dxpcb:160/proxy/: foo (200; 12.773553ms)
Dec 16 23:44:05.221: INFO: (19) /api/v1/namespaces/proxy-6335/pods/https:proxy-service-z768r-dxpcb:460/proxy/: tls baz (200; 12.978548ms)
Dec 16 23:44:05.221: INFO: (19) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname2/proxy/: tls qux (200; 13.1358ms)
Dec 16 23:44:05.222: INFO: (19) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname1/proxy/: foo (200; 14.003123ms)
Dec 16 23:44:05.222: INFO: (19) /api/v1/namespaces/proxy-6335/services/http:proxy-service-z768r:portname2/proxy/: bar (200; 14.186155ms)
Dec 16 23:44:05.222: INFO: (19) /api/v1/namespaces/proxy-6335/services/https:proxy-service-z768r:tlsportname1/proxy/: tls baz (200; 14.318925ms)
STEP: deleting ReplicationController proxy-service-z768r in namespace proxy-6335, will wait for the garbage collector to delete the pods
Dec 16 23:44:05.288: INFO: Deleting ReplicationController proxy-service-z768r took: 9.790657ms
Dec 16 23:44:05.888: INFO: Terminating ReplicationController proxy-service-z768r pods took: 600.152634ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:44:27.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6335" for this suite.

• [SLOW TEST:32.228 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":205,"skipped":3562,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:44:27.921: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 16 23:44:33.079: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:44:34.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5232" for this suite.

• [SLOW TEST:6.239 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":206,"skipped":3577,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:44:34.161: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Dec 16 23:44:34.223: INFO: observed Pod pod-test in namespace pods-6261 in phase Pending conditions []
Dec 16 23:44:34.225: INFO: observed Pod pod-test in namespace pods-6261 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:44:34 +0000 UTC  }]
Dec 16 23:44:34.240: INFO: observed Pod pod-test in namespace pods-6261 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:44:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:44:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:44:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:44:34 +0000 UTC  }]
Dec 16 23:44:34.705: INFO: observed Pod pod-test in namespace pods-6261 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:44:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:44:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:44:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-16 23:44:34 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Dec 16 23:44:35.287: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Dec 16 23:44:35.330: INFO: observed event type ADDED
Dec 16 23:44:35.331: INFO: observed event type MODIFIED
Dec 16 23:44:35.331: INFO: observed event type MODIFIED
Dec 16 23:44:35.331: INFO: observed event type MODIFIED
Dec 16 23:44:35.331: INFO: observed event type MODIFIED
Dec 16 23:44:35.331: INFO: observed event type MODIFIED
Dec 16 23:44:35.334: INFO: observed event type MODIFIED
Dec 16 23:44:35.334: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:44:35.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6261" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":207,"skipped":3578,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:44:35.349: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:44:35.392: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:44:35.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2113" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":208,"skipped":3584,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:44:36.013: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 16 23:44:36.116: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:44:39.589: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:44:52.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6128" for this suite.

• [SLOW TEST:16.825 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":209,"skipped":3588,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:44:52.838: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-5171a6eb-5b44-410c-be42-4b475d91f850
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-5171a6eb-5b44-410c-be42-4b475d91f850
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:44:56.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4139" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3598,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:44:56.977: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-0f7e3ef3-b4c3-4d46-8191-1807743168b9 in namespace container-probe-1051
Dec 16 23:44:59.054: INFO: Started pod liveness-0f7e3ef3-b4c3-4d46-8191-1807743168b9 in namespace container-probe-1051
STEP: checking the pod's current state and verifying that restartCount is present
Dec 16 23:44:59.056: INFO: Initial restart count of pod liveness-0f7e3ef3-b4c3-4d46-8191-1807743168b9 is 0
Dec 16 23:45:15.132: INFO: Restart count of pod container-probe-1051/liveness-0f7e3ef3-b4c3-4d46-8191-1807743168b9 is now 1 (16.075751749s elapsed)
Dec 16 23:45:35.204: INFO: Restart count of pod container-probe-1051/liveness-0f7e3ef3-b4c3-4d46-8191-1807743168b9 is now 2 (36.147789341s elapsed)
Dec 16 23:45:55.278: INFO: Restart count of pod container-probe-1051/liveness-0f7e3ef3-b4c3-4d46-8191-1807743168b9 is now 3 (56.222102454s elapsed)
Dec 16 23:46:15.353: INFO: Restart count of pod container-probe-1051/liveness-0f7e3ef3-b4c3-4d46-8191-1807743168b9 is now 4 (1m16.296942917s elapsed)
Dec 16 23:47:29.647: INFO: Restart count of pod container-probe-1051/liveness-0f7e3ef3-b4c3-4d46-8191-1807743168b9 is now 5 (2m30.590803706s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:47:29.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1051" for this suite.

• [SLOW TEST:152.698 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":211,"skipped":3601,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:47:29.676: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-3580909e-4f12-491a-a73f-77b6ffe6cd6c
STEP: Creating configMap with name cm-test-opt-upd-20ec528c-edf3-4af8-bb01-13a25c295c1f
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3580909e-4f12-491a-a73f-77b6ffe6cd6c
STEP: Updating configmap cm-test-opt-upd-20ec528c-edf3-4af8-bb01-13a25c295c1f
STEP: Creating configMap with name cm-test-opt-create-989a8af1-0acb-4c04-bbee-7d1782f359d3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:47:33.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8387" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3610,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:47:33.839: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:47:35.955: INFO: Waiting up to 5m0s for pod "client-envvars-45ef4b62-2b22-47f3-b8bc-8112974e9660" in namespace "pods-6994" to be "Succeeded or Failed"
Dec 16 23:47:35.976: INFO: Pod "client-envvars-45ef4b62-2b22-47f3-b8bc-8112974e9660": Phase="Pending", Reason="", readiness=false. Elapsed: 20.322991ms
Dec 16 23:47:37.983: INFO: Pod "client-envvars-45ef4b62-2b22-47f3-b8bc-8112974e9660": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027767123s
Dec 16 23:47:39.988: INFO: Pod "client-envvars-45ef4b62-2b22-47f3-b8bc-8112974e9660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032890045s
STEP: Saw pod success
Dec 16 23:47:39.988: INFO: Pod "client-envvars-45ef4b62-2b22-47f3-b8bc-8112974e9660" satisfied condition "Succeeded or Failed"
Dec 16 23:47:39.991: INFO: Trying to get logs from node ip-172-31-6-174 pod client-envvars-45ef4b62-2b22-47f3-b8bc-8112974e9660 container env3cont: <nil>
STEP: delete the pod
Dec 16 23:47:40.038: INFO: Waiting for pod client-envvars-45ef4b62-2b22-47f3-b8bc-8112974e9660 to disappear
Dec 16 23:47:40.046: INFO: Pod client-envvars-45ef4b62-2b22-47f3-b8bc-8112974e9660 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:47:40.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6994" for this suite.

• [SLOW TEST:6.217 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":213,"skipped":3617,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:47:40.059: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-eb715f91-8148-4d4b-9569-2a234b06398f
STEP: Creating a pod to test consume secrets
Dec 16 23:47:40.107: INFO: Waiting up to 5m0s for pod "pod-secrets-2bfd9eef-f9a7-49fc-b134-2db90973ad9c" in namespace "secrets-1710" to be "Succeeded or Failed"
Dec 16 23:47:40.110: INFO: Pod "pod-secrets-2bfd9eef-f9a7-49fc-b134-2db90973ad9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.816504ms
Dec 16 23:47:42.117: INFO: Pod "pod-secrets-2bfd9eef-f9a7-49fc-b134-2db90973ad9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009440753s
STEP: Saw pod success
Dec 16 23:47:42.117: INFO: Pod "pod-secrets-2bfd9eef-f9a7-49fc-b134-2db90973ad9c" satisfied condition "Succeeded or Failed"
Dec 16 23:47:42.120: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-secrets-2bfd9eef-f9a7-49fc-b134-2db90973ad9c container secret-volume-test: <nil>
STEP: delete the pod
Dec 16 23:47:42.140: INFO: Waiting for pod pod-secrets-2bfd9eef-f9a7-49fc-b134-2db90973ad9c to disappear
Dec 16 23:47:42.143: INFO: Pod pod-secrets-2bfd9eef-f9a7-49fc-b134-2db90973ad9c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:47:42.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1710" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":214,"skipped":3704,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:47:42.159: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 23:47:42.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82816dd8-6603-49fa-a41e-6cc3c0291035" in namespace "downward-api-8716" to be "Succeeded or Failed"
Dec 16 23:47:42.209: INFO: Pod "downwardapi-volume-82816dd8-6603-49fa-a41e-6cc3c0291035": Phase="Pending", Reason="", readiness=false. Elapsed: 4.364396ms
Dec 16 23:47:44.216: INFO: Pod "downwardapi-volume-82816dd8-6603-49fa-a41e-6cc3c0291035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011458558s
STEP: Saw pod success
Dec 16 23:47:44.216: INFO: Pod "downwardapi-volume-82816dd8-6603-49fa-a41e-6cc3c0291035" satisfied condition "Succeeded or Failed"
Dec 16 23:47:44.219: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-82816dd8-6603-49fa-a41e-6cc3c0291035 container client-container: <nil>
STEP: delete the pod
Dec 16 23:47:44.245: INFO: Waiting for pod downwardapi-volume-82816dd8-6603-49fa-a41e-6cc3c0291035 to disappear
Dec 16 23:47:44.250: INFO: Pod downwardapi-volume-82816dd8-6603-49fa-a41e-6cc3c0291035 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:47:44.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8716" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":215,"skipped":3722,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:47:44.266: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 16 23:47:50.381: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 16 23:47:50.385: INFO: Pod pod-with-poststart-http-hook still exists
Dec 16 23:47:52.385: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 16 23:47:52.392: INFO: Pod pod-with-poststart-http-hook still exists
Dec 16 23:47:54.385: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 16 23:47:54.392: INFO: Pod pod-with-poststart-http-hook still exists
Dec 16 23:47:56.385: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 16 23:47:56.392: INFO: Pod pod-with-poststart-http-hook still exists
Dec 16 23:47:58.385: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 16 23:47:58.392: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:47:58.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6363" for this suite.

• [SLOW TEST:14.138 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":216,"skipped":3743,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:47:58.405: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-dedd8d9e-6c63-4b1d-addb-2a5a4dfecc02
STEP: Creating a pod to test consume configMaps
Dec 16 23:47:58.465: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e12e0fa-8864-43a8-b4cf-12b8f2b6cad0" in namespace "configmap-6118" to be "Succeeded or Failed"
Dec 16 23:47:58.468: INFO: Pod "pod-configmaps-7e12e0fa-8864-43a8-b4cf-12b8f2b6cad0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588856ms
Dec 16 23:48:00.475: INFO: Pod "pod-configmaps-7e12e0fa-8864-43a8-b4cf-12b8f2b6cad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008958017s
STEP: Saw pod success
Dec 16 23:48:00.475: INFO: Pod "pod-configmaps-7e12e0fa-8864-43a8-b4cf-12b8f2b6cad0" satisfied condition "Succeeded or Failed"
Dec 16 23:48:00.478: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-configmaps-7e12e0fa-8864-43a8-b4cf-12b8f2b6cad0 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 16 23:48:00.495: INFO: Waiting for pod pod-configmaps-7e12e0fa-8864-43a8-b4cf-12b8f2b6cad0 to disappear
Dec 16 23:48:00.498: INFO: Pod pod-configmaps-7e12e0fa-8864-43a8-b4cf-12b8f2b6cad0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:48:00.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6118" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3780,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:48:00.508: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-6364
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 16 23:48:00.548: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 16 23:48:00.670: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 23:48:02.677: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:48:04.678: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:48:06.677: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:48:08.679: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:48:10.682: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:48:12.678: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:48:14.678: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:48:16.678: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 16 23:48:16.684: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 16 23:48:18.691: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 16 23:48:20.732: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 16 23:48:20.732: INFO: Going to poll 192.168.140.11 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Dec 16 23:48:20.735: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.140.11:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6364 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:48:20.735: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:48:20.827: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 16 23:48:20.827: INFO: Going to poll 192.168.91.6 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Dec 16 23:48:20.831: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.91.6:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6364 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:48:20.831: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:48:20.916: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:48:20.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6364" for this suite.

• [SLOW TEST:20.421 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":218,"skipped":3781,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:48:20.931: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 16 23:48:20.986: INFO: Waiting up to 5m0s for pod "pod-41dc76ff-b439-4ff0-a596-c1825a1352b4" in namespace "emptydir-2398" to be "Succeeded or Failed"
Dec 16 23:48:20.990: INFO: Pod "pod-41dc76ff-b439-4ff0-a596-c1825a1352b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.340477ms
Dec 16 23:48:22.998: INFO: Pod "pod-41dc76ff-b439-4ff0-a596-c1825a1352b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011666411s
STEP: Saw pod success
Dec 16 23:48:22.998: INFO: Pod "pod-41dc76ff-b439-4ff0-a596-c1825a1352b4" satisfied condition "Succeeded or Failed"
Dec 16 23:48:23.003: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-41dc76ff-b439-4ff0-a596-c1825a1352b4 container test-container: <nil>
STEP: delete the pod
Dec 16 23:48:23.030: INFO: Waiting for pod pod-41dc76ff-b439-4ff0-a596-c1825a1352b4 to disappear
Dec 16 23:48:23.033: INFO: Pod pod-41dc76ff-b439-4ff0-a596-c1825a1352b4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:48:23.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2398" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":219,"skipped":3796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:48:23.056: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Dec 16 23:48:23.109: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 23:48:23.109: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 23:48:23.127: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 23:48:23.127: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 23:48:23.155: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 23:48:23.155: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 23:48:23.181: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 23:48:23.181: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 23:48:24.647: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 16 23:48:24.647: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 16 23:48:25.191: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Dec 16 23:48:25.206: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 0
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 2
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 2
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 2
Dec 16 23:48:25.208: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 2
Dec 16 23:48:25.225: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 2
Dec 16 23:48:25.225: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 2
Dec 16 23:48:25.262: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 2
Dec 16 23:48:25.262: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 2
Dec 16 23:48:25.271: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1
STEP: listing Deployments
Dec 16 23:48:25.284: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Dec 16 23:48:25.308: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Dec 16 23:48:25.320: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 23:48:25.332: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 23:48:25.364: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 23:48:25.403: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 23:48:25.422: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 23:48:25.446: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Dec 16 23:48:27.732: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1
Dec 16 23:48:27.732: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1
Dec 16 23:48:27.732: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1
Dec 16 23:48:27.732: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1
Dec 16 23:48:27.732: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1
Dec 16 23:48:27.732: INFO: observed Deployment test-deployment in namespace deployment-6596 with ReadyReplicas 1
STEP: deleting the Deployment
Dec 16 23:48:27.746: INFO: observed event type MODIFIED
Dec 16 23:48:27.746: INFO: observed event type MODIFIED
Dec 16 23:48:27.746: INFO: observed event type MODIFIED
Dec 16 23:48:27.746: INFO: observed event type MODIFIED
Dec 16 23:48:27.746: INFO: observed event type MODIFIED
Dec 16 23:48:27.746: INFO: observed event type MODIFIED
Dec 16 23:48:27.746: INFO: observed event type MODIFIED
Dec 16 23:48:27.746: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 16 23:48:27.756: INFO: Log out all the ReplicaSets if there is no deployment created
Dec 16 23:48:27.761: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-6596  71b6aa59-d207-44c4-a80a-ae1cc64fb03b 43256 3 2020-12-16 23:48:25 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment aa7dff54-e3f8-4ddd-a75c-fc9a620b58eb 0xc000926597 0xc000926598}] []  [{kube-controller-manager Update apps/v1 2020-12-16 23:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa7dff54-e3f8-4ddd-a75c-fc9a620b58eb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000926600 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Dec 16 23:48:27.765: INFO: pod: "test-deployment-768947d6f5-7swsf":
&Pod{ObjectMeta:{test-deployment-768947d6f5-7swsf test-deployment-768947d6f5- deployment-6596  d5e159b0-1b69-4dfa-af0f-5dd8eaa8bda9 43255 0 2020-12-16 23:48:27 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 71b6aa59-d207-44c4-a80a-ae1cc64fb03b 0xc006af8c17 0xc006af8c18}] []  [{kube-controller-manager Update v1 2020-12-16 23:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71b6aa59-d207-44c4-a80a-ae1cc64fb03b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 23:48:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mc7rs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mc7rs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mc7rs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:,StartTime:2020-12-16 23:48:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 16 23:48:27.765: INFO: pod: "test-deployment-768947d6f5-d4rzg":
&Pod{ObjectMeta:{test-deployment-768947d6f5-d4rzg test-deployment-768947d6f5- deployment-6596  9dd07d3f-55e0-4953-9079-229300912f10 43239 0 2020-12-16 23:48:25 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[cni.projectcalico.org/podIP:192.168.140.16/32 cni.projectcalico.org/podIPs:192.168.140.16/32] [{apps/v1 ReplicaSet test-deployment-768947d6f5 71b6aa59-d207-44c4-a80a-ae1cc64fb03b 0xc006af8e47 0xc006af8e48}] []  [{kube-controller-manager Update v1 2020-12-16 23:48:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71b6aa59-d207-44c4-a80a-ae1cc64fb03b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-16 23:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-16 23:48:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.140.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mc7rs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mc7rs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mc7rs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:192.168.140.16,StartTime:2020-12-16 23:48:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 23:48:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://84c0301669e603faa40735788e4a7a7fad4e363c7cdf500f394a3648ed161754,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.140.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 16 23:48:27.766: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-6596  f45b4ce2-da4d-4681-8dd5-47bbda6de119 43258 4 2020-12-16 23:48:25 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment aa7dff54-e3f8-4ddd-a75c-fc9a620b58eb 0xc000926707 0xc000926708}] []  [{kube-controller-manager Update apps/v1 2020-12-16 23:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa7dff54-e3f8-4ddd-a75c-fc9a620b58eb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0009267a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec 16 23:48:27.775: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-6596  081deb34-db64-4561-b7fd-88425dbb2f1f 43142 2 2020-12-16 23:48:23 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment aa7dff54-e3f8-4ddd-a75c-fc9a620b58eb 0xc000926837 0xc000926838}] []  [{kube-controller-manager Update apps/v1 2020-12-16 23:48:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa7dff54-e3f8-4ddd-a75c-fc9a620b58eb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0009268f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Dec 16 23:48:27.787: INFO: pod: "test-deployment-8b6954bfb-lrkc8":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-lrkc8 test-deployment-8b6954bfb- deployment-6596  12b98401-1bf2-44f8-b30e-b901cc0c6169 43111 0 2020-12-16 23:48:23 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[cni.projectcalico.org/podIP:192.168.140.14/32 cni.projectcalico.org/podIPs:192.168.140.14/32] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 081deb34-db64-4561-b7fd-88425dbb2f1f 0xc0007421c7 0xc0007421c8}] []  [{calico Update v1 2020-12-16 23:48:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-12-16 23:48:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"081deb34-db64-4561-b7fd-88425dbb2f1f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-16 23:48:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.140.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mc7rs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mc7rs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mc7rs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-16 23:48:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:192.168.140.14,StartTime:2020-12-16 23:48:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-16 23:48:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://318fa8f99e0d4a31775a2458dba2f222ae842d55745eb768c9668b4bffe870ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.140.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:48:27.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6596" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":220,"skipped":3831,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:48:27.799: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Dec 16 23:48:30.396: INFO: Successfully updated pod "annotationupdate38de4e5d-6b6f-4cba-8f3a-b81cc7137eeb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:48:32.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2504" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3832,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:48:32.428: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:48:32.511: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 16 23:48:35.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-4287 --namespace=crd-publish-openapi-4287 create -f -'
Dec 16 23:48:36.423: INFO: stderr: ""
Dec 16 23:48:36.424: INFO: stdout: "e2e-test-crd-publish-openapi-7198-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 16 23:48:36.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-4287 --namespace=crd-publish-openapi-4287 delete e2e-test-crd-publish-openapi-7198-crds test-cr'
Dec 16 23:48:36.512: INFO: stderr: ""
Dec 16 23:48:36.512: INFO: stdout: "e2e-test-crd-publish-openapi-7198-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 16 23:48:36.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-4287 --namespace=crd-publish-openapi-4287 apply -f -'
Dec 16 23:48:36.781: INFO: stderr: ""
Dec 16 23:48:36.781: INFO: stdout: "e2e-test-crd-publish-openapi-7198-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 16 23:48:36.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-4287 --namespace=crd-publish-openapi-4287 delete e2e-test-crd-publish-openapi-7198-crds test-cr'
Dec 16 23:48:36.866: INFO: stderr: ""
Dec 16 23:48:36.866: INFO: stdout: "e2e-test-crd-publish-openapi-7198-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 16 23:48:36.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-4287 explain e2e-test-crd-publish-openapi-7198-crds'
Dec 16 23:48:37.139: INFO: stderr: ""
Dec 16 23:48:37.139: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7198-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:48:40.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4287" for this suite.

• [SLOW TEST:8.174 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":222,"skipped":3835,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:48:40.602: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-917ac760-6bfc-4814-ac07-db8b89136cf0
STEP: Creating secret with name s-test-opt-upd-c5e1231b-6c85-45bd-b85d-2e33f66dbef2
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-917ac760-6bfc-4814-ac07-db8b89136cf0
STEP: Updating secret s-test-opt-upd-c5e1231b-6c85-45bd-b85d-2e33f66dbef2
STEP: Creating secret with name s-test-opt-create-24225b0c-a571-4dbe-b94e-96db03904992
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:49:55.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-996" for this suite.

• [SLOW TEST:74.487 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":223,"skipped":3870,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:49:55.092: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-5940f7f5-aed1-4aee-a67c-734029372a71
STEP: Creating a pod to test consume configMaps
Dec 16 23:49:55.160: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad088b4c-9fb1-4140-adc1-3736d08520df" in namespace "configmap-2406" to be "Succeeded or Failed"
Dec 16 23:49:55.163: INFO: Pod "pod-configmaps-ad088b4c-9fb1-4140-adc1-3736d08520df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.617475ms
Dec 16 23:49:57.170: INFO: Pod "pod-configmaps-ad088b4c-9fb1-4140-adc1-3736d08520df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00947726s
STEP: Saw pod success
Dec 16 23:49:57.170: INFO: Pod "pod-configmaps-ad088b4c-9fb1-4140-adc1-3736d08520df" satisfied condition "Succeeded or Failed"
Dec 16 23:49:57.172: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-configmaps-ad088b4c-9fb1-4140-adc1-3736d08520df container agnhost-container: <nil>
STEP: delete the pod
Dec 16 23:49:57.199: INFO: Waiting for pod pod-configmaps-ad088b4c-9fb1-4140-adc1-3736d08520df to disappear
Dec 16 23:49:57.202: INFO: Pod pod-configmaps-ad088b4c-9fb1-4140-adc1-3736d08520df no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:49:57.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2406" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":224,"skipped":3885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:49:57.219: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Dec 16 23:49:57.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 create -f -'
Dec 16 23:49:57.558: INFO: stderr: ""
Dec 16 23:49:57.558: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 16 23:49:57.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 23:49:57.678: INFO: stderr: ""
Dec 16 23:49:57.678: INFO: stdout: "update-demo-nautilus-9tfsn update-demo-nautilus-zmktr "
Dec 16 23:49:57.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-9tfsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 23:49:57.773: INFO: stderr: ""
Dec 16 23:49:57.773: INFO: stdout: ""
Dec 16 23:49:57.773: INFO: update-demo-nautilus-9tfsn is created but not running
Dec 16 23:50:02.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 23:50:02.981: INFO: stderr: ""
Dec 16 23:50:02.981: INFO: stdout: "update-demo-nautilus-9tfsn update-demo-nautilus-zmktr "
Dec 16 23:50:02.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-9tfsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 23:50:03.095: INFO: stderr: ""
Dec 16 23:50:03.095: INFO: stdout: "true"
Dec 16 23:50:03.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-9tfsn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 23:50:03.187: INFO: stderr: ""
Dec 16 23:50:03.187: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 16 23:50:03.187: INFO: validating pod update-demo-nautilus-9tfsn
Dec 16 23:50:03.195: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 23:50:03.195: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 23:50:03.195: INFO: update-demo-nautilus-9tfsn is verified up and running
Dec 16 23:50:03.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-zmktr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 23:50:03.295: INFO: stderr: ""
Dec 16 23:50:03.295: INFO: stdout: "true"
Dec 16 23:50:03.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-zmktr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 23:50:03.385: INFO: stderr: ""
Dec 16 23:50:03.385: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 16 23:50:03.385: INFO: validating pod update-demo-nautilus-zmktr
Dec 16 23:50:03.390: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 23:50:03.390: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 23:50:03.390: INFO: update-demo-nautilus-zmktr is verified up and running
STEP: scaling down the replication controller
Dec 16 23:50:03.392: INFO: scanned /root for discovery docs: <nil>
Dec 16 23:50:03.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec 16 23:50:04.525: INFO: stderr: ""
Dec 16 23:50:04.525: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 16 23:50:04.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 23:50:04.604: INFO: stderr: ""
Dec 16 23:50:04.604: INFO: stdout: "update-demo-nautilus-9tfsn update-demo-nautilus-zmktr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 16 23:50:09.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 23:50:09.684: INFO: stderr: ""
Dec 16 23:50:09.684: INFO: stdout: "update-demo-nautilus-9tfsn "
Dec 16 23:50:09.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-9tfsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 23:50:09.763: INFO: stderr: ""
Dec 16 23:50:09.763: INFO: stdout: "true"
Dec 16 23:50:09.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-9tfsn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 23:50:09.853: INFO: stderr: ""
Dec 16 23:50:09.853: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 16 23:50:09.853: INFO: validating pod update-demo-nautilus-9tfsn
Dec 16 23:50:09.856: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 23:50:09.856: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 23:50:09.856: INFO: update-demo-nautilus-9tfsn is verified up and running
STEP: scaling up the replication controller
Dec 16 23:50:09.858: INFO: scanned /root for discovery docs: <nil>
Dec 16 23:50:09.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec 16 23:50:10.967: INFO: stderr: ""
Dec 16 23:50:10.967: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 16 23:50:10.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 23:50:11.046: INFO: stderr: ""
Dec 16 23:50:11.046: INFO: stdout: "update-demo-nautilus-9tfsn update-demo-nautilus-qzczg "
Dec 16 23:50:11.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-9tfsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 23:50:11.121: INFO: stderr: ""
Dec 16 23:50:11.121: INFO: stdout: "true"
Dec 16 23:50:11.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-9tfsn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 23:50:11.203: INFO: stderr: ""
Dec 16 23:50:11.203: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 16 23:50:11.203: INFO: validating pod update-demo-nautilus-9tfsn
Dec 16 23:50:11.212: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 23:50:11.212: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 23:50:11.212: INFO: update-demo-nautilus-9tfsn is verified up and running
Dec 16 23:50:11.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-qzczg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 23:50:11.294: INFO: stderr: ""
Dec 16 23:50:11.294: INFO: stdout: ""
Dec 16 23:50:11.294: INFO: update-demo-nautilus-qzczg is created but not running
Dec 16 23:50:16.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 23:50:16.433: INFO: stderr: ""
Dec 16 23:50:16.433: INFO: stdout: "update-demo-nautilus-9tfsn update-demo-nautilus-qzczg "
Dec 16 23:50:16.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-9tfsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 23:50:16.550: INFO: stderr: ""
Dec 16 23:50:16.550: INFO: stdout: "true"
Dec 16 23:50:16.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-9tfsn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 23:50:16.640: INFO: stderr: ""
Dec 16 23:50:16.640: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 16 23:50:16.640: INFO: validating pod update-demo-nautilus-9tfsn
Dec 16 23:50:16.645: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 23:50:16.645: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 23:50:16.645: INFO: update-demo-nautilus-9tfsn is verified up and running
Dec 16 23:50:16.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-qzczg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 23:50:16.768: INFO: stderr: ""
Dec 16 23:50:16.768: INFO: stdout: "true"
Dec 16 23:50:16.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods update-demo-nautilus-qzczg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 23:50:16.857: INFO: stderr: ""
Dec 16 23:50:16.857: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 16 23:50:16.857: INFO: validating pod update-demo-nautilus-qzczg
Dec 16 23:50:16.870: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 23:50:16.870: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 23:50:16.870: INFO: update-demo-nautilus-qzczg is verified up and running
STEP: using delete to clean up resources
Dec 16 23:50:16.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 delete --grace-period=0 --force -f -'
Dec 16 23:50:16.970: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 23:50:16.970: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 16 23:50:16.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get rc,svc -l name=update-demo --no-headers'
Dec 16 23:50:17.102: INFO: stderr: "No resources found in kubectl-4965 namespace.\n"
Dec 16 23:50:17.103: INFO: stdout: ""
Dec 16 23:50:17.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 16 23:50:17.178: INFO: stderr: ""
Dec 16 23:50:17.178: INFO: stdout: "update-demo-nautilus-9tfsn\nupdate-demo-nautilus-qzczg\n"
Dec 16 23:50:17.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get rc,svc -l name=update-demo --no-headers'
Dec 16 23:50:17.764: INFO: stderr: "No resources found in kubectl-4965 namespace.\n"
Dec 16 23:50:17.764: INFO: stdout: ""
Dec 16 23:50:17.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-4965 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 16 23:50:17.845: INFO: stderr: ""
Dec 16 23:50:17.845: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:50:17.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4965" for this suite.

• [SLOW TEST:20.639 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":225,"skipped":3912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:50:17.860: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:50:19.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7564" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":226,"skipped":3990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:50:19.944: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:50:22.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8425" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":227,"skipped":4057,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:50:22.022: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-ddea9dec-8eb6-423b-97e6-632f346ad60f
STEP: Creating a pod to test consume configMaps
Dec 16 23:50:22.092: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-17097560-f57e-4f41-8242-4a02483b6a83" in namespace "projected-9060" to be "Succeeded or Failed"
Dec 16 23:50:22.097: INFO: Pod "pod-projected-configmaps-17097560-f57e-4f41-8242-4a02483b6a83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.787946ms
Dec 16 23:50:24.103: INFO: Pod "pod-projected-configmaps-17097560-f57e-4f41-8242-4a02483b6a83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010824699s
STEP: Saw pod success
Dec 16 23:50:24.103: INFO: Pod "pod-projected-configmaps-17097560-f57e-4f41-8242-4a02483b6a83" satisfied condition "Succeeded or Failed"
Dec 16 23:50:24.106: INFO: Trying to get logs from node ip-172-31-6-174 pod pod-projected-configmaps-17097560-f57e-4f41-8242-4a02483b6a83 container agnhost-container: <nil>
STEP: delete the pod
Dec 16 23:50:24.135: INFO: Waiting for pod pod-projected-configmaps-17097560-f57e-4f41-8242-4a02483b6a83 to disappear
Dec 16 23:50:24.138: INFO: Pod pod-projected-configmaps-17097560-f57e-4f41-8242-4a02483b6a83 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:50:24.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9060" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":4081,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:50:24.150: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 16 23:50:24.201: INFO: Waiting up to 5m0s for pod "pod-35b12042-06e9-4f58-9c6c-f2292e7dcec9" in namespace "emptydir-4085" to be "Succeeded or Failed"
Dec 16 23:50:24.204: INFO: Pod "pod-35b12042-06e9-4f58-9c6c-f2292e7dcec9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.732592ms
Dec 16 23:50:26.211: INFO: Pod "pod-35b12042-06e9-4f58-9c6c-f2292e7dcec9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00952013s
STEP: Saw pod success
Dec 16 23:50:26.211: INFO: Pod "pod-35b12042-06e9-4f58-9c6c-f2292e7dcec9" satisfied condition "Succeeded or Failed"
Dec 16 23:50:26.213: INFO: Trying to get logs from node ip-172-31-6-174 pod pod-35b12042-06e9-4f58-9c6c-f2292e7dcec9 container test-container: <nil>
STEP: delete the pod
Dec 16 23:50:26.233: INFO: Waiting for pod pod-35b12042-06e9-4f58-9c6c-f2292e7dcec9 to disappear
Dec 16 23:50:26.236: INFO: Pod pod-35b12042-06e9-4f58-9c6c-f2292e7dcec9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:50:26.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4085" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":229,"skipped":4084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:50:26.252: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:50:27.091: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 16 23:50:29.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759427, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759427, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759427, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759427, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:50:32.124: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 16 23:50:32.144: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:50:32.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-554" for this suite.
STEP: Destroying namespace "webhook-554-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.032 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":230,"skipped":4109,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:50:32.284: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5252.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5252.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 16.42.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.42.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.42.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.42.16_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5252.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5252.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5252.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 16.42.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.42.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.42.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.42.16_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 16 23:50:34.469: INFO: Unable to read wheezy_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:34.472: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:34.475: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:34.478: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:34.497: INFO: Unable to read jessie_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:34.500: INFO: Unable to read jessie_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:34.502: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:34.505: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:34.528: INFO: Lookups using dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb failed for: [wheezy_udp@dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_udp@dns-test-service.dns-5252.svc.cluster.local jessie_tcp@dns-test-service.dns-5252.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local]

Dec 16 23:50:39.532: INFO: Unable to read wheezy_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:39.535: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:39.539: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:39.541: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:39.559: INFO: Unable to read jessie_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:39.562: INFO: Unable to read jessie_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:39.565: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:39.569: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:39.588: INFO: Lookups using dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb failed for: [wheezy_udp@dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_udp@dns-test-service.dns-5252.svc.cluster.local jessie_tcp@dns-test-service.dns-5252.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local]

Dec 16 23:50:44.540: INFO: Unable to read wheezy_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:44.555: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:44.558: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:44.561: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:44.604: INFO: Unable to read jessie_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:44.610: INFO: Unable to read jessie_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:44.616: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:44.621: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:44.660: INFO: Lookups using dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb failed for: [wheezy_udp@dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_udp@dns-test-service.dns-5252.svc.cluster.local jessie_tcp@dns-test-service.dns-5252.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local]

Dec 16 23:50:49.532: INFO: Unable to read wheezy_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:49.535: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:49.538: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:49.543: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:49.561: INFO: Unable to read jessie_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:49.564: INFO: Unable to read jessie_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:49.567: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:49.569: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:49.585: INFO: Lookups using dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb failed for: [wheezy_udp@dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_udp@dns-test-service.dns-5252.svc.cluster.local jessie_tcp@dns-test-service.dns-5252.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local]

Dec 16 23:50:54.532: INFO: Unable to read wheezy_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:54.535: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:54.538: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:54.540: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:54.560: INFO: Unable to read jessie_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:54.562: INFO: Unable to read jessie_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:54.565: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:54.568: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:54.583: INFO: Lookups using dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb failed for: [wheezy_udp@dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_udp@dns-test-service.dns-5252.svc.cluster.local jessie_tcp@dns-test-service.dns-5252.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local]

Dec 16 23:50:59.534: INFO: Unable to read wheezy_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:59.538: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:59.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:59.544: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:59.575: INFO: Unable to read jessie_udp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:59.579: INFO: Unable to read jessie_tcp@dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:59.582: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:59.584: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local from pod dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb: the server could not find the requested resource (get pods dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb)
Dec 16 23:50:59.600: INFO: Lookups using dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb failed for: [wheezy_udp@dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@dns-test-service.dns-5252.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_udp@dns-test-service.dns-5252.svc.cluster.local jessie_tcp@dns-test-service.dns-5252.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5252.svc.cluster.local]

Dec 16 23:51:04.583: INFO: DNS probes using dns-5252/dns-test-ed8361be-544b-4aa8-ab43-2d45ec1eadcb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:51:04.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5252" for this suite.

• [SLOW TEST:32.430 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":231,"skipped":4113,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:51:04.722: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Dec 16 23:51:04.771: INFO: namespace kubectl-2495
Dec 16 23:51:04.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2495 create -f -'
Dec 16 23:51:05.173: INFO: stderr: ""
Dec 16 23:51:05.173: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 16 23:51:06.178: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 23:51:06.178: INFO: Found 0 / 1
Dec 16 23:51:07.178: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 23:51:07.178: INFO: Found 1 / 1
Dec 16 23:51:07.178: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 16 23:51:07.186: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 23:51:07.186: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 16 23:51:07.186: INFO: wait on agnhost-primary startup in kubectl-2495 
Dec 16 23:51:07.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2495 logs agnhost-primary-6kfvc agnhost-primary'
Dec 16 23:51:07.278: INFO: stderr: ""
Dec 16 23:51:07.278: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec 16 23:51:07.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2495 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec 16 23:51:07.412: INFO: stderr: ""
Dec 16 23:51:07.412: INFO: stdout: "service/rm2 exposed\n"
Dec 16 23:51:07.416: INFO: Service rm2 in namespace kubectl-2495 found.
STEP: exposing service
Dec 16 23:51:09.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=kubectl-2495 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec 16 23:51:09.537: INFO: stderr: ""
Dec 16 23:51:09.537: INFO: stdout: "service/rm3 exposed\n"
Dec 16 23:51:09.547: INFO: Service rm3 in namespace kubectl-2495 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:51:11.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2495" for this suite.

• [SLOW TEST:6.848 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":232,"skipped":4125,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:51:11.570: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:51:13.656: INFO: Deleting pod "var-expansion-3ca10af1-2424-42e8-95b6-b702ff5c8641" in namespace "var-expansion-8038"
Dec 16 23:51:13.662: INFO: Wait up to 5m0s for pod "var-expansion-3ca10af1-2424-42e8-95b6-b702ff5c8641" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:51:29.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8038" for this suite.

• [SLOW TEST:18.125 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":233,"skipped":4143,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:51:29.696: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Dec 16 23:51:29.767: INFO: Waiting up to 5m0s for pod "downward-api-dba840cf-392e-4252-9e68-44f55d345525" in namespace "downward-api-9272" to be "Succeeded or Failed"
Dec 16 23:51:29.780: INFO: Pod "downward-api-dba840cf-392e-4252-9e68-44f55d345525": Phase="Pending", Reason="", readiness=false. Elapsed: 13.481007ms
Dec 16 23:51:31.788: INFO: Pod "downward-api-dba840cf-392e-4252-9e68-44f55d345525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021364786s
STEP: Saw pod success
Dec 16 23:51:31.788: INFO: Pod "downward-api-dba840cf-392e-4252-9e68-44f55d345525" satisfied condition "Succeeded or Failed"
Dec 16 23:51:31.792: INFO: Trying to get logs from node ip-172-31-1-217 pod downward-api-dba840cf-392e-4252-9e68-44f55d345525 container dapi-container: <nil>
STEP: delete the pod
Dec 16 23:51:31.813: INFO: Waiting for pod downward-api-dba840cf-392e-4252-9e68-44f55d345525 to disappear
Dec 16 23:51:31.816: INFO: Pod downward-api-dba840cf-392e-4252-9e68-44f55d345525 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:51:31.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9272" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":234,"skipped":4155,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:51:31.834: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-528
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 16 23:51:31.879: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 16 23:51:31.916: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 23:51:33.923: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:51:35.923: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:51:37.923: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:51:39.920: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:51:41.923: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:51:43.924: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:51:45.923: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:51:47.923: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:51:49.920: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 16 23:51:51.924: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 16 23:51:51.934: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 16 23:51:53.956: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 16 23:51:53.956: INFO: Breadth first check of 192.168.140.33 on host 172.31.1.217...
Dec 16 23:51:53.959: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.140.28:9080/dial?request=hostname&protocol=http&host=192.168.140.33&port=8080&tries=1'] Namespace:pod-network-test-528 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:51:53.959: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:51:54.040: INFO: Waiting for responses: map[]
Dec 16 23:51:54.040: INFO: reached 192.168.140.33 after 0/1 tries
Dec 16 23:51:54.040: INFO: Breadth first check of 192.168.91.18 on host 172.31.6.174...
Dec 16 23:51:54.043: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.140.28:9080/dial?request=hostname&protocol=http&host=192.168.91.18&port=8080&tries=1'] Namespace:pod-network-test-528 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:51:54.043: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:51:54.118: INFO: Waiting for responses: map[]
Dec 16 23:51:54.118: INFO: reached 192.168.91.18 after 0/1 tries
Dec 16 23:51:54.118: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:51:54.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-528" for this suite.

• [SLOW TEST:22.304 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":235,"skipped":4155,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:51:54.141: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:52:02.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9046" for this suite.

• [SLOW TEST:8.087 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":236,"skipped":4164,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:52:02.228: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:52:13.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6788" for this suite.

• [SLOW TEST:11.196 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":237,"skipped":4171,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:52:13.427: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:52:37.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4673" for this suite.

• [SLOW TEST:24.365 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":238,"skipped":4178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:52:37.796: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Dec 16 23:52:39.861: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3102 PodName:pod-sharedvolume-0e6da7c6-2a4b-4dde-86e7-8300ad7e0253 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:52:39.861: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
Dec 16 23:52:39.924: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:52:39.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3102" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":239,"skipped":4227,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:52:39.946: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Dec 16 23:52:40.010: INFO: Waiting up to 5m0s for pod "downward-api-24428fa3-91ce-4bac-b7ae-b463cd71a3dc" in namespace "downward-api-4357" to be "Succeeded or Failed"
Dec 16 23:52:40.016: INFO: Pod "downward-api-24428fa3-91ce-4bac-b7ae-b463cd71a3dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.142017ms
Dec 16 23:52:42.027: INFO: Pod "downward-api-24428fa3-91ce-4bac-b7ae-b463cd71a3dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017031653s
STEP: Saw pod success
Dec 16 23:52:42.027: INFO: Pod "downward-api-24428fa3-91ce-4bac-b7ae-b463cd71a3dc" satisfied condition "Succeeded or Failed"
Dec 16 23:52:42.032: INFO: Trying to get logs from node ip-172-31-1-217 pod downward-api-24428fa3-91ce-4bac-b7ae-b463cd71a3dc container dapi-container: <nil>
STEP: delete the pod
Dec 16 23:52:42.059: INFO: Waiting for pod downward-api-24428fa3-91ce-4bac-b7ae-b463cd71a3dc to disappear
Dec 16 23:52:42.062: INFO: Pod downward-api-24428fa3-91ce-4bac-b7ae-b463cd71a3dc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:52:42.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4357" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":240,"skipped":4233,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:52:42.074: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-1958
STEP: creating service affinity-nodeport in namespace services-1958
STEP: creating replication controller affinity-nodeport in namespace services-1958
I1216 23:52:42.141934      24 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-1958, replica count: 3
I1216 23:52:45.192690      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 23:52:45.211: INFO: Creating new exec pod
Dec 16 23:52:48.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-1958 exec execpod-affinitycv58b -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Dec 16 23:52:48.389: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec 16 23:52:48.389: INFO: stdout: ""
Dec 16 23:52:48.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-1958 exec execpod-affinitycv58b -- /bin/sh -x -c nc -zv -t -w 2 10.98.235.141 80'
Dec 16 23:52:48.531: INFO: stderr: "+ nc -zv -t -w 2 10.98.235.141 80\nConnection to 10.98.235.141 80 port [tcp/http] succeeded!\n"
Dec 16 23:52:48.531: INFO: stdout: ""
Dec 16 23:52:48.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-1958 exec execpod-affinitycv58b -- /bin/sh -x -c nc -zv -t -w 2 172.31.1.217 30916'
Dec 16 23:52:48.706: INFO: stderr: "+ nc -zv -t -w 2 172.31.1.217 30916\nConnection to 172.31.1.217 30916 port [tcp/30916] succeeded!\n"
Dec 16 23:52:48.706: INFO: stdout: ""
Dec 16 23:52:48.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-1958 exec execpod-affinitycv58b -- /bin/sh -x -c nc -zv -t -w 2 172.31.6.174 30916'
Dec 16 23:52:48.891: INFO: stderr: "+ nc -zv -t -w 2 172.31.6.174 30916\nConnection to 172.31.6.174 30916 port [tcp/30916] succeeded!\n"
Dec 16 23:52:48.891: INFO: stdout: ""
Dec 16 23:52:48.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-1958 exec execpod-affinitycv58b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.1.217:30916/ ; done'
Dec 16 23:52:49.116: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.217:30916/\n"
Dec 16 23:52:49.116: INFO: stdout: "\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r\naffinity-nodeport-pq66r"
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Received response from host: affinity-nodeport-pq66r
Dec 16 23:52:49.116: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1958, will wait for the garbage collector to delete the pods
Dec 16 23:52:49.192: INFO: Deleting ReplicationController affinity-nodeport took: 7.260877ms
Dec 16 23:52:49.793: INFO: Terminating ReplicationController affinity-nodeport pods took: 600.664499ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:52:57.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1958" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.922 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":241,"skipped":4241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:52:58.021: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Dec 16 23:52:58.646: INFO: created pod pod-service-account-defaultsa
Dec 16 23:52:58.646: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 16 23:52:58.652: INFO: created pod pod-service-account-mountsa
Dec 16 23:52:58.652: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 16 23:52:58.662: INFO: created pod pod-service-account-nomountsa
Dec 16 23:52:58.662: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 16 23:52:58.670: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 16 23:52:58.670: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 16 23:52:58.675: INFO: created pod pod-service-account-mountsa-mountspec
Dec 16 23:52:58.675: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 16 23:52:58.681: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 16 23:52:58.681: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 16 23:52:58.693: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 16 23:52:58.693: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 16 23:52:58.701: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 16 23:52:58.701: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 16 23:52:58.711: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 16 23:52:58.711: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:52:58.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5180" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":242,"skipped":4289,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:52:58.727: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 16 23:52:58.790: INFO: Waiting up to 5m0s for pod "pod-a7464edd-5ae1-4aac-9676-f9acb3f52557" in namespace "emptydir-6525" to be "Succeeded or Failed"
Dec 16 23:52:58.799: INFO: Pod "pod-a7464edd-5ae1-4aac-9676-f9acb3f52557": Phase="Pending", Reason="", readiness=false. Elapsed: 8.771313ms
Dec 16 23:53:00.806: INFO: Pod "pod-a7464edd-5ae1-4aac-9676-f9acb3f52557": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015710515s
Dec 16 23:53:02.821: INFO: Pod "pod-a7464edd-5ae1-4aac-9676-f9acb3f52557": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030549052s
STEP: Saw pod success
Dec 16 23:53:02.821: INFO: Pod "pod-a7464edd-5ae1-4aac-9676-f9acb3f52557" satisfied condition "Succeeded or Failed"
Dec 16 23:53:02.826: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-a7464edd-5ae1-4aac-9676-f9acb3f52557 container test-container: <nil>
STEP: delete the pod
Dec 16 23:53:02.852: INFO: Waiting for pod pod-a7464edd-5ae1-4aac-9676-f9acb3f52557 to disappear
Dec 16 23:53:02.859: INFO: Pod pod-a7464edd-5ae1-4aac-9676-f9acb3f52557 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:53:02.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6525" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4300,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:53:02.897: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Dec 16 23:53:06.975: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-314 PodName:var-expansion-ebf23da2-3c41-41aa-ba46-c5dad67d1784 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:53:06.975: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: test for file in mounted path
Dec 16 23:53:07.039: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-314 PodName:var-expansion-ebf23da2-3c41-41aa-ba46-c5dad67d1784 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 23:53:07.039: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: updating the annotation value
Dec 16 23:53:07.606: INFO: Successfully updated pod "var-expansion-ebf23da2-3c41-41aa-ba46-c5dad67d1784"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Dec 16 23:53:07.609: INFO: Deleting pod "var-expansion-ebf23da2-3c41-41aa-ba46-c5dad67d1784" in namespace "var-expansion-314"
Dec 16 23:53:07.618: INFO: Wait up to 5m0s for pod "var-expansion-ebf23da2-3c41-41aa-ba46-c5dad67d1784" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:54:29.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-314" for this suite.

• [SLOW TEST:86.744 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":244,"skipped":4313,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:54:29.648: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Dec 16 23:54:32.233: INFO: Successfully updated pod "labelsupdate156b2032-2b33-4a96-a379-ee3c3da92a5f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:54:34.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3979" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4376,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:54:34.271: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Dec 16 23:54:34.308: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 16 23:54:34.314: INFO: Waiting for terminating namespaces to be deleted...
Dec 16 23:54:34.316: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-1-217 before test
Dec 16 23:54:34.322: INFO: calico-node-ldh6v from kube-system started at 2020-12-16 21:14:55 +0000 UTC (1 container statuses recorded)
Dec 16 23:54:34.322: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 23:54:34.322: INFO: kube-proxy-v4mt9 from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 16 23:54:34.322: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 23:54:34.322: INFO: nginx-proxy-ip-172-31-1-217 from kube-system started at 2020-12-16 21:14:55 +0000 UTC (1 container statuses recorded)
Dec 16 23:54:34.322: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 16 23:54:34.322: INFO: labelsupdate156b2032-2b33-4a96-a379-ee3c3da92a5f from projected-3979 started at 2020-12-16 23:54:29 +0000 UTC (1 container statuses recorded)
Dec 16 23:54:34.322: INFO: 	Container client-container ready: true, restart count 0
Dec 16 23:54:34.322: INFO: sonobuoy from sonobuoy started at 2020-12-16 22:33:28 +0000 UTC (1 container statuses recorded)
Dec 16 23:54:34.322: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 16 23:54:34.322: INFO: sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-jkfz6 from sonobuoy started at 2020-12-16 22:33:30 +0000 UTC (2 container statuses recorded)
Dec 16 23:54:34.322: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Dec 16 23:54:34.322: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 23:54:34.322: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-174 before test
Dec 16 23:54:34.333: INFO: calico-node-84s5l from kube-system started at 2020-12-16 21:14:54 +0000 UTC (1 container statuses recorded)
Dec 16 23:54:34.333: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 23:54:34.333: INFO: coredns-74ff55c5b-96bcf from kube-system started at 2020-12-16 21:15:19 +0000 UTC (1 container statuses recorded)
Dec 16 23:54:34.333: INFO: 	Container coredns ready: true, restart count 0
Dec 16 23:54:34.333: INFO: coredns-74ff55c5b-t4nvx from kube-system started at 2020-12-16 22:37:05 +0000 UTC (1 container statuses recorded)
Dec 16 23:54:34.333: INFO: 	Container coredns ready: true, restart count 0
Dec 16 23:54:34.333: INFO: kube-proxy-zpdbw from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 16 23:54:34.333: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 23:54:34.333: INFO: nginx-proxy-ip-172-31-6-174 from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 16 23:54:34.333: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 16 23:54:34.334: INFO: sonobuoy-e2e-job-1783c268d2664b69 from sonobuoy started at 2020-12-16 22:33:29 +0000 UTC (2 container statuses recorded)
Dec 16 23:54:34.334: INFO: 	Container e2e ready: true, restart count 0
Dec 16 23:54:34.334: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 23:54:34.334: INFO: sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-g7kjv from sonobuoy started at 2020-12-16 22:33:30 +0000 UTC (2 container statuses recorded)
Dec 16 23:54:34.334: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Dec 16 23:54:34.334: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-bafff788-387d-4fb6-9231-1144f17b200c 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-bafff788-387d-4fb6-9231-1144f17b200c off the node ip-172-31-1-217
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bafff788-387d-4fb6-9231-1144f17b200c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:54:38.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8876" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":246,"skipped":4377,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:54:38.484: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-9076/configmap-test-ae573e8b-0570-4d00-b98f-6756d00363e3
STEP: Creating a pod to test consume configMaps
Dec 16 23:54:38.545: INFO: Waiting up to 5m0s for pod "pod-configmaps-15dd5fb9-68cb-47b1-a2e2-c599c8c174e8" in namespace "configmap-9076" to be "Succeeded or Failed"
Dec 16 23:54:38.548: INFO: Pod "pod-configmaps-15dd5fb9-68cb-47b1-a2e2-c599c8c174e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.482342ms
Dec 16 23:54:40.557: INFO: Pod "pod-configmaps-15dd5fb9-68cb-47b1-a2e2-c599c8c174e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011558318s
Dec 16 23:54:42.565: INFO: Pod "pod-configmaps-15dd5fb9-68cb-47b1-a2e2-c599c8c174e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020326888s
STEP: Saw pod success
Dec 16 23:54:42.571: INFO: Pod "pod-configmaps-15dd5fb9-68cb-47b1-a2e2-c599c8c174e8" satisfied condition "Succeeded or Failed"
Dec 16 23:54:42.574: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-configmaps-15dd5fb9-68cb-47b1-a2e2-c599c8c174e8 container env-test: <nil>
STEP: delete the pod
Dec 16 23:54:42.605: INFO: Waiting for pod pod-configmaps-15dd5fb9-68cb-47b1-a2e2-c599c8c174e8 to disappear
Dec 16 23:54:42.608: INFO: Pod pod-configmaps-15dd5fb9-68cb-47b1-a2e2-c599c8c174e8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:54:42.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9076" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":247,"skipped":4396,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:54:42.621: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-a71c2ace-dc2e-4138-88e6-8aad262cb711
STEP: Creating a pod to test consume secrets
Dec 16 23:54:42.730: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f98ce874-6e82-4052-9ca9-6a6d336c1476" in namespace "projected-4801" to be "Succeeded or Failed"
Dec 16 23:54:42.735: INFO: Pod "pod-projected-secrets-f98ce874-6e82-4052-9ca9-6a6d336c1476": Phase="Pending", Reason="", readiness=false. Elapsed: 4.604825ms
Dec 16 23:54:44.754: INFO: Pod "pod-projected-secrets-f98ce874-6e82-4052-9ca9-6a6d336c1476": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023842166s
STEP: Saw pod success
Dec 16 23:54:44.754: INFO: Pod "pod-projected-secrets-f98ce874-6e82-4052-9ca9-6a6d336c1476" satisfied condition "Succeeded or Failed"
Dec 16 23:54:44.771: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-secrets-f98ce874-6e82-4052-9ca9-6a6d336c1476 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 16 23:54:44.827: INFO: Waiting for pod pod-projected-secrets-f98ce874-6e82-4052-9ca9-6a6d336c1476 to disappear
Dec 16 23:54:44.840: INFO: Pod pod-projected-secrets-f98ce874-6e82-4052-9ca9-6a6d336c1476 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:54:44.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4801" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:54:44.863: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-rdpz
STEP: Creating a pod to test atomic-volume-subpath
Dec 16 23:54:44.954: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rdpz" in namespace "subpath-8621" to be "Succeeded or Failed"
Dec 16 23:54:44.964: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Pending", Reason="", readiness=false. Elapsed: 9.779133ms
Dec 16 23:54:46.970: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Running", Reason="", readiness=true. Elapsed: 2.016481s
Dec 16 23:54:48.979: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Running", Reason="", readiness=true. Elapsed: 4.02471508s
Dec 16 23:54:50.983: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Running", Reason="", readiness=true. Elapsed: 6.028660412s
Dec 16 23:54:52.992: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Running", Reason="", readiness=true. Elapsed: 8.03788912s
Dec 16 23:54:54.998: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Running", Reason="", readiness=true. Elapsed: 10.04424307s
Dec 16 23:54:57.007: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Running", Reason="", readiness=true. Elapsed: 12.053268869s
Dec 16 23:54:59.014: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Running", Reason="", readiness=true. Elapsed: 14.060011095s
Dec 16 23:55:01.019: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Running", Reason="", readiness=true. Elapsed: 16.064594442s
Dec 16 23:55:03.029: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Running", Reason="", readiness=true. Elapsed: 18.075157371s
Dec 16 23:55:05.036: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Running", Reason="", readiness=true. Elapsed: 20.081751711s
Dec 16 23:55:07.042: INFO: Pod "pod-subpath-test-projected-rdpz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.088192549s
STEP: Saw pod success
Dec 16 23:55:07.042: INFO: Pod "pod-subpath-test-projected-rdpz" satisfied condition "Succeeded or Failed"
Dec 16 23:55:07.045: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-subpath-test-projected-rdpz container test-container-subpath-projected-rdpz: <nil>
STEP: delete the pod
Dec 16 23:55:07.066: INFO: Waiting for pod pod-subpath-test-projected-rdpz to disappear
Dec 16 23:55:07.069: INFO: Pod pod-subpath-test-projected-rdpz no longer exists
STEP: Deleting pod pod-subpath-test-projected-rdpz
Dec 16 23:55:07.069: INFO: Deleting pod "pod-subpath-test-projected-rdpz" in namespace "subpath-8621"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:55:07.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8621" for this suite.

• [SLOW TEST:22.218 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":249,"skipped":4444,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:55:07.081: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3268
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3268
STEP: creating replication controller externalsvc in namespace services-3268
I1216 23:55:07.171113      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3268, replica count: 2
I1216 23:55:10.222270      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 16 23:55:10.278: INFO: Creating new exec pod
Dec 16 23:55:12.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-3268 exec execpodh2gwf -- /bin/sh -x -c nslookup nodeport-service.services-3268.svc.cluster.local'
Dec 16 23:55:12.718: INFO: stderr: "+ nslookup nodeport-service.services-3268.svc.cluster.local\n"
Dec 16 23:55:12.718: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3268.svc.cluster.local\tcanonical name = externalsvc.services-3268.svc.cluster.local.\nName:\texternalsvc.services-3268.svc.cluster.local\nAddress: 10.111.193.81\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3268, will wait for the garbage collector to delete the pods
Dec 16 23:55:12.782: INFO: Deleting ReplicationController externalsvc took: 8.109047ms
Dec 16 23:55:13.382: INFO: Terminating ReplicationController externalsvc pods took: 600.07831ms
Dec 16 23:55:27.935: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:55:27.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3268" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:20.904 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":250,"skipped":4454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:55:27.987: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:55:28.079: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 16 23:55:31.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-3394 --namespace=crd-publish-openapi-3394 create -f -'
Dec 16 23:55:32.100: INFO: stderr: ""
Dec 16 23:55:32.100: INFO: stdout: "e2e-test-crd-publish-openapi-1427-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 16 23:55:32.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-3394 --namespace=crd-publish-openapi-3394 delete e2e-test-crd-publish-openapi-1427-crds test-cr'
Dec 16 23:55:32.181: INFO: stderr: ""
Dec 16 23:55:32.181: INFO: stdout: "e2e-test-crd-publish-openapi-1427-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 16 23:55:32.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-3394 --namespace=crd-publish-openapi-3394 apply -f -'
Dec 16 23:55:32.463: INFO: stderr: ""
Dec 16 23:55:32.463: INFO: stdout: "e2e-test-crd-publish-openapi-1427-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 16 23:55:32.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-3394 --namespace=crd-publish-openapi-3394 delete e2e-test-crd-publish-openapi-1427-crds test-cr'
Dec 16 23:55:32.542: INFO: stderr: ""
Dec 16 23:55:32.542: INFO: stdout: "e2e-test-crd-publish-openapi-1427-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 16 23:55:32.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=crd-publish-openapi-3394 explain e2e-test-crd-publish-openapi-1427-crds'
Dec 16 23:55:32.734: INFO: stderr: ""
Dec 16 23:55:32.734: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1427-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:55:35.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3394" for this suite.

• [SLOW TEST:7.752 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":251,"skipped":4484,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:55:35.739: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Dec 16 23:55:35.856: INFO: Waiting up to 5m0s for pod "downward-api-76fa4c41-0bc5-4c9b-9366-9b386c236476" in namespace "downward-api-2941" to be "Succeeded or Failed"
Dec 16 23:55:35.863: INFO: Pod "downward-api-76fa4c41-0bc5-4c9b-9366-9b386c236476": Phase="Pending", Reason="", readiness=false. Elapsed: 6.864929ms
Dec 16 23:55:37.871: INFO: Pod "downward-api-76fa4c41-0bc5-4c9b-9366-9b386c236476": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01459039s
STEP: Saw pod success
Dec 16 23:55:37.871: INFO: Pod "downward-api-76fa4c41-0bc5-4c9b-9366-9b386c236476" satisfied condition "Succeeded or Failed"
Dec 16 23:55:37.874: INFO: Trying to get logs from node ip-172-31-1-217 pod downward-api-76fa4c41-0bc5-4c9b-9366-9b386c236476 container dapi-container: <nil>
STEP: delete the pod
Dec 16 23:55:37.900: INFO: Waiting for pod downward-api-76fa4c41-0bc5-4c9b-9366-9b386c236476 to disappear
Dec 16 23:55:37.904: INFO: Pod downward-api-76fa4c41-0bc5-4c9b-9366-9b386c236476 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:55:37.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2941" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":252,"skipped":4498,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:55:37.922: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:55:37.977: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:55:39.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8202" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":253,"skipped":4501,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:55:39.200: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Dec 16 23:55:39.278: INFO: Waiting up to 5m0s for pod "test-pod-63e45910-f43a-480f-b7bd-e7ac2cd97f7c" in namespace "svcaccounts-9005" to be "Succeeded or Failed"
Dec 16 23:55:39.287: INFO: Pod "test-pod-63e45910-f43a-480f-b7bd-e7ac2cd97f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.158991ms
Dec 16 23:55:41.296: INFO: Pod "test-pod-63e45910-f43a-480f-b7bd-e7ac2cd97f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018624869s
STEP: Saw pod success
Dec 16 23:55:41.296: INFO: Pod "test-pod-63e45910-f43a-480f-b7bd-e7ac2cd97f7c" satisfied condition "Succeeded or Failed"
Dec 16 23:55:41.299: INFO: Trying to get logs from node ip-172-31-1-217 pod test-pod-63e45910-f43a-480f-b7bd-e7ac2cd97f7c container agnhost-container: <nil>
STEP: delete the pod
Dec 16 23:55:41.322: INFO: Waiting for pod test-pod-63e45910-f43a-480f-b7bd-e7ac2cd97f7c to disappear
Dec 16 23:55:41.326: INFO: Pod test-pod-63e45910-f43a-480f-b7bd-e7ac2cd97f7c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:55:41.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9005" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":254,"skipped":4520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:55:41.339: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:55:41.405: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Creating first CR 
Dec 16 23:55:41.965: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-16T23:55:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-16T23:55:41Z]] name:name1 resourceVersion:46129 uid:9cbef444-bc22-40b7-b97f-ea064e6f2a94] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 16 23:55:51.974: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-16T23:55:51Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-16T23:55:51Z]] name:name2 resourceVersion:46174 uid:78b49f03-c27e-4c52-b7dd-60ef1efe094e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 16 23:56:01.984: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-16T23:55:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-16T23:56:01Z]] name:name1 resourceVersion:46193 uid:9cbef444-bc22-40b7-b97f-ea064e6f2a94] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 16 23:56:11.996: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-16T23:55:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-16T23:56:11Z]] name:name2 resourceVersion:46213 uid:78b49f03-c27e-4c52-b7dd-60ef1efe094e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 16 23:56:22.012: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-16T23:55:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-16T23:56:01Z]] name:name1 resourceVersion:46232 uid:9cbef444-bc22-40b7-b97f-ea064e6f2a94] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 16 23:56:32.022: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-16T23:55:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-16T23:56:11Z]] name:name2 resourceVersion:46251 uid:78b49f03-c27e-4c52-b7dd-60ef1efe094e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:56:42.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9362" for this suite.

• [SLOW TEST:61.220 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":255,"skipped":4543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:56:42.561: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 16 23:56:46.657: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 16 23:56:46.661: INFO: Pod pod-with-prestop-http-hook still exists
Dec 16 23:56:48.662: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 16 23:56:48.669: INFO: Pod pod-with-prestop-http-hook still exists
Dec 16 23:56:50.661: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 16 23:56:50.669: INFO: Pod pod-with-prestop-http-hook still exists
Dec 16 23:56:52.661: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 16 23:56:52.667: INFO: Pod pod-with-prestop-http-hook still exists
Dec 16 23:56:54.661: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 16 23:56:54.668: INFO: Pod pod-with-prestop-http-hook still exists
Dec 16 23:56:56.661: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 16 23:56:56.673: INFO: Pod pod-with-prestop-http-hook still exists
Dec 16 23:56:58.661: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 16 23:56:58.671: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:56:58.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8347" for this suite.

• [SLOW TEST:16.135 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4566,"failed":0}
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:56:58.696: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Dec 16 23:56:58.740: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:57:04.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1127" for this suite.

• [SLOW TEST:6.203 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":257,"skipped":4566,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:57:04.899: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1216 23:57:15.072504      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 16 23:58:17.104: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Dec 16 23:58:17.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d5k8" in namespace "gc-3949"
Dec 16 23:58:17.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vzwx" in namespace "gc-3949"
Dec 16 23:58:17.162: INFO: Deleting pod "simpletest-rc-to-be-deleted-755kj" in namespace "gc-3949"
Dec 16 23:58:17.192: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2x2v" in namespace "gc-3949"
Dec 16 23:58:17.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpcbr" in namespace "gc-3949"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:58:17.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3949" for this suite.

• [SLOW TEST:72.359 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":258,"skipped":4572,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:58:17.261: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:58:18.197: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 16 23:58:20.210: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759898, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759898, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759898, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759898, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:58:23.238: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:58:35.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4143" for this suite.
STEP: Destroying namespace "webhook-4143-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:18.325 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":259,"skipped":4589,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:58:35.588: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Dec 16 23:58:35.651: INFO: Waiting up to 5m0s for pod "client-containers-5f4b6a7d-0faf-401e-b5fd-922500fb9124" in namespace "containers-9606" to be "Succeeded or Failed"
Dec 16 23:58:35.656: INFO: Pod "client-containers-5f4b6a7d-0faf-401e-b5fd-922500fb9124": Phase="Pending", Reason="", readiness=false. Elapsed: 5.02977ms
Dec 16 23:58:37.663: INFO: Pod "client-containers-5f4b6a7d-0faf-401e-b5fd-922500fb9124": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012243597s
STEP: Saw pod success
Dec 16 23:58:37.663: INFO: Pod "client-containers-5f4b6a7d-0faf-401e-b5fd-922500fb9124" satisfied condition "Succeeded or Failed"
Dec 16 23:58:37.666: INFO: Trying to get logs from node ip-172-31-1-217 pod client-containers-5f4b6a7d-0faf-401e-b5fd-922500fb9124 container agnhost-container: <nil>
STEP: delete the pod
Dec 16 23:58:37.696: INFO: Waiting for pod client-containers-5f4b6a7d-0faf-401e-b5fd-922500fb9124 to disappear
Dec 16 23:58:37.700: INFO: Pod client-containers-5f4b6a7d-0faf-401e-b5fd-922500fb9124 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:58:37.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9606" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4593,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:58:37.724: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 16 23:58:38.123: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 16 23:58:40.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759918, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759918, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759918, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743759918, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 16 23:58:43.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:58:43.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8492" for this suite.
STEP: Destroying namespace "webhook-8492-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.564 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":261,"skipped":4605,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:58:43.288: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 16 23:58:43.339: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ce38700-c6a9-4d93-8753-7ed218167c95" in namespace "downward-api-9576" to be "Succeeded or Failed"
Dec 16 23:58:43.342: INFO: Pod "downwardapi-volume-8ce38700-c6a9-4d93-8753-7ed218167c95": Phase="Pending", Reason="", readiness=false. Elapsed: 3.483711ms
Dec 16 23:58:45.351: INFO: Pod "downwardapi-volume-8ce38700-c6a9-4d93-8753-7ed218167c95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012164737s
STEP: Saw pod success
Dec 16 23:58:45.351: INFO: Pod "downwardapi-volume-8ce38700-c6a9-4d93-8753-7ed218167c95" satisfied condition "Succeeded or Failed"
Dec 16 23:58:45.354: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-8ce38700-c6a9-4d93-8753-7ed218167c95 container client-container: <nil>
STEP: delete the pod
Dec 16 23:58:45.379: INFO: Waiting for pod downwardapi-volume-8ce38700-c6a9-4d93-8753-7ed218167c95 to disappear
Dec 16 23:58:45.382: INFO: Pod downwardapi-volume-8ce38700-c6a9-4d93-8753-7ed218167c95 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:58:45.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9576" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":262,"skipped":4624,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:58:45.399: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Dec 16 23:58:45.475: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 16 23:58:45.483: INFO: Waiting for terminating namespaces to be deleted...
Dec 16 23:58:45.486: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-1-217 before test
Dec 16 23:58:45.492: INFO: calico-node-ldh6v from kube-system started at 2020-12-16 21:14:55 +0000 UTC (1 container statuses recorded)
Dec 16 23:58:45.492: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 23:58:45.492: INFO: kube-proxy-v4mt9 from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 16 23:58:45.492: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 23:58:45.492: INFO: nginx-proxy-ip-172-31-1-217 from kube-system started at 2020-12-16 21:14:55 +0000 UTC (1 container statuses recorded)
Dec 16 23:58:45.492: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 16 23:58:45.492: INFO: sonobuoy from sonobuoy started at 2020-12-16 22:33:28 +0000 UTC (1 container statuses recorded)
Dec 16 23:58:45.492: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 16 23:58:45.492: INFO: sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-jkfz6 from sonobuoy started at 2020-12-16 22:33:30 +0000 UTC (2 container statuses recorded)
Dec 16 23:58:45.492: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Dec 16 23:58:45.492: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 23:58:45.492: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-174 before test
Dec 16 23:58:45.498: INFO: calico-node-84s5l from kube-system started at 2020-12-16 21:14:54 +0000 UTC (1 container statuses recorded)
Dec 16 23:58:45.498: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 23:58:45.498: INFO: coredns-74ff55c5b-96bcf from kube-system started at 2020-12-16 21:15:19 +0000 UTC (1 container statuses recorded)
Dec 16 23:58:45.498: INFO: 	Container coredns ready: true, restart count 0
Dec 16 23:58:45.498: INFO: coredns-74ff55c5b-t4nvx from kube-system started at 2020-12-16 22:37:05 +0000 UTC (1 container statuses recorded)
Dec 16 23:58:45.498: INFO: 	Container coredns ready: true, restart count 0
Dec 16 23:58:45.498: INFO: kube-proxy-zpdbw from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 16 23:58:45.498: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 23:58:45.498: INFO: nginx-proxy-ip-172-31-6-174 from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 16 23:58:45.498: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 16 23:58:45.498: INFO: sonobuoy-e2e-job-1783c268d2664b69 from sonobuoy started at 2020-12-16 22:33:29 +0000 UTC (2 container statuses recorded)
Dec 16 23:58:45.498: INFO: 	Container e2e ready: true, restart count 0
Dec 16 23:58:45.498: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 23:58:45.498: INFO: sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-g7kjv from sonobuoy started at 2020-12-16 22:33:30 +0000 UTC (2 container statuses recorded)
Dec 16 23:58:45.498: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Dec 16 23:58:45.498: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node ip-172-31-1-217
STEP: verifying the node has the label node ip-172-31-6-174
Dec 16 23:58:45.557: INFO: Pod calico-node-84s5l requesting resource cpu=250m on Node ip-172-31-6-174
Dec 16 23:58:45.557: INFO: Pod calico-node-ldh6v requesting resource cpu=250m on Node ip-172-31-1-217
Dec 16 23:58:45.557: INFO: Pod coredns-74ff55c5b-96bcf requesting resource cpu=100m on Node ip-172-31-6-174
Dec 16 23:58:45.557: INFO: Pod coredns-74ff55c5b-t4nvx requesting resource cpu=100m on Node ip-172-31-6-174
Dec 16 23:58:45.557: INFO: Pod kube-proxy-v4mt9 requesting resource cpu=0m on Node ip-172-31-1-217
Dec 16 23:58:45.557: INFO: Pod kube-proxy-zpdbw requesting resource cpu=0m on Node ip-172-31-6-174
Dec 16 23:58:45.557: INFO: Pod nginx-proxy-ip-172-31-1-217 requesting resource cpu=25m on Node ip-172-31-1-217
Dec 16 23:58:45.557: INFO: Pod nginx-proxy-ip-172-31-6-174 requesting resource cpu=25m on Node ip-172-31-6-174
Dec 16 23:58:45.557: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-1-217
Dec 16 23:58:45.557: INFO: Pod sonobuoy-e2e-job-1783c268d2664b69 requesting resource cpu=0m on Node ip-172-31-6-174
Dec 16 23:58:45.557: INFO: Pod sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-g7kjv requesting resource cpu=0m on Node ip-172-31-6-174
Dec 16 23:58:45.557: INFO: Pod sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-jkfz6 requesting resource cpu=0m on Node ip-172-31-1-217
STEP: Starting Pods to consume most of the cluster CPU.
Dec 16 23:58:45.557: INFO: Creating a pod which consumes cpu=1207m on Node ip-172-31-1-217
Dec 16 23:58:45.567: INFO: Creating a pod which consumes cpu=1067m on Node ip-172-31-6-174
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-057f41a7-788a-4db0-ae72-4d5f055be1da.165157d7a12529b7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-801/filler-pod-057f41a7-788a-4db0-ae72-4d5f055be1da to ip-172-31-6-174]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-057f41a7-788a-4db0-ae72-4d5f055be1da.165157d7d58371d6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-057f41a7-788a-4db0-ae72-4d5f055be1da.165157d7d8dd0433], Reason = [Created], Message = [Created container filler-pod-057f41a7-788a-4db0-ae72-4d5f055be1da]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-057f41a7-788a-4db0-ae72-4d5f055be1da.165157d7ddb616e4], Reason = [Started], Message = [Started container filler-pod-057f41a7-788a-4db0-ae72-4d5f055be1da]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-305bf177-9476-40b6-8d97-3e204cb16c16.165157d7a0daf2d5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-801/filler-pod-305bf177-9476-40b6-8d97-3e204cb16c16 to ip-172-31-1-217]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-305bf177-9476-40b6-8d97-3e204cb16c16.165157d7d637dfd5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-305bf177-9476-40b6-8d97-3e204cb16c16.165157d7d9573dc8], Reason = [Created], Message = [Created container filler-pod-305bf177-9476-40b6-8d97-3e204cb16c16]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-305bf177-9476-40b6-8d97-3e204cb16c16.165157d7de134e49], Reason = [Started], Message = [Started container filler-pod-305bf177-9476-40b6-8d97-3e204cb16c16]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.165157d819f3ecbd], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.165157d81abeaa6f], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node ip-172-31-1-217
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-6-174
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:58:48.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-801" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":263,"skipped":4624,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:58:48.675: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-500a9e95-382b-4a7e-a4b7-b31222d1c8ec
STEP: Creating configMap with name cm-test-opt-upd-efd2b03b-43b0-4917-8db1-03fd91e8b385
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-500a9e95-382b-4a7e-a4b7-b31222d1c8ec
STEP: Updating configmap cm-test-opt-upd-efd2b03b-43b0-4917-8db1-03fd91e8b385
STEP: Creating configMap with name cm-test-opt-create-dfbd9bf6-b589-4653-aa46-b48e6d4fc072
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:58:52.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3556" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":264,"skipped":4642,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:58:52.986: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-83e566ae-f740-4dd4-9354-160d11e916bf
STEP: Creating a pod to test consume configMaps
Dec 16 23:58:53.073: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e771599-f761-41b1-8cf4-25a202db8edb" in namespace "projected-5660" to be "Succeeded or Failed"
Dec 16 23:58:53.080: INFO: Pod "pod-projected-configmaps-0e771599-f761-41b1-8cf4-25a202db8edb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.963509ms
Dec 16 23:58:55.087: INFO: Pod "pod-projected-configmaps-0e771599-f761-41b1-8cf4-25a202db8edb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012234062s
STEP: Saw pod success
Dec 16 23:58:55.087: INFO: Pod "pod-projected-configmaps-0e771599-f761-41b1-8cf4-25a202db8edb" satisfied condition "Succeeded or Failed"
Dec 16 23:58:55.090: INFO: Trying to get logs from node ip-172-31-6-174 pod pod-projected-configmaps-0e771599-f761-41b1-8cf4-25a202db8edb container agnhost-container: <nil>
STEP: delete the pod
Dec 16 23:58:55.122: INFO: Waiting for pod pod-projected-configmaps-0e771599-f761-41b1-8cf4-25a202db8edb to disappear
Dec 16 23:58:55.124: INFO: Pod pod-projected-configmaps-0e771599-f761-41b1-8cf4-25a202db8edb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:58:55.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5660" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:58:55.137: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 16 23:58:57.709: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ebf6a0d5-720e-4506-96f4-697b39cfcdc0"
Dec 16 23:58:57.709: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ebf6a0d5-720e-4506-96f4-697b39cfcdc0" in namespace "pods-4280" to be "terminated due to deadline exceeded"
Dec 16 23:58:57.712: INFO: Pod "pod-update-activedeadlineseconds-ebf6a0d5-720e-4506-96f4-697b39cfcdc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.840782ms
Dec 16 23:58:59.719: INFO: Pod "pod-update-activedeadlineseconds-ebf6a0d5-720e-4506-96f4-697b39cfcdc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.009455339s
Dec 16 23:59:01.725: INFO: Pod "pod-update-activedeadlineseconds-ebf6a0d5-720e-4506-96f4-697b39cfcdc0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.016232071s
Dec 16 23:59:01.725: INFO: Pod "pod-update-activedeadlineseconds-ebf6a0d5-720e-4506-96f4-697b39cfcdc0" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:59:01.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4280" for this suite.

• [SLOW TEST:6.601 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":266,"skipped":4718,"failed":0}
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:59:01.741: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:59:03.824: INFO: Deleting pod "var-expansion-cc435ea6-0491-4317-ad5b-3256dea22019" in namespace "var-expansion-3663"
Dec 16 23:59:03.837: INFO: Wait up to 5m0s for pod "var-expansion-cc435ea6-0491-4317-ad5b-3256dea22019" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:59:37.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3663" for this suite.

• [SLOW TEST:36.121 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":267,"skipped":4718,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:59:37.862: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 16 23:59:37.919: INFO: Waiting up to 5m0s for pod "pod-09b0bdf2-a1c1-4c26-a383-01af15f4c24c" in namespace "emptydir-459" to be "Succeeded or Failed"
Dec 16 23:59:37.922: INFO: Pod "pod-09b0bdf2-a1c1-4c26-a383-01af15f4c24c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.911782ms
Dec 16 23:59:39.929: INFO: Pod "pod-09b0bdf2-a1c1-4c26-a383-01af15f4c24c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01003296s
STEP: Saw pod success
Dec 16 23:59:39.929: INFO: Pod "pod-09b0bdf2-a1c1-4c26-a383-01af15f4c24c" satisfied condition "Succeeded or Failed"
Dec 16 23:59:39.932: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-09b0bdf2-a1c1-4c26-a383-01af15f4c24c container test-container: <nil>
STEP: delete the pod
Dec 16 23:59:39.950: INFO: Waiting for pod pod-09b0bdf2-a1c1-4c26-a383-01af15f4c24c to disappear
Dec 16 23:59:39.953: INFO: Pod pod-09b0bdf2-a1c1-4c26-a383-01af15f4c24c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:59:39.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-459" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":268,"skipped":4727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:59:39.971: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 16 23:59:40.336: INFO: Checking APIGroup: apiregistration.k8s.io
Dec 16 23:59:40.336: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec 16 23:59:40.336: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.337: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec 16 23:59:40.337: INFO: Checking APIGroup: apps
Dec 16 23:59:40.337: INFO: PreferredVersion.GroupVersion: apps/v1
Dec 16 23:59:40.337: INFO: Versions found [{apps/v1 v1}]
Dec 16 23:59:40.337: INFO: apps/v1 matches apps/v1
Dec 16 23:59:40.337: INFO: Checking APIGroup: events.k8s.io
Dec 16 23:59:40.338: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec 16 23:59:40.338: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.338: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec 16 23:59:40.338: INFO: Checking APIGroup: authentication.k8s.io
Dec 16 23:59:40.339: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec 16 23:59:40.339: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.339: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec 16 23:59:40.339: INFO: Checking APIGroup: authorization.k8s.io
Dec 16 23:59:40.340: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec 16 23:59:40.340: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.340: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec 16 23:59:40.340: INFO: Checking APIGroup: autoscaling
Dec 16 23:59:40.340: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Dec 16 23:59:40.340: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Dec 16 23:59:40.341: INFO: autoscaling/v1 matches autoscaling/v1
Dec 16 23:59:40.341: INFO: Checking APIGroup: batch
Dec 16 23:59:40.342: INFO: PreferredVersion.GroupVersion: batch/v1
Dec 16 23:59:40.342: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Dec 16 23:59:40.342: INFO: batch/v1 matches batch/v1
Dec 16 23:59:40.342: INFO: Checking APIGroup: certificates.k8s.io
Dec 16 23:59:40.343: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec 16 23:59:40.343: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.343: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec 16 23:59:40.343: INFO: Checking APIGroup: networking.k8s.io
Dec 16 23:59:40.343: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec 16 23:59:40.344: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.344: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec 16 23:59:40.344: INFO: Checking APIGroup: extensions
Dec 16 23:59:40.344: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Dec 16 23:59:40.344: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Dec 16 23:59:40.345: INFO: extensions/v1beta1 matches extensions/v1beta1
Dec 16 23:59:40.345: INFO: Checking APIGroup: policy
Dec 16 23:59:40.345: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Dec 16 23:59:40.345: INFO: Versions found [{policy/v1beta1 v1beta1}]
Dec 16 23:59:40.345: INFO: policy/v1beta1 matches policy/v1beta1
Dec 16 23:59:40.345: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec 16 23:59:40.346: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec 16 23:59:40.346: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.347: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec 16 23:59:40.347: INFO: Checking APIGroup: storage.k8s.io
Dec 16 23:59:40.347: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec 16 23:59:40.347: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.347: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec 16 23:59:40.347: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec 16 23:59:40.348: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec 16 23:59:40.348: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.348: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec 16 23:59:40.348: INFO: Checking APIGroup: apiextensions.k8s.io
Dec 16 23:59:40.350: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec 16 23:59:40.350: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.350: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec 16 23:59:40.350: INFO: Checking APIGroup: scheduling.k8s.io
Dec 16 23:59:40.351: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec 16 23:59:40.351: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.351: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec 16 23:59:40.351: INFO: Checking APIGroup: coordination.k8s.io
Dec 16 23:59:40.352: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec 16 23:59:40.352: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.352: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec 16 23:59:40.352: INFO: Checking APIGroup: node.k8s.io
Dec 16 23:59:40.353: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec 16 23:59:40.353: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.353: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec 16 23:59:40.353: INFO: Checking APIGroup: discovery.k8s.io
Dec 16 23:59:40.354: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Dec 16 23:59:40.354: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.354: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Dec 16 23:59:40.354: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec 16 23:59:40.355: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Dec 16 23:59:40.355: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Dec 16 23:59:40.355: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Dec 16 23:59:40.355: INFO: Checking APIGroup: crd.projectcalico.org
Dec 16 23:59:40.356: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Dec 16 23:59:40.356: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Dec 16 23:59:40.356: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:59:40.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-739" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":269,"skipped":4749,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:59:40.376: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 16 23:59:40.419: INFO: Waiting up to 5m0s for pod "pod-5d407e98-1e61-4a5f-8f3d-1a2834583e0a" in namespace "emptydir-1613" to be "Succeeded or Failed"
Dec 16 23:59:40.423: INFO: Pod "pod-5d407e98-1e61-4a5f-8f3d-1a2834583e0a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.983614ms
Dec 16 23:59:42.430: INFO: Pod "pod-5d407e98-1e61-4a5f-8f3d-1a2834583e0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010964237s
STEP: Saw pod success
Dec 16 23:59:42.430: INFO: Pod "pod-5d407e98-1e61-4a5f-8f3d-1a2834583e0a" satisfied condition "Succeeded or Failed"
Dec 16 23:59:42.439: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-5d407e98-1e61-4a5f-8f3d-1a2834583e0a container test-container: <nil>
STEP: delete the pod
Dec 16 23:59:42.461: INFO: Waiting for pod pod-5d407e98-1e61-4a5f-8f3d-1a2834583e0a to disappear
Dec 16 23:59:42.464: INFO: Pod pod-5d407e98-1e61-4a5f-8f3d-1a2834583e0a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 16 23:59:42.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1613" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":270,"skipped":4752,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 16 23:59:42.474: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:00:42.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3197" for this suite.

• [SLOW TEST:60.081 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:00:42.556: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-70d2f9d1-8ddf-40a1-a546-722ee256ecc8
STEP: Creating a pod to test consume configMaps
Dec 17 00:00:42.623: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e8f2881-b9eb-4679-9900-70df3cc9234e" in namespace "configmap-2068" to be "Succeeded or Failed"
Dec 17 00:00:42.629: INFO: Pod "pod-configmaps-7e8f2881-b9eb-4679-9900-70df3cc9234e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16558ms
Dec 17 00:00:44.645: INFO: Pod "pod-configmaps-7e8f2881-b9eb-4679-9900-70df3cc9234e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020508201s
STEP: Saw pod success
Dec 17 00:00:44.645: INFO: Pod "pod-configmaps-7e8f2881-b9eb-4679-9900-70df3cc9234e" satisfied condition "Succeeded or Failed"
Dec 17 00:00:44.648: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-configmaps-7e8f2881-b9eb-4679-9900-70df3cc9234e container agnhost-container: <nil>
STEP: delete the pod
Dec 17 00:00:44.698: INFO: Waiting for pod pod-configmaps-7e8f2881-b9eb-4679-9900-70df3cc9234e to disappear
Dec 17 00:00:44.701: INFO: Pod pod-configmaps-7e8f2881-b9eb-4679-9900-70df3cc9234e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:00:44.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2068" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4790,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:00:44.716: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Dec 17 00:00:44.767: INFO: Waiting up to 5m0s for pod "var-expansion-1e70e5d2-0196-4ed6-aea1-cb95b9df7932" in namespace "var-expansion-4880" to be "Succeeded or Failed"
Dec 17 00:00:44.770: INFO: Pod "var-expansion-1e70e5d2-0196-4ed6-aea1-cb95b9df7932": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643473ms
Dec 17 00:00:46.777: INFO: Pod "var-expansion-1e70e5d2-0196-4ed6-aea1-cb95b9df7932": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009987129s
STEP: Saw pod success
Dec 17 00:00:46.777: INFO: Pod "var-expansion-1e70e5d2-0196-4ed6-aea1-cb95b9df7932" satisfied condition "Succeeded or Failed"
Dec 17 00:00:46.780: INFO: Trying to get logs from node ip-172-31-1-217 pod var-expansion-1e70e5d2-0196-4ed6-aea1-cb95b9df7932 container dapi-container: <nil>
STEP: delete the pod
Dec 17 00:00:46.801: INFO: Waiting for pod var-expansion-1e70e5d2-0196-4ed6-aea1-cb95b9df7932 to disappear
Dec 17 00:00:46.806: INFO: Pod var-expansion-1e70e5d2-0196-4ed6-aea1-cb95b9df7932 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:00:46.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4880" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":273,"skipped":4811,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:00:46.830: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-5be96acd-06cb-4bb9-b2b0-2617a5fc201c
STEP: Creating a pod to test consume configMaps
Dec 17 00:00:46.887: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a458b0c-604e-43db-95f6-849ad838e604" in namespace "projected-3030" to be "Succeeded or Failed"
Dec 17 00:00:46.890: INFO: Pod "pod-projected-configmaps-7a458b0c-604e-43db-95f6-849ad838e604": Phase="Pending", Reason="", readiness=false. Elapsed: 2.90408ms
Dec 17 00:00:48.897: INFO: Pod "pod-projected-configmaps-7a458b0c-604e-43db-95f6-849ad838e604": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010120126s
STEP: Saw pod success
Dec 17 00:00:48.898: INFO: Pod "pod-projected-configmaps-7a458b0c-604e-43db-95f6-849ad838e604" satisfied condition "Succeeded or Failed"
Dec 17 00:00:48.900: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-configmaps-7a458b0c-604e-43db-95f6-849ad838e604 container agnhost-container: <nil>
STEP: delete the pod
Dec 17 00:00:48.919: INFO: Waiting for pod pod-projected-configmaps-7a458b0c-604e-43db-95f6-849ad838e604 to disappear
Dec 17 00:00:48.923: INFO: Pod pod-projected-configmaps-7a458b0c-604e-43db-95f6-849ad838e604 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:00:48.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3030" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4825,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:00:48.933: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-6d16b454-d45b-4f19-996d-7726cc222eee
STEP: Creating a pod to test consume secrets
Dec 17 00:00:48.984: INFO: Waiting up to 5m0s for pod "pod-secrets-a8ce0761-806c-498b-a916-7ab6e860349b" in namespace "secrets-3508" to be "Succeeded or Failed"
Dec 17 00:00:48.988: INFO: Pod "pod-secrets-a8ce0761-806c-498b-a916-7ab6e860349b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016187ms
Dec 17 00:00:50.995: INFO: Pod "pod-secrets-a8ce0761-806c-498b-a916-7ab6e860349b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011023893s
STEP: Saw pod success
Dec 17 00:00:50.995: INFO: Pod "pod-secrets-a8ce0761-806c-498b-a916-7ab6e860349b" satisfied condition "Succeeded or Failed"
Dec 17 00:00:50.998: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-secrets-a8ce0761-806c-498b-a916-7ab6e860349b container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 00:00:51.041: INFO: Waiting for pod pod-secrets-a8ce0761-806c-498b-a916-7ab6e860349b to disappear
Dec 17 00:00:51.044: INFO: Pod pod-secrets-a8ce0761-806c-498b-a916-7ab6e860349b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:00:51.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3508" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4828,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:00:51.057: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-2c30d377-bdbb-4465-a382-ee148add6cd8
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:00:51.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6588" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":276,"skipped":4841,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:00:51.122: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Dec 17 00:00:51.167: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 17 00:00:51.174: INFO: Waiting for terminating namespaces to be deleted...
Dec 17 00:00:51.177: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-1-217 before test
Dec 17 00:00:51.183: INFO: test-webserver-ec51d59d-4a7a-45ae-b0b2-598d06d43308 from container-probe-3197 started at 2020-12-16 23:59:42 +0000 UTC (1 container statuses recorded)
Dec 17 00:00:51.183: INFO: 	Container test-webserver ready: false, restart count 0
Dec 17 00:00:51.183: INFO: calico-node-ldh6v from kube-system started at 2020-12-16 21:14:55 +0000 UTC (1 container statuses recorded)
Dec 17 00:00:51.183: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 00:00:51.183: INFO: kube-proxy-v4mt9 from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 17 00:00:51.183: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 00:00:51.183: INFO: nginx-proxy-ip-172-31-1-217 from kube-system started at 2020-12-16 21:14:55 +0000 UTC (1 container statuses recorded)
Dec 17 00:00:51.183: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 00:00:51.183: INFO: sonobuoy from sonobuoy started at 2020-12-16 22:33:28 +0000 UTC (1 container statuses recorded)
Dec 17 00:00:51.183: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 17 00:00:51.183: INFO: sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-jkfz6 from sonobuoy started at 2020-12-16 22:33:30 +0000 UTC (2 container statuses recorded)
Dec 17 00:00:51.183: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Dec 17 00:00:51.184: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 00:00:51.184: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-174 before test
Dec 17 00:00:51.191: INFO: calico-node-84s5l from kube-system started at 2020-12-16 21:14:54 +0000 UTC (1 container statuses recorded)
Dec 17 00:00:51.191: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 00:00:51.191: INFO: coredns-74ff55c5b-96bcf from kube-system started at 2020-12-16 21:15:19 +0000 UTC (1 container statuses recorded)
Dec 17 00:00:51.191: INFO: 	Container coredns ready: true, restart count 0
Dec 17 00:00:51.191: INFO: coredns-74ff55c5b-t4nvx from kube-system started at 2020-12-16 22:37:05 +0000 UTC (1 container statuses recorded)
Dec 17 00:00:51.191: INFO: 	Container coredns ready: true, restart count 0
Dec 17 00:00:51.191: INFO: kube-proxy-zpdbw from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 17 00:00:51.191: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 00:00:51.191: INFO: nginx-proxy-ip-172-31-6-174 from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 17 00:00:51.191: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 00:00:51.191: INFO: sonobuoy-e2e-job-1783c268d2664b69 from sonobuoy started at 2020-12-16 22:33:29 +0000 UTC (2 container statuses recorded)
Dec 17 00:00:51.191: INFO: 	Container e2e ready: true, restart count 0
Dec 17 00:00:51.191: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 00:00:51.191: INFO: sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-g7kjv from sonobuoy started at 2020-12-16 22:33:30 +0000 UTC (2 container statuses recorded)
Dec 17 00:00:51.192: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Dec 17 00:00:51.192: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cb41a169-2ace-4031-be29-902e166c76bd 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.1.217 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-cb41a169-2ace-4031-be29-902e166c76bd off the node ip-172-31-1-217
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cb41a169-2ace-4031-be29-902e166c76bd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:05:55.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2556" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.217 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":277,"skipped":4847,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:05:55.342: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:05:55.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7956" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":278,"skipped":4868,"failed":0}
SSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:05:55.543: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Dec 17 00:05:55.594: INFO: created test-podtemplate-1
Dec 17 00:05:55.600: INFO: created test-podtemplate-2
Dec 17 00:05:55.604: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Dec 17 00:05:55.607: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Dec 17 00:05:55.626: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:05:55.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4977" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":279,"skipped":4875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:05:55.642: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5867
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5867
STEP: creating replication controller externalsvc in namespace services-5867
I1217 00:05:55.794795      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5867, replica count: 2
I1217 00:05:58.845293      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 17 00:05:58.883: INFO: Creating new exec pod
Dec 17 00:06:02.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-5867 exec execpodbfvdx -- /bin/sh -x -c nslookup clusterip-service.services-5867.svc.cluster.local'
Dec 17 00:06:03.412: INFO: stderr: "+ nslookup clusterip-service.services-5867.svc.cluster.local\n"
Dec 17 00:06:03.412: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-5867.svc.cluster.local\tcanonical name = externalsvc.services-5867.svc.cluster.local.\nName:\texternalsvc.services-5867.svc.cluster.local\nAddress: 10.98.57.47\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5867, will wait for the garbage collector to delete the pods
Dec 17 00:06:03.474: INFO: Deleting ReplicationController externalsvc took: 8.030274ms
Dec 17 00:06:04.075: INFO: Terminating ReplicationController externalsvc pods took: 600.164522ms
Dec 17 00:06:17.936: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:06:17.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5867" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:22.329 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":280,"skipped":4916,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:06:17.975: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 17 00:06:20.066: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:06:20.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6876" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4932,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:06:20.110: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-1062/secret-test-f8e6dd83-53a3-4651-9bf4-f9af138be826
STEP: Creating a pod to test consume secrets
Dec 17 00:06:20.173: INFO: Waiting up to 5m0s for pod "pod-configmaps-7fa76a3c-7395-4985-8bef-457f6e193073" in namespace "secrets-1062" to be "Succeeded or Failed"
Dec 17 00:06:20.180: INFO: Pod "pod-configmaps-7fa76a3c-7395-4985-8bef-457f6e193073": Phase="Pending", Reason="", readiness=false. Elapsed: 6.785388ms
Dec 17 00:06:22.189: INFO: Pod "pod-configmaps-7fa76a3c-7395-4985-8bef-457f6e193073": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016221949s
STEP: Saw pod success
Dec 17 00:06:22.189: INFO: Pod "pod-configmaps-7fa76a3c-7395-4985-8bef-457f6e193073" satisfied condition "Succeeded or Failed"
Dec 17 00:06:22.194: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-configmaps-7fa76a3c-7395-4985-8bef-457f6e193073 container env-test: <nil>
STEP: delete the pod
Dec 17 00:06:22.241: INFO: Waiting for pod pod-configmaps-7fa76a3c-7395-4985-8bef-457f6e193073 to disappear
Dec 17 00:06:22.244: INFO: Pod pod-configmaps-7fa76a3c-7395-4985-8bef-457f6e193073 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:06:22.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1062" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":282,"skipped":4944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:06:22.258: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 17 00:06:22.306: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:06:24.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6532" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:06:24.355: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:06:24.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4290" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":284,"skipped":5015,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:06:24.436: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-5ff03129-a629-43ee-a062-2135ad9cabaa in namespace container-probe-2299
Dec 17 00:06:26.489: INFO: Started pod busybox-5ff03129-a629-43ee-a062-2135ad9cabaa in namespace container-probe-2299
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 00:06:26.496: INFO: Initial restart count of pod busybox-5ff03129-a629-43ee-a062-2135ad9cabaa is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:10:27.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2299" for this suite.

• [SLOW TEST:243.133 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":5037,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:10:27.570: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-6798
STEP: creating service affinity-clusterip-transition in namespace services-6798
STEP: creating replication controller affinity-clusterip-transition in namespace services-6798
I1217 00:10:27.687008      24 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-6798, replica count: 3
I1217 00:10:30.742902      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 17 00:10:30.753: INFO: Creating new exec pod
Dec 17 00:10:33.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-6798 exec execpod-affinitywvv6n -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Dec 17 00:10:33.951: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec 17 00:10:33.951: INFO: stdout: ""
Dec 17 00:10:33.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-6798 exec execpod-affinitywvv6n -- /bin/sh -x -c nc -zv -t -w 2 10.110.69.97 80'
Dec 17 00:10:34.108: INFO: stderr: "+ nc -zv -t -w 2 10.110.69.97 80\nConnection to 10.110.69.97 80 port [tcp/http] succeeded!\n"
Dec 17 00:10:34.108: INFO: stdout: ""
Dec 17 00:10:34.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-6798 exec execpod-affinitywvv6n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.69.97:80/ ; done'
Dec 17 00:10:34.400: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n"
Dec 17 00:10:34.400: INFO: stdout: "\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-r9krw\naffinity-clusterip-transition-r9krw\naffinity-clusterip-transition-r9krw\naffinity-clusterip-transition-r9krw\naffinity-clusterip-transition-lchfm\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-lchfm\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-r9krw\naffinity-clusterip-transition-r9krw\naffinity-clusterip-transition-r9krw\naffinity-clusterip-transition-lchfm"
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-r9krw
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-r9krw
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-r9krw
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-r9krw
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-lchfm
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-lchfm
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-r9krw
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-r9krw
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-r9krw
Dec 17 00:10:34.400: INFO: Received response from host: affinity-clusterip-transition-lchfm
Dec 17 00:10:34.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-864705134 --namespace=services-6798 exec execpod-affinitywvv6n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.69.97:80/ ; done'
Dec 17 00:10:34.651: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.69.97:80/\n"
Dec 17 00:10:34.651: INFO: stdout: "\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4\naffinity-clusterip-transition-7qvz4"
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Received response from host: affinity-clusterip-transition-7qvz4
Dec 17 00:10:34.651: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6798, will wait for the garbage collector to delete the pods
Dec 17 00:10:34.731: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.324978ms
Dec 17 00:10:35.331: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 600.291254ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:10:47.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6798" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:20.438 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":286,"skipped":5068,"failed":0}
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:10:48.015: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 17 00:10:48.073: INFO: Creating ReplicaSet my-hostname-basic-e36820fd-8641-458b-98f5-9e294b56153c
Dec 17 00:10:48.085: INFO: Pod name my-hostname-basic-e36820fd-8641-458b-98f5-9e294b56153c: Found 0 pods out of 1
Dec 17 00:10:53.097: INFO: Pod name my-hostname-basic-e36820fd-8641-458b-98f5-9e294b56153c: Found 1 pods out of 1
Dec 17 00:10:53.097: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e36820fd-8641-458b-98f5-9e294b56153c" is running
Dec 17 00:10:53.100: INFO: Pod "my-hostname-basic-e36820fd-8641-458b-98f5-9e294b56153c-fx4cw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 00:10:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 00:10:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 00:10:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 00:10:48 +0000 UTC Reason: Message:}])
Dec 17 00:10:53.101: INFO: Trying to dial the pod
Dec 17 00:10:58.115: INFO: Controller my-hostname-basic-e36820fd-8641-458b-98f5-9e294b56153c: Got expected result from replica 1 [my-hostname-basic-e36820fd-8641-458b-98f5-9e294b56153c-fx4cw]: "my-hostname-basic-e36820fd-8641-458b-98f5-9e294b56153c-fx4cw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:10:58.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-281" for this suite.

• [SLOW TEST:10.111 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":287,"skipped":5068,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:10:58.127: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 17 00:10:58.184: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0657d808-ba13-4ed2-87ce-700d6964c58a" in namespace "projected-5676" to be "Succeeded or Failed"
Dec 17 00:10:58.190: INFO: Pod "downwardapi-volume-0657d808-ba13-4ed2-87ce-700d6964c58a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448088ms
Dec 17 00:11:00.201: INFO: Pod "downwardapi-volume-0657d808-ba13-4ed2-87ce-700d6964c58a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016482275s
STEP: Saw pod success
Dec 17 00:11:00.201: INFO: Pod "downwardapi-volume-0657d808-ba13-4ed2-87ce-700d6964c58a" satisfied condition "Succeeded or Failed"
Dec 17 00:11:00.204: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-0657d808-ba13-4ed2-87ce-700d6964c58a container client-container: <nil>
STEP: delete the pod
Dec 17 00:11:00.234: INFO: Waiting for pod downwardapi-volume-0657d808-ba13-4ed2-87ce-700d6964c58a to disappear
Dec 17 00:11:00.237: INFO: Pod downwardapi-volume-0657d808-ba13-4ed2-87ce-700d6964c58a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:11:00.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5676" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":288,"skipped":5076,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:11:00.267: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-dchl
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 00:11:00.339: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dchl" in namespace "subpath-4945" to be "Succeeded or Failed"
Dec 17 00:11:00.343: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.455678ms
Dec 17 00:11:02.352: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012742491s
Dec 17 00:11:04.359: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Running", Reason="", readiness=true. Elapsed: 4.019733403s
Dec 17 00:11:06.366: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Running", Reason="", readiness=true. Elapsed: 6.026577302s
Dec 17 00:11:08.373: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Running", Reason="", readiness=true. Elapsed: 8.03361608s
Dec 17 00:11:10.379: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Running", Reason="", readiness=true. Elapsed: 10.040354732s
Dec 17 00:11:12.387: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Running", Reason="", readiness=true. Elapsed: 12.048256854s
Dec 17 00:11:14.396: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Running", Reason="", readiness=true. Elapsed: 14.056857463s
Dec 17 00:11:16.405: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Running", Reason="", readiness=true. Elapsed: 16.066303147s
Dec 17 00:11:18.413: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Running", Reason="", readiness=true. Elapsed: 18.073969513s
Dec 17 00:11:20.420: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Running", Reason="", readiness=true. Elapsed: 20.081309957s
Dec 17 00:11:22.428: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Running", Reason="", readiness=true. Elapsed: 22.088813743s
Dec 17 00:11:24.435: INFO: Pod "pod-subpath-test-configmap-dchl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.095898862s
STEP: Saw pod success
Dec 17 00:11:24.435: INFO: Pod "pod-subpath-test-configmap-dchl" satisfied condition "Succeeded or Failed"
Dec 17 00:11:24.438: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-subpath-test-configmap-dchl container test-container-subpath-configmap-dchl: <nil>
STEP: delete the pod
Dec 17 00:11:24.466: INFO: Waiting for pod pod-subpath-test-configmap-dchl to disappear
Dec 17 00:11:24.469: INFO: Pod pod-subpath-test-configmap-dchl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dchl
Dec 17 00:11:24.469: INFO: Deleting pod "pod-subpath-test-configmap-dchl" in namespace "subpath-4945"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:11:24.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4945" for this suite.

• [SLOW TEST:24.219 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":289,"skipped":5092,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:11:24.486: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 17 00:11:24.523: INFO: Creating deployment "test-recreate-deployment"
Dec 17 00:11:24.529: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 17 00:11:24.547: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 17 00:11:26.560: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 17 00:11:26.563: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 17 00:11:26.577: INFO: Updating deployment test-recreate-deployment
Dec 17 00:11:26.577: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 17 00:11:26.705: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9762  7acf8b37-c0f1-4333-89b1-4e4d57194142 49810 2 2020-12-17 00:11:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-17 00:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-17 00:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a086e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-12-17 00:11:26 +0000 UTC,LastTransitionTime:2020-12-17 00:11:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-12-17 00:11:26 +0000 UTC,LastTransitionTime:2020-12-17 00:11:24 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 17 00:11:26.709: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-9762  6bd17046-50a8-46bf-92cb-1504458f46a4 49807 1 2020-12-17 00:11:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 7acf8b37-c0f1-4333-89b1-4e4d57194142 0xc005a08b50 0xc005a08b51}] []  [{kube-controller-manager Update apps/v1 2020-12-17 00:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7acf8b37-c0f1-4333-89b1-4e4d57194142\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a08bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 00:11:26.709: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 17 00:11:26.709: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-9762  3d5c44a8-2675-48bf-a16e-f27c0e7a9da7 49798 2 2020-12-17 00:11:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 7acf8b37-c0f1-4333-89b1-4e4d57194142 0xc005a08a57 0xc005a08a58}] []  [{kube-controller-manager Update apps/v1 2020-12-17 00:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7acf8b37-c0f1-4333-89b1-4e4d57194142\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a08ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 00:11:26.713: INFO: Pod "test-recreate-deployment-f79dd4667-r8g8p" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-r8g8p test-recreate-deployment-f79dd4667- deployment-9762  469ac6cd-c2fe-4859-a6a7-2cce014a9107 49809 0 2020-12-17 00:11:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 6bd17046-50a8-46bf-92cb-1504458f46a4 0xc005a08fe0 0xc005a08fe1}] []  [{kube-controller-manager Update v1 2020-12-17 00:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6bd17046-50a8-46bf-92cb-1504458f46a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-17 00:11:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-phwbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-phwbn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-phwbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:11:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:11:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:11:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:11:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:,StartTime:2020-12-17 00:11:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:11:26.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9762" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":290,"skipped":5094,"failed":0}

------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:11:26.725: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 17 00:11:27.821: INFO: starting watch
STEP: patching
STEP: updating
Dec 17 00:11:27.834: INFO: waiting for watch events with expected annotations
Dec 17 00:11:27.834: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:11:27.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4565" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":291,"skipped":5094,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:11:27.933: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-9949bdea-3dba-4959-9668-ca291892cb4b
STEP: Creating a pod to test consume configMaps
Dec 17 00:11:27.995: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-440c36ca-42a5-4be0-bcfd-db55776b6166" in namespace "projected-349" to be "Succeeded or Failed"
Dec 17 00:11:27.998: INFO: Pod "pod-projected-configmaps-440c36ca-42a5-4be0-bcfd-db55776b6166": Phase="Pending", Reason="", readiness=false. Elapsed: 2.411744ms
Dec 17 00:11:30.014: INFO: Pod "pod-projected-configmaps-440c36ca-42a5-4be0-bcfd-db55776b6166": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018511114s
STEP: Saw pod success
Dec 17 00:11:30.014: INFO: Pod "pod-projected-configmaps-440c36ca-42a5-4be0-bcfd-db55776b6166" satisfied condition "Succeeded or Failed"
Dec 17 00:11:30.018: INFO: Trying to get logs from node ip-172-31-1-217 pod pod-projected-configmaps-440c36ca-42a5-4be0-bcfd-db55776b6166 container agnhost-container: <nil>
STEP: delete the pod
Dec 17 00:11:30.048: INFO: Waiting for pod pod-projected-configmaps-440c36ca-42a5-4be0-bcfd-db55776b6166 to disappear
Dec 17 00:11:30.051: INFO: Pod pod-projected-configmaps-440c36ca-42a5-4be0-bcfd-db55776b6166 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:11:30.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-349" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":5101,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:11:30.069: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-436
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-436
Dec 17 00:11:30.149: INFO: Found 0 stateful pods, waiting for 1
Dec 17 00:11:40.170: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 17 00:11:40.203: INFO: Deleting all statefulset in ns statefulset-436
Dec 17 00:11:40.209: INFO: Scaling statefulset ss to 0
Dec 17 00:12:00.260: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 00:12:00.266: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:12:00.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-436" for this suite.

• [SLOW TEST:30.238 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":293,"skipped":5107,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:12:00.309: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 17 00:12:00.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-888cfe0c-eb7f-4bc4-89f0-c4e206cf7b38" in namespace "downward-api-9035" to be "Succeeded or Failed"
Dec 17 00:12:00.392: INFO: Pod "downwardapi-volume-888cfe0c-eb7f-4bc4-89f0-c4e206cf7b38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.502775ms
Dec 17 00:12:02.412: INFO: Pod "downwardapi-volume-888cfe0c-eb7f-4bc4-89f0-c4e206cf7b38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023328341s
Dec 17 00:12:04.419: INFO: Pod "downwardapi-volume-888cfe0c-eb7f-4bc4-89f0-c4e206cf7b38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030870693s
STEP: Saw pod success
Dec 17 00:12:04.420: INFO: Pod "downwardapi-volume-888cfe0c-eb7f-4bc4-89f0-c4e206cf7b38" satisfied condition "Succeeded or Failed"
Dec 17 00:12:04.423: INFO: Trying to get logs from node ip-172-31-1-217 pod downwardapi-volume-888cfe0c-eb7f-4bc4-89f0-c4e206cf7b38 container client-container: <nil>
STEP: delete the pod
Dec 17 00:12:04.448: INFO: Waiting for pod downwardapi-volume-888cfe0c-eb7f-4bc4-89f0-c4e206cf7b38 to disappear
Dec 17 00:12:04.451: INFO: Pod downwardapi-volume-888cfe0c-eb7f-4bc4-89f0-c4e206cf7b38 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:12:04.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9035" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":294,"skipped":5120,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:12:04.464: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Dec 17 00:12:04.515: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Dec 17 00:12:04.534: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:12:04.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-664" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":295,"skipped":5130,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:12:04.572: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 17 00:12:04.630: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:04.630: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:04.630: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:04.635: INFO: Number of nodes with available pods: 0
Dec 17 00:12:04.635: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 17 00:12:05.641: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:05.641: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:05.641: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:05.644: INFO: Number of nodes with available pods: 0
Dec 17 00:12:05.644: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 17 00:12:06.660: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:06.660: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:06.660: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:06.666: INFO: Number of nodes with available pods: 2
Dec 17 00:12:06.666: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 17 00:12:06.693: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:06.693: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:06.693: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:06.696: INFO: Number of nodes with available pods: 1
Dec 17 00:12:06.696: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 17 00:12:07.703: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:07.703: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:07.703: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:07.706: INFO: Number of nodes with available pods: 1
Dec 17 00:12:07.706: INFO: Node ip-172-31-1-217 is running more than one daemon pod
Dec 17 00:12:08.702: INFO: DaemonSet pods can't tolerate node ip-172-31-1-201 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:08.702: INFO: DaemonSet pods can't tolerate node ip-172-31-2-238 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:08.702: INFO: DaemonSet pods can't tolerate node ip-172-31-4-4 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 00:12:08.706: INFO: Number of nodes with available pods: 2
Dec 17 00:12:08.706: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8478, will wait for the garbage collector to delete the pods
Dec 17 00:12:08.773: INFO: Deleting DaemonSet.extensions daemon-set took: 8.27176ms
Dec 17 00:12:09.373: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.227931ms
Dec 17 00:12:17.915: INFO: Number of nodes with available pods: 0
Dec 17 00:12:17.915: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 00:12:17.927: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50237"},"items":null}

Dec 17 00:12:17.941: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50237"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:12:17.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8478" for this suite.

• [SLOW TEST:13.462 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":296,"skipped":5130,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:12:18.036: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 17 00:12:18.097: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 17 00:12:18.108: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 17 00:12:23.127: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 17 00:12:23.127: INFO: Creating deployment "test-rolling-update-deployment"
Dec 17 00:12:23.143: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 17 00:12:23.165: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 17 00:12:25.173: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 17 00:12:25.175: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 17 00:12:25.182: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-671  3387146e-7b26-4010-a349-7f48fc38eeab 50329 1 2020-12-17 00:12:23 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-12-17 00:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-17 00:12:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a093f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-17 00:12:23 +0000 UTC,LastTransitionTime:2020-12-17 00:12:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2020-12-17 00:12:24 +0000 UTC,LastTransitionTime:2020-12-17 00:12:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 17 00:12:25.185: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-671  cbad4607-007b-44e1-a6c6-e6f82d52f917 50318 1 2020-12-17 00:12:23 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3387146e-7b26-4010-a349-7f48fc38eeab 0xc005a098a7 0xc005a098a8}] []  [{kube-controller-manager Update apps/v1 2020-12-17 00:12:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3387146e-7b26-4010-a349-7f48fc38eeab\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a09938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 17 00:12:25.185: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 17 00:12:25.185: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-671  ddecb955-15c0-433a-b6fe-296c435893d5 50328 2 2020-12-17 00:12:18 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3387146e-7b26-4010-a349-7f48fc38eeab 0xc005a09797 0xc005a09798}] []  [{e2e.test Update apps/v1 2020-12-17 00:12:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-17 00:12:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3387146e-7b26-4010-a349-7f48fc38eeab\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005a09838 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 00:12:25.188: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-8drmh" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-8drmh test-rolling-update-deployment-6b6bf9df46- deployment-671  aec0f568-51d1-4cd6-a90b-efb98a106bf1 50317 0 2020-12-17 00:12:23 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:192.168.140.48/32 cni.projectcalico.org/podIPs:192.168.140.48/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 cbad4607-007b-44e1-a6c6-e6f82d52f917 0xc005a09d57 0xc005a09d58}] []  [{calico Update v1 2020-12-17 00:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-12-17 00:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbad4607-007b-44e1-a6c6-e6f82d52f917\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-17 00:12:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.140.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4wwkn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4wwkn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4wwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:12:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:12:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:192.168.140.48,StartTime:2020-12-17 00:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 00:12:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://7e2ed2de146f33736fbd169f6bf383eda8bf5edddd585bac098334235b6adedf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.140.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:12:25.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-671" for this suite.

• [SLOW TEST:7.162 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":297,"skipped":5144,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:12:25.199: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Dec 17 00:12:25.240: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:12:41.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9872" for this suite.

• [SLOW TEST:16.764 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":298,"skipped":5164,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:12:41.963: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Dec 17 00:12:42.013: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 17 00:12:42.020: INFO: Waiting for terminating namespaces to be deleted...
Dec 17 00:12:42.022: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-1-217 before test
Dec 17 00:12:42.028: INFO: calico-node-ldh6v from kube-system started at 2020-12-16 21:14:55 +0000 UTC (1 container statuses recorded)
Dec 17 00:12:42.028: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 00:12:42.028: INFO: kube-proxy-v4mt9 from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 17 00:12:42.028: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 00:12:42.029: INFO: nginx-proxy-ip-172-31-1-217 from kube-system started at 2020-12-16 21:14:55 +0000 UTC (1 container statuses recorded)
Dec 17 00:12:42.029: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 00:12:42.029: INFO: sonobuoy from sonobuoy started at 2020-12-16 22:33:28 +0000 UTC (1 container statuses recorded)
Dec 17 00:12:42.029: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 17 00:12:42.029: INFO: sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-jkfz6 from sonobuoy started at 2020-12-16 22:33:30 +0000 UTC (2 container statuses recorded)
Dec 17 00:12:42.029: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Dec 17 00:12:42.029: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 00:12:42.029: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-174 before test
Dec 17 00:12:42.037: INFO: calico-node-84s5l from kube-system started at 2020-12-16 21:14:54 +0000 UTC (1 container statuses recorded)
Dec 17 00:12:42.037: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 00:12:42.037: INFO: coredns-74ff55c5b-96bcf from kube-system started at 2020-12-16 21:15:19 +0000 UTC (1 container statuses recorded)
Dec 17 00:12:42.037: INFO: 	Container coredns ready: true, restart count 0
Dec 17 00:12:42.037: INFO: coredns-74ff55c5b-t4nvx from kube-system started at 2020-12-16 22:37:05 +0000 UTC (1 container statuses recorded)
Dec 17 00:12:42.037: INFO: 	Container coredns ready: true, restart count 0
Dec 17 00:12:42.037: INFO: kube-proxy-zpdbw from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 17 00:12:42.037: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 00:12:42.037: INFO: nginx-proxy-ip-172-31-6-174 from kube-system started at 2020-12-16 21:14:40 +0000 UTC (1 container statuses recorded)
Dec 17 00:12:42.037: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 00:12:42.037: INFO: sonobuoy-e2e-job-1783c268d2664b69 from sonobuoy started at 2020-12-16 22:33:29 +0000 UTC (2 container statuses recorded)
Dec 17 00:12:42.037: INFO: 	Container e2e ready: true, restart count 0
Dec 17 00:12:42.037: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 00:12:42.037: INFO: sonobuoy-systemd-logs-daemon-set-231ed4a1bf184648-g7kjv from sonobuoy started at 2020-12-16 22:33:30 +0000 UTC (2 container statuses recorded)
Dec 17 00:12:42.037: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Dec 17 00:12:42.037: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1651589a63339746], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1651589a648efd14], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:12:43.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8517" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":299,"skipped":5182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:12:43.075: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Dec 17 00:12:45.689: INFO: Successfully updated pod "labelsupdatecc36ebd3-31b2-4583-9236-74631057c5ee"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:12:49.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-927" for this suite.

• [SLOW TEST:6.655 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":300,"skipped":5226,"failed":0}
SSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:12:49.732: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Dec 17 00:12:49.790: INFO: created test-event-1
Dec 17 00:12:49.802: INFO: created test-event-2
Dec 17 00:12:49.807: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Dec 17 00:12:49.813: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Dec 17 00:12:49.842: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:12:49.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9746" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":301,"skipped":5229,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:12:49.860: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:12:49.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3471" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":302,"skipped":5239,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:12:49.957: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1217 00:13:30.123128      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 17 00:14:32.146: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Dec 17 00:14:32.146: INFO: Deleting pod "simpletest.rc-5nh5c" in namespace "gc-4349"
Dec 17 00:14:32.171: INFO: Deleting pod "simpletest.rc-78g52" in namespace "gc-4349"
Dec 17 00:14:32.187: INFO: Deleting pod "simpletest.rc-df5lz" in namespace "gc-4349"
Dec 17 00:14:32.215: INFO: Deleting pod "simpletest.rc-fwcxm" in namespace "gc-4349"
Dec 17 00:14:32.239: INFO: Deleting pod "simpletest.rc-jr6tl" in namespace "gc-4349"
Dec 17 00:14:32.261: INFO: Deleting pod "simpletest.rc-kf7mq" in namespace "gc-4349"
Dec 17 00:14:32.297: INFO: Deleting pod "simpletest.rc-m574m" in namespace "gc-4349"
Dec 17 00:14:32.317: INFO: Deleting pod "simpletest.rc-pw8w7" in namespace "gc-4349"
Dec 17 00:14:32.341: INFO: Deleting pod "simpletest.rc-qmz9r" in namespace "gc-4349"
Dec 17 00:14:32.384: INFO: Deleting pod "simpletest.rc-smrtp" in namespace "gc-4349"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:14:32.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4349" for this suite.

• [SLOW TEST:102.473 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":303,"skipped":5242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:14:32.433: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 17 00:14:35.550: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:14:35.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2165" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":304,"skipped":5283,"failed":0}

------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:14:35.627: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 17 00:14:35.918: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 17 00:14:40.928: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 17 00:14:40.929: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 17 00:14:42.936: INFO: Creating deployment "test-rollover-deployment"
Dec 17 00:14:42.952: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 17 00:14:44.998: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 17 00:14:45.061: INFO: Ensure that both replica sets have 1 created replica
Dec 17 00:14:45.090: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 17 00:14:45.119: INFO: Updating deployment test-rollover-deployment
Dec 17 00:14:45.119: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 17 00:14:47.145: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 17 00:14:47.150: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 17 00:14:47.157: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 00:14:47.157: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760886, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 00:14:49.168: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 00:14:49.168: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760886, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 00:14:51.169: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 00:14:51.169: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760886, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 00:14:53.172: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 00:14:53.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760886, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 00:14:55.178: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 00:14:55.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760886, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743760882, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 00:14:57.167: INFO: 
Dec 17 00:14:57.167: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 17 00:14:57.174: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1341  d260a930-693c-4490-81bf-27b8d3697a97 51186 2 2020-12-17 00:14:42 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-17 00:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-17 00:14:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003568328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-17 00:14:42 +0000 UTC,LastTransitionTime:2020-12-17 00:14:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2020-12-17 00:14:57 +0000 UTC,LastTransitionTime:2020-12-17 00:14:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 17 00:14:57.176: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-1341  06f2eef1-0cd1-40e7-872f-0047d3214e65 51176 2 2020-12-17 00:14:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d260a930-693c-4490-81bf-27b8d3697a97 0xc004d977b7 0xc004d977b8}] []  [{kube-controller-manager Update apps/v1 2020-12-17 00:14:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d260a930-693c-4490-81bf-27b8d3697a97\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d97848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 17 00:14:57.176: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 17 00:14:57.177: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1341  859a4561-9be9-4645-b297-d18d085e2a78 51184 2 2020-12-17 00:14:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d260a930-693c-4490-81bf-27b8d3697a97 0xc004d976a7 0xc004d976a8}] []  [{e2e.test Update apps/v1 2020-12-17 00:14:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-17 00:14:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d260a930-693c-4490-81bf-27b8d3697a97\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004d97748 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 00:14:57.177: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-1341  8ff7fa78-e876-4eda-9d21-70f9b0b59a13 51136 2 2020-12-17 00:14:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d260a930-693c-4490-81bf-27b8d3697a97 0xc004d978b7 0xc004d978b8}] []  [{kube-controller-manager Update apps/v1 2020-12-17 00:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d260a930-693c-4490-81bf-27b8d3697a97\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d97948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 00:14:57.180: INFO: Pod "test-rollover-deployment-668db69979-lvgx9" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-lvgx9 test-rollover-deployment-668db69979- deployment-1341  f3d916ed-5b85-4b54-a6cf-140fbc6ec533 51155 0 2020-12-17 00:14:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:192.168.140.49/32 cni.projectcalico.org/podIPs:192.168.140.49/32] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 06f2eef1-0cd1-40e7-872f-0047d3214e65 0xc0035690a7 0xc0035690a8}] []  [{calico Update v1 2020-12-17 00:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-12-17 00:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06f2eef1-0cd1-40e7-872f-0047d3214e65\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-17 00:14:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.140.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lb94r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lb94r,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lb94r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:14:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:14:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:14:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 00:14:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.217,PodIP:192.168.140.49,StartTime:2020-12-17 00:14:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 00:14:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://b3fd8f42dab35af515101289978f017fac92fdc11750651387ba682592a7d997,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.140.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:14:57.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1341" for this suite.

• [SLOW TEST:21.561 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":305,"skipped":5283,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:14:57.190: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-39da0034-cae6-4e9b-9cba-c959994b5921 in namespace container-probe-6716
Dec 17 00:14:59.247: INFO: Started pod test-webserver-39da0034-cae6-4e9b-9cba-c959994b5921 in namespace container-probe-6716
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 00:14:59.250: INFO: Initial restart count of pod test-webserver-39da0034-cae6-4e9b-9cba-c959994b5921 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:19:00.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6716" for this suite.

• [SLOW TEST:243.249 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5284,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:19:00.439: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 17 00:19:04.641: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 00:19:04.645: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 00:19:06.645: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 00:19:06.655: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 00:19:08.645: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 00:19:08.651: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 00:19:10.646: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 00:19:10.657: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 00:19:12.645: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 00:19:12.653: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 00:19:14.645: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 00:19:14.665: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 00:19:16.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 00:19:16.655: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 00:19:18.645: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 00:19:18.653: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:19:18.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1652" for this suite.

• [SLOW TEST:18.224 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":307,"skipped":5289,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:19:18.665: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Dec 17 00:19:18.742: INFO: created test-pod-1
Dec 17 00:19:18.748: INFO: created test-pod-2
Dec 17 00:19:18.756: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:19:18.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5279" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":308,"skipped":5294,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:19:18.870: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 17 00:19:18.923: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:19:19.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5399" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":309,"skipped":5306,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:19:19.982: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:19:48.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2602" for this suite.

• [SLOW TEST:28.163 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":310,"skipped":5317,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 17 00:19:48.147: INFO: >>> kubeConfig: /tmp/kubeconfig-864705134
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 17 00:19:48.214: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 17 00:20:48.261: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Dec 17 00:20:48.299: INFO: Created pod: pod0-sched-preemption-low-priority
Dec 17 00:20:48.327: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 17 00:21:10.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9776" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:82.384 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":311,"skipped":5322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSDec 17 00:21:10.531: INFO: Running AfterSuite actions on all nodes
Dec 17 00:21:10.531: INFO: Running AfterSuite actions on node 1
Dec 17 00:21:10.531: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 6452.363 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h47m33.919579109s
Test Suite Passed
