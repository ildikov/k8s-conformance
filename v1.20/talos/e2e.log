I0324 14:20:28.071079      22 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-088565179
I0324 14:20:28.071101      22 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0324 14:20:28.071299      22 e2e.go:129] Starting e2e run "4da38307-b270-4025-b811-f34998ca1f3a" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1616595626 - Will randomize all specs
Will run 311 of 5667 specs

Mar 24 14:20:28.081: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:20:28.084: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0324 14:20:28.084395      22 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar 24 14:20:28.122: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 24 14:20:28.164: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 24 14:20:28.164: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar 24 14:20:28.164: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 24 14:20:28.175: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel' (0 seconds elapsed)
Mar 24 14:20:28.175: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar 24 14:20:28.175: INFO: e2e test version: v1.20.5
Mar 24 14:20:28.177: INFO: kube-apiserver version: v1.20.5
Mar 24 14:20:28.177: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:20:28.184: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:20:28.184: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pods
Mar 24 14:20:28.280: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Mar 24 14:20:28.291: INFO: PSP annotation exists on dry run pod: "privileged"; assuming PodSecurityPolicy is enabled
Mar 24 14:20:28.306: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:20:28.431: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:20:32.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1123" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":1,"skipped":25,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:20:32.569: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6937.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6937.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6937.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6937.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6937.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6937.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 14:20:50.864: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local from pod dns-6937/dns-test-99466b0a-74e9-4d16-816e-0a38cb3dcb6c: the server could not find the requested resource (get pods dns-test-99466b0a-74e9-4d16-816e-0a38cb3dcb6c)
Mar 24 14:20:50.868: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local from pod dns-6937/dns-test-99466b0a-74e9-4d16-816e-0a38cb3dcb6c: the server could not find the requested resource (get pods dns-test-99466b0a-74e9-4d16-816e-0a38cb3dcb6c)
Mar 24 14:20:50.876: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6937.svc.cluster.local from pod dns-6937/dns-test-99466b0a-74e9-4d16-816e-0a38cb3dcb6c: the server could not find the requested resource (get pods dns-test-99466b0a-74e9-4d16-816e-0a38cb3dcb6c)
Mar 24 14:20:50.882: INFO: Lookups using dns-6937/dns-test-99466b0a-74e9-4d16-816e-0a38cb3dcb6c failed for: [jessie_udp@dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6937.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6937.svc.cluster.local]

Mar 24 14:20:55.939: INFO: DNS probes using dns-6937/dns-test-99466b0a-74e9-4d16-816e-0a38cb3dcb6c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:20:56.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6937" for this suite.

• [SLOW TEST:23.547 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":2,"skipped":49,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:20:56.116: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3890
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3890
I0324 14:20:56.429669      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3890, replica count: 2
I0324 14:20:59.479958      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0324 14:21:02.480038      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 14:21:05.480: INFO: Creating new exec pod
I0324 14:21:05.480282      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 14:21:08.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3890 exec execpodk6pzz -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 24 14:21:09.360: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 24 14:21:09.360: INFO: stdout: ""
Mar 24 14:21:09.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3890 exec execpodk6pzz -- /bin/sh -x -c nc -zv -t -w 2 10.107.134.159 80'
Mar 24 14:21:09.522: INFO: stderr: "+ nc -zv -t -w 2 10.107.134.159 80\nConnection to 10.107.134.159 80 port [tcp/http] succeeded!\n"
Mar 24 14:21:09.522: INFO: stdout: ""
Mar 24 14:21:09.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3890 exec execpodk6pzz -- /bin/sh -x -c nc -zv -t -w 2 10.5.0.5 30851'
Mar 24 14:21:09.695: INFO: stderr: "+ nc -zv -t -w 2 10.5.0.5 30851\nConnection to 10.5.0.5 30851 port [tcp/30851] succeeded!\n"
Mar 24 14:21:09.695: INFO: stdout: ""
Mar 24 14:21:09.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3890 exec execpodk6pzz -- /bin/sh -x -c nc -zv -t -w 2 10.5.0.6 30851'
Mar 24 14:21:09.847: INFO: stderr: "+ nc -zv -t -w 2 10.5.0.6 30851\nConnection to 10.5.0.6 30851 port [tcp/30851] succeeded!\n"
Mar 24 14:21:09.847: INFO: stdout: ""
Mar 24 14:21:09.847: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:21:09.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3890" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:13.832 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":3,"skipped":68,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:21:09.949: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:21:10.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8555" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":4,"skipped":76,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:21:10.168: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 14:21:13.501: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:21:13.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6404" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":5,"skipped":98,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:21:13.555: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 14:21:15.778: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:21:15.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9687" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":117,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:21:16.000: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:21:16.170: INFO: Creating ReplicaSet my-hostname-basic-39a46fed-2c36-48b6-aa95-bd0e1a373a7a
Mar 24 14:21:16.203: INFO: Pod name my-hostname-basic-39a46fed-2c36-48b6-aa95-bd0e1a373a7a: Found 1 pods out of 1
Mar 24 14:21:16.203: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-39a46fed-2c36-48b6-aa95-bd0e1a373a7a" is running
Mar 24 14:21:18.219: INFO: Pod "my-hostname-basic-39a46fed-2c36-48b6-aa95-bd0e1a373a7a-hvb6k" is running (conditions: [])
Mar 24 14:21:18.220: INFO: Trying to dial the pod
Mar 24 14:21:23.239: INFO: Controller my-hostname-basic-39a46fed-2c36-48b6-aa95-bd0e1a373a7a: Got expected result from replica 1 [my-hostname-basic-39a46fed-2c36-48b6-aa95-bd0e1a373a7a-hvb6k]: "my-hostname-basic-39a46fed-2c36-48b6-aa95-bd0e1a373a7a-hvb6k", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:21:23.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5559" for this suite.

• [SLOW TEST:7.267 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":7,"skipped":131,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:21:23.270: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-c2317c32-700b-4b87-8c83-dc049117e6ac in namespace container-probe-7
Mar 24 14:21:25.486: INFO: Started pod liveness-c2317c32-700b-4b87-8c83-dc049117e6ac in namespace container-probe-7
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 14:21:25.489: INFO: Initial restart count of pod liveness-c2317c32-700b-4b87-8c83-dc049117e6ac is 0
Mar 24 14:21:41.589: INFO: Restart count of pod container-probe-7/liveness-c2317c32-700b-4b87-8c83-dc049117e6ac is now 1 (16.09913779s elapsed)
Mar 24 14:21:59.696: INFO: Restart count of pod container-probe-7/liveness-c2317c32-700b-4b87-8c83-dc049117e6ac is now 2 (34.206735102s elapsed)
Mar 24 14:22:19.808: INFO: Restart count of pod container-probe-7/liveness-c2317c32-700b-4b87-8c83-dc049117e6ac is now 3 (54.318311326s elapsed)
Mar 24 14:22:39.924: INFO: Restart count of pod container-probe-7/liveness-c2317c32-700b-4b87-8c83-dc049117e6ac is now 4 (1m14.43470478s elapsed)
Mar 24 14:23:50.365: INFO: Restart count of pod container-probe-7/liveness-c2317c32-700b-4b87-8c83-dc049117e6ac is now 5 (2m24.875196071s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:23:50.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7" for this suite.

• [SLOW TEST:147.157 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":146,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:23:50.428: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Mar 24 14:23:50.642: INFO: Waiting up to 5m0s for pod "test-pod-8242025e-07d4-40f8-be27-15aab9dc7ffa" in namespace "svcaccounts-9324" to be "Succeeded or Failed"
Mar 24 14:23:50.648: INFO: Pod "test-pod-8242025e-07d4-40f8-be27-15aab9dc7ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.661561ms
Mar 24 14:23:52.660: INFO: Pod "test-pod-8242025e-07d4-40f8-be27-15aab9dc7ffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018062745s
STEP: Saw pod success
Mar 24 14:23:52.660: INFO: Pod "test-pod-8242025e-07d4-40f8-be27-15aab9dc7ffa" satisfied condition "Succeeded or Failed"
Mar 24 14:23:52.663: INFO: Trying to get logs from node talos-default-worker-1 pod test-pod-8242025e-07d4-40f8-be27-15aab9dc7ffa container agnhost-container: <nil>
STEP: delete the pod
Mar 24 14:23:52.704: INFO: Waiting for pod test-pod-8242025e-07d4-40f8-be27-15aab9dc7ffa to disappear
Mar 24 14:23:52.708: INFO: Pod test-pod-8242025e-07d4-40f8-be27-15aab9dc7ffa no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:23:52.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9324" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":9,"skipped":149,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:23:52.722: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 14:23:52.949: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a5df845-c057-402f-8d94-4852f12338ea" in namespace "downward-api-1823" to be "Succeeded or Failed"
Mar 24 14:23:52.952: INFO: Pod "downwardapi-volume-2a5df845-c057-402f-8d94-4852f12338ea": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426161ms
Mar 24 14:23:54.964: INFO: Pod "downwardapi-volume-2a5df845-c057-402f-8d94-4852f12338ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015554225s
STEP: Saw pod success
Mar 24 14:23:54.965: INFO: Pod "downwardapi-volume-2a5df845-c057-402f-8d94-4852f12338ea" satisfied condition "Succeeded or Failed"
Mar 24 14:23:54.968: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-2a5df845-c057-402f-8d94-4852f12338ea container client-container: <nil>
STEP: delete the pod
Mar 24 14:23:55.013: INFO: Waiting for pod downwardapi-volume-2a5df845-c057-402f-8d94-4852f12338ea to disappear
Mar 24 14:23:55.017: INFO: Pod downwardapi-volume-2a5df845-c057-402f-8d94-4852f12338ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:23:55.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1823" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":10,"skipped":154,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:23:55.044: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:24:55.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5429" for this suite.

• [SLOW TEST:60.311 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":11,"skipped":165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:24:55.356: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3060
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:25:06.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3060" for this suite.

• [SLOW TEST:11.383 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":12,"skipped":211,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:25:06.740: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 24 14:25:06.970: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 14:26:07.000: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:26:07.003: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-5958
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Mar 24 14:26:09.394: INFO: found a healthy node: talos-default-worker-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:26:21.592: INFO: pods created so far: [1 1 1]
Mar 24 14:26:21.592: INFO: length of pods created so far: 3
Mar 24 14:26:31.632: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:26:38.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5958" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:26:38.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5125" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:92.069 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":13,"skipped":211,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:26:38.811: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-f757795d-8ee8-4d0e-a9fc-9c3a52db1247
STEP: Creating a pod to test consume secrets
Mar 24 14:26:39.048: INFO: Waiting up to 5m0s for pod "pod-secrets-6079a775-2cd0-46f4-9551-46233c05d500" in namespace "secrets-4091" to be "Succeeded or Failed"
Mar 24 14:26:39.061: INFO: Pod "pod-secrets-6079a775-2cd0-46f4-9551-46233c05d500": Phase="Pending", Reason="", readiness=false. Elapsed: 13.348196ms
Mar 24 14:26:41.074: INFO: Pod "pod-secrets-6079a775-2cd0-46f4-9551-46233c05d500": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026009613s
STEP: Saw pod success
Mar 24 14:26:41.074: INFO: Pod "pod-secrets-6079a775-2cd0-46f4-9551-46233c05d500" satisfied condition "Succeeded or Failed"
Mar 24 14:26:41.077: INFO: Trying to get logs from node talos-default-worker-2 pod pod-secrets-6079a775-2cd0-46f4-9551-46233c05d500 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 14:26:41.128: INFO: Waiting for pod pod-secrets-6079a775-2cd0-46f4-9551-46233c05d500 to disappear
Mar 24 14:26:41.133: INFO: Pod pod-secrets-6079a775-2cd0-46f4-9551-46233c05d500 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:26:41.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4091" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:26:41.160: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar 24 14:26:41.384: INFO: Waiting up to 5m0s for pod "downward-api-a65898d3-7d05-46cd-baf9-cc5f5a344b4f" in namespace "downward-api-3782" to be "Succeeded or Failed"
Mar 24 14:26:41.390: INFO: Pod "downward-api-a65898d3-7d05-46cd-baf9-cc5f5a344b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.543194ms
Mar 24 14:26:43.403: INFO: Pod "downward-api-a65898d3-7d05-46cd-baf9-cc5f5a344b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01904565s
STEP: Saw pod success
Mar 24 14:26:43.404: INFO: Pod "downward-api-a65898d3-7d05-46cd-baf9-cc5f5a344b4f" satisfied condition "Succeeded or Failed"
Mar 24 14:26:43.408: INFO: Trying to get logs from node talos-default-worker-2 pod downward-api-a65898d3-7d05-46cd-baf9-cc5f5a344b4f container dapi-container: <nil>
STEP: delete the pod
Mar 24 14:26:43.456: INFO: Waiting for pod downward-api-a65898d3-7d05-46cd-baf9-cc5f5a344b4f to disappear
Mar 24 14:26:43.469: INFO: Pod downward-api-a65898d3-7d05-46cd-baf9-cc5f5a344b4f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:26:43.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3782" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":15,"skipped":249,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:26:43.485: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-980
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Mar 24 14:26:43.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-980 cluster-info'
Mar 24 14:26:43.742: INFO: stderr: ""
Mar 24 14:26:43.742: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:26:43.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-980" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":16,"skipped":263,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:26:43.760: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6833
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Mar 24 14:26:43.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6833 create -f -'
Mar 24 14:26:44.409: INFO: stderr: ""
Mar 24 14:26:44.409: INFO: stdout: "pod/pause created\n"
Mar 24 14:26:44.409: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 24 14:26:44.410: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6833" to be "running and ready"
Mar 24 14:26:44.429: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 19.793898ms
Mar 24 14:26:46.446: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.036128397s
Mar 24 14:26:46.446: INFO: Pod "pause" satisfied condition "running and ready"
Mar 24 14:26:46.446: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 24 14:26:46.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6833 label pods pause testing-label=testing-label-value'
Mar 24 14:26:46.552: INFO: stderr: ""
Mar 24 14:26:46.552: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 24 14:26:46.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6833 get pod pause -L testing-label'
Mar 24 14:26:46.629: INFO: stderr: ""
Mar 24 14:26:46.629: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 24 14:26:46.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6833 label pods pause testing-label-'
Mar 24 14:26:46.742: INFO: stderr: ""
Mar 24 14:26:46.742: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 24 14:26:46.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6833 get pod pause -L testing-label'
Mar 24 14:26:46.814: INFO: stderr: ""
Mar 24 14:26:46.814: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Mar 24 14:26:46.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6833 delete --grace-period=0 --force -f -'
Mar 24 14:26:46.952: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 14:26:46.952: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 24 14:26:46.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6833 get rc,svc -l name=pause --no-headers'
Mar 24 14:26:47.029: INFO: stderr: "No resources found in kubectl-6833 namespace.\n"
Mar 24 14:26:47.029: INFO: stdout: ""
Mar 24 14:26:47.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6833 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 14:26:47.109: INFO: stderr: ""
Mar 24 14:26:47.109: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:26:47.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6833" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":17,"skipped":287,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:26:47.142: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8611.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8611.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8611.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8611.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8611.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8611.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 14:26:49.459: INFO: DNS probes using dns-8611/dns-test-5f8b05d2-4c8f-41c6-8c17-0e43ffeeb181 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:26:49.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8611" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":18,"skipped":290,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:26:49.612: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-5207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:26:50.030: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 24 14:26:50.032: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 24 14:26:50.032: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.032: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 24 14:26:50.032: INFO: Checking APIGroup: apps
Mar 24 14:26:50.033: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 24 14:26:50.034: INFO: Versions found [{apps/v1 v1}]
Mar 24 14:26:50.034: INFO: apps/v1 matches apps/v1
Mar 24 14:26:50.034: INFO: Checking APIGroup: events.k8s.io
Mar 24 14:26:50.036: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 24 14:26:50.036: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.036: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 24 14:26:50.036: INFO: Checking APIGroup: authentication.k8s.io
Mar 24 14:26:50.038: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 24 14:26:50.038: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.038: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 24 14:26:50.038: INFO: Checking APIGroup: authorization.k8s.io
Mar 24 14:26:50.039: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 24 14:26:50.040: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.040: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 24 14:26:50.040: INFO: Checking APIGroup: autoscaling
Mar 24 14:26:50.041: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Mar 24 14:26:50.041: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar 24 14:26:50.042: INFO: autoscaling/v1 matches autoscaling/v1
Mar 24 14:26:50.042: INFO: Checking APIGroup: batch
Mar 24 14:26:50.043: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 24 14:26:50.043: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar 24 14:26:50.043: INFO: batch/v1 matches batch/v1
Mar 24 14:26:50.043: INFO: Checking APIGroup: certificates.k8s.io
Mar 24 14:26:50.044: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 24 14:26:50.044: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.044: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 24 14:26:50.045: INFO: Checking APIGroup: networking.k8s.io
Mar 24 14:26:50.046: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 24 14:26:50.046: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.046: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 24 14:26:50.046: INFO: Checking APIGroup: extensions
Mar 24 14:26:50.047: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Mar 24 14:26:50.047: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Mar 24 14:26:50.047: INFO: extensions/v1beta1 matches extensions/v1beta1
Mar 24 14:26:50.047: INFO: Checking APIGroup: policy
Mar 24 14:26:50.048: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Mar 24 14:26:50.048: INFO: Versions found [{policy/v1beta1 v1beta1}]
Mar 24 14:26:50.048: INFO: policy/v1beta1 matches policy/v1beta1
Mar 24 14:26:50.048: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 24 14:26:50.050: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 24 14:26:50.050: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.050: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 24 14:26:50.050: INFO: Checking APIGroup: storage.k8s.io
Mar 24 14:26:50.052: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 24 14:26:50.052: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.052: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 24 14:26:50.052: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 24 14:26:50.053: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 24 14:26:50.053: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.053: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 24 14:26:50.053: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 24 14:26:50.055: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 24 14:26:50.055: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.055: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 24 14:26:50.055: INFO: Checking APIGroup: scheduling.k8s.io
Mar 24 14:26:50.056: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 24 14:26:50.056: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.056: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 24 14:26:50.056: INFO: Checking APIGroup: coordination.k8s.io
Mar 24 14:26:50.057: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 24 14:26:50.057: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.057: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 24 14:26:50.057: INFO: Checking APIGroup: node.k8s.io
Mar 24 14:26:50.058: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 24 14:26:50.058: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.058: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 24 14:26:50.058: INFO: Checking APIGroup: discovery.k8s.io
Mar 24 14:26:50.060: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Mar 24 14:26:50.060: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.060: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Mar 24 14:26:50.060: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 24 14:26:50.062: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Mar 24 14:26:50.062: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar 24 14:26:50.062: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:26:50.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5207" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":19,"skipped":301,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:26:50.078: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2987
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 14:26:50.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ddd0dd2a-ee8c-4914-84d1-5df931a6f4f4" in namespace "projected-2987" to be "Succeeded or Failed"
Mar 24 14:26:50.374: INFO: Pod "downwardapi-volume-ddd0dd2a-ee8c-4914-84d1-5df931a6f4f4": Phase="Pending", Reason="", readiness=false. Elapsed: 31.601873ms
Mar 24 14:26:52.509: INFO: Pod "downwardapi-volume-ddd0dd2a-ee8c-4914-84d1-5df931a6f4f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.166684719s
STEP: Saw pod success
Mar 24 14:26:52.509: INFO: Pod "downwardapi-volume-ddd0dd2a-ee8c-4914-84d1-5df931a6f4f4" satisfied condition "Succeeded or Failed"
Mar 24 14:26:52.514: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-ddd0dd2a-ee8c-4914-84d1-5df931a6f4f4 container client-container: <nil>
STEP: delete the pod
Mar 24 14:26:52.569: INFO: Waiting for pod downwardapi-volume-ddd0dd2a-ee8c-4914-84d1-5df931a6f4f4 to disappear
Mar 24 14:26:52.573: INFO: Pod downwardapi-volume-ddd0dd2a-ee8c-4914-84d1-5df931a6f4f4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:26:52.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2987" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":20,"skipped":301,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:26:52.595: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 14:26:53.179: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 24 14:26:55.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752192813, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752192813, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752192813, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752192813, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 14:26:58.261: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:26:58.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3223" for this suite.
STEP: Destroying namespace "webhook-3223-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.371 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":21,"skipped":302,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:26:58.967: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-274
STEP: creating replication controller nodeport-test in namespace services-274
I0324 14:26:59.314891      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-274, replica count: 2
I0324 14:27:02.365183      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 14:27:02.365: INFO: Creating new exec pod
Mar 24 14:27:05.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-274 exec execpod5p9ms -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 24 14:27:05.595: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 24 14:27:05.595: INFO: stdout: ""
Mar 24 14:27:05.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-274 exec execpod5p9ms -- /bin/sh -x -c nc -zv -t -w 2 10.96.115.242 80'
Mar 24 14:27:05.750: INFO: stderr: "+ nc -zv -t -w 2 10.96.115.242 80\nConnection to 10.96.115.242 80 port [tcp/http] succeeded!\n"
Mar 24 14:27:05.750: INFO: stdout: ""
Mar 24 14:27:05.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-274 exec execpod5p9ms -- /bin/sh -x -c nc -zv -t -w 2 10.5.0.5 31495'
Mar 24 14:27:05.895: INFO: stderr: "+ nc -zv -t -w 2 10.5.0.5 31495\nConnection to 10.5.0.5 31495 port [tcp/31495] succeeded!\n"
Mar 24 14:27:05.895: INFO: stdout: ""
Mar 24 14:27:05.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-274 exec execpod5p9ms -- /bin/sh -x -c nc -zv -t -w 2 10.5.0.6 31495'
Mar 24 14:27:06.051: INFO: stderr: "+ nc -zv -t -w 2 10.5.0.6 31495\nConnection to 10.5.0.6 31495 port [tcp/31495] succeeded!\n"
Mar 24 14:27:06.051: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:27:06.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-274" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.108 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":22,"skipped":309,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:27:06.075: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:27:29.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6097" for this suite.

• [SLOW TEST:23.769 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":320,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:27:29.845: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar 24 14:27:30.021: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:27:35.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5952" for this suite.

• [SLOW TEST:5.441 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":24,"skipped":323,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:27:35.286: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Mar 24 14:27:35.475: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-5822 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:27:35.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5822" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":25,"skipped":324,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:27:35.566: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-5307
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5307 to expose endpoints map[]
Mar 24 14:27:35.841: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar 24 14:27:36.861: INFO: successfully validated that service endpoint-test2 in namespace services-5307 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5307
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5307 to expose endpoints map[pod1:[80]]
Mar 24 14:27:38.917: INFO: successfully validated that service endpoint-test2 in namespace services-5307 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-5307
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5307 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 24 14:27:40.984: INFO: successfully validated that service endpoint-test2 in namespace services-5307 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-5307
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5307 to expose endpoints map[pod2:[80]]
Mar 24 14:27:41.060: INFO: successfully validated that service endpoint-test2 in namespace services-5307 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-5307
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5307 to expose endpoints map[]
Mar 24 14:27:41.244: INFO: successfully validated that service endpoint-test2 in namespace services-5307 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:27:41.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5307" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.741 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":26,"skipped":337,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:27:41.307: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7500
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0324 14:28:21.625967      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 14:28:23.658: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 24 14:28:23.658: INFO: Deleting pod "simpletest.rc-2rct6" in namespace "gc-7500"
Mar 24 14:28:23.694: INFO: Deleting pod "simpletest.rc-4v2r4" in namespace "gc-7500"
Mar 24 14:28:23.840: INFO: Deleting pod "simpletest.rc-795dr" in namespace "gc-7500"
Mar 24 14:28:23.880: INFO: Deleting pod "simpletest.rc-9nsd2" in namespace "gc-7500"
Mar 24 14:28:23.953: INFO: Deleting pod "simpletest.rc-k8n9n" in namespace "gc-7500"
Mar 24 14:28:23.983: INFO: Deleting pod "simpletest.rc-q9drj" in namespace "gc-7500"
Mar 24 14:28:24.045: INFO: Deleting pod "simpletest.rc-qcddn" in namespace "gc-7500"
Mar 24 14:28:24.089: INFO: Deleting pod "simpletest.rc-t4chx" in namespace "gc-7500"
Mar 24 14:28:24.125: INFO: Deleting pod "simpletest.rc-vkp86" in namespace "gc-7500"
Mar 24 14:28:24.163: INFO: Deleting pod "simpletest.rc-wb6pn" in namespace "gc-7500"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:24.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7500" for this suite.

• [SLOW TEST:42.961 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":27,"skipped":339,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:24.269: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Mar 24 14:28:24.558: INFO: Waiting up to 5m0s for pod "var-expansion-cf14aa1a-659d-49d4-8287-b24dbd9d3067" in namespace "var-expansion-4341" to be "Succeeded or Failed"
Mar 24 14:28:24.562: INFO: Pod "var-expansion-cf14aa1a-659d-49d4-8287-b24dbd9d3067": Phase="Pending", Reason="", readiness=false. Elapsed: 3.587097ms
Mar 24 14:28:26.571: INFO: Pod "var-expansion-cf14aa1a-659d-49d4-8287-b24dbd9d3067": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013427999s
STEP: Saw pod success
Mar 24 14:28:26.571: INFO: Pod "var-expansion-cf14aa1a-659d-49d4-8287-b24dbd9d3067" satisfied condition "Succeeded or Failed"
Mar 24 14:28:26.575: INFO: Trying to get logs from node talos-default-worker-1 pod var-expansion-cf14aa1a-659d-49d4-8287-b24dbd9d3067 container dapi-container: <nil>
STEP: delete the pod
Mar 24 14:28:26.613: INFO: Waiting for pod var-expansion-cf14aa1a-659d-49d4-8287-b24dbd9d3067 to disappear
Mar 24 14:28:26.619: INFO: Pod var-expansion-cf14aa1a-659d-49d4-8287-b24dbd9d3067 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:26.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4341" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":28,"skipped":341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:26.639: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:31.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3080" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":29,"skipped":369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:31.419: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 14:28:31.628: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea9d5faf-25d1-4f97-a150-27c55b31eaa0" in namespace "projected-6493" to be "Succeeded or Failed"
Mar 24 14:28:31.643: INFO: Pod "downwardapi-volume-ea9d5faf-25d1-4f97-a150-27c55b31eaa0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.905523ms
Mar 24 14:28:33.657: INFO: Pod "downwardapi-volume-ea9d5faf-25d1-4f97-a150-27c55b31eaa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02843801s
STEP: Saw pod success
Mar 24 14:28:33.657: INFO: Pod "downwardapi-volume-ea9d5faf-25d1-4f97-a150-27c55b31eaa0" satisfied condition "Succeeded or Failed"
Mar 24 14:28:33.661: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-ea9d5faf-25d1-4f97-a150-27c55b31eaa0 container client-container: <nil>
STEP: delete the pod
Mar 24 14:28:33.709: INFO: Waiting for pod downwardapi-volume-ea9d5faf-25d1-4f97-a150-27c55b31eaa0 to disappear
Mar 24 14:28:33.720: INFO: Pod downwardapi-volume-ea9d5faf-25d1-4f97-a150-27c55b31eaa0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:33.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6493" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":415,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:33.737: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7211
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 24 14:28:33.959: INFO: Waiting up to 5m0s for pod "pod-666f64c1-00ff-43f7-9a95-234ec96093dc" in namespace "emptydir-7211" to be "Succeeded or Failed"
Mar 24 14:28:33.968: INFO: Pod "pod-666f64c1-00ff-43f7-9a95-234ec96093dc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058193ms
Mar 24 14:28:35.976: INFO: Pod "pod-666f64c1-00ff-43f7-9a95-234ec96093dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016186631s
STEP: Saw pod success
Mar 24 14:28:35.976: INFO: Pod "pod-666f64c1-00ff-43f7-9a95-234ec96093dc" satisfied condition "Succeeded or Failed"
Mar 24 14:28:35.979: INFO: Trying to get logs from node talos-default-worker-1 pod pod-666f64c1-00ff-43f7-9a95-234ec96093dc container test-container: <nil>
STEP: delete the pod
Mar 24 14:28:36.012: INFO: Waiting for pod pod-666f64c1-00ff-43f7-9a95-234ec96093dc to disappear
Mar 24 14:28:36.015: INFO: Pod pod-666f64c1-00ff-43f7-9a95-234ec96093dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:36.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7211" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":417,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:36.030: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-4dce71c0-df63-46a9-b873-68cc482fdf7a
STEP: Creating a pod to test consume configMaps
Mar 24 14:28:36.260: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c99a1d08-bc4f-4b3b-86f7-f946233dda15" in namespace "projected-3757" to be "Succeeded or Failed"
Mar 24 14:28:36.281: INFO: Pod "pod-projected-configmaps-c99a1d08-bc4f-4b3b-86f7-f946233dda15": Phase="Pending", Reason="", readiness=false. Elapsed: 21.037264ms
Mar 24 14:28:38.294: INFO: Pod "pod-projected-configmaps-c99a1d08-bc4f-4b3b-86f7-f946233dda15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034108053s
STEP: Saw pod success
Mar 24 14:28:38.294: INFO: Pod "pod-projected-configmaps-c99a1d08-bc4f-4b3b-86f7-f946233dda15" satisfied condition "Succeeded or Failed"
Mar 24 14:28:38.298: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-configmaps-c99a1d08-bc4f-4b3b-86f7-f946233dda15 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 14:28:38.329: INFO: Waiting for pod pod-projected-configmaps-c99a1d08-bc4f-4b3b-86f7-f946233dda15 to disappear
Mar 24 14:28:38.339: INFO: Pod pod-projected-configmaps-c99a1d08-bc4f-4b3b-86f7-f946233dda15 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:38.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3757" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":32,"skipped":425,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:38.363: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-363f6c63-b626-4868-999a-b31d64dcf754
STEP: Creating a pod to test consume secrets
Mar 24 14:28:38.573: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ed9d7b8-244d-430b-9105-d7201cb4ffbf" in namespace "projected-8158" to be "Succeeded or Failed"
Mar 24 14:28:38.581: INFO: Pod "pod-projected-secrets-6ed9d7b8-244d-430b-9105-d7201cb4ffbf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.485234ms
Mar 24 14:28:40.594: INFO: Pod "pod-projected-secrets-6ed9d7b8-244d-430b-9105-d7201cb4ffbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021181486s
STEP: Saw pod success
Mar 24 14:28:40.594: INFO: Pod "pod-projected-secrets-6ed9d7b8-244d-430b-9105-d7201cb4ffbf" satisfied condition "Succeeded or Failed"
Mar 24 14:28:40.598: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-secrets-6ed9d7b8-244d-430b-9105-d7201cb4ffbf container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 14:28:40.636: INFO: Waiting for pod pod-projected-secrets-6ed9d7b8-244d-430b-9105-d7201cb4ffbf to disappear
Mar 24 14:28:40.640: INFO: Pod pod-projected-secrets-6ed9d7b8-244d-430b-9105-d7201cb4ffbf no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:40.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8158" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":33,"skipped":432,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:40.654: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:28:42.881: INFO: Deleting pod "var-expansion-f7cd1328-908f-4749-9d62-36a5982b1a4b" in namespace "var-expansion-5293"
Mar 24 14:28:42.917: INFO: Wait up to 5m0s for pod "var-expansion-f7cd1328-908f-4749-9d62-36a5982b1a4b" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:44.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5293" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":34,"skipped":437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:44.993: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3113
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9815
STEP: Creating secret with name secret-test-5cfb435d-d4f0-4259-90ae-a85b6d4a033b
STEP: Creating a pod to test consume secrets
Mar 24 14:28:45.441: INFO: Waiting up to 5m0s for pod "pod-secrets-3baf7000-6aec-47e4-95ea-050c228c71c1" in namespace "secrets-3113" to be "Succeeded or Failed"
Mar 24 14:28:45.445: INFO: Pod "pod-secrets-3baf7000-6aec-47e4-95ea-050c228c71c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007439ms
Mar 24 14:28:47.459: INFO: Pod "pod-secrets-3baf7000-6aec-47e4-95ea-050c228c71c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017521274s
STEP: Saw pod success
Mar 24 14:28:47.459: INFO: Pod "pod-secrets-3baf7000-6aec-47e4-95ea-050c228c71c1" satisfied condition "Succeeded or Failed"
Mar 24 14:28:47.463: INFO: Trying to get logs from node talos-default-worker-1 pod pod-secrets-3baf7000-6aec-47e4-95ea-050c228c71c1 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 14:28:47.501: INFO: Waiting for pod pod-secrets-3baf7000-6aec-47e4-95ea-050c228c71c1 to disappear
Mar 24 14:28:47.506: INFO: Pod pod-secrets-3baf7000-6aec-47e4-95ea-050c228c71c1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:47.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3113" for this suite.
STEP: Destroying namespace "secret-namespace-9815" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":462,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:47.534: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2455
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:28:47.722: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:48.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2455" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":36,"skipped":477,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:48.827: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 14:28:49.630: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 14:28:52.691: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:28:52.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8260" for this suite.
STEP: Destroying namespace "webhook-8260-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":37,"skipped":477,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:28:52.907: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 24 14:28:53.151: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 14:29:53.181: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Mar 24 14:29:53.228: INFO: Created pod: pod0-sched-preemption-low-priority
Mar 24 14:29:53.299: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:30:19.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5537" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:86.587 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":38,"skipped":481,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:30:19.494: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 14:30:21.735: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:30:21.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6578" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":39,"skipped":484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:30:21.795: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7204
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:30:38.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7204" for this suite.

• [SLOW TEST:16.460 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":40,"skipped":506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:30:38.255: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-cdec0ce1-1bbd-4ea7-afeb-cb858185eea0
STEP: Creating a pod to test consume secrets
Mar 24 14:30:38.448: INFO: Waiting up to 5m0s for pod "pod-secrets-3c7f1895-c6c1-4878-ac6d-f344d4edd362" in namespace "secrets-9561" to be "Succeeded or Failed"
Mar 24 14:30:38.452: INFO: Pod "pod-secrets-3c7f1895-c6c1-4878-ac6d-f344d4edd362": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572466ms
Mar 24 14:30:40.464: INFO: Pod "pod-secrets-3c7f1895-c6c1-4878-ac6d-f344d4edd362": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015441249s
STEP: Saw pod success
Mar 24 14:30:40.464: INFO: Pod "pod-secrets-3c7f1895-c6c1-4878-ac6d-f344d4edd362" satisfied condition "Succeeded or Failed"
Mar 24 14:30:40.467: INFO: Trying to get logs from node talos-default-worker-1 pod pod-secrets-3c7f1895-c6c1-4878-ac6d-f344d4edd362 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 14:30:40.499: INFO: Waiting for pod pod-secrets-3c7f1895-c6c1-4878-ac6d-f344d4edd362 to disappear
Mar 24 14:30:40.503: INFO: Pod pod-secrets-3c7f1895-c6c1-4878-ac6d-f344d4edd362 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:30:40.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9561" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":41,"skipped":539,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:30:40.517: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Mar 24 14:30:40.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-7966 api-versions'
Mar 24 14:30:40.791: INFO: stderr: ""
Mar 24 14:30:40.791: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:30:40.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7966" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":42,"skipped":539,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:30:40.815: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5887
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-ba7dfd50-b1eb-4682-9877-d88555564ce9
Mar 24 14:30:41.025: INFO: Pod name my-hostname-basic-ba7dfd50-b1eb-4682-9877-d88555564ce9: Found 0 pods out of 1
Mar 24 14:30:46.046: INFO: Pod name my-hostname-basic-ba7dfd50-b1eb-4682-9877-d88555564ce9: Found 1 pods out of 1
Mar 24 14:30:46.046: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ba7dfd50-b1eb-4682-9877-d88555564ce9" are running
Mar 24 14:30:46.049: INFO: Pod "my-hostname-basic-ba7dfd50-b1eb-4682-9877-d88555564ce9-b5w2z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 14:30:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 14:30:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 14:30:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 14:30:41 +0000 UTC Reason: Message:}])
Mar 24 14:30:46.049: INFO: Trying to dial the pod
Mar 24 14:30:51.071: INFO: Controller my-hostname-basic-ba7dfd50-b1eb-4682-9877-d88555564ce9: Got expected result from replica 1 [my-hostname-basic-ba7dfd50-b1eb-4682-9877-d88555564ce9-b5w2z]: "my-hostname-basic-ba7dfd50-b1eb-4682-9877-d88555564ce9-b5w2z", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:30:51.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5887" for this suite.

• [SLOW TEST:10.276 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":43,"skipped":539,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:30:51.092: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2112
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Mar 24 14:30:51.298: INFO: Waiting up to 5m0s for pod "client-containers-04ed3311-ea60-402e-9302-325173cc83d7" in namespace "containers-2112" to be "Succeeded or Failed"
Mar 24 14:30:51.301: INFO: Pod "client-containers-04ed3311-ea60-402e-9302-325173cc83d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.343983ms
Mar 24 14:30:53.315: INFO: Pod "client-containers-04ed3311-ea60-402e-9302-325173cc83d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017373654s
STEP: Saw pod success
Mar 24 14:30:53.315: INFO: Pod "client-containers-04ed3311-ea60-402e-9302-325173cc83d7" satisfied condition "Succeeded or Failed"
Mar 24 14:30:53.319: INFO: Trying to get logs from node talos-default-worker-1 pod client-containers-04ed3311-ea60-402e-9302-325173cc83d7 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 14:30:53.358: INFO: Waiting for pod client-containers-04ed3311-ea60-402e-9302-325173cc83d7 to disappear
Mar 24 14:30:53.362: INFO: Pod client-containers-04ed3311-ea60-402e-9302-325173cc83d7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:30:53.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2112" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":44,"skipped":541,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:30:53.381: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 24 14:30:54.254: INFO: Pod name wrapped-volume-race-02398617-8f11-4503-87e2-ac33a37b17e6: Found 0 pods out of 5
Mar 24 14:30:59.266: INFO: Pod name wrapped-volume-race-02398617-8f11-4503-87e2-ac33a37b17e6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-02398617-8f11-4503-87e2-ac33a37b17e6 in namespace emptydir-wrapper-2422, will wait for the garbage collector to delete the pods
Mar 24 14:31:09.363: INFO: Deleting ReplicationController wrapped-volume-race-02398617-8f11-4503-87e2-ac33a37b17e6 took: 13.254781ms
Mar 24 14:31:09.963: INFO: Terminating ReplicationController wrapped-volume-race-02398617-8f11-4503-87e2-ac33a37b17e6 pods took: 600.501321ms
STEP: Creating RC which spawns configmap-volume pods
Mar 24 14:31:16.023: INFO: Pod name wrapped-volume-race-3d716a24-fb13-41b6-b215-8ae0cb8bc454: Found 0 pods out of 5
Mar 24 14:31:21.041: INFO: Pod name wrapped-volume-race-3d716a24-fb13-41b6-b215-8ae0cb8bc454: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3d716a24-fb13-41b6-b215-8ae0cb8bc454 in namespace emptydir-wrapper-2422, will wait for the garbage collector to delete the pods
Mar 24 14:31:31.141: INFO: Deleting ReplicationController wrapped-volume-race-3d716a24-fb13-41b6-b215-8ae0cb8bc454 took: 17.215031ms
Mar 24 14:31:31.341: INFO: Terminating ReplicationController wrapped-volume-race-3d716a24-fb13-41b6-b215-8ae0cb8bc454 pods took: 200.20653ms
STEP: Creating RC which spawns configmap-volume pods
Mar 24 14:31:35.712: INFO: Pod name wrapped-volume-race-5071eeaa-77ae-4cd0-8de4-0ee4d8b00088: Found 0 pods out of 5
Mar 24 14:31:40.726: INFO: Pod name wrapped-volume-race-5071eeaa-77ae-4cd0-8de4-0ee4d8b00088: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5071eeaa-77ae-4cd0-8de4-0ee4d8b00088 in namespace emptydir-wrapper-2422, will wait for the garbage collector to delete the pods
Mar 24 14:31:50.830: INFO: Deleting ReplicationController wrapped-volume-race-5071eeaa-77ae-4cd0-8de4-0ee4d8b00088 took: 20.004038ms
Mar 24 14:31:51.431: INFO: Terminating ReplicationController wrapped-volume-race-5071eeaa-77ae-4cd0-8de4-0ee4d8b00088 pods took: 600.36966ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:31:58.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2422" for this suite.

• [SLOW TEST:65.590 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":45,"skipped":563,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:31:58.972: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 14:31:59.490: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 24 14:32:01.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752193119, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752193119, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752193119, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752193119, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 14:32:04.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 24 14:32:06.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=webhook-6003 attach --namespace=webhook-6003 to-be-attached-pod -i -c=container1'
Mar 24 14:32:07.075: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:07.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6003" for this suite.
STEP: Destroying namespace "webhook-6003-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.267 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":46,"skipped":567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:07.242: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Mar 24 14:32:09.514: INFO: Pod pod-hostip-c40b44ad-74c4-4fe4-877f-f525dea8c8a0 has hostIP: 10.5.0.6
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:09.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8796" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":47,"skipped":610,"failed":0}
S
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:09.529: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6913
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:32:09.763: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ba7ad84a-ee9a-4358-8d26-ae45fd911f40" in namespace "security-context-test-6913" to be "Succeeded or Failed"
Mar 24 14:32:09.773: INFO: Pod "busybox-readonly-false-ba7ad84a-ee9a-4358-8d26-ae45fd911f40": Phase="Pending", Reason="", readiness=false. Elapsed: 10.429357ms
Mar 24 14:32:11.787: INFO: Pod "busybox-readonly-false-ba7ad84a-ee9a-4358-8d26-ae45fd911f40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024092513s
Mar 24 14:32:11.787: INFO: Pod "busybox-readonly-false-ba7ad84a-ee9a-4358-8d26-ae45fd911f40" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:11.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6913" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":48,"skipped":611,"failed":0}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:11.807: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1561
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 24 14:32:16.107: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 14:32:16.115: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 14:32:18.115: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 14:32:18.130: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 14:32:20.115: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 14:32:20.126: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 14:32:22.115: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 14:32:22.128: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 14:32:24.115: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 14:32:24.128: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 14:32:26.115: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 14:32:26.128: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:26.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1561" for this suite.

• [SLOW TEST:14.351 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":49,"skipped":613,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:26.158: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-5959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 24 14:32:26.408: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 24 14:32:26.414: INFO: starting watch
STEP: patching
STEP: updating
Mar 24 14:32:26.441: INFO: waiting for watch events with expected annotations
Mar 24 14:32:26.441: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:26.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5959" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":50,"skipped":624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:26.615: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1851
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 24 14:32:30.888: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1851 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:32:30.888: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:32:30.961: INFO: Exec stderr: ""
Mar 24 14:32:30.961: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1851 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:32:30.961: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:32:31.035: INFO: Exec stderr: ""
Mar 24 14:32:31.035: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1851 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:32:31.035: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:32:31.097: INFO: Exec stderr: ""
Mar 24 14:32:31.097: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1851 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:32:31.097: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:32:31.168: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 24 14:32:31.168: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1851 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:32:31.168: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:32:31.234: INFO: Exec stderr: ""
Mar 24 14:32:31.234: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1851 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:32:31.234: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:32:31.304: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 24 14:32:31.305: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1851 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:32:31.305: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:32:31.388: INFO: Exec stderr: ""
Mar 24 14:32:31.388: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1851 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:32:31.388: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:32:31.465: INFO: Exec stderr: ""
Mar 24 14:32:31.465: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1851 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:32:31.465: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:32:31.536: INFO: Exec stderr: ""
Mar 24 14:32:31.537: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1851 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:32:31.537: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:32:31.606: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:31.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1851" for this suite.

• [SLOW TEST:5.016 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":664,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:31.631: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2817
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:32:31.833: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:38.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2817" for this suite.

• [SLOW TEST:6.764 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":52,"skipped":680,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:38.396: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:49.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4604" for this suite.

• [SLOW TEST:11.279 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":53,"skipped":692,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:49.675: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar 24 14:32:49.872: INFO: Waiting up to 5m0s for pod "downward-api-ad83dc7f-3d8d-46a0-8251-8397d2e496db" in namespace "downward-api-5917" to be "Succeeded or Failed"
Mar 24 14:32:49.882: INFO: Pod "downward-api-ad83dc7f-3d8d-46a0-8251-8397d2e496db": Phase="Pending", Reason="", readiness=false. Elapsed: 9.848187ms
Mar 24 14:32:51.907: INFO: Pod "downward-api-ad83dc7f-3d8d-46a0-8251-8397d2e496db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034906626s
STEP: Saw pod success
Mar 24 14:32:51.908: INFO: Pod "downward-api-ad83dc7f-3d8d-46a0-8251-8397d2e496db" satisfied condition "Succeeded or Failed"
Mar 24 14:32:51.912: INFO: Trying to get logs from node talos-default-worker-1 pod downward-api-ad83dc7f-3d8d-46a0-8251-8397d2e496db container dapi-container: <nil>
STEP: delete the pod
Mar 24 14:32:51.949: INFO: Waiting for pod downward-api-ad83dc7f-3d8d-46a0-8251-8397d2e496db to disappear
Mar 24 14:32:51.953: INFO: Pod downward-api-ad83dc7f-3d8d-46a0-8251-8397d2e496db no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:51.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5917" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:52.004: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-8904
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:52.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8904" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":55,"skipped":726,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:52.285: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7789
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar 24 14:32:55.058: INFO: Successfully updated pod "labelsupdate07fc2262-1a43-4881-ba10-391cef12cafe"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:32:59.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7789" for this suite.

• [SLOW TEST:6.833 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":740,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:32:59.118: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8328
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8328
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 14:32:59.297: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 24 14:32:59.350: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 24 14:33:01.361: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:33:03.361: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:33:05.364: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:33:07.363: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:33:09.359: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:33:11.366: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:33:13.363: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:33:15.363: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:33:17.363: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 24 14:33:17.370: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 24 14:33:19.418: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 24 14:33:19.418: INFO: Breadth first check of 10.244.2.69 on host 10.5.0.5...
Mar 24 14:33:19.421: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.70:9080/dial?request=hostname&protocol=http&host=10.244.2.69&port=8080&tries=1'] Namespace:pod-network-test-8328 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:33:19.421: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:33:19.493: INFO: Waiting for responses: map[]
Mar 24 14:33:19.493: INFO: reached 10.244.2.69 after 0/1 tries
Mar 24 14:33:19.493: INFO: Breadth first check of 10.244.3.22 on host 10.5.0.6...
Mar 24 14:33:19.498: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.70:9080/dial?request=hostname&protocol=http&host=10.244.3.22&port=8080&tries=1'] Namespace:pod-network-test-8328 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:33:19.498: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:33:19.573: INFO: Waiting for responses: map[]
Mar 24 14:33:19.573: INFO: reached 10.244.3.22 after 0/1 tries
Mar 24 14:33:19.573: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:33:19.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8328" for this suite.

• [SLOW TEST:20.471 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":57,"skipped":754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:33:19.591: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4508.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4508.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 157.97.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.97.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.97.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.97.157_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4508.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4508.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 157.97.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.97.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.97.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.97.157_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 14:33:21.985: INFO: Unable to read wheezy_udp@dns-test-service.dns-4508.svc.cluster.local from pod dns-4508/dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60: the server could not find the requested resource (get pods dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60)
Mar 24 14:33:21.993: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4508.svc.cluster.local from pod dns-4508/dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60: the server could not find the requested resource (get pods dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60)
Mar 24 14:33:22.000: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local from pod dns-4508/dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60: the server could not find the requested resource (get pods dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60)
Mar 24 14:33:22.006: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local from pod dns-4508/dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60: the server could not find the requested resource (get pods dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60)
Mar 24 14:33:22.040: INFO: Unable to read jessie_udp@dns-test-service.dns-4508.svc.cluster.local from pod dns-4508/dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60: the server could not find the requested resource (get pods dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60)
Mar 24 14:33:22.045: INFO: Unable to read jessie_tcp@dns-test-service.dns-4508.svc.cluster.local from pod dns-4508/dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60: the server could not find the requested resource (get pods dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60)
Mar 24 14:33:22.049: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local from pod dns-4508/dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60: the server could not find the requested resource (get pods dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60)
Mar 24 14:33:22.054: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local from pod dns-4508/dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60: the server could not find the requested resource (get pods dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60)
Mar 24 14:33:22.086: INFO: Lookups using dns-4508/dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60 failed for: [wheezy_udp@dns-test-service.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service.dns-4508.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local jessie_udp@dns-test-service.dns-4508.svc.cluster.local jessie_tcp@dns-test-service.dns-4508.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4508.svc.cluster.local]

Mar 24 14:33:27.182: INFO: DNS probes using dns-4508/dns-test-1f16dfb5-e070-4973-af3d-01847e64bf60 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:33:27.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4508" for this suite.

• [SLOW TEST:7.903 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":58,"skipped":802,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:33:27.495: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8993
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 24 14:33:27.689: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 24 14:33:38.689: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:33:40.552: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:33:50.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8993" for this suite.

• [SLOW TEST:23.093 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":59,"skipped":805,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:33:50.589: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4594
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-245733bb-894d-4b98-8940-f9821bdd0463
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:33:50.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4594" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":60,"skipped":813,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:33:50.798: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Mar 24 14:33:51.009: INFO: Waiting up to 5m0s for pod "client-containers-abeae96a-328f-45d0-9fdd-033a10b8c040" in namespace "containers-9504" to be "Succeeded or Failed"
Mar 24 14:33:51.013: INFO: Pod "client-containers-abeae96a-328f-45d0-9fdd-033a10b8c040": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201246ms
Mar 24 14:33:53.025: INFO: Pod "client-containers-abeae96a-328f-45d0-9fdd-033a10b8c040": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01576798s
STEP: Saw pod success
Mar 24 14:33:53.025: INFO: Pod "client-containers-abeae96a-328f-45d0-9fdd-033a10b8c040" satisfied condition "Succeeded or Failed"
Mar 24 14:33:53.029: INFO: Trying to get logs from node talos-default-worker-1 pod client-containers-abeae96a-328f-45d0-9fdd-033a10b8c040 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 14:33:53.065: INFO: Waiting for pod client-containers-abeae96a-328f-45d0-9fdd-033a10b8c040 to disappear
Mar 24 14:33:53.069: INFO: Pod client-containers-abeae96a-328f-45d0-9fdd-033a10b8c040 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:33:53.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9504" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":61,"skipped":828,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:33:53.084: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8470
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:33:53.315: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 24 14:33:58.335: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 24 14:33:58.335: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 14:33:58.373: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8470  175242ff-e32b-4ab3-9d17-a7358cc0d062 7014 1 2021-03-24 14:33:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-03-24 14:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006679a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 24 14:33:58.378: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:33:58.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8470" for this suite.

• [SLOW TEST:5.313 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":62,"skipped":842,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:33:58.398: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8190
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 24 14:33:58.650: INFO: Waiting up to 5m0s for pod "pod-cd24cc31-5dbd-4d16-a1e8-79eca8a0565d" in namespace "emptydir-8190" to be "Succeeded or Failed"
Mar 24 14:33:58.655: INFO: Pod "pod-cd24cc31-5dbd-4d16-a1e8-79eca8a0565d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.141713ms
Mar 24 14:34:00.668: INFO: Pod "pod-cd24cc31-5dbd-4d16-a1e8-79eca8a0565d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017823785s
STEP: Saw pod success
Mar 24 14:34:00.668: INFO: Pod "pod-cd24cc31-5dbd-4d16-a1e8-79eca8a0565d" satisfied condition "Succeeded or Failed"
Mar 24 14:34:00.672: INFO: Trying to get logs from node talos-default-worker-1 pod pod-cd24cc31-5dbd-4d16-a1e8-79eca8a0565d container test-container: <nil>
STEP: delete the pod
Mar 24 14:34:00.710: INFO: Waiting for pod pod-cd24cc31-5dbd-4d16-a1e8-79eca8a0565d to disappear
Mar 24 14:34:00.714: INFO: Pod pod-cd24cc31-5dbd-4d16-a1e8-79eca8a0565d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:34:00.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8190" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":842,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:34:00.852: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar 24 14:34:03.607: INFO: Successfully updated pod "annotationupdatea89a3844-e54c-476e-807b-f41f0838aeac"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:34:07.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5909" for this suite.

• [SLOW TEST:6.808 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":64,"skipped":846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:34:07.666: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-adff6f10-a2fd-45cc-8bbc-3d2f9be92e29
STEP: Creating a pod to test consume secrets
Mar 24 14:34:07.855: INFO: Waiting up to 5m0s for pod "pod-secrets-0662f21b-f511-46d3-b62f-11354caeee92" in namespace "secrets-5864" to be "Succeeded or Failed"
Mar 24 14:34:07.859: INFO: Pod "pod-secrets-0662f21b-f511-46d3-b62f-11354caeee92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07015ms
Mar 24 14:34:09.870: INFO: Pod "pod-secrets-0662f21b-f511-46d3-b62f-11354caeee92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014658161s
STEP: Saw pod success
Mar 24 14:34:09.870: INFO: Pod "pod-secrets-0662f21b-f511-46d3-b62f-11354caeee92" satisfied condition "Succeeded or Failed"
Mar 24 14:34:09.874: INFO: Trying to get logs from node talos-default-worker-1 pod pod-secrets-0662f21b-f511-46d3-b62f-11354caeee92 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 14:34:09.922: INFO: Waiting for pod pod-secrets-0662f21b-f511-46d3-b62f-11354caeee92 to disappear
Mar 24 14:34:09.925: INFO: Pod pod-secrets-0662f21b-f511-46d3-b62f-11354caeee92 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:34:09.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5864" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":949,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:34:09.938: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2380
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:34:10.127: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 24 14:34:10.166: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 24 14:34:12.186: INFO: Creating deployment "test-rolling-update-deployment"
Mar 24 14:34:12.200: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 24 14:34:12.207: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 24 14:34:14.222: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 24 14:34:14.225: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 14:34:14.234: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2380  cbbc832e-e90b-4e3a-91ba-3fc616cc2f47 7197 1 2021-03-24 14:34:12 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-03-24 14:34:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 14:34:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034254b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-24 14:34:12 +0000 UTC,LastTransitionTime:2021-03-24 14:34:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-03-24 14:34:14 +0000 UTC,LastTransitionTime:2021-03-24 14:34:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 24 14:34:14.238: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-2380  41a9fdb3-fe34-442d-9983-96e48b62b4a1 7186 1 2021-03-24 14:34:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment cbbc832e-e90b-4e3a-91ba-3fc616cc2f47 0xc003ffa2b7 0xc003ffa2b8}] []  [{kube-controller-manager Update apps/v1 2021-03-24 14:34:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbbc832e-e90b-4e3a-91ba-3fc616cc2f47\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ffa348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 24 14:34:14.238: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 24 14:34:14.238: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2380  ea39d948-8905-4bd7-b031-5685ad634d9c 7195 2 2021-03-24 14:34:10 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment cbbc832e-e90b-4e3a-91ba-3fc616cc2f47 0xc003ffa1af 0xc003ffa1c0}] []  [{e2e.test Update apps/v1 2021-03-24 14:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 14:34:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbbc832e-e90b-4e3a-91ba-3fc616cc2f47\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ffa258 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 14:34:14.242: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-w2x8r" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-w2x8r test-rolling-update-deployment-6b6bf9df46- deployment-2380  2e88d0ea-3fbb-4b15-b08c-a12c1a055dd7 7185 0 2021-03-24 14:34:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 41a9fdb3-fe34-442d-9983-96e48b62b4a1 0xc003ffa757 0xc003ffa758}] []  [{kube-controller-manager Update v1 2021-03-24 14:34:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41a9fdb3-fe34-442d-9983-96e48b62b4a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:34:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zpm7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zpm7l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zpm7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:34:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:34:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:34:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:34:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:10.244.3.23,StartTime:2021-03-24 14:34:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:34:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://ec761ebfa8e6ef87021da08fc01102b52d8a872e18e1d68c33d42b0c275cd127,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:34:14.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2380" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":66,"skipped":951,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:34:14.261: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 14:34:14.835: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 14:34:17.896: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:34:18.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5876" for this suite.
STEP: Destroying namespace "webhook-5876-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":67,"skipped":961,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:34:18.246: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6098
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-071ab6bc-1c1e-4a50-94e8-10bb819d855e
STEP: Creating a pod to test consume configMaps
Mar 24 14:34:18.522: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6f93f8e9-e9cd-4e9f-b599-30826374ddcb" in namespace "projected-6098" to be "Succeeded or Failed"
Mar 24 14:34:18.548: INFO: Pod "pod-projected-configmaps-6f93f8e9-e9cd-4e9f-b599-30826374ddcb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.631553ms
Mar 24 14:34:20.561: INFO: Pod "pod-projected-configmaps-6f93f8e9-e9cd-4e9f-b599-30826374ddcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039686901s
STEP: Saw pod success
Mar 24 14:34:20.561: INFO: Pod "pod-projected-configmaps-6f93f8e9-e9cd-4e9f-b599-30826374ddcb" satisfied condition "Succeeded or Failed"
Mar 24 14:34:20.565: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-configmaps-6f93f8e9-e9cd-4e9f-b599-30826374ddcb container agnhost-container: <nil>
STEP: delete the pod
Mar 24 14:34:20.688: INFO: Waiting for pod pod-projected-configmaps-6f93f8e9-e9cd-4e9f-b599-30826374ddcb to disappear
Mar 24 14:34:20.693: INFO: Pod pod-projected-configmaps-6f93f8e9-e9cd-4e9f-b599-30826374ddcb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:34:20.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6098" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":68,"skipped":963,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:34:20.712: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Mar 24 14:34:22.936: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7820 PodName:var-expansion-9957b3df-0b71-4031-a7d9-69556bee2a28 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:34:22.936: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: test for file in mounted path
Mar 24 14:34:23.012: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7820 PodName:var-expansion-9957b3df-0b71-4031-a7d9-69556bee2a28 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:34:23.012: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: updating the annotation value
Mar 24 14:34:23.613: INFO: Successfully updated pod "var-expansion-9957b3df-0b71-4031-a7d9-69556bee2a28"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Mar 24 14:34:23.619: INFO: Deleting pod "var-expansion-9957b3df-0b71-4031-a7d9-69556bee2a28" in namespace "var-expansion-7820"
Mar 24 14:34:23.660: INFO: Wait up to 5m0s for pod "var-expansion-9957b3df-0b71-4031-a7d9-69556bee2a28" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:35:07.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7820" for this suite.

• [SLOW TEST:46.998 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":69,"skipped":985,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:35:07.710: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 14:35:07.916: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4fec604-3755-4a64-a312-811e2ab73772" in namespace "projected-9176" to be "Succeeded or Failed"
Mar 24 14:35:07.924: INFO: Pod "downwardapi-volume-b4fec604-3755-4a64-a312-811e2ab73772": Phase="Pending", Reason="", readiness=false. Elapsed: 7.586614ms
Mar 24 14:35:09.932: INFO: Pod "downwardapi-volume-b4fec604-3755-4a64-a312-811e2ab73772": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015998793s
STEP: Saw pod success
Mar 24 14:35:09.932: INFO: Pod "downwardapi-volume-b4fec604-3755-4a64-a312-811e2ab73772" satisfied condition "Succeeded or Failed"
Mar 24 14:35:09.936: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-b4fec604-3755-4a64-a312-811e2ab73772 container client-container: <nil>
STEP: delete the pod
Mar 24 14:35:09.973: INFO: Waiting for pod downwardapi-volume-b4fec604-3755-4a64-a312-811e2ab73772 to disappear
Mar 24 14:35:09.977: INFO: Pod downwardapi-volume-b4fec604-3755-4a64-a312-811e2ab73772 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:35:09.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9176" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:35:10.009: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-6885/configmap-test-58e4da03-9710-49f7-a8e8-53e79fc80ccb
STEP: Creating a pod to test consume configMaps
Mar 24 14:35:10.223: INFO: Waiting up to 5m0s for pod "pod-configmaps-39dd44a1-cf86-4213-8f37-94e554efc0bd" in namespace "configmap-6885" to be "Succeeded or Failed"
Mar 24 14:35:10.227: INFO: Pod "pod-configmaps-39dd44a1-cf86-4213-8f37-94e554efc0bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.209407ms
Mar 24 14:35:12.241: INFO: Pod "pod-configmaps-39dd44a1-cf86-4213-8f37-94e554efc0bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018147774s
STEP: Saw pod success
Mar 24 14:35:12.242: INFO: Pod "pod-configmaps-39dd44a1-cf86-4213-8f37-94e554efc0bd" satisfied condition "Succeeded or Failed"
Mar 24 14:35:12.246: INFO: Trying to get logs from node talos-default-worker-1 pod pod-configmaps-39dd44a1-cf86-4213-8f37-94e554efc0bd container env-test: <nil>
STEP: delete the pod
Mar 24 14:35:12.276: INFO: Waiting for pod pod-configmaps-39dd44a1-cf86-4213-8f37-94e554efc0bd to disappear
Mar 24 14:35:12.284: INFO: Pod pod-configmaps-39dd44a1-cf86-4213-8f37-94e554efc0bd no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:35:12.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6885" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":71,"skipped":1105,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:35:12.308: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6023
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-309c1ff1-4cfe-4360-92e2-c7221b37da7d
STEP: Creating a pod to test consume configMaps
Mar 24 14:35:12.547: INFO: Waiting up to 5m0s for pod "pod-configmaps-569d3859-222e-4f26-842f-fa384c6eb144" in namespace "configmap-6023" to be "Succeeded or Failed"
Mar 24 14:35:12.551: INFO: Pod "pod-configmaps-569d3859-222e-4f26-842f-fa384c6eb144": Phase="Pending", Reason="", readiness=false. Elapsed: 4.334611ms
Mar 24 14:35:14.564: INFO: Pod "pod-configmaps-569d3859-222e-4f26-842f-fa384c6eb144": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017601098s
STEP: Saw pod success
Mar 24 14:35:14.564: INFO: Pod "pod-configmaps-569d3859-222e-4f26-842f-fa384c6eb144" satisfied condition "Succeeded or Failed"
Mar 24 14:35:14.568: INFO: Trying to get logs from node talos-default-worker-1 pod pod-configmaps-569d3859-222e-4f26-842f-fa384c6eb144 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 14:35:14.605: INFO: Waiting for pod pod-configmaps-569d3859-222e-4f26-842f-fa384c6eb144 to disappear
Mar 24 14:35:14.608: INFO: Pod pod-configmaps-569d3859-222e-4f26-842f-fa384c6eb144 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:35:14.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6023" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":72,"skipped":1124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:35:14.645: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4490
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Mar 24 14:37:15.414: INFO: Successfully updated pod "var-expansion-e682cee9-a5d8-4064-9b49-8120aa55b111"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Mar 24 14:37:17.430: INFO: Deleting pod "var-expansion-e682cee9-a5d8-4064-9b49-8120aa55b111" in namespace "var-expansion-4490"
Mar 24 14:37:17.452: INFO: Wait up to 5m0s for pod "var-expansion-e682cee9-a5d8-4064-9b49-8120aa55b111" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:37:57.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4490" for this suite.

• [SLOW TEST:162.862 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":73,"skipped":1172,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:37:57.507: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-6715/configmap-test-49903382-e2fa-4f03-9cb8-e288f85f5c8c
STEP: Creating a pod to test consume configMaps
Mar 24 14:37:57.795: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e7f5fb8-99f9-43da-9b99-51ecf93e0ceb" in namespace "configmap-6715" to be "Succeeded or Failed"
Mar 24 14:37:57.799: INFO: Pod "pod-configmaps-4e7f5fb8-99f9-43da-9b99-51ecf93e0ceb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.787471ms
Mar 24 14:37:59.865: INFO: Pod "pod-configmaps-4e7f5fb8-99f9-43da-9b99-51ecf93e0ceb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.070008829s
STEP: Saw pod success
Mar 24 14:37:59.865: INFO: Pod "pod-configmaps-4e7f5fb8-99f9-43da-9b99-51ecf93e0ceb" satisfied condition "Succeeded or Failed"
Mar 24 14:37:59.869: INFO: Trying to get logs from node talos-default-worker-1 pod pod-configmaps-4e7f5fb8-99f9-43da-9b99-51ecf93e0ceb container env-test: <nil>
STEP: delete the pod
Mar 24 14:37:59.903: INFO: Waiting for pod pod-configmaps-4e7f5fb8-99f9-43da-9b99-51ecf93e0ceb to disappear
Mar 24 14:37:59.907: INFO: Pod pod-configmaps-4e7f5fb8-99f9-43da-9b99-51ecf93e0ceb no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:37:59.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6715" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1177,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:37:59.927: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 24 14:38:00.118: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5252  e2429aa2-bf04-4c25-9b06-041988e27549 7944 0 2021-03-24 14:38:00 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-03-24 14:38:00 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gz5k4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gz5k4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gz5k4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:38:00.121: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 24 14:38:02.135: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 24 14:38:02.135: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5252 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:38:02.135: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Verifying customized DNS server is configured on pod...
Mar 24 14:38:02.215: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5252 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:38:02.215: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:38:02.291: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:38:02.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5252" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":75,"skipped":1183,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:38:02.369: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 24 14:38:02.556: INFO: Waiting up to 5m0s for pod "pod-65d0823d-68f8-48ff-91f0-4627bdcaca05" in namespace "emptydir-4021" to be "Succeeded or Failed"
Mar 24 14:38:02.559: INFO: Pod "pod-65d0823d-68f8-48ff-91f0-4627bdcaca05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.159636ms
Mar 24 14:38:04.572: INFO: Pod "pod-65d0823d-68f8-48ff-91f0-4627bdcaca05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015758171s
STEP: Saw pod success
Mar 24 14:38:04.572: INFO: Pod "pod-65d0823d-68f8-48ff-91f0-4627bdcaca05" satisfied condition "Succeeded or Failed"
Mar 24 14:38:04.577: INFO: Trying to get logs from node talos-default-worker-1 pod pod-65d0823d-68f8-48ff-91f0-4627bdcaca05 container test-container: <nil>
STEP: delete the pod
Mar 24 14:38:04.611: INFO: Waiting for pod pod-65d0823d-68f8-48ff-91f0-4627bdcaca05 to disappear
Mar 24 14:38:04.614: INFO: Pod pod-65d0823d-68f8-48ff-91f0-4627bdcaca05 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:38:04.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4021" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1199,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:38:04.629: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 14:38:06.860: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:38:06.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8885" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:38:06.931: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 24 14:38:07.151: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:07.152: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:07.152: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:07.155: INFO: Number of nodes with available pods: 0
Mar 24 14:38:07.155: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:38:08.164: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:08.164: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:08.165: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:08.168: INFO: Number of nodes with available pods: 0
Mar 24 14:38:08.168: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:38:09.163: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:09.163: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:09.163: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:09.167: INFO: Number of nodes with available pods: 1
Mar 24 14:38:09.167: INFO: Node talos-default-worker-2 is running more than one daemon pod
Mar 24 14:38:10.162: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:10.162: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:10.162: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:10.168: INFO: Number of nodes with available pods: 1
Mar 24 14:38:10.168: INFO: Node talos-default-worker-2 is running more than one daemon pod
Mar 24 14:38:11.163: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:11.163: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:11.163: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:11.167: INFO: Number of nodes with available pods: 1
Mar 24 14:38:11.167: INFO: Node talos-default-worker-2 is running more than one daemon pod
Mar 24 14:38:12.168: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:12.168: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:12.168: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:12.173: INFO: Number of nodes with available pods: 1
Mar 24 14:38:12.173: INFO: Node talos-default-worker-2 is running more than one daemon pod
Mar 24 14:38:13.767: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:13.767: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:13.767: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:14.261: INFO: Number of nodes with available pods: 1
Mar 24 14:38:14.261: INFO: Node talos-default-worker-2 is running more than one daemon pod
Mar 24 14:38:15.166: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:15.166: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:15.166: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:15.169: INFO: Number of nodes with available pods: 2
Mar 24 14:38:15.169: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 24 14:38:15.208: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:15.208: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:15.208: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:38:15.212: INFO: Number of nodes with available pods: 2
Mar 24 14:38:15.212: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2356, will wait for the garbage collector to delete the pods
Mar 24 14:38:16.371: INFO: Deleting DaemonSet.extensions daemon-set took: 12.202795ms
Mar 24 14:38:16.871: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.263517ms
Mar 24 14:39:27.895: INFO: Number of nodes with available pods: 0
Mar 24 14:39:27.895: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 14:39:27.903: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8269"},"items":null}

Mar 24 14:39:27.906: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8269"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:39:27.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2356" for this suite.

• [SLOW TEST:81.015 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":78,"skipped":1311,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:39:27.946: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4235
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-4235
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 14:39:28.138: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 24 14:39:28.204: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 24 14:39:30.216: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:39:32.217: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:39:34.217: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:39:36.219: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:39:38.216: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:39:40.213: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:39:42.215: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:39:44.218: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:39:46.217: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 14:39:48.214: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 24 14:39:48.222: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 24 14:39:50.267: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 24 14:39:50.267: INFO: Breadth first check of 10.244.2.91 on host 10.5.0.5...
Mar 24 14:39:50.271: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.92:9080/dial?request=hostname&protocol=udp&host=10.244.2.91&port=8081&tries=1'] Namespace:pod-network-test-4235 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:39:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:39:50.343: INFO: Waiting for responses: map[]
Mar 24 14:39:50.343: INFO: reached 10.244.2.91 after 0/1 tries
Mar 24 14:39:50.344: INFO: Breadth first check of 10.244.3.27 on host 10.5.0.6...
Mar 24 14:39:50.348: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.92:9080/dial?request=hostname&protocol=udp&host=10.244.3.27&port=8081&tries=1'] Namespace:pod-network-test-4235 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:39:50.348: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:39:50.421: INFO: Waiting for responses: map[]
Mar 24 14:39:50.421: INFO: reached 10.244.3.27 after 0/1 tries
Mar 24 14:39:50.421: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:39:50.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4235" for this suite.

• [SLOW TEST:22.495 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":79,"skipped":1312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:39:50.443: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6459
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:39:50.638: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 24 14:39:53.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-6459 --namespace=crd-publish-openapi-6459 create -f -'
Mar 24 14:39:54.249: INFO: stderr: ""
Mar 24 14:39:54.249: INFO: stdout: "e2e-test-crd-publish-openapi-9076-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 24 14:39:54.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-6459 --namespace=crd-publish-openapi-6459 delete e2e-test-crd-publish-openapi-9076-crds test-cr'
Mar 24 14:39:54.391: INFO: stderr: ""
Mar 24 14:39:54.391: INFO: stdout: "e2e-test-crd-publish-openapi-9076-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 24 14:39:54.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-6459 --namespace=crd-publish-openapi-6459 apply -f -'
Mar 24 14:39:54.717: INFO: stderr: ""
Mar 24 14:39:54.717: INFO: stdout: "e2e-test-crd-publish-openapi-9076-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 24 14:39:54.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-6459 --namespace=crd-publish-openapi-6459 delete e2e-test-crd-publish-openapi-9076-crds test-cr'
Mar 24 14:39:54.818: INFO: stderr: ""
Mar 24 14:39:54.818: INFO: stdout: "e2e-test-crd-publish-openapi-9076-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 24 14:39:54.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-6459 explain e2e-test-crd-publish-openapi-9076-crds'
Mar 24 14:39:55.029: INFO: stderr: ""
Mar 24 14:39:55.029: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9076-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:39:57.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6459" for this suite.

• [SLOW TEST:7.400 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":80,"skipped":1344,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:39:57.844: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1508
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar 24 14:39:58.019: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 14:39:58.027: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 14:39:58.030: INFO: 
Logging pods the apiserver thinks is on node talos-default-worker-1 before test
Mar 24 14:39:58.036: INFO: kube-flannel-9nwp5 from kube-system started at 2021-03-24 14:11:00 +0000 UTC (1 container statuses recorded)
Mar 24 14:39:58.036: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 14:39:58.036: INFO: kube-proxy-vzljq from kube-system started at 2021-03-24 14:11:00 +0000 UTC (1 container statuses recorded)
Mar 24 14:39:58.036: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 14:39:58.036: INFO: netserver-0 from pod-network-test-4235 started at 2021-03-24 14:39:28 +0000 UTC (1 container statuses recorded)
Mar 24 14:39:58.036: INFO: 	Container webserver ready: false, restart count 0
Mar 24 14:39:58.036: INFO: test-container-pod from pod-network-test-4235 started at 2021-03-24 14:39:48 +0000 UTC (1 container statuses recorded)
Mar 24 14:39:58.036: INFO: 	Container webserver ready: false, restart count 0
Mar 24 14:39:58.036: INFO: sonobuoy from sonobuoy started at 2021-03-24 14:20:14 +0000 UTC (1 container statuses recorded)
Mar 24 14:39:58.036: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 24 14:39:58.036: INFO: 
Logging pods the apiserver thinks is on node talos-default-worker-2 before test
Mar 24 14:39:58.043: INFO: coredns-6bd7f94d9b-sq6kb from kube-system started at 2021-03-24 14:11:20 +0000 UTC (1 container statuses recorded)
Mar 24 14:39:58.043: INFO: 	Container coredns ready: true, restart count 0
Mar 24 14:39:58.043: INFO: kube-flannel-fnwwj from kube-system started at 2021-03-24 14:10:52 +0000 UTC (1 container statuses recorded)
Mar 24 14:39:58.043: INFO: 	Container kube-flannel ready: true, restart count 1
Mar 24 14:39:58.043: INFO: kube-proxy-swlm2 from kube-system started at 2021-03-24 14:10:52 +0000 UTC (1 container statuses recorded)
Mar 24 14:39:58.043: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 14:39:58.043: INFO: netserver-1 from pod-network-test-4235 started at 2021-03-24 14:39:28 +0000 UTC (1 container statuses recorded)
Mar 24 14:39:58.043: INFO: 	Container webserver ready: false, restart count 0
Mar 24 14:39:58.043: INFO: sonobuoy-e2e-job-865e6f320bce471a from sonobuoy started at 2021-03-24 14:20:18 +0000 UTC (2 container statuses recorded)
Mar 24 14:39:58.043: INFO: 	Container e2e ready: true, restart count 0
Mar 24 14:39:58.043: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.166f4e39078b2bdc], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.166f4e3909314ef0], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:39:59.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1508" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":81,"skipped":1345,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:39:59.107: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:40:01.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3803" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":82,"skipped":1355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:40:01.342: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 14:40:02.074: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 14:40:05.137: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:40:05.145: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:40:06.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7728" for this suite.
STEP: Destroying namespace "webhook-7728-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.154 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":83,"skipped":1405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:40:06.497: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7679.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7679.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7679.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7679.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 14:40:08.763: INFO: DNS probes using dns-test-f4363c8a-2401-45ee-ba30-64d4fd50a85f succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7679.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7679.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7679.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7679.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 14:40:10.871: INFO: File wheezy_udp@dns-test-service-3.dns-7679.svc.cluster.local from pod  dns-7679/dns-test-5c943fd8-37ea-4870-91f8-1b96a833370a contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 14:40:10.875: INFO: File jessie_udp@dns-test-service-3.dns-7679.svc.cluster.local from pod  dns-7679/dns-test-5c943fd8-37ea-4870-91f8-1b96a833370a contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 14:40:10.875: INFO: Lookups using dns-7679/dns-test-5c943fd8-37ea-4870-91f8-1b96a833370a failed for: [wheezy_udp@dns-test-service-3.dns-7679.svc.cluster.local jessie_udp@dns-test-service-3.dns-7679.svc.cluster.local]

Mar 24 14:40:15.881: INFO: Unable to read wheezy_udp@dns-test-service-3.dns-7679.svc.cluster.local from pod dns-7679/dns-test-5c943fd8-37ea-4870-91f8-1b96a833370a: Get "https://10.96.0.1:443/api/v1/namespaces/dns-7679/pods/dns-test-5c943fd8-37ea-4870-91f8-1b96a833370a/proxy/results/wheezy_udp@dns-test-service-3.dns-7679.svc.cluster.local": stream error: stream ID 5537; INTERNAL_ERROR
Mar 24 14:40:15.887: INFO: Lookups using dns-7679/dns-test-5c943fd8-37ea-4870-91f8-1b96a833370a failed for: [wheezy_udp@dns-test-service-3.dns-7679.svc.cluster.local]

Mar 24 14:40:20.885: INFO: DNS probes using dns-test-5c943fd8-37ea-4870-91f8-1b96a833370a succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7679.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7679.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7679.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7679.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 14:40:23.123: INFO: DNS probes using dns-test-620219c7-3bac-4f7d-8337-8416e863ef40 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:40:23.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7679" for this suite.

• [SLOW TEST:16.843 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":84,"skipped":1449,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:40:23.340: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 24 14:40:23.563: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 14:41:23.594: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:41:23.598: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-2919
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:41:23.829: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar 24 14:41:23.833: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:41:23.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2919" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:41:23.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5112" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.756 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":85,"skipped":1465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:41:24.102: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar 24 14:41:26.853: INFO: Successfully updated pod "annotationupdate952f4b2c-aeea-41b2-a205-b86680eef59a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:41:30.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4994" for this suite.

• [SLOW TEST:6.805 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":86,"skipped":1503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:41:30.908: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1059
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 24 14:41:31.147: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1059  596773f1-f7dc-4fc4-9380-b3e9a31017fc 8944 0 2021-03-24 14:41:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-24 14:41:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 14:41:31.148: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1059  596773f1-f7dc-4fc4-9380-b3e9a31017fc 8945 0 2021-03-24 14:41:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-24 14:41:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:41:31.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1059" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":87,"skipped":1526,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:41:31.160: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 14:41:31.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cef18e56-5d21-4767-8b2f-02aeb9176a66" in namespace "projected-2798" to be "Succeeded or Failed"
Mar 24 14:41:31.362: INFO: Pod "downwardapi-volume-cef18e56-5d21-4767-8b2f-02aeb9176a66": Phase="Pending", Reason="", readiness=false. Elapsed: 7.4771ms
Mar 24 14:41:33.374: INFO: Pod "downwardapi-volume-cef18e56-5d21-4767-8b2f-02aeb9176a66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018617235s
STEP: Saw pod success
Mar 24 14:41:33.374: INFO: Pod "downwardapi-volume-cef18e56-5d21-4767-8b2f-02aeb9176a66" satisfied condition "Succeeded or Failed"
Mar 24 14:41:33.377: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-cef18e56-5d21-4767-8b2f-02aeb9176a66 container client-container: <nil>
STEP: delete the pod
Mar 24 14:41:33.416: INFO: Waiting for pod downwardapi-volume-cef18e56-5d21-4767-8b2f-02aeb9176a66 to disappear
Mar 24 14:41:33.419: INFO: Pod downwardapi-volume-cef18e56-5d21-4767-8b2f-02aeb9176a66 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:41:33.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2798" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":88,"skipped":1540,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:41:33.437: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3158
Mar 24 14:41:35.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3158 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 24 14:41:35.819: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 24 14:41:35.819: INFO: stdout: "iptables"
Mar 24 14:41:35.819: INFO: proxyMode: iptables
Mar 24 14:41:35.842: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 24 14:41:35.845: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3158
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3158
I0324 14:41:35.918053      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3158, replica count: 3
I0324 14:41:38.968335      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 14:41:38.991: INFO: Creating new exec pod
Mar 24 14:41:42.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3158 exec execpod-affinityf26rb -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Mar 24 14:41:42.183: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar 24 14:41:42.183: INFO: stdout: ""
Mar 24 14:41:42.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3158 exec execpod-affinityf26rb -- /bin/sh -x -c nc -zv -t -w 2 10.97.192.38 80'
Mar 24 14:41:42.349: INFO: stderr: "+ nc -zv -t -w 2 10.97.192.38 80\nConnection to 10.97.192.38 80 port [tcp/http] succeeded!\n"
Mar 24 14:41:42.349: INFO: stdout: ""
Mar 24 14:41:42.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3158 exec execpod-affinityf26rb -- /bin/sh -x -c nc -zv -t -w 2 10.5.0.5 32535'
Mar 24 14:41:42.511: INFO: stderr: "+ nc -zv -t -w 2 10.5.0.5 32535\nConnection to 10.5.0.5 32535 port [tcp/32535] succeeded!\n"
Mar 24 14:41:42.511: INFO: stdout: ""
Mar 24 14:41:42.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3158 exec execpod-affinityf26rb -- /bin/sh -x -c nc -zv -t -w 2 10.5.0.6 32535'
Mar 24 14:41:42.678: INFO: stderr: "+ nc -zv -t -w 2 10.5.0.6 32535\nConnection to 10.5.0.6 32535 port [tcp/32535] succeeded!\n"
Mar 24 14:41:42.678: INFO: stdout: ""
Mar 24 14:41:42.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3158 exec execpod-affinityf26rb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.5.0.5:32535/ ; done'
Mar 24 14:41:42.894: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n"
Mar 24 14:41:42.894: INFO: stdout: "\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4\naffinity-nodeport-timeout-qmmj4"
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.894: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.895: INFO: Received response from host: affinity-nodeport-timeout-qmmj4
Mar 24 14:41:42.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3158 exec execpod-affinityf26rb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.5.0.5:32535/'
Mar 24 14:41:43.042: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n"
Mar 24 14:41:43.042: INFO: stdout: "affinity-nodeport-timeout-qmmj4"
Mar 24 14:42:03.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-3158 exec execpod-affinityf26rb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.5.0.5:32535/'
Mar 24 14:42:03.196: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.5.0.5:32535/\n"
Mar 24 14:42:03.196: INFO: stdout: "affinity-nodeport-timeout-c6224"
Mar 24 14:42:03.196: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3158, will wait for the garbage collector to delete the pods
Mar 24 14:42:03.349: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 15.018449ms
Mar 24 14:42:03.949: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.23467ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:42:18.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3158" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:44.625 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":89,"skipped":1541,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:42:18.064: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-507
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-aade5a96-52fa-476e-b46c-caf6e790c518
STEP: Creating secret with name s-test-opt-upd-c123c705-094f-4264-aade-b4920d9aed95
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-aade5a96-52fa-476e-b46c-caf6e790c518
STEP: Updating secret s-test-opt-upd-c123c705-094f-4264-aade-b4920d9aed95
STEP: Creating secret with name s-test-opt-create-ecaa7f5e-57d0-4e83-840d-2622dae70285
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:42:22.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-507" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1547,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:42:22.441: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8156
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8156
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8156
I0324 14:42:22.748618      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8156, replica count: 2
I0324 14:42:25.798957      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 14:42:25.799: INFO: Creating new exec pod
Mar 24 14:42:28.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-8156 exec execpodr5ngq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 24 14:42:29.007: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 24 14:42:29.007: INFO: stdout: ""
Mar 24 14:42:29.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-8156 exec execpodr5ngq -- /bin/sh -x -c nc -zv -t -w 2 10.99.225.211 80'
Mar 24 14:42:29.150: INFO: stderr: "+ nc -zv -t -w 2 10.99.225.211 80\nConnection to 10.99.225.211 80 port [tcp/http] succeeded!\n"
Mar 24 14:42:29.151: INFO: stdout: ""
Mar 24 14:42:29.151: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:42:29.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8156" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.782 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":91,"skipped":1547,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:42:29.223: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9018
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-ccf3ae71-194d-4052-9a5c-658a968fecfd
STEP: Creating a pod to test consume configMaps
Mar 24 14:42:29.457: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a1b2c27-7f4a-4d99-b2c8-8e3f18de0e07" in namespace "configmap-9018" to be "Succeeded or Failed"
Mar 24 14:42:29.461: INFO: Pod "pod-configmaps-8a1b2c27-7f4a-4d99-b2c8-8e3f18de0e07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.531682ms
Mar 24 14:42:31.473: INFO: Pod "pod-configmaps-8a1b2c27-7f4a-4d99-b2c8-8e3f18de0e07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015351382s
STEP: Saw pod success
Mar 24 14:42:31.473: INFO: Pod "pod-configmaps-8a1b2c27-7f4a-4d99-b2c8-8e3f18de0e07" satisfied condition "Succeeded or Failed"
Mar 24 14:42:31.477: INFO: Trying to get logs from node talos-default-worker-1 pod pod-configmaps-8a1b2c27-7f4a-4d99-b2c8-8e3f18de0e07 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 14:42:31.519: INFO: Waiting for pod pod-configmaps-8a1b2c27-7f4a-4d99-b2c8-8e3f18de0e07 to disappear
Mar 24 14:42:31.522: INFO: Pod pod-configmaps-8a1b2c27-7f4a-4d99-b2c8-8e3f18de0e07 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:42:31.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9018" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":92,"skipped":1558,"failed":0}

------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:42:31.535: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:42:31.723: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:42:33.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6846" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":93,"skipped":1558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:42:33.846: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:42:34.038: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 24 14:42:36.107: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:42:36.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2927" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":94,"skipped":1622,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:42:36.131: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 14:42:36.584: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 14:42:39.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:42:39.656: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1564-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:42:40.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9609" for this suite.
STEP: Destroying namespace "webhook-9609-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":95,"skipped":1648,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:42:41.063: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6053
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-e87b8078-d9a7-45fe-a051-a62e52faab24
STEP: Creating a pod to test consume configMaps
Mar 24 14:42:41.285: INFO: Waiting up to 5m0s for pod "pod-configmaps-94614909-e2c4-4541-8182-c25701b32250" in namespace "configmap-6053" to be "Succeeded or Failed"
Mar 24 14:42:41.292: INFO: Pod "pod-configmaps-94614909-e2c4-4541-8182-c25701b32250": Phase="Pending", Reason="", readiness=false. Elapsed: 6.42108ms
Mar 24 14:42:43.302: INFO: Pod "pod-configmaps-94614909-e2c4-4541-8182-c25701b32250": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016976549s
STEP: Saw pod success
Mar 24 14:42:43.302: INFO: Pod "pod-configmaps-94614909-e2c4-4541-8182-c25701b32250" satisfied condition "Succeeded or Failed"
Mar 24 14:42:43.306: INFO: Trying to get logs from node talos-default-worker-2 pod pod-configmaps-94614909-e2c4-4541-8182-c25701b32250 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 14:42:43.352: INFO: Waiting for pod pod-configmaps-94614909-e2c4-4541-8182-c25701b32250 to disappear
Mar 24 14:42:43.356: INFO: Pod pod-configmaps-94614909-e2c4-4541-8182-c25701b32250 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:42:43.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6053" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":96,"skipped":1662,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:42:43.374: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7577
Mar 24 14:42:45.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-7577 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 24 14:42:45.770: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 24 14:42:45.770: INFO: stdout: "iptables"
Mar 24 14:42:45.770: INFO: proxyMode: iptables
Mar 24 14:42:45.805: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 24 14:42:45.809: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7577
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7577
I0324 14:42:45.858958      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7577, replica count: 3
I0324 14:42:48.909332      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 14:42:48.928: INFO: Creating new exec pod
Mar 24 14:42:51.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-7577 exec execpod-affinitylk22w -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Mar 24 14:42:52.131: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar 24 14:42:52.131: INFO: stdout: ""
Mar 24 14:42:52.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-7577 exec execpod-affinitylk22w -- /bin/sh -x -c nc -zv -t -w 2 10.104.37.16 80'
Mar 24 14:42:52.280: INFO: stderr: "+ nc -zv -t -w 2 10.104.37.16 80\nConnection to 10.104.37.16 80 port [tcp/http] succeeded!\n"
Mar 24 14:42:52.280: INFO: stdout: ""
Mar 24 14:42:52.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-7577 exec execpod-affinitylk22w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.37.16:80/ ; done'
Mar 24 14:42:52.488: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n"
Mar 24 14:42:52.488: INFO: stdout: "\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6\naffinity-clusterip-timeout-lxjm6"
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.488: INFO: Received response from host: affinity-clusterip-timeout-lxjm6
Mar 24 14:42:52.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-7577 exec execpod-affinitylk22w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.104.37.16:80/'
Mar 24 14:42:52.654: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n"
Mar 24 14:42:52.654: INFO: stdout: "affinity-clusterip-timeout-lxjm6"
Mar 24 14:43:12.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-7577 exec execpod-affinitylk22w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.104.37.16:80/'
Mar 24 14:43:12.912: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n"
Mar 24 14:43:12.912: INFO: stdout: "affinity-clusterip-timeout-lxjm6"
Mar 24 14:43:32.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-7577 exec execpod-affinitylk22w -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.104.37.16:80/'
Mar 24 14:43:33.082: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.104.37.16:80/\n"
Mar 24 14:43:33.082: INFO: stdout: "affinity-clusterip-timeout-gb88r"
Mar 24 14:43:33.082: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7577, will wait for the garbage collector to delete the pods
Mar 24 14:43:33.197: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.631509ms
Mar 24 14:43:33.797: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.163684ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:43:48.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7577" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:64.667 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":97,"skipped":1667,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:43:48.041: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:43:48.245: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 24 14:43:48.263: INFO: Number of nodes with available pods: 0
Mar 24 14:43:48.263: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 24 14:43:48.298: INFO: Number of nodes with available pods: 0
Mar 24 14:43:48.298: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:43:49.306: INFO: Number of nodes with available pods: 0
Mar 24 14:43:49.306: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:43:50.307: INFO: Number of nodes with available pods: 1
Mar 24 14:43:50.307: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 24 14:43:50.340: INFO: Number of nodes with available pods: 1
Mar 24 14:43:50.340: INFO: Number of running nodes: 0, number of available pods: 1
Mar 24 14:43:51.350: INFO: Number of nodes with available pods: 0
Mar 24 14:43:51.350: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 24 14:43:51.392: INFO: Number of nodes with available pods: 0
Mar 24 14:43:51.392: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:43:52.403: INFO: Number of nodes with available pods: 0
Mar 24 14:43:52.403: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:43:53.401: INFO: Number of nodes with available pods: 0
Mar 24 14:43:53.401: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:43:54.401: INFO: Number of nodes with available pods: 0
Mar 24 14:43:54.401: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:43:55.401: INFO: Number of nodes with available pods: 1
Mar 24 14:43:55.401: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8953, will wait for the garbage collector to delete the pods
Mar 24 14:43:55.483: INFO: Deleting DaemonSet.extensions daemon-set took: 21.745573ms
Mar 24 14:43:55.583: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.172035ms
Mar 24 14:44:06.000: INFO: Number of nodes with available pods: 0
Mar 24 14:44:06.000: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 14:44:06.005: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9999"},"items":null}

Mar 24 14:44:06.009: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9999"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:44:06.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8953" for this suite.

• [SLOW TEST:18.018 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":98,"skipped":1673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:44:06.060: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-684.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-684.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-684.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-684.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-684.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-684.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 14:44:08.325: INFO: DNS probes using dns-684/dns-test-93586b36-d675-47c5-997a-29bd05f1dbb2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:44:08.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-684" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":99,"skipped":1699,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:44:08.384: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9388
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-f9d7c6de-54fa-4ed0-8173-28fd99a73cb9
STEP: Creating secret with name s-test-opt-upd-a37f73e7-bded-42a4-9d0f-8cd2ffbe009a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f9d7c6de-54fa-4ed0-8173-28fd99a73cb9
STEP: Updating secret s-test-opt-upd-a37f73e7-bded-42a4-9d0f-8cd2ffbe009a
STEP: Creating secret with name s-test-opt-create-b3090d96-01f7-4196-b9de-4ec4792e0d01
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:44:12.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9388" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":100,"skipped":1731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:44:12.757: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar 24 14:44:12.946: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:44:16.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2177" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":101,"skipped":1763,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:44:16.597: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Mar 24 14:44:16.782: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 14:44:16.782: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 14:44:16.822: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 14:44:16.822: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 14:44:16.968: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 14:44:16.968: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 14:44:17.092: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 14:44:17.092: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 14:44:18.571: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 24 14:44:18.571: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 24 14:44:18.601: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Mar 24 14:44:18.634: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Mar 24 14:44:18.636: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0
Mar 24 14:44:18.637: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0
Mar 24 14:44:18.637: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0
Mar 24 14:44:18.637: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0
Mar 24 14:44:18.637: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0
Mar 24 14:44:18.637: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0
Mar 24 14:44:18.637: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0
Mar 24 14:44:18.637: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 0
Mar 24 14:44:18.639: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
Mar 24 14:44:18.639: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
Mar 24 14:44:18.639: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 2
Mar 24 14:44:18.639: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 2
Mar 24 14:44:18.639: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 2
Mar 24 14:44:18.639: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 2
Mar 24 14:44:18.683: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 2
Mar 24 14:44:18.683: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 2
Mar 24 14:44:18.718: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 2
Mar 24 14:44:18.718: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 2
Mar 24 14:44:18.768: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
STEP: listing Deployments
Mar 24 14:44:18.776: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Mar 24 14:44:18.815: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Mar 24 14:44:18.827: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 14:44:18.902: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 14:44:19.029: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 14:44:19.135: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 14:44:19.195: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 14:44:19.242: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 14:44:19.283: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Mar 24 14:44:21.168: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
Mar 24 14:44:21.168: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
Mar 24 14:44:21.168: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
Mar 24 14:44:21.168: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
Mar 24 14:44:21.168: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
Mar 24 14:44:21.168: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
Mar 24 14:44:21.168: INFO: observed Deployment test-deployment in namespace deployment-4231 with ReadyReplicas 1
STEP: deleting the Deployment
Mar 24 14:44:21.228: INFO: observed event type MODIFIED
Mar 24 14:44:21.228: INFO: observed event type MODIFIED
Mar 24 14:44:21.228: INFO: observed event type MODIFIED
Mar 24 14:44:21.229: INFO: observed event type MODIFIED
Mar 24 14:44:21.229: INFO: observed event type MODIFIED
Mar 24 14:44:21.229: INFO: observed event type MODIFIED
Mar 24 14:44:21.229: INFO: observed event type MODIFIED
Mar 24 14:44:21.229: INFO: observed event type MODIFIED
Mar 24 14:44:21.229: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 14:44:21.236: INFO: Log out all the ReplicaSets if there is no deployment created
Mar 24 14:44:21.241: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-4231  460edcb2-603b-49dc-9226-18c09db5edfb 10287 3 2021-03-24 14:44:18 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 77a8d66a-26be-4466-952f-923b5e67307b 0xc00348da07 0xc00348da08}] []  [{kube-controller-manager Update apps/v1 2021-03-24 14:44:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77a8d66a-26be-4466-952f-923b5e67307b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00348da70 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Mar 24 14:44:21.246: INFO: pod: "test-deployment-768947d6f5-2wdrd":
&Pod{ObjectMeta:{test-deployment-768947d6f5-2wdrd test-deployment-768947d6f5- deployment-4231  655bc78b-2002-4872-8885-7aa7331357da 10268 0 2021-03-24 14:44:19 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-768947d6f5 460edcb2-603b-49dc-9226-18c09db5edfb 0xc00348de67 0xc00348de68}] []  [{kube-controller-manager Update v1 2021-03-24 14:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"460edcb2-603b-49dc-9226-18c09db5edfb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:44:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7pz7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7pz7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7pz7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:10.244.2.115,StartTime:2021-03-24 14:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:44:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://18e4cbc4f716ef0312ea53743cbded5dc8144cf6004834306e09f2c669663f2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 24 14:44:21.246: INFO: pod: "test-deployment-768947d6f5-4d762":
&Pod{ObjectMeta:{test-deployment-768947d6f5-4d762 test-deployment-768947d6f5- deployment-4231  a6a69113-b2ca-42c3-8543-324f3398eb70 10289 0 2021-03-24 14:44:21 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-768947d6f5 460edcb2-603b-49dc-9226-18c09db5edfb 0xc0008d6077 0xc0008d6078}] []  [{kube-controller-manager Update v1 2021-03-24 14:44:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"460edcb2-603b-49dc-9226-18c09db5edfb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:44:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7pz7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7pz7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7pz7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 14:44:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 24 14:44:21.247: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-4231  34943afc-7c51-43fd-b818-d1585f7ac9cd 10281 4 2021-03-24 14:44:18 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 77a8d66a-26be-4466-952f-923b5e67307b 0xc00348dad7 0xc00348dad8}] []  [{kube-controller-manager Update apps/v1 2021-03-24 14:44:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77a8d66a-26be-4466-952f-923b5e67307b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00348db58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 24 14:44:21.251: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-4231  0b78880a-9adb-4f16-9fb9-ca934d0e68cf 10216 2 2021-03-24 14:44:16 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 77a8d66a-26be-4466-952f-923b5e67307b 0xc00348dbb7 0xc00348dbb8}] []  [{kube-controller-manager Update apps/v1 2021-03-24 14:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77a8d66a-26be-4466-952f-923b5e67307b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00348dc20 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Mar 24 14:44:21.255: INFO: pod: "test-deployment-8b6954bfb-6dg4z":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-6dg4z test-deployment-8b6954bfb- deployment-4231  17356393-52e6-4a25-b0fb-f38c6b7e518b 10188 0 2021-03-24 14:44:16 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 0b78880a-9adb-4f16-9fb9-ca934d0e68cf 0xc0001170f7 0xc0001170f8}] []  [{kube-controller-manager Update v1 2021-03-24 14:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b78880a-9adb-4f16-9fb9-ca934d0e68cf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:44:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7pz7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7pz7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7pz7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:44:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:10.244.3.38,StartTime:2021-03-24 14:44:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:44:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://8ebcecb8b5cd3473a107f01f0719ac09d1be2cb3640a4a0f09f925a8d58f18ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:44:21.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4231" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":102,"skipped":1772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:44:21.273: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2033
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-32f6104f-c7ff-4d91-ba2c-fac4d7001987
STEP: Creating secret with name secret-projected-all-test-volume-6e4963c9-9421-4f45-a7a2-0818ce2584bf
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 24 14:44:21.493: INFO: Waiting up to 5m0s for pod "projected-volume-44373330-7505-43a9-8473-55e0be15e800" in namespace "projected-2033" to be "Succeeded or Failed"
Mar 24 14:44:21.500: INFO: Pod "projected-volume-44373330-7505-43a9-8473-55e0be15e800": Phase="Pending", Reason="", readiness=false. Elapsed: 6.306407ms
Mar 24 14:44:23.516: INFO: Pod "projected-volume-44373330-7505-43a9-8473-55e0be15e800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022310919s
STEP: Saw pod success
Mar 24 14:44:23.516: INFO: Pod "projected-volume-44373330-7505-43a9-8473-55e0be15e800" satisfied condition "Succeeded or Failed"
Mar 24 14:44:23.520: INFO: Trying to get logs from node talos-default-worker-1 pod projected-volume-44373330-7505-43a9-8473-55e0be15e800 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 24 14:44:23.567: INFO: Waiting for pod projected-volume-44373330-7505-43a9-8473-55e0be15e800 to disappear
Mar 24 14:44:23.570: INFO: Pod projected-volume-44373330-7505-43a9-8473-55e0be15e800 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:44:23.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2033" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":103,"skipped":1804,"failed":0}
SSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:44:23.585: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:44:23.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-493" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":104,"skipped":1810,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:44:23.918: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 14:44:24.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ffaa3757-e0f3-4f97-ae13-499d032c491b" in namespace "projected-8614" to be "Succeeded or Failed"
Mar 24 14:44:24.156: INFO: Pod "downwardapi-volume-ffaa3757-e0f3-4f97-ae13-499d032c491b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.756701ms
Mar 24 14:44:26.168: INFO: Pod "downwardapi-volume-ffaa3757-e0f3-4f97-ae13-499d032c491b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030662012s
STEP: Saw pod success
Mar 24 14:44:26.168: INFO: Pod "downwardapi-volume-ffaa3757-e0f3-4f97-ae13-499d032c491b" satisfied condition "Succeeded or Failed"
Mar 24 14:44:26.171: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-ffaa3757-e0f3-4f97-ae13-499d032c491b container client-container: <nil>
STEP: delete the pod
Mar 24 14:44:26.206: INFO: Waiting for pod downwardapi-volume-ffaa3757-e0f3-4f97-ae13-499d032c491b to disappear
Mar 24 14:44:26.209: INFO: Pod downwardapi-volume-ffaa3757-e0f3-4f97-ae13-499d032c491b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:44:26.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8614" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":105,"skipped":1816,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:44:26.225: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4921
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-4921
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4921
Mar 24 14:44:26.486: INFO: Found 0 stateful pods, waiting for 1
Mar 24 14:44:36.507: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 24 14:44:36.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-4921 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 14:44:36.669: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 14:44:36.669: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 14:44:36.669: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 14:44:36.674: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 24 14:44:46.694: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 14:44:46.694: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 14:44:46.733: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:44:46.733: INFO: ss-0  talos-default-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:26 +0000 UTC  }]
Mar 24 14:44:46.733: INFO: 
Mar 24 14:44:46.733: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 24 14:44:47.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995720636s
Mar 24 14:44:48.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985306456s
Mar 24 14:44:49.768: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968912522s
Mar 24 14:44:50.778: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.960980363s
Mar 24 14:44:51.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.951471353s
Mar 24 14:44:52.797: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.939847329s
Mar 24 14:44:53.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.931703052s
Mar 24 14:44:54.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.924198949s
Mar 24 14:44:55.824: INFO: Verifying statefulset ss doesn't scale past 3 for another 913.975378ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4921
Mar 24 14:44:56.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-4921 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 14:44:56.974: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 14:44:56.974: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 14:44:56.974: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 14:44:56.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-4921 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 14:44:57.131: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 24 14:44:57.131: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 14:44:57.131: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 14:44:57.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-4921 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 14:44:57.279: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 24 14:44:57.279: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 14:44:57.279: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 14:44:57.286: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 24 14:45:07.306: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 14:45:07.306: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 14:45:07.306: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 24 14:45:07.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-4921 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 14:45:07.456: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 14:45:07.456: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 14:45:07.456: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 14:45:07.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-4921 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 14:45:07.626: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 14:45:07.626: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 14:45:07.626: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 14:45:07.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-4921 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 14:45:07.788: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 14:45:07.788: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 14:45:07.788: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 14:45:07.788: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 14:45:07.803: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 24 14:45:17.826: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 14:45:17.826: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 14:45:17.826: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 14:45:17.874: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:45:17.874: INFO: ss-0  talos-default-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:26 +0000 UTC  }]
Mar 24 14:45:17.874: INFO: ss-1  talos-default-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:17.874: INFO: ss-2  talos-default-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:17.874: INFO: 
Mar 24 14:45:17.874: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 14:45:18.884: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:45:18.884: INFO: ss-0  talos-default-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:26 +0000 UTC  }]
Mar 24 14:45:18.885: INFO: ss-1  talos-default-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:18.885: INFO: ss-2  talos-default-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:18.885: INFO: 
Mar 24 14:45:18.885: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 14:45:19.894: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:45:19.894: INFO: ss-0  talos-default-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:26 +0000 UTC  }]
Mar 24 14:45:19.894: INFO: ss-1  talos-default-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:19.894: INFO: ss-2  talos-default-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:19.894: INFO: 
Mar 24 14:45:19.894: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 14:45:20.905: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:45:20.905: INFO: ss-1  talos-default-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:20.905: INFO: ss-2  talos-default-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:20.905: INFO: 
Mar 24 14:45:20.905: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 24 14:45:21.915: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:45:21.915: INFO: ss-1  talos-default-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:21.915: INFO: ss-2  talos-default-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:21.915: INFO: 
Mar 24 14:45:21.915: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 24 14:45:22.925: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:45:22.925: INFO: ss-1  talos-default-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:22.925: INFO: ss-2  talos-default-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:22.925: INFO: 
Mar 24 14:45:22.926: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 24 14:45:23.934: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:45:23.934: INFO: ss-1  talos-default-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:23.934: INFO: ss-2  talos-default-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:23.934: INFO: 
Mar 24 14:45:23.934: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 24 14:45:24.942: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:45:24.942: INFO: ss-1  talos-default-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:24.943: INFO: ss-2  talos-default-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:24.943: INFO: 
Mar 24 14:45:24.943: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 24 14:45:25.953: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:45:25.953: INFO: ss-2  talos-default-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:25.953: INFO: 
Mar 24 14:45:25.953: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 24 14:45:26.968: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Mar 24 14:45:26.968: INFO: ss-2  talos-default-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:45:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 14:44:46 +0000 UTC  }]
Mar 24 14:45:26.968: INFO: 
Mar 24 14:45:26.968: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4921
Mar 24 14:45:27.978: INFO: Scaling statefulset ss to 0
Mar 24 14:45:28.005: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 14:45:28.009: INFO: Deleting all statefulset in ns statefulset-4921
Mar 24 14:45:28.013: INFO: Scaling statefulset ss to 0
Mar 24 14:45:28.024: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 14:45:28.028: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:45:28.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4921" for this suite.

• [SLOW TEST:61.844 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":106,"skipped":1819,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:45:28.069: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9897
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 24 14:45:31.397: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:45:32.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9897" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":107,"skipped":1835,"failed":0}

------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:45:32.457: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5306
STEP: creating service affinity-clusterip in namespace services-5306
STEP: creating replication controller affinity-clusterip in namespace services-5306
I0324 14:45:32.717034      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-5306, replica count: 3
I0324 14:45:35.767513      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 14:45:35.783: INFO: Creating new exec pod
Mar 24 14:45:38.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-5306 exec execpod-affinityt9gtf -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Mar 24 14:45:39.019: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 24 14:45:39.019: INFO: stdout: ""
Mar 24 14:45:39.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-5306 exec execpod-affinityt9gtf -- /bin/sh -x -c nc -zv -t -w 2 10.103.146.153 80'
Mar 24 14:45:39.176: INFO: stderr: "+ nc -zv -t -w 2 10.103.146.153 80\nConnection to 10.103.146.153 80 port [tcp/http] succeeded!\n"
Mar 24 14:45:39.176: INFO: stdout: ""
Mar 24 14:45:39.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-5306 exec execpod-affinityt9gtf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.146.153:80/ ; done'
Mar 24 14:45:39.372: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.146.153:80/\n"
Mar 24 14:45:39.372: INFO: stdout: "\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7\naffinity-clusterip-hmrf7"
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.372: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.373: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.373: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.373: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.373: INFO: Received response from host: affinity-clusterip-hmrf7
Mar 24 14:45:39.373: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5306, will wait for the garbage collector to delete the pods
Mar 24 14:45:39.499: INFO: Deleting ReplicationController affinity-clusterip took: 11.187603ms
Mar 24 14:45:39.599: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.156066ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:45:47.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5306" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.539 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":108,"skipped":1835,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:45:48.002: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:45:48.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5972" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":109,"skipped":1908,"failed":0}

------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:45:48.226: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-19934887-0fe6-4c35-8d81-e28039834400 in namespace container-probe-478
Mar 24 14:45:50.477: INFO: Started pod busybox-19934887-0fe6-4c35-8d81-e28039834400 in namespace container-probe-478
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 14:45:50.482: INFO: Initial restart count of pod busybox-19934887-0fe6-4c35-8d81-e28039834400 is 0
Mar 24 14:46:42.792: INFO: Restart count of pod container-probe-478/busybox-19934887-0fe6-4c35-8d81-e28039834400 is now 1 (52.310840778s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:46:42.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-478" for this suite.

• [SLOW TEST:54.619 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":1908,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:46:42.846: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Mar 24 14:46:43.110: INFO: Waiting up to 5m0s for pod "var-expansion-c8dd1cb7-e12f-4b92-ae1e-9b356f48a8c9" in namespace "var-expansion-9151" to be "Succeeded or Failed"
Mar 24 14:46:43.123: INFO: Pod "var-expansion-c8dd1cb7-e12f-4b92-ae1e-9b356f48a8c9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.192082ms
Mar 24 14:46:45.133: INFO: Pod "var-expansion-c8dd1cb7-e12f-4b92-ae1e-9b356f48a8c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023428616s
STEP: Saw pod success
Mar 24 14:46:45.133: INFO: Pod "var-expansion-c8dd1cb7-e12f-4b92-ae1e-9b356f48a8c9" satisfied condition "Succeeded or Failed"
Mar 24 14:46:45.137: INFO: Trying to get logs from node talos-default-worker-1 pod var-expansion-c8dd1cb7-e12f-4b92-ae1e-9b356f48a8c9 container dapi-container: <nil>
STEP: delete the pod
Mar 24 14:46:45.227: INFO: Waiting for pod var-expansion-c8dd1cb7-e12f-4b92-ae1e-9b356f48a8c9 to disappear
Mar 24 14:46:45.231: INFO: Pod var-expansion-c8dd1cb7-e12f-4b92-ae1e-9b356f48a8c9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:46:45.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9151" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":111,"skipped":1921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:46:46.126: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Mar 24 14:46:46.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 create -f -'
Mar 24 14:46:46.723: INFO: stderr: ""
Mar 24 14:46:46.723: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 14:46:46.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 14:46:46.805: INFO: stderr: ""
Mar 24 14:46:46.805: INFO: stdout: "update-demo-nautilus-47qhc update-demo-nautilus-tmrmk "
Mar 24 14:46:46.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-47qhc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 14:46:46.907: INFO: stderr: ""
Mar 24 14:46:46.907: INFO: stdout: ""
Mar 24 14:46:46.907: INFO: update-demo-nautilus-47qhc is created but not running
Mar 24 14:46:51.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 14:46:52.000: INFO: stderr: ""
Mar 24 14:46:52.000: INFO: stdout: "update-demo-nautilus-47qhc update-demo-nautilus-tmrmk "
Mar 24 14:46:52.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-47qhc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 14:46:52.081: INFO: stderr: ""
Mar 24 14:46:52.081: INFO: stdout: ""
Mar 24 14:46:52.081: INFO: update-demo-nautilus-47qhc is created but not running
Mar 24 14:46:57.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 14:46:57.171: INFO: stderr: ""
Mar 24 14:46:57.171: INFO: stdout: "update-demo-nautilus-47qhc update-demo-nautilus-tmrmk "
Mar 24 14:46:57.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-47qhc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 14:46:57.251: INFO: stderr: ""
Mar 24 14:46:57.251: INFO: stdout: "true"
Mar 24 14:46:57.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-47qhc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 14:46:57.327: INFO: stderr: ""
Mar 24 14:46:57.327: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 14:46:57.327: INFO: validating pod update-demo-nautilus-47qhc
Mar 24 14:46:57.332: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 14:46:57.333: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 14:46:57.333: INFO: update-demo-nautilus-47qhc is verified up and running
Mar 24 14:46:57.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-tmrmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 14:46:57.450: INFO: stderr: ""
Mar 24 14:46:57.450: INFO: stdout: "true"
Mar 24 14:46:57.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-tmrmk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 14:46:57.523: INFO: stderr: ""
Mar 24 14:46:57.523: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 14:46:57.523: INFO: validating pod update-demo-nautilus-tmrmk
Mar 24 14:46:57.528: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 14:46:57.528: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 14:46:57.528: INFO: update-demo-nautilus-tmrmk is verified up and running
STEP: scaling down the replication controller
Mar 24 14:46:57.531: INFO: scanned /root for discovery docs: <nil>
Mar 24 14:46:57.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 24 14:46:58.646: INFO: stderr: ""
Mar 24 14:46:58.647: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 14:46:58.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 14:46:58.746: INFO: stderr: ""
Mar 24 14:46:58.746: INFO: stdout: "update-demo-nautilus-47qhc update-demo-nautilus-tmrmk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 24 14:47:03.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 14:47:03.836: INFO: stderr: ""
Mar 24 14:47:03.836: INFO: stdout: "update-demo-nautilus-47qhc update-demo-nautilus-tmrmk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 24 14:47:08.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 14:47:08.924: INFO: stderr: ""
Mar 24 14:47:08.924: INFO: stdout: "update-demo-nautilus-47qhc "
Mar 24 14:47:08.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-47qhc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 14:47:08.994: INFO: stderr: ""
Mar 24 14:47:08.994: INFO: stdout: "true"
Mar 24 14:47:08.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-47qhc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 14:47:09.095: INFO: stderr: ""
Mar 24 14:47:09.095: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 14:47:09.096: INFO: validating pod update-demo-nautilus-47qhc
Mar 24 14:47:09.100: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 14:47:09.100: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 14:47:09.100: INFO: update-demo-nautilus-47qhc is verified up and running
STEP: scaling up the replication controller
Mar 24 14:47:09.102: INFO: scanned /root for discovery docs: <nil>
Mar 24 14:47:09.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 24 14:47:10.224: INFO: stderr: ""
Mar 24 14:47:10.224: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 14:47:10.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 14:47:10.308: INFO: stderr: ""
Mar 24 14:47:10.308: INFO: stdout: "update-demo-nautilus-47qhc update-demo-nautilus-bzfh7 "
Mar 24 14:47:10.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-47qhc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 14:47:10.388: INFO: stderr: ""
Mar 24 14:47:10.388: INFO: stdout: "true"
Mar 24 14:47:10.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-47qhc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 14:47:10.459: INFO: stderr: ""
Mar 24 14:47:10.459: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 14:47:10.459: INFO: validating pod update-demo-nautilus-47qhc
Mar 24 14:47:10.463: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 14:47:10.463: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 14:47:10.463: INFO: update-demo-nautilus-47qhc is verified up and running
Mar 24 14:47:10.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-bzfh7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 14:47:10.546: INFO: stderr: ""
Mar 24 14:47:10.546: INFO: stdout: "true"
Mar 24 14:47:10.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods update-demo-nautilus-bzfh7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 14:47:10.616: INFO: stderr: ""
Mar 24 14:47:10.616: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 14:47:10.616: INFO: validating pod update-demo-nautilus-bzfh7
Mar 24 14:47:10.621: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 14:47:10.621: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 14:47:10.621: INFO: update-demo-nautilus-bzfh7 is verified up and running
STEP: using delete to clean up resources
Mar 24 14:47:10.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 delete --grace-period=0 --force -f -'
Mar 24 14:47:10.699: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 14:47:10.699: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 24 14:47:10.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get rc,svc -l name=update-demo --no-headers'
Mar 24 14:47:10.781: INFO: stderr: "No resources found in kubectl-3402 namespace.\n"
Mar 24 14:47:10.781: INFO: stdout: ""
Mar 24 14:47:10.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 14:47:10.852: INFO: stderr: ""
Mar 24 14:47:10.852: INFO: stdout: "update-demo-nautilus-47qhc\nupdate-demo-nautilus-bzfh7\n"
Mar 24 14:47:11.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get rc,svc -l name=update-demo --no-headers'
Mar 24 14:47:11.436: INFO: stderr: "No resources found in kubectl-3402 namespace.\n"
Mar 24 14:47:11.436: INFO: stdout: ""
Mar 24 14:47:11.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3402 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 14:47:11.514: INFO: stderr: ""
Mar 24 14:47:11.514: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:47:11.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3402" for this suite.

• [SLOW TEST:25.407 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":112,"skipped":1964,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:47:11.534: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:47:24.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1357" for this suite.

• [SLOW TEST:13.360 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":113,"skipped":1973,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:47:24.895: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6862
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-3d808b67-2f4f-4bcd-a921-f6f0d5973f48 in namespace container-probe-6862
Mar 24 14:47:27.142: INFO: Started pod liveness-3d808b67-2f4f-4bcd-a921-f6f0d5973f48 in namespace container-probe-6862
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 14:47:27.146: INFO: Initial restart count of pod liveness-3d808b67-2f4f-4bcd-a921-f6f0d5973f48 is 0
Mar 24 14:47:47.269: INFO: Restart count of pod container-probe-6862/liveness-3d808b67-2f4f-4bcd-a921-f6f0d5973f48 is now 1 (20.123400209s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:47:47.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6862" for this suite.

• [SLOW TEST:22.452 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":1979,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:47:47.348: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5173
STEP: creating service affinity-nodeport-transition in namespace services-5173
STEP: creating replication controller affinity-nodeport-transition in namespace services-5173
I0324 14:47:47.593774      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-5173, replica count: 3
I0324 14:47:50.644080      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 14:47:50.664: INFO: Creating new exec pod
Mar 24 14:47:53.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-5173 exec execpod-affinitydvm4d -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Mar 24 14:47:53.884: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 24 14:47:53.884: INFO: stdout: ""
Mar 24 14:47:53.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-5173 exec execpod-affinitydvm4d -- /bin/sh -x -c nc -zv -t -w 2 10.99.216.231 80'
Mar 24 14:47:54.052: INFO: stderr: "+ nc -zv -t -w 2 10.99.216.231 80\nConnection to 10.99.216.231 80 port [tcp/http] succeeded!\n"
Mar 24 14:47:54.052: INFO: stdout: ""
Mar 24 14:47:54.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-5173 exec execpod-affinitydvm4d -- /bin/sh -x -c nc -zv -t -w 2 10.5.0.5 31823'
Mar 24 14:47:54.217: INFO: stderr: "+ nc -zv -t -w 2 10.5.0.5 31823\nConnection to 10.5.0.5 31823 port [tcp/31823] succeeded!\n"
Mar 24 14:47:54.217: INFO: stdout: ""
Mar 24 14:47:54.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-5173 exec execpod-affinitydvm4d -- /bin/sh -x -c nc -zv -t -w 2 10.5.0.6 31823'
Mar 24 14:47:54.360: INFO: stderr: "+ nc -zv -t -w 2 10.5.0.6 31823\nConnection to 10.5.0.6 31823 port [tcp/31823] succeeded!\n"
Mar 24 14:47:54.360: INFO: stdout: ""
Mar 24 14:47:54.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-5173 exec execpod-affinitydvm4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.5.0.5:31823/ ; done'
Mar 24 14:47:54.621: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n"
Mar 24 14:47:54.621: INFO: stdout: "\naffinity-nodeport-transition-5lnxb\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-bwvfd\naffinity-nodeport-transition-bwvfd\naffinity-nodeport-transition-5lnxb\naffinity-nodeport-transition-bwvfd\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-5lnxb\naffinity-nodeport-transition-bwvfd\naffinity-nodeport-transition-bwvfd\naffinity-nodeport-transition-5lnxb\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-bwvfd\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-5lnxb"
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-5lnxb
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-bwvfd
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-bwvfd
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-5lnxb
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-bwvfd
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-5lnxb
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-bwvfd
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-bwvfd
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-5lnxb
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-bwvfd
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.621: INFO: Received response from host: affinity-nodeport-transition-5lnxb
Mar 24 14:47:54.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-5173 exec execpod-affinitydvm4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.5.0.5:31823/ ; done'
Mar 24 14:47:54.967: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:31823/\n"
Mar 24 14:47:54.967: INFO: stdout: "\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g\naffinity-nodeport-transition-9kg9g"
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Received response from host: affinity-nodeport-transition-9kg9g
Mar 24 14:47:54.967: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5173, will wait for the garbage collector to delete the pods
Mar 24 14:47:55.079: INFO: Deleting ReplicationController affinity-nodeport-transition took: 13.038513ms
Mar 24 14:47:55.679: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 600.167413ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:48:07.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5173" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:20.643 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":115,"skipped":1992,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:48:07.992: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-29q6
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 14:48:08.212: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-29q6" in namespace "subpath-4385" to be "Succeeded or Failed"
Mar 24 14:48:08.216: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.712557ms
Mar 24 14:48:10.228: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Running", Reason="", readiness=true. Elapsed: 2.016501525s
Mar 24 14:48:12.241: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Running", Reason="", readiness=true. Elapsed: 4.029043027s
Mar 24 14:48:14.254: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Running", Reason="", readiness=true. Elapsed: 6.041871524s
Mar 24 14:48:16.265: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Running", Reason="", readiness=true. Elapsed: 8.053248389s
Mar 24 14:48:18.277: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Running", Reason="", readiness=true. Elapsed: 10.065199551s
Mar 24 14:48:20.289: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Running", Reason="", readiness=true. Elapsed: 12.077768825s
Mar 24 14:48:22.302: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Running", Reason="", readiness=true. Elapsed: 14.090783045s
Mar 24 14:48:24.313: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Running", Reason="", readiness=true. Elapsed: 16.101000899s
Mar 24 14:48:26.325: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Running", Reason="", readiness=true. Elapsed: 18.113069318s
Mar 24 14:48:28.337: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Running", Reason="", readiness=true. Elapsed: 20.125022876s
Mar 24 14:48:30.345: INFO: Pod "pod-subpath-test-configmap-29q6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.133757791s
STEP: Saw pod success
Mar 24 14:48:30.345: INFO: Pod "pod-subpath-test-configmap-29q6" satisfied condition "Succeeded or Failed"
Mar 24 14:48:30.350: INFO: Trying to get logs from node talos-default-worker-1 pod pod-subpath-test-configmap-29q6 container test-container-subpath-configmap-29q6: <nil>
STEP: delete the pod
Mar 24 14:48:30.386: INFO: Waiting for pod pod-subpath-test-configmap-29q6 to disappear
Mar 24 14:48:30.390: INFO: Pod pod-subpath-test-configmap-29q6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-29q6
Mar 24 14:48:30.390: INFO: Deleting pod "pod-subpath-test-configmap-29q6" in namespace "subpath-4385"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:48:30.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4385" for this suite.

• [SLOW TEST:22.417 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":116,"skipped":2002,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:48:30.409: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 24 14:48:30.612: INFO: Waiting up to 5m0s for pod "pod-9dc0c6ae-c5a8-46c1-96da-87ec2674fb27" in namespace "emptydir-7212" to be "Succeeded or Failed"
Mar 24 14:48:30.616: INFO: Pod "pod-9dc0c6ae-c5a8-46c1-96da-87ec2674fb27": Phase="Pending", Reason="", readiness=false. Elapsed: 3.612905ms
Mar 24 14:48:32.628: INFO: Pod "pod-9dc0c6ae-c5a8-46c1-96da-87ec2674fb27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015913353s
STEP: Saw pod success
Mar 24 14:48:32.628: INFO: Pod "pod-9dc0c6ae-c5a8-46c1-96da-87ec2674fb27" satisfied condition "Succeeded or Failed"
Mar 24 14:48:32.632: INFO: Trying to get logs from node talos-default-worker-1 pod pod-9dc0c6ae-c5a8-46c1-96da-87ec2674fb27 container test-container: <nil>
STEP: delete the pod
Mar 24 14:48:32.667: INFO: Waiting for pod pod-9dc0c6ae-c5a8-46c1-96da-87ec2674fb27 to disappear
Mar 24 14:48:32.671: INFO: Pod pod-9dc0c6ae-c5a8-46c1-96da-87ec2674fb27 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:48:32.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7212" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":2007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:48:32.686: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6299
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:48:32.873: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6299
I0324 14:48:32.899786      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6299, replica count: 1
I0324 14:48:33.950224      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0324 14:48:34.950407      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 14:48:35.091: INFO: Created: latency-svc-h6jfd
Mar 24 14:48:35.112: INFO: Got endpoints: latency-svc-h6jfd [61.433738ms]
Mar 24 14:48:35.164: INFO: Created: latency-svc-2p7fc
Mar 24 14:48:35.175: INFO: Got endpoints: latency-svc-2p7fc [63.276514ms]
Mar 24 14:48:35.202: INFO: Created: latency-svc-62pvw
Mar 24 14:48:35.314: INFO: Got endpoints: latency-svc-62pvw [201.898569ms]
Mar 24 14:48:35.327: INFO: Created: latency-svc-fgcrd
Mar 24 14:48:35.350: INFO: Got endpoints: latency-svc-fgcrd [237.146959ms]
Mar 24 14:48:35.381: INFO: Created: latency-svc-jz64t
Mar 24 14:48:35.412: INFO: Got endpoints: latency-svc-jz64t [299.237354ms]
Mar 24 14:48:35.433: INFO: Created: latency-svc-xxxkz
Mar 24 14:48:35.463: INFO: Created: latency-svc-jm2tc
Mar 24 14:48:35.570: INFO: Got endpoints: latency-svc-xxxkz [457.236793ms]
Mar 24 14:48:35.576: INFO: Got endpoints: latency-svc-jm2tc [462.652148ms]
Mar 24 14:48:35.588: INFO: Created: latency-svc-g5x22
Mar 24 14:48:35.621: INFO: Got endpoints: latency-svc-g5x22 [507.759479ms]
Mar 24 14:48:35.641: INFO: Created: latency-svc-x8phd
Mar 24 14:48:35.692: INFO: Got endpoints: latency-svc-x8phd [578.657634ms]
Mar 24 14:48:35.708: INFO: Created: latency-svc-7b2n8
Mar 24 14:48:35.739: INFO: Got endpoints: latency-svc-7b2n8 [626.49081ms]
Mar 24 14:48:35.840: INFO: Created: latency-svc-29cwf
Mar 24 14:48:35.868: INFO: Got endpoints: latency-svc-29cwf [754.496243ms]
Mar 24 14:48:35.878: INFO: Created: latency-svc-5rkcv
Mar 24 14:48:35.915: INFO: Got endpoints: latency-svc-5rkcv [802.478898ms]
Mar 24 14:48:35.917: INFO: Created: latency-svc-5z2xt
Mar 24 14:48:36.028: INFO: Got endpoints: latency-svc-5z2xt [914.852646ms]
Mar 24 14:48:36.077: INFO: Created: latency-svc-htrnn
Mar 24 14:48:36.086: INFO: Got endpoints: latency-svc-htrnn [972.791082ms]
Mar 24 14:48:36.122: INFO: Created: latency-svc-4h59l
Mar 24 14:48:36.153: INFO: Got endpoints: latency-svc-4h59l [1.039225213s]
Mar 24 14:48:36.169: INFO: Created: latency-svc-6v9q5
Mar 24 14:48:36.190: INFO: Got endpoints: latency-svc-6v9q5 [1.077359518s]
Mar 24 14:48:36.216: INFO: Created: latency-svc-mgcf2
Mar 24 14:48:36.251: INFO: Created: latency-svc-5ffwn
Mar 24 14:48:36.386: INFO: Got endpoints: latency-svc-mgcf2 [1.210836035s]
Mar 24 14:48:36.386: INFO: Got endpoints: latency-svc-5ffwn [1.072350737s]
Mar 24 14:48:36.396: INFO: Created: latency-svc-xfhqs
Mar 24 14:48:36.442: INFO: Got endpoints: latency-svc-xfhqs [1.091770149s]
Mar 24 14:48:36.451: INFO: Created: latency-svc-w296w
Mar 24 14:48:36.498: INFO: Got endpoints: latency-svc-w296w [1.086150324s]
Mar 24 14:48:36.586: INFO: Created: latency-svc-dqxtw
Mar 24 14:48:36.637: INFO: Got endpoints: latency-svc-dqxtw [1.066856813s]
Mar 24 14:48:36.673: INFO: Created: latency-svc-pfwt9
Mar 24 14:48:36.729: INFO: Created: latency-svc-pxwkk
Mar 24 14:48:36.766: INFO: Got endpoints: latency-svc-pfwt9 [1.190074875s]
Mar 24 14:48:36.796: INFO: Got endpoints: latency-svc-pxwkk [1.17509129s]
Mar 24 14:48:36.815: INFO: Created: latency-svc-dpq4z
Mar 24 14:48:36.845: INFO: Got endpoints: latency-svc-dpq4z [1.153015137s]
Mar 24 14:48:36.939: INFO: Created: latency-svc-vd568
Mar 24 14:48:36.986: INFO: Got endpoints: latency-svc-vd568 [1.246418408s]
Mar 24 14:48:37.004: INFO: Created: latency-svc-cs9kp
Mar 24 14:48:37.013: INFO: Got endpoints: latency-svc-cs9kp [1.145241999s]
Mar 24 14:48:37.060: INFO: Created: latency-svc-rc68x
Mar 24 14:48:37.074: INFO: Got endpoints: latency-svc-rc68x [1.158817261s]
Mar 24 14:48:37.120: INFO: Created: latency-svc-plrql
Mar 24 14:48:37.222: INFO: Got endpoints: latency-svc-plrql [1.19289039s]
Mar 24 14:48:37.246: INFO: Created: latency-svc-gd55m
Mar 24 14:48:37.312: INFO: Created: latency-svc-4vwph
Mar 24 14:48:37.357: INFO: Got endpoints: latency-svc-gd55m [1.270862899s]
Mar 24 14:48:37.357: INFO: Got endpoints: latency-svc-4vwph [1.204176495s]
Mar 24 14:48:37.372: INFO: Created: latency-svc-47w72
Mar 24 14:48:37.450: INFO: Got endpoints: latency-svc-47w72 [1.259119336s]
Mar 24 14:48:37.453: INFO: Created: latency-svc-9k882
Mar 24 14:48:37.514: INFO: Got endpoints: latency-svc-9k882 [1.126495225s]
Mar 24 14:48:37.523: INFO: Created: latency-svc-c7bqb
Mar 24 14:48:37.565: INFO: Got endpoints: latency-svc-c7bqb [1.178480373s]
Mar 24 14:48:37.572: INFO: Created: latency-svc-5shp8
Mar 24 14:48:37.658: INFO: Got endpoints: latency-svc-5shp8 [1.215921597s]
Mar 24 14:48:37.673: INFO: Created: latency-svc-mrs7t
Mar 24 14:48:37.705: INFO: Got endpoints: latency-svc-mrs7t [1.206516526s]
Mar 24 14:48:37.715: INFO: Created: latency-svc-vdnz2
Mar 24 14:48:37.855: INFO: Got endpoints: latency-svc-vdnz2 [1.217721426s]
Mar 24 14:48:37.857: INFO: Created: latency-svc-ddtkt
Mar 24 14:48:37.947: INFO: Got endpoints: latency-svc-ddtkt [1.180779221s]
Mar 24 14:48:37.954: INFO: Created: latency-svc-nff9t
Mar 24 14:48:37.986: INFO: Got endpoints: latency-svc-nff9t [1.190315961s]
Mar 24 14:48:37.992: INFO: Created: latency-svc-5g74p
Mar 24 14:48:38.048: INFO: Created: latency-svc-mzshg
Mar 24 14:48:38.069: INFO: Got endpoints: latency-svc-5g74p [1.223765939s]
Mar 24 14:48:38.208: INFO: Got endpoints: latency-svc-mzshg [1.22211678s]
Mar 24 14:48:38.222: INFO: Created: latency-svc-gww29
Mar 24 14:48:38.263: INFO: Got endpoints: latency-svc-gww29 [1.250361145s]
Mar 24 14:48:38.270: INFO: Created: latency-svc-nrfft
Mar 24 14:48:38.301: INFO: Got endpoints: latency-svc-nrfft [1.226852213s]
Mar 24 14:48:38.342: INFO: Created: latency-svc-nsg74
Mar 24 14:48:38.390: INFO: Got endpoints: latency-svc-nsg74 [1.168412426s]
Mar 24 14:48:38.392: INFO: Created: latency-svc-45nfj
Mar 24 14:48:38.511: INFO: Got endpoints: latency-svc-45nfj [1.154023996s]
Mar 24 14:48:38.525: INFO: Created: latency-svc-9j6wx
Mar 24 14:48:38.548: INFO: Got endpoints: latency-svc-9j6wx [1.19039489s]
Mar 24 14:48:38.557: INFO: Created: latency-svc-7pxmp
Mar 24 14:48:38.596: INFO: Got endpoints: latency-svc-7pxmp [1.146079148s]
Mar 24 14:48:38.597: INFO: Created: latency-svc-9t8vp
Mar 24 14:48:38.687: INFO: Got endpoints: latency-svc-9t8vp [1.172917099s]
Mar 24 14:48:38.701: INFO: Created: latency-svc-rk7m4
Mar 24 14:48:38.727: INFO: Got endpoints: latency-svc-rk7m4 [1.162281437s]
Mar 24 14:48:38.745: INFO: Created: latency-svc-bv5x7
Mar 24 14:48:38.775: INFO: Got endpoints: latency-svc-bv5x7 [1.116979615s]
Mar 24 14:48:38.787: INFO: Created: latency-svc-hdwp4
Mar 24 14:48:38.877: INFO: Created: latency-svc-2vh44
Mar 24 14:48:38.938: INFO: Got endpoints: latency-svc-2vh44 [1.083498727s]
Mar 24 14:48:38.938: INFO: Got endpoints: latency-svc-hdwp4 [1.233296429s]
Mar 24 14:48:38.941: INFO: Created: latency-svc-bkxwf
Mar 24 14:48:38.958: INFO: Got endpoints: latency-svc-bkxwf [1.011571283s]
Mar 24 14:48:39.013: INFO: Created: latency-svc-txq7x
Mar 24 14:48:39.049: INFO: Got endpoints: latency-svc-txq7x [1.062658265s]
Mar 24 14:48:39.054: INFO: Created: latency-svc-2qt95
Mar 24 14:48:39.071: INFO: Got endpoints: latency-svc-2qt95 [1.001428321s]
Mar 24 14:48:39.098: INFO: Created: latency-svc-mjdfh
Mar 24 14:48:39.196: INFO: Got endpoints: latency-svc-mjdfh [988.193312ms]
Mar 24 14:48:39.214: INFO: Created: latency-svc-929rx
Mar 24 14:48:39.256: INFO: Got endpoints: latency-svc-929rx [991.880744ms]
Mar 24 14:48:39.270: INFO: Created: latency-svc-x692p
Mar 24 14:48:39.350: INFO: Got endpoints: latency-svc-x692p [1.048595107s]
Mar 24 14:48:39.368: INFO: Created: latency-svc-8sr8r
Mar 24 14:48:39.437: INFO: Got endpoints: latency-svc-8sr8r [1.046852928s]
Mar 24 14:48:39.438: INFO: Created: latency-svc-4gqxj
Mar 24 14:48:39.485: INFO: Got endpoints: latency-svc-4gqxj [973.957137ms]
Mar 24 14:48:39.507: INFO: Created: latency-svc-275mj
Mar 24 14:48:39.519: INFO: Got endpoints: latency-svc-275mj [971.227109ms]
Mar 24 14:48:39.606: INFO: Created: latency-svc-pgvcf
Mar 24 14:48:39.622: INFO: Got endpoints: latency-svc-pgvcf [1.025692752s]
Mar 24 14:48:39.654: INFO: Created: latency-svc-qs6z7
Mar 24 14:48:39.660: INFO: Got endpoints: latency-svc-qs6z7 [973.725807ms]
Mar 24 14:48:39.694: INFO: Created: latency-svc-9mjrf
Mar 24 14:48:39.745: INFO: Got endpoints: latency-svc-9mjrf [1.01800256s]
Mar 24 14:48:39.757: INFO: Created: latency-svc-k82nm
Mar 24 14:48:39.818: INFO: Got endpoints: latency-svc-k82nm [1.043742054s]
Mar 24 14:48:39.825: INFO: Created: latency-svc-cr8z6
Mar 24 14:48:39.884: INFO: Got endpoints: latency-svc-cr8z6 [945.607883ms]
Mar 24 14:48:39.890: INFO: Created: latency-svc-vkcx2
Mar 24 14:48:39.919: INFO: Got endpoints: latency-svc-vkcx2 [980.30565ms]
Mar 24 14:48:39.938: INFO: Created: latency-svc-82bv8
Mar 24 14:48:40.020: INFO: Created: latency-svc-fb9vh
Mar 24 14:48:40.029: INFO: Got endpoints: latency-svc-82bv8 [1.070826064s]
Mar 24 14:48:40.038: INFO: Got endpoints: latency-svc-fb9vh [989.094368ms]
Mar 24 14:48:40.060: INFO: Created: latency-svc-lcjfg
Mar 24 14:48:40.098: INFO: Got endpoints: latency-svc-lcjfg [1.026882598s]
Mar 24 14:48:40.103: INFO: Created: latency-svc-fs7np
Mar 24 14:48:40.192: INFO: Got endpoints: latency-svc-fs7np [995.562776ms]
Mar 24 14:48:40.197: INFO: Created: latency-svc-x5sw8
Mar 24 14:48:40.243: INFO: Got endpoints: latency-svc-x5sw8 [987.001226ms]
Mar 24 14:48:40.251: INFO: Created: latency-svc-tvbgq
Mar 24 14:48:40.319: INFO: Got endpoints: latency-svc-tvbgq [968.86145ms]
Mar 24 14:48:40.336: INFO: Created: latency-svc-cpv4f
Mar 24 14:48:40.390: INFO: Got endpoints: latency-svc-cpv4f [953.038123ms]
Mar 24 14:48:40.392: INFO: Created: latency-svc-tbmnl
Mar 24 14:48:40.483: INFO: Created: latency-svc-8glgc
Mar 24 14:48:40.492: INFO: Got endpoints: latency-svc-tbmnl [1.006711608s]
Mar 24 14:48:40.533: INFO: Got endpoints: latency-svc-8glgc [1.014409936s]
Mar 24 14:48:40.540: INFO: Created: latency-svc-j98qh
Mar 24 14:48:40.571: INFO: Got endpoints: latency-svc-j98qh [949.505658ms]
Mar 24 14:48:40.652: INFO: Created: latency-svc-qm2d8
Mar 24 14:48:40.652: INFO: Got endpoints: latency-svc-qm2d8 [991.319617ms]
Mar 24 14:48:40.683: INFO: Created: latency-svc-db2tn
Mar 24 14:48:40.708: INFO: Got endpoints: latency-svc-db2tn [962.513124ms]
Mar 24 14:48:40.731: INFO: Created: latency-svc-x4qd7
Mar 24 14:48:40.747: INFO: Got endpoints: latency-svc-x4qd7 [927.868179ms]
Mar 24 14:48:40.800: INFO: Created: latency-svc-x7c8f
Mar 24 14:48:40.831: INFO: Got endpoints: latency-svc-x7c8f [946.293753ms]
Mar 24 14:48:40.879: INFO: Created: latency-svc-9q9pv
Mar 24 14:48:40.963: INFO: Got endpoints: latency-svc-9q9pv [1.043966141s]
Mar 24 14:48:40.970: INFO: Created: latency-svc-6cmn7
Mar 24 14:48:41.022: INFO: Got endpoints: latency-svc-6cmn7 [992.614142ms]
Mar 24 14:48:41.026: INFO: Created: latency-svc-svxpk
Mar 24 14:48:41.052: INFO: Got endpoints: latency-svc-svxpk [1.013433108s]
Mar 24 14:48:41.117: INFO: Created: latency-svc-2dvks
Mar 24 14:48:41.154: INFO: Got endpoints: latency-svc-2dvks [1.055913562s]
Mar 24 14:48:41.162: INFO: Created: latency-svc-vj9db
Mar 24 14:48:41.206: INFO: Created: latency-svc-7vswv
Mar 24 14:48:41.208: INFO: Got endpoints: latency-svc-vj9db [1.015807436s]
Mar 24 14:48:41.261: INFO: Got endpoints: latency-svc-7vswv [1.017802609s]
Mar 24 14:48:41.272: INFO: Created: latency-svc-ccd9x
Mar 24 14:48:41.425: INFO: Got endpoints: latency-svc-ccd9x [1.106253002s]
Mar 24 14:48:41.442: INFO: Created: latency-svc-lbvml
Mar 24 14:48:41.463: INFO: Created: latency-svc-98j76
Mar 24 14:48:41.482: INFO: Got endpoints: latency-svc-lbvml [1.091575441s]
Mar 24 14:48:41.499: INFO: Got endpoints: latency-svc-98j76 [1.00735001s]
Mar 24 14:48:41.507: INFO: Created: latency-svc-8lq8v
Mar 24 14:48:41.587: INFO: Got endpoints: latency-svc-8lq8v [1.054001668s]
Mar 24 14:48:41.598: INFO: Created: latency-svc-xn55q
Mar 24 14:48:41.633: INFO: Got endpoints: latency-svc-xn55q [1.061811448s]
Mar 24 14:48:41.643: INFO: Created: latency-svc-tc9hx
Mar 24 14:48:41.681: INFO: Got endpoints: latency-svc-tc9hx [1.029606046s]
Mar 24 14:48:41.719: INFO: Created: latency-svc-qwf5d
Mar 24 14:48:41.764: INFO: Got endpoints: latency-svc-qwf5d [1.056707134s]
Mar 24 14:48:41.773: INFO: Created: latency-svc-54bbl
Mar 24 14:48:41.856: INFO: Got endpoints: latency-svc-54bbl [1.109227203s]
Mar 24 14:48:41.868: INFO: Created: latency-svc-zfmv5
Mar 24 14:48:41.902: INFO: Created: latency-svc-sbr5l
Mar 24 14:48:41.931: INFO: Got endpoints: latency-svc-zfmv5 [1.099684449s]
Mar 24 14:48:41.940: INFO: Got endpoints: latency-svc-sbr5l [976.902692ms]
Mar 24 14:48:41.946: INFO: Created: latency-svc-p25jl
Mar 24 14:48:42.028: INFO: Got endpoints: latency-svc-p25jl [1.005808758s]
Mar 24 14:48:42.070: INFO: Created: latency-svc-fpntn
Mar 24 14:48:42.099: INFO: Got endpoints: latency-svc-fpntn [1.047119976s]
Mar 24 14:48:42.110: INFO: Created: latency-svc-hwrwt
Mar 24 14:48:42.126: INFO: Got endpoints: latency-svc-hwrwt [972.253112ms]
Mar 24 14:48:42.172: INFO: Created: latency-svc-wsc6x
Mar 24 14:48:42.190: INFO: Got endpoints: latency-svc-wsc6x [981.869859ms]
Mar 24 14:48:42.218: INFO: Created: latency-svc-6k559
Mar 24 14:48:42.295: INFO: Got endpoints: latency-svc-6k559 [1.033566186s]
Mar 24 14:48:42.302: INFO: Created: latency-svc-dw862
Mar 24 14:48:42.330: INFO: Got endpoints: latency-svc-dw862 [904.657645ms]
Mar 24 14:48:42.343: INFO: Created: latency-svc-2w2bj
Mar 24 14:48:42.377: INFO: Got endpoints: latency-svc-2w2bj [894.722405ms]
Mar 24 14:48:42.385: INFO: Created: latency-svc-9gjgb
Mar 24 14:48:42.481: INFO: Got endpoints: latency-svc-9gjgb [981.617066ms]
Mar 24 14:48:42.501: INFO: Created: latency-svc-slvgl
Mar 24 14:48:42.511: INFO: Got endpoints: latency-svc-slvgl [923.229402ms]
Mar 24 14:48:42.543: INFO: Created: latency-svc-tvrf5
Mar 24 14:48:42.601: INFO: Got endpoints: latency-svc-tvrf5 [967.048611ms]
Mar 24 14:48:42.608: INFO: Created: latency-svc-b899x
Mar 24 14:48:42.653: INFO: Got endpoints: latency-svc-b899x [971.316292ms]
Mar 24 14:48:42.695: INFO: Created: latency-svc-l9kh6
Mar 24 14:48:42.765: INFO: Created: latency-svc-cjn82
Mar 24 14:48:42.804: INFO: Created: latency-svc-wbf65
Mar 24 14:48:42.813: INFO: Got endpoints: latency-svc-cjn82 [956.522151ms]
Mar 24 14:48:42.813: INFO: Got endpoints: latency-svc-l9kh6 [1.048565286s]
Mar 24 14:48:42.829: INFO: Got endpoints: latency-svc-wbf65 [898.064671ms]
Mar 24 14:48:42.922: INFO: Created: latency-svc-48gk9
Mar 24 14:48:42.983: INFO: Got endpoints: latency-svc-48gk9 [1.042597181s]
Mar 24 14:48:42.983: INFO: Created: latency-svc-jzxlr
Mar 24 14:48:42.991: INFO: Got endpoints: latency-svc-jzxlr [963.646404ms]
Mar 24 14:48:43.070: INFO: Created: latency-svc-qxt9x
Mar 24 14:48:43.211: INFO: Created: latency-svc-9l4jq
Mar 24 14:48:43.213: INFO: Got endpoints: latency-svc-qxt9x [1.113725469s]
Mar 24 14:48:43.246: INFO: Created: latency-svc-tb56g
Mar 24 14:48:43.253: INFO: Got endpoints: latency-svc-9l4jq [1.126341813s]
Mar 24 14:48:43.262: INFO: Got endpoints: latency-svc-tb56g [1.072072045s]
Mar 24 14:48:43.296: INFO: Created: latency-svc-zbtn6
Mar 24 14:48:43.387: INFO: Got endpoints: latency-svc-zbtn6 [1.091952181s]
Mar 24 14:48:43.393: INFO: Created: latency-svc-5r8mv
Mar 24 14:48:43.431: INFO: Got endpoints: latency-svc-5r8mv [1.1009465s]
Mar 24 14:48:43.432: INFO: Created: latency-svc-kdnqv
Mar 24 14:48:43.455: INFO: Got endpoints: latency-svc-kdnqv [1.07789297s]
Mar 24 14:48:43.471: INFO: Created: latency-svc-vf6c7
Mar 24 14:48:43.529: INFO: Got endpoints: latency-svc-vf6c7 [1.047862567s]
Mar 24 14:48:43.538: INFO: Created: latency-svc-4mf58
Mar 24 14:48:43.605: INFO: Got endpoints: latency-svc-4mf58 [1.093973921s]
Mar 24 14:48:43.614: INFO: Created: latency-svc-w28ms
Mar 24 14:48:43.682: INFO: Got endpoints: latency-svc-w28ms [1.080969703s]
Mar 24 14:48:43.707: INFO: Created: latency-svc-przlq
Mar 24 14:48:43.721: INFO: Got endpoints: latency-svc-przlq [1.068005254s]
Mar 24 14:48:43.745: INFO: Created: latency-svc-s2jb6
Mar 24 14:48:43.771: INFO: Got endpoints: latency-svc-s2jb6 [957.870762ms]
Mar 24 14:48:43.839: INFO: Created: latency-svc-k8x2m
Mar 24 14:48:43.871: INFO: Got endpoints: latency-svc-k8x2m [1.057638618s]
Mar 24 14:48:43.880: INFO: Created: latency-svc-lgshx
Mar 24 14:48:43.912: INFO: Got endpoints: latency-svc-lgshx [1.08302786s]
Mar 24 14:48:43.924: INFO: Created: latency-svc-wtl6q
Mar 24 14:48:43.968: INFO: Got endpoints: latency-svc-wtl6q [985.637672ms]
Mar 24 14:48:43.978: INFO: Created: latency-svc-ljhpq
Mar 24 14:48:44.040: INFO: Got endpoints: latency-svc-ljhpq [1.048510312s]
Mar 24 14:48:44.062: INFO: Created: latency-svc-j9dzd
Mar 24 14:48:44.110: INFO: Got endpoints: latency-svc-j9dzd [897.562388ms]
Mar 24 14:48:44.128: INFO: Created: latency-svc-58gph
Mar 24 14:48:44.180: INFO: Created: latency-svc-slqbr
Mar 24 14:48:44.180: INFO: Got endpoints: latency-svc-58gph [927.722116ms]
Mar 24 14:48:44.195: INFO: Got endpoints: latency-svc-slqbr [932.266245ms]
Mar 24 14:48:44.289: INFO: Created: latency-svc-h24bf
Mar 24 14:48:44.343: INFO: Created: latency-svc-86lhs
Mar 24 14:48:44.346: INFO: Got endpoints: latency-svc-h24bf [958.956643ms]
Mar 24 14:48:44.408: INFO: Got endpoints: latency-svc-86lhs [976.150403ms]
Mar 24 14:48:44.409: INFO: Created: latency-svc-r8b2b
Mar 24 14:48:44.417: INFO: Got endpoints: latency-svc-r8b2b [962.548849ms]
Mar 24 14:48:44.535: INFO: Created: latency-svc-m4mp6
Mar 24 14:48:44.572: INFO: Got endpoints: latency-svc-m4mp6 [1.043242135s]
Mar 24 14:48:44.595: INFO: Created: latency-svc-7ff6z
Mar 24 14:48:44.632: INFO: Got endpoints: latency-svc-7ff6z [1.02693777s]
Mar 24 14:48:44.673: INFO: Created: latency-svc-hknlv
Mar 24 14:48:44.674: INFO: Got endpoints: latency-svc-hknlv [992.204687ms]
Mar 24 14:48:44.748: INFO: Created: latency-svc-9v8pm
Mar 24 14:48:44.801: INFO: Got endpoints: latency-svc-9v8pm [1.080432096s]
Mar 24 14:48:44.813: INFO: Created: latency-svc-6zwch
Mar 24 14:48:44.848: INFO: Got endpoints: latency-svc-6zwch [1.07682553s]
Mar 24 14:48:44.857: INFO: Created: latency-svc-j85kb
Mar 24 14:48:44.901: INFO: Created: latency-svc-5n2v4
Mar 24 14:48:44.974: INFO: Got endpoints: latency-svc-j85kb [1.103180033s]
Mar 24 14:48:44.983: INFO: Got endpoints: latency-svc-5n2v4 [1.071028669s]
Mar 24 14:48:44.989: INFO: Created: latency-svc-2mn6g
Mar 24 14:48:45.031: INFO: Created: latency-svc-xdcw6
Mar 24 14:48:45.063: INFO: Got endpoints: latency-svc-2mn6g [1.094495746s]
Mar 24 14:48:45.103: INFO: Created: latency-svc-4qglh
Mar 24 14:48:45.216: INFO: Got endpoints: latency-svc-xdcw6 [1.175633946s]
Mar 24 14:48:45.257: INFO: Created: latency-svc-h8hwr
Mar 24 14:48:45.274: INFO: Got endpoints: latency-svc-4qglh [1.164058092s]
Mar 24 14:48:45.285: INFO: Got endpoints: latency-svc-h8hwr [1.104178433s]
Mar 24 14:48:45.310: INFO: Created: latency-svc-ktdkq
Mar 24 14:48:45.344: INFO: Got endpoints: latency-svc-ktdkq [1.148839983s]
Mar 24 14:48:45.352: INFO: Created: latency-svc-qm8fz
Mar 24 14:48:45.467: INFO: Got endpoints: latency-svc-qm8fz [1.120838024s]
Mar 24 14:48:45.470: INFO: Created: latency-svc-snrtr
Mar 24 14:48:45.472: INFO: Got endpoints: latency-svc-snrtr [1.063353788s]
Mar 24 14:48:45.527: INFO: Created: latency-svc-596q6
Mar 24 14:48:45.592: INFO: Got endpoints: latency-svc-596q6 [1.17494902s]
Mar 24 14:48:45.605: INFO: Created: latency-svc-xp8cj
Mar 24 14:48:45.726: INFO: Got endpoints: latency-svc-xp8cj [1.153825692s]
Mar 24 14:48:45.728: INFO: Created: latency-svc-xf7dx
Mar 24 14:48:45.731: INFO: Got endpoints: latency-svc-xf7dx [1.098760152s]
Mar 24 14:48:45.734: INFO: Created: latency-svc-hp75t
Mar 24 14:48:45.771: INFO: Got endpoints: latency-svc-hp75t [1.096477623s]
Mar 24 14:48:45.791: INFO: Created: latency-svc-kq2vx
Mar 24 14:48:45.904: INFO: Got endpoints: latency-svc-kq2vx [1.102494154s]
Mar 24 14:48:45.906: INFO: Created: latency-svc-bbjkb
Mar 24 14:48:45.913: INFO: Got endpoints: latency-svc-bbjkb [1.065079527s]
Mar 24 14:48:45.944: INFO: Created: latency-svc-rqfhc
Mar 24 14:48:45.977: INFO: Got endpoints: latency-svc-rqfhc [1.003016488s]
Mar 24 14:48:45.985: INFO: Created: latency-svc-qrfkh
Mar 24 14:48:46.077: INFO: Got endpoints: latency-svc-qrfkh [1.093793361s]
Mar 24 14:48:46.084: INFO: Created: latency-svc-w59mn
Mar 24 14:48:46.126: INFO: Got endpoints: latency-svc-w59mn [1.062667381s]
Mar 24 14:48:46.130: INFO: Created: latency-svc-c6g2n
Mar 24 14:48:46.166: INFO: Got endpoints: latency-svc-c6g2n [949.781706ms]
Mar 24 14:48:46.173: INFO: Created: latency-svc-nnvq8
Mar 24 14:48:46.230: INFO: Got endpoints: latency-svc-nnvq8 [955.769863ms]
Mar 24 14:48:46.239: INFO: Created: latency-svc-fxddh
Mar 24 14:48:46.306: INFO: Got endpoints: latency-svc-fxddh [1.021187368s]
Mar 24 14:48:46.346: INFO: Created: latency-svc-ggg7k
Mar 24 14:48:46.364: INFO: Got endpoints: latency-svc-ggg7k [1.019852347s]
Mar 24 14:48:46.392: INFO: Created: latency-svc-825kj
Mar 24 14:48:46.427: INFO: Got endpoints: latency-svc-825kj [960.408959ms]
Mar 24 14:48:46.440: INFO: Created: latency-svc-p9nph
Mar 24 14:48:46.516: INFO: Got endpoints: latency-svc-p9nph [1.044083727s]
Mar 24 14:48:46.565: INFO: Created: latency-svc-rz9fq
Mar 24 14:48:46.582: INFO: Got endpoints: latency-svc-rz9fq [989.843432ms]
Mar 24 14:48:46.608: INFO: Created: latency-svc-df874
Mar 24 14:48:46.665: INFO: Got endpoints: latency-svc-df874 [938.907321ms]
Mar 24 14:48:46.674: INFO: Created: latency-svc-sr9jg
Mar 24 14:48:46.702: INFO: Got endpoints: latency-svc-sr9jg [971.037318ms]
Mar 24 14:48:46.727: INFO: Created: latency-svc-zcmlv
Mar 24 14:48:46.744: INFO: Got endpoints: latency-svc-zcmlv [973.219088ms]
Mar 24 14:48:46.832: INFO: Created: latency-svc-htcp8
Mar 24 14:48:46.868: INFO: Got endpoints: latency-svc-htcp8 [964.076166ms]
Mar 24 14:48:46.877: INFO: Created: latency-svc-n58hf
Mar 24 14:48:46.916: INFO: Got endpoints: latency-svc-n58hf [1.002905819s]
Mar 24 14:48:46.926: INFO: Created: latency-svc-kljt7
Mar 24 14:48:47.017: INFO: Got endpoints: latency-svc-kljt7 [1.040045405s]
Mar 24 14:48:47.027: INFO: Created: latency-svc-vtjjx
Mar 24 14:48:47.060: INFO: Got endpoints: latency-svc-vtjjx [982.677449ms]
Mar 24 14:48:47.067: INFO: Created: latency-svc-lbkk2
Mar 24 14:48:47.173: INFO: Got endpoints: latency-svc-lbkk2 [1.047395985s]
Mar 24 14:48:47.175: INFO: Created: latency-svc-rt9nh
Mar 24 14:48:47.178: INFO: Got endpoints: latency-svc-rt9nh [1.011743419s]
Mar 24 14:48:47.211: INFO: Created: latency-svc-64c2n
Mar 24 14:48:47.236: INFO: Got endpoints: latency-svc-64c2n [1.005857381s]
Mar 24 14:48:47.246: INFO: Created: latency-svc-wpk9r
Mar 24 14:48:47.308: INFO: Got endpoints: latency-svc-wpk9r [1.001121029s]
Mar 24 14:48:47.317: INFO: Created: latency-svc-q2dsj
Mar 24 14:48:47.385: INFO: Got endpoints: latency-svc-q2dsj [1.020809388s]
Mar 24 14:48:47.396: INFO: Created: latency-svc-wgdsr
Mar 24 14:48:47.444: INFO: Got endpoints: latency-svc-wgdsr [1.016510813s]
Mar 24 14:48:47.450: INFO: Created: latency-svc-dxsm8
Mar 24 14:48:47.479: INFO: Got endpoints: latency-svc-dxsm8 [963.255351ms]
Mar 24 14:48:47.491: INFO: Created: latency-svc-vlgxp
Mar 24 14:48:47.516: INFO: Got endpoints: latency-svc-vlgxp [933.654641ms]
Mar 24 14:48:47.526: INFO: Created: latency-svc-7dlvl
Mar 24 14:48:47.636: INFO: Got endpoints: latency-svc-7dlvl [970.728506ms]
Mar 24 14:48:47.642: INFO: Created: latency-svc-rmsqp
Mar 24 14:48:47.670: INFO: Got endpoints: latency-svc-rmsqp [968.36194ms]
Mar 24 14:48:47.682: INFO: Created: latency-svc-xfvx9
Mar 24 14:48:47.707: INFO: Got endpoints: latency-svc-xfvx9 [962.425182ms]
Mar 24 14:48:47.726: INFO: Created: latency-svc-8ggz2
Mar 24 14:48:47.812: INFO: Got endpoints: latency-svc-8ggz2 [943.62454ms]
Mar 24 14:48:47.821: INFO: Created: latency-svc-vqvsj
Mar 24 14:48:47.849: INFO: Got endpoints: latency-svc-vqvsj [932.531677ms]
Mar 24 14:48:47.855: INFO: Created: latency-svc-wrn6s
Mar 24 14:48:47.884: INFO: Got endpoints: latency-svc-wrn6s [866.302077ms]
Mar 24 14:48:47.891: INFO: Created: latency-svc-qrqkp
Mar 24 14:48:47.938: INFO: Got endpoints: latency-svc-qrqkp [878.662873ms]
Mar 24 14:48:47.950: INFO: Created: latency-svc-tcwhg
Mar 24 14:48:47.993: INFO: Got endpoints: latency-svc-tcwhg [819.164939ms]
Mar 24 14:48:48.000: INFO: Created: latency-svc-xcgft
Mar 24 14:48:48.024: INFO: Got endpoints: latency-svc-xcgft [846.629921ms]
Mar 24 14:48:48.069: INFO: Created: latency-svc-t9qlh
Mar 24 14:48:48.079: INFO: Got endpoints: latency-svc-t9qlh [841.876863ms]
Mar 24 14:48:48.124: INFO: Created: latency-svc-zbq8z
Mar 24 14:48:48.150: INFO: Got endpoints: latency-svc-zbq8z [842.2171ms]
Mar 24 14:48:48.155: INFO: Created: latency-svc-7kqkj
Mar 24 14:48:48.195: INFO: Got endpoints: latency-svc-7kqkj [810.404808ms]
Mar 24 14:48:48.205: INFO: Created: latency-svc-gs26k
Mar 24 14:48:48.238: INFO: Got endpoints: latency-svc-gs26k [793.743097ms]
Mar 24 14:48:48.250: INFO: Created: latency-svc-4sgkd
Mar 24 14:48:48.275: INFO: Got endpoints: latency-svc-4sgkd [795.126956ms]
Mar 24 14:48:48.282: INFO: Created: latency-svc-xm2mt
Mar 24 14:48:48.339: INFO: Got endpoints: latency-svc-xm2mt [823.23516ms]
Mar 24 14:48:48.354: INFO: Created: latency-svc-2fmlx
Mar 24 14:48:48.366: INFO: Got endpoints: latency-svc-2fmlx [729.805156ms]
Mar 24 14:48:48.399: INFO: Created: latency-svc-t28gj
Mar 24 14:48:48.468: INFO: Got endpoints: latency-svc-t28gj [797.537719ms]
Mar 24 14:48:48.472: INFO: Created: latency-svc-ftr8q
Mar 24 14:48:48.506: INFO: Got endpoints: latency-svc-ftr8q [799.290954ms]
Mar 24 14:48:48.516: INFO: Created: latency-svc-ltc28
Mar 24 14:48:48.562: INFO: Created: latency-svc-crqbd
Mar 24 14:48:48.612: INFO: Got endpoints: latency-svc-ltc28 [800.049376ms]
Mar 24 14:48:48.619: INFO: Got endpoints: latency-svc-crqbd [769.846913ms]
Mar 24 14:48:48.650: INFO: Created: latency-svc-48kzp
Mar 24 14:48:48.690: INFO: Created: latency-svc-dvtz2
Mar 24 14:48:48.697: INFO: Got endpoints: latency-svc-48kzp [812.071346ms]
Mar 24 14:48:48.823: INFO: Got endpoints: latency-svc-dvtz2 [884.520211ms]
Mar 24 14:48:48.838: INFO: Created: latency-svc-tt72t
Mar 24 14:48:48.877: INFO: Got endpoints: latency-svc-tt72t [884.290186ms]
Mar 24 14:48:48.877: INFO: Latencies: [63.276514ms 201.898569ms 237.146959ms 299.237354ms 457.236793ms 462.652148ms 507.759479ms 578.657634ms 626.49081ms 729.805156ms 754.496243ms 769.846913ms 793.743097ms 795.126956ms 797.537719ms 799.290954ms 800.049376ms 802.478898ms 810.404808ms 812.071346ms 819.164939ms 823.23516ms 841.876863ms 842.2171ms 846.629921ms 866.302077ms 878.662873ms 884.290186ms 884.520211ms 894.722405ms 897.562388ms 898.064671ms 904.657645ms 914.852646ms 923.229402ms 927.722116ms 927.868179ms 932.266245ms 932.531677ms 933.654641ms 938.907321ms 943.62454ms 945.607883ms 946.293753ms 949.505658ms 949.781706ms 953.038123ms 955.769863ms 956.522151ms 957.870762ms 958.956643ms 960.408959ms 962.425182ms 962.513124ms 962.548849ms 963.255351ms 963.646404ms 964.076166ms 967.048611ms 968.36194ms 968.86145ms 970.728506ms 971.037318ms 971.227109ms 971.316292ms 972.253112ms 972.791082ms 973.219088ms 973.725807ms 973.957137ms 976.150403ms 976.902692ms 980.30565ms 981.617066ms 981.869859ms 982.677449ms 985.637672ms 987.001226ms 988.193312ms 989.094368ms 989.843432ms 991.319617ms 991.880744ms 992.204687ms 992.614142ms 995.562776ms 1.001121029s 1.001428321s 1.002905819s 1.003016488s 1.005808758s 1.005857381s 1.006711608s 1.00735001s 1.011571283s 1.011743419s 1.013433108s 1.014409936s 1.015807436s 1.016510813s 1.017802609s 1.01800256s 1.019852347s 1.020809388s 1.021187368s 1.025692752s 1.026882598s 1.02693777s 1.029606046s 1.033566186s 1.039225213s 1.040045405s 1.042597181s 1.043242135s 1.043742054s 1.043966141s 1.044083727s 1.046852928s 1.047119976s 1.047395985s 1.047862567s 1.048510312s 1.048565286s 1.048595107s 1.054001668s 1.055913562s 1.056707134s 1.057638618s 1.061811448s 1.062658265s 1.062667381s 1.063353788s 1.065079527s 1.066856813s 1.068005254s 1.070826064s 1.071028669s 1.072072045s 1.072350737s 1.07682553s 1.077359518s 1.07789297s 1.080432096s 1.080969703s 1.08302786s 1.083498727s 1.086150324s 1.091575441s 1.091770149s 1.091952181s 1.093793361s 1.093973921s 1.094495746s 1.096477623s 1.098760152s 1.099684449s 1.1009465s 1.102494154s 1.103180033s 1.104178433s 1.106253002s 1.109227203s 1.113725469s 1.116979615s 1.120838024s 1.126341813s 1.126495225s 1.145241999s 1.146079148s 1.148839983s 1.153015137s 1.153825692s 1.154023996s 1.158817261s 1.162281437s 1.164058092s 1.168412426s 1.172917099s 1.17494902s 1.17509129s 1.175633946s 1.178480373s 1.180779221s 1.190074875s 1.190315961s 1.19039489s 1.19289039s 1.204176495s 1.206516526s 1.210836035s 1.215921597s 1.217721426s 1.22211678s 1.223765939s 1.226852213s 1.233296429s 1.246418408s 1.250361145s 1.259119336s 1.270862899s]
Mar 24 14:48:48.878: INFO: 50 %ile: 1.017802609s
Mar 24 14:48:48.878: INFO: 90 %ile: 1.175633946s
Mar 24 14:48:48.878: INFO: 99 %ile: 1.259119336s
Mar 24 14:48:48.878: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:48:48.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6299" for this suite.

• [SLOW TEST:16.220 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":118,"skipped":2038,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:48:48.907: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 14:48:49.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7aa2baa8-0471-429d-b404-347d74cd12fe" in namespace "downward-api-6857" to be "Succeeded or Failed"
Mar 24 14:48:49.116: INFO: Pod "downwardapi-volume-7aa2baa8-0471-429d-b404-347d74cd12fe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.236323ms
Mar 24 14:48:51.127: INFO: Pod "downwardapi-volume-7aa2baa8-0471-429d-b404-347d74cd12fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023081224s
STEP: Saw pod success
Mar 24 14:48:51.127: INFO: Pod "downwardapi-volume-7aa2baa8-0471-429d-b404-347d74cd12fe" satisfied condition "Succeeded or Failed"
Mar 24 14:48:51.130: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-7aa2baa8-0471-429d-b404-347d74cd12fe container client-container: <nil>
STEP: delete the pod
Mar 24 14:48:51.163: INFO: Waiting for pod downwardapi-volume-7aa2baa8-0471-429d-b404-347d74cd12fe to disappear
Mar 24 14:48:51.166: INFO: Pod downwardapi-volume-7aa2baa8-0471-429d-b404-347d74cd12fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:48:51.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6857" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":2052,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:48:51.185: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4911
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-b0784acb-3628-427a-893c-2f150fa640f5
STEP: Creating a pod to test consume configMaps
Mar 24 14:48:51.405: INFO: Waiting up to 5m0s for pod "pod-configmaps-abab30d5-1845-4bd4-a01a-2146888a885c" in namespace "configmap-4911" to be "Succeeded or Failed"
Mar 24 14:48:51.409: INFO: Pod "pod-configmaps-abab30d5-1845-4bd4-a01a-2146888a885c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.761582ms
Mar 24 14:48:53.422: INFO: Pod "pod-configmaps-abab30d5-1845-4bd4-a01a-2146888a885c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016888616s
STEP: Saw pod success
Mar 24 14:48:53.422: INFO: Pod "pod-configmaps-abab30d5-1845-4bd4-a01a-2146888a885c" satisfied condition "Succeeded or Failed"
Mar 24 14:48:53.425: INFO: Trying to get logs from node talos-default-worker-1 pod pod-configmaps-abab30d5-1845-4bd4-a01a-2146888a885c container agnhost-container: <nil>
STEP: delete the pod
Mar 24 14:48:53.466: INFO: Waiting for pod pod-configmaps-abab30d5-1845-4bd4-a01a-2146888a885c to disappear
Mar 24 14:48:53.476: INFO: Pod pod-configmaps-abab30d5-1845-4bd4-a01a-2146888a885c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:48:53.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4911" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":2052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:48:53.510: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-859
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-859
STEP: creating replication controller externalsvc in namespace services-859
I0324 14:48:53.806975      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-859, replica count: 2
I0324 14:48:56.857302      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 24 14:48:56.948: INFO: Creating new exec pod
Mar 24 14:48:59.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-859 exec execpoddcpwb -- /bin/sh -x -c nslookup clusterip-service.services-859.svc.cluster.local'
Mar 24 14:48:59.240: INFO: stderr: "+ nslookup clusterip-service.services-859.svc.cluster.local\n"
Mar 24 14:48:59.240: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-859.svc.cluster.local\tcanonical name = externalsvc.services-859.svc.cluster.local.\nName:\texternalsvc.services-859.svc.cluster.local\nAddress: 10.105.87.217\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-859, will wait for the garbage collector to delete the pods
Mar 24 14:48:59.327: INFO: Deleting ReplicationController externalsvc took: 29.251494ms
Mar 24 14:48:59.927: INFO: Terminating ReplicationController externalsvc pods took: 600.169047ms
Mar 24 14:49:06.124: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:49:06.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-859" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:12.792 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":121,"skipped":2077,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:49:06.303: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9532
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-778b5a4f-bdbc-4111-a5ae-3be27a8a830f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-778b5a4f-bdbc-4111-a5ae-3be27a8a830f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:49:10.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9532" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:49:10.728: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Mar 24 14:49:11.056: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:49:11.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2977" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":123,"skipped":2147,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:49:11.141: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5880
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:49:11.376: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:49:12.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5880" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":124,"skipped":2163,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:49:12.074: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5727
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0324 14:49:22.524580      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 14:49:24.550: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:49:24.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5727" for this suite.

• [SLOW TEST:12.527 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":125,"skipped":2169,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:49:24.599: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3892
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Mar 24 14:49:24.754: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3892 proxy --unix-socket=/tmp/kubectl-proxy-unix227713194/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:49:24.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3892" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":126,"skipped":2173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:49:24.817: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2734
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-2734/secret-test-346446ec-bf51-4874-a3b9-27b09f5e302b
STEP: Creating a pod to test consume secrets
Mar 24 14:49:25.038: INFO: Waiting up to 5m0s for pod "pod-configmaps-28c83c01-672c-424c-b809-961fe1cbb18e" in namespace "secrets-2734" to be "Succeeded or Failed"
Mar 24 14:49:25.042: INFO: Pod "pod-configmaps-28c83c01-672c-424c-b809-961fe1cbb18e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.549836ms
Mar 24 14:49:27.054: INFO: Pod "pod-configmaps-28c83c01-672c-424c-b809-961fe1cbb18e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015667839s
STEP: Saw pod success
Mar 24 14:49:27.054: INFO: Pod "pod-configmaps-28c83c01-672c-424c-b809-961fe1cbb18e" satisfied condition "Succeeded or Failed"
Mar 24 14:49:27.058: INFO: Trying to get logs from node talos-default-worker-1 pod pod-configmaps-28c83c01-672c-424c-b809-961fe1cbb18e container env-test: <nil>
STEP: delete the pod
Mar 24 14:49:27.125: INFO: Waiting for pod pod-configmaps-28c83c01-672c-424c-b809-961fe1cbb18e to disappear
Mar 24 14:49:27.129: INFO: Pod pod-configmaps-28c83c01-672c-424c-b809-961fe1cbb18e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:49:27.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2734" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":127,"skipped":2202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:49:27.141: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Mar 24 14:49:27.310: INFO: namespace kubectl-3972
Mar 24 14:49:27.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3972 create -f -'
Mar 24 14:49:27.694: INFO: stderr: ""
Mar 24 14:49:27.694: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 24 14:49:28.704: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 14:49:28.704: INFO: Found 1 / 1
Mar 24 14:49:28.704: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 24 14:49:28.708: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 14:49:28.708: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 24 14:49:28.708: INFO: wait on agnhost-primary startup in kubectl-3972 
Mar 24 14:49:28.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3972 logs agnhost-primary-vd2p2 agnhost-primary'
Mar 24 14:49:28.783: INFO: stderr: ""
Mar 24 14:49:28.783: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 24 14:49:28.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3972 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 24 14:49:28.926: INFO: stderr: ""
Mar 24 14:49:28.926: INFO: stdout: "service/rm2 exposed\n"
Mar 24 14:49:28.932: INFO: Service rm2 in namespace kubectl-3972 found.
STEP: exposing service
Mar 24 14:49:30.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3972 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 24 14:49:31.105: INFO: stderr: ""
Mar 24 14:49:31.105: INFO: stdout: "service/rm3 exposed\n"
Mar 24 14:49:31.128: INFO: Service rm3 in namespace kubectl-3972 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:49:33.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3972" for this suite.

• [SLOW TEST:6.027 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":128,"skipped":2251,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:49:33.168: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6083
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 24 14:49:33.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6083 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar 24 14:49:33.448: INFO: stderr: ""
Mar 24 14:49:33.448: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Mar 24 14:49:33.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6083 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Mar 24 14:49:33.820: INFO: stderr: ""
Mar 24 14:49:33.820: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Mar 24 14:49:33.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-6083 delete pods e2e-test-httpd-pod'
Mar 24 14:49:36.602: INFO: stderr: ""
Mar 24 14:49:36.602: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:49:36.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6083" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":129,"skipped":2295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:49:36.626: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:49:36.876: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 24 14:49:36.893: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:36.893: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:36.893: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:36.899: INFO: Number of nodes with available pods: 0
Mar 24 14:49:36.899: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:49:37.908: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:37.908: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:37.908: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:37.912: INFO: Number of nodes with available pods: 0
Mar 24 14:49:37.912: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:49:38.911: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:38.911: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:38.911: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:38.916: INFO: Number of nodes with available pods: 2
Mar 24 14:49:38.916: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 24 14:49:38.959: INFO: Wrong image for pod: daemon-set-n4vns. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:38.959: INFO: Wrong image for pod: daemon-set-xfjt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:38.964: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:38.964: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:38.964: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:39.972: INFO: Wrong image for pod: daemon-set-n4vns. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:39.972: INFO: Wrong image for pod: daemon-set-xfjt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:39.976: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:39.976: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:39.977: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:40.973: INFO: Wrong image for pod: daemon-set-n4vns. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:40.973: INFO: Wrong image for pod: daemon-set-xfjt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:40.978: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:40.978: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:40.978: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:41.976: INFO: Wrong image for pod: daemon-set-n4vns. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:41.977: INFO: Wrong image for pod: daemon-set-xfjt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:41.977: INFO: Pod daemon-set-xfjt5 is not available
Mar 24 14:49:41.985: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:41.986: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:41.986: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:42.973: INFO: Wrong image for pod: daemon-set-n4vns. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:42.973: INFO: Pod daemon-set-vcv99 is not available
Mar 24 14:49:42.978: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:42.978: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:42.979: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:43.973: INFO: Wrong image for pod: daemon-set-n4vns. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:43.997: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:43.997: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:43.997: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:44.972: INFO: Wrong image for pod: daemon-set-n4vns. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:44.972: INFO: Pod daemon-set-n4vns is not available
Mar 24 14:49:44.976: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:44.976: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:44.976: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:45.971: INFO: Wrong image for pod: daemon-set-n4vns. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:45.971: INFO: Pod daemon-set-n4vns is not available
Mar 24 14:49:45.976: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:45.976: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:45.976: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:46.973: INFO: Wrong image for pod: daemon-set-n4vns. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 14:49:46.973: INFO: Pod daemon-set-n4vns is not available
Mar 24 14:49:46.977: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:46.977: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:46.977: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:47.970: INFO: Pod daemon-set-z4sw9 is not available
Mar 24 14:49:47.974: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:47.974: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:47.974: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 24 14:49:47.979: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:47.979: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:47.979: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:47.982: INFO: Number of nodes with available pods: 1
Mar 24 14:49:47.982: INFO: Node talos-default-worker-2 is running more than one daemon pod
Mar 24 14:49:49.006: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:49.006: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:49.006: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:49:49.011: INFO: Number of nodes with available pods: 2
Mar 24 14:49:49.011: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6040, will wait for the garbage collector to delete the pods
Mar 24 14:49:49.119: INFO: Deleting DaemonSet.extensions daemon-set took: 34.339487ms
Mar 24 14:49:49.219: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.153425ms
Mar 24 14:49:57.939: INFO: Number of nodes with available pods: 0
Mar 24 14:49:57.939: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 14:49:57.942: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14020"},"items":null}

Mar 24 14:49:57.946: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14020"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:49:57.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6040" for this suite.

• [SLOW TEST:21.366 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":130,"skipped":2322,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:49:57.992: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-8ce4aade-a3ff-4122-9f4f-749c912a6db3
STEP: Creating a pod to test consume configMaps
Mar 24 14:49:58.208: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3cc2fb8-2c50-48a9-9ddf-b2936a284a58" in namespace "configmap-6937" to be "Succeeded or Failed"
Mar 24 14:49:58.219: INFO: Pod "pod-configmaps-d3cc2fb8-2c50-48a9-9ddf-b2936a284a58": Phase="Pending", Reason="", readiness=false. Elapsed: 10.849087ms
Mar 24 14:50:00.230: INFO: Pod "pod-configmaps-d3cc2fb8-2c50-48a9-9ddf-b2936a284a58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021684227s
STEP: Saw pod success
Mar 24 14:50:00.230: INFO: Pod "pod-configmaps-d3cc2fb8-2c50-48a9-9ddf-b2936a284a58" satisfied condition "Succeeded or Failed"
Mar 24 14:50:00.233: INFO: Trying to get logs from node talos-default-worker-1 pod pod-configmaps-d3cc2fb8-2c50-48a9-9ddf-b2936a284a58 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 14:50:00.271: INFO: Waiting for pod pod-configmaps-d3cc2fb8-2c50-48a9-9ddf-b2936a284a58 to disappear
Mar 24 14:50:00.275: INFO: Pod pod-configmaps-d3cc2fb8-2c50-48a9-9ddf-b2936a284a58 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:50:00.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6937" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2330,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:50:00.290: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4164 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4164;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4164 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4164;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4164.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4164.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4164.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4164.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4164.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4164.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4164.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4164.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4164.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4164.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4164.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4164.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4164.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 249.207.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.207.249_udp@PTR;check="$$(dig +tcp +noall +answer +search 249.207.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.207.249_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4164 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4164;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4164 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4164;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4164.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4164.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4164.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4164.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4164.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4164.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4164.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4164.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4164.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4164.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4164.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4164.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4164.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 249.207.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.207.249_udp@PTR;check="$$(dig +tcp +noall +answer +search 249.207.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.207.249_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 14:50:04.656: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.660: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.664: INFO: Unable to read wheezy_udp@dns-test-service.dns-4164 from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.672: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4164 from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.676: INFO: Unable to read wheezy_udp@dns-test-service.dns-4164.svc from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.681: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4164.svc from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.685: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4164.svc from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.712: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.715: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.738: INFO: Unable to read jessie_udp@dns-test-service.dns-4164 from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.743: INFO: Unable to read jessie_tcp@dns-test-service.dns-4164 from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.747: INFO: Unable to read jessie_udp@dns-test-service.dns-4164.svc from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.751: INFO: Unable to read jessie_tcp@dns-test-service.dns-4164.svc from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.755: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4164.svc from pod dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85: the server could not find the requested resource (get pods dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85)
Mar 24 14:50:04.784: INFO: Lookups using dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4164 wheezy_tcp@dns-test-service.dns-4164 wheezy_udp@dns-test-service.dns-4164.svc wheezy_tcp@dns-test-service.dns-4164.svc wheezy_udp@_http._tcp.dns-test-service.dns-4164.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4164 jessie_tcp@dns-test-service.dns-4164 jessie_udp@dns-test-service.dns-4164.svc jessie_tcp@dns-test-service.dns-4164.svc jessie_udp@_http._tcp.dns-test-service.dns-4164.svc]

Mar 24 14:50:09.896: INFO: DNS probes using dns-4164/dns-test-11d32119-bc9a-4adb-95ab-eaca10027d85 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:50:10.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4164" for this suite.

• [SLOW TEST:9.936 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":132,"skipped":2337,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:50:10.235: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 24 14:50:10.447: INFO: Waiting up to 5m0s for pod "pod-d1e35e5a-be76-44e5-90f2-720b3f52f16e" in namespace "emptydir-8643" to be "Succeeded or Failed"
Mar 24 14:50:10.452: INFO: Pod "pod-d1e35e5a-be76-44e5-90f2-720b3f52f16e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.975177ms
Mar 24 14:50:12.464: INFO: Pod "pod-d1e35e5a-be76-44e5-90f2-720b3f52f16e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016805083s
STEP: Saw pod success
Mar 24 14:50:12.464: INFO: Pod "pod-d1e35e5a-be76-44e5-90f2-720b3f52f16e" satisfied condition "Succeeded or Failed"
Mar 24 14:50:12.468: INFO: Trying to get logs from node talos-default-worker-1 pod pod-d1e35e5a-be76-44e5-90f2-720b3f52f16e container test-container: <nil>
STEP: delete the pod
Mar 24 14:50:12.496: INFO: Waiting for pod pod-d1e35e5a-be76-44e5-90f2-720b3f52f16e to disappear
Mar 24 14:50:12.503: INFO: Pod pod-d1e35e5a-be76-44e5-90f2-720b3f52f16e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:50:12.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8643" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":133,"skipped":2342,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:50:12.523: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0324 14:50:13.782751      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 14:50:15.879: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:50:15.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9432" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":134,"skipped":2346,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:50:15.916: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3098
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 24 14:50:16.116: INFO: Waiting up to 5m0s for pod "pod-bffea5cc-4689-4742-959e-c60d6d389a6c" in namespace "emptydir-3098" to be "Succeeded or Failed"
Mar 24 14:50:16.129: INFO: Pod "pod-bffea5cc-4689-4742-959e-c60d6d389a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.643522ms
Mar 24 14:50:18.141: INFO: Pod "pod-bffea5cc-4689-4742-959e-c60d6d389a6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024519128s
STEP: Saw pod success
Mar 24 14:50:18.141: INFO: Pod "pod-bffea5cc-4689-4742-959e-c60d6d389a6c" satisfied condition "Succeeded or Failed"
Mar 24 14:50:18.144: INFO: Trying to get logs from node talos-default-worker-1 pod pod-bffea5cc-4689-4742-959e-c60d6d389a6c container test-container: <nil>
STEP: delete the pod
Mar 24 14:50:18.174: INFO: Waiting for pod pod-bffea5cc-4689-4742-959e-c60d6d389a6c to disappear
Mar 24 14:50:18.177: INFO: Pod pod-bffea5cc-4689-4742-959e-c60d6d389a6c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:50:18.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3098" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":135,"skipped":2363,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:50:18.191: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1221
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 24 14:50:18.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-1221 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar 24 14:50:18.867: INFO: stderr: ""
Mar 24 14:50:18.867: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 24 14:50:23.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-1221 get pod e2e-test-httpd-pod -o json'
Mar 24 14:50:24.009: INFO: stderr: ""
Mar 24 14:50:24.009: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-03-24T14:50:18Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-24T14:50:18Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.2.153\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-24T14:50:19Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1221\",\n        \"resourceVersion\": \"14304\",\n        \"uid\": \"d32821f5-7ac0-4d47-8b27-2d7d0bdce7ab\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-6jzks\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"talos-default-worker-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-6jzks\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-6jzks\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-24T14:50:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-24T14:50:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-24T14:50:19Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-24T14:50:18Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://676f4da40bfb5170846589febe8dfd00bfacfe184f95890629f4dd095973c395\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-03-24T14:50:19Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.5.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.2.153\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.2.153\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-03-24T14:50:18Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 24 14:50:24.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-1221 replace -f -'
Mar 24 14:50:24.368: INFO: stderr: ""
Mar 24 14:50:24.369: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Mar 24 14:50:24.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-1221 delete pods e2e-test-httpd-pod'
Mar 24 14:50:36.008: INFO: stderr: ""
Mar 24 14:50:36.008: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:50:36.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1221" for this suite.

• [SLOW TEST:17.880 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":136,"skipped":2378,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:50:36.071: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 14:50:36.277: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e79d46ad-f0ed-45e5-8396-c8f1461d5c43" in namespace "downward-api-6669" to be "Succeeded or Failed"
Mar 24 14:50:36.289: INFO: Pod "downwardapi-volume-e79d46ad-f0ed-45e5-8396-c8f1461d5c43": Phase="Pending", Reason="", readiness=false. Elapsed: 11.918761ms
Mar 24 14:50:38.302: INFO: Pod "downwardapi-volume-e79d46ad-f0ed-45e5-8396-c8f1461d5c43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024460871s
STEP: Saw pod success
Mar 24 14:50:38.302: INFO: Pod "downwardapi-volume-e79d46ad-f0ed-45e5-8396-c8f1461d5c43" satisfied condition "Succeeded or Failed"
Mar 24 14:50:38.306: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-e79d46ad-f0ed-45e5-8396-c8f1461d5c43 container client-container: <nil>
STEP: delete the pod
Mar 24 14:50:38.341: INFO: Waiting for pod downwardapi-volume-e79d46ad-f0ed-45e5-8396-c8f1461d5c43 to disappear
Mar 24 14:50:38.345: INFO: Pod downwardapi-volume-e79d46ad-f0ed-45e5-8396-c8f1461d5c43 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:50:38.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6669" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2387,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:50:38.362: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:50:38.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9314 version'
Mar 24 14:50:38.645: INFO: stderr: ""
Mar 24 14:50:38.645: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.5\", GitCommit:\"6b1d87acf3c8253c123756b9e61dac642678305f\", GitTreeState:\"clean\", BuildDate:\"2021-03-18T01:10:43Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.5\", GitCommit:\"6b1d87acf3c8253c123756b9e61dac642678305f\", GitTreeState:\"clean\", BuildDate:\"2021-03-18T01:02:01Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:50:38.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9314" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":138,"skipped":2407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:50:38.665: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2518
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2518
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2518
Mar 24 14:50:38.982: INFO: Found 0 stateful pods, waiting for 1
Mar 24 14:50:48.999: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 24 14:50:49.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-2518 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 14:50:49.168: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 14:50:49.168: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 14:50:49.168: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 14:50:49.173: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 24 14:50:59.193: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 14:50:59.193: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 14:50:59.228: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999958s
Mar 24 14:51:00.237: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996628358s
Mar 24 14:51:01.244: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987885453s
Mar 24 14:51:02.253: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980227047s
Mar 24 14:51:03.263: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.971717673s
Mar 24 14:51:04.272: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.961570234s
Mar 24 14:51:05.281: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.952503409s
Mar 24 14:51:06.286: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.943495913s
Mar 24 14:51:07.295: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.93809981s
Mar 24 14:51:08.304: INFO: Verifying statefulset ss doesn't scale past 1 for another 929.926028ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2518
Mar 24 14:51:09.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-2518 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 14:51:09.486: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 14:51:09.486: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 14:51:09.486: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 14:51:09.491: INFO: Found 1 stateful pods, waiting for 3
Mar 24 14:51:19.512: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 14:51:19.512: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 14:51:19.512: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 24 14:51:19.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-2518 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 14:51:19.673: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 14:51:19.673: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 14:51:19.673: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 14:51:19.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-2518 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 14:51:19.873: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 14:51:19.873: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 14:51:19.873: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 14:51:19.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-2518 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 14:51:20.046: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 14:51:20.046: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 14:51:20.046: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 14:51:20.046: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 14:51:20.067: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 24 14:51:30.091: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 14:51:30.091: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 14:51:30.091: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 14:51:30.130: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999995s
Mar 24 14:51:31.200: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991243876s
Mar 24 14:51:32.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.921511335s
Mar 24 14:51:33.218: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.91242745s
Mar 24 14:51:34.228: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.903293039s
Mar 24 14:51:35.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.893903735s
Mar 24 14:51:36.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.88449513s
Mar 24 14:51:37.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.876695857s
Mar 24 14:51:38.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.869273552s
Mar 24 14:51:39.271: INFO: Verifying statefulset ss doesn't scale past 3 for another 859.42355ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2518
Mar 24 14:51:40.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-2518 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 14:51:40.444: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 14:51:40.444: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 14:51:40.444: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 14:51:40.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-2518 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 14:51:40.612: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 14:51:40.612: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 14:51:40.612: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 14:51:40.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-2518 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 14:51:40.948: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 14:51:40.948: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 14:51:40.948: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 14:51:40.948: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 14:52:01.005: INFO: Deleting all statefulset in ns statefulset-2518
Mar 24 14:52:01.008: INFO: Scaling statefulset ss to 0
Mar 24 14:52:01.020: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 14:52:01.023: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:52:01.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2518" for this suite.

• [SLOW TEST:82.395 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":139,"skipped":2432,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:52:01.065: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 14:52:01.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-509c46bf-bc90-4328-bf6a-c87b23437ea3" in namespace "projected-6087" to be "Succeeded or Failed"
Mar 24 14:52:01.309: INFO: Pod "downwardapi-volume-509c46bf-bc90-4328-bf6a-c87b23437ea3": Phase="Pending", Reason="", readiness=false. Elapsed: 42.302426ms
Mar 24 14:52:03.321: INFO: Pod "downwardapi-volume-509c46bf-bc90-4328-bf6a-c87b23437ea3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.053716712s
STEP: Saw pod success
Mar 24 14:52:03.321: INFO: Pod "downwardapi-volume-509c46bf-bc90-4328-bf6a-c87b23437ea3" satisfied condition "Succeeded or Failed"
Mar 24 14:52:03.324: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-509c46bf-bc90-4328-bf6a-c87b23437ea3 container client-container: <nil>
STEP: delete the pod
Mar 24 14:52:03.372: INFO: Waiting for pod downwardapi-volume-509c46bf-bc90-4328-bf6a-c87b23437ea3 to disappear
Mar 24 14:52:03.376: INFO: Pod downwardapi-volume-509c46bf-bc90-4328-bf6a-c87b23437ea3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:52:03.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6087" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":140,"skipped":2433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:52:03.409: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-4296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar 24 14:52:03.610: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar 24 14:52:03.620: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 24 14:52:03.621: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar 24 14:52:03.642: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 24 14:52:03.642: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar 24 14:52:03.671: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 24 14:52:03.672: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar 24 14:52:10.809: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:52:10.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4296" for this suite.

• [SLOW TEST:7.433 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":141,"skipped":2508,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:52:10.848: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 14:52:11.259: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 24 14:52:13.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194331, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194331, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194331, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194331, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 14:52:16.329: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:52:16.337: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4550-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:52:17.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-609" for this suite.
STEP: Destroying namespace "webhook-609-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.958 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":142,"skipped":2532,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:52:17.808: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1563
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Mar 24 14:52:20.045: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1563 PodName:pod-sharedvolume-432f7ffb-4c2c-47d5-9b55-5a752afc4c5f ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 14:52:20.045: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:52:20.115: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:52:20.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1563" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":143,"skipped":2538,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:52:20.136: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 24 14:52:20.371: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5600  73cb0a8e-b34c-4bcb-a899-8f2634a94b0b 15037 0 2021-03-24 14:52:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-24 14:52:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 14:52:20.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5600  73cb0a8e-b34c-4bcb-a899-8f2634a94b0b 15038 0 2021-03-24 14:52:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-24 14:52:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 24 14:52:20.397: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5600  73cb0a8e-b34c-4bcb-a899-8f2634a94b0b 15039 0 2021-03-24 14:52:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-24 14:52:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 14:52:20.397: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5600  73cb0a8e-b34c-4bcb-a899-8f2634a94b0b 15040 0 2021-03-24 14:52:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-24 14:52:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:52:20.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5600" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":144,"skipped":2542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:52:20.414: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 24 14:52:24.779: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 14:52:24.785: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 14:52:26.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 14:52:26.799: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 14:52:28.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 14:52:28.796: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 14:52:30.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 14:52:30.796: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 14:52:32.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 14:52:32.797: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 14:52:34.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 14:52:34.806: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 14:52:36.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 14:52:36.799: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:52:36.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-929" for this suite.

• [SLOW TEST:16.428 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2567,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:52:36.844: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4158
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:52:37.120: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 24 14:52:39.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-4158 --namespace=crd-publish-openapi-4158 create -f -'
Mar 24 14:52:40.969: INFO: stderr: ""
Mar 24 14:52:40.969: INFO: stdout: "e2e-test-crd-publish-openapi-3451-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 24 14:52:40.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-4158 --namespace=crd-publish-openapi-4158 delete e2e-test-crd-publish-openapi-3451-crds test-cr'
Mar 24 14:52:41.090: INFO: stderr: ""
Mar 24 14:52:41.091: INFO: stdout: "e2e-test-crd-publish-openapi-3451-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 24 14:52:41.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-4158 --namespace=crd-publish-openapi-4158 apply -f -'
Mar 24 14:52:41.452: INFO: stderr: ""
Mar 24 14:52:41.452: INFO: stdout: "e2e-test-crd-publish-openapi-3451-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 24 14:52:41.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-4158 --namespace=crd-publish-openapi-4158 delete e2e-test-crd-publish-openapi-3451-crds test-cr'
Mar 24 14:52:41.542: INFO: stderr: ""
Mar 24 14:52:41.542: INFO: stdout: "e2e-test-crd-publish-openapi-3451-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 24 14:52:41.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-4158 explain e2e-test-crd-publish-openapi-3451-crds'
Mar 24 14:52:41.885: INFO: stderr: ""
Mar 24 14:52:41.885: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3451-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:52:44.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4158" for this suite.

• [SLOW TEST:7.894 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":146,"skipped":2574,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:52:44.740: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-992
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 24 14:52:44.924: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 14:52:47.808: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:52:56.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-992" for this suite.

• [SLOW TEST:11.403 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":147,"skipped":2577,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:52:56.143: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:52:58.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5600" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":148,"skipped":2581,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:52:58.383: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8324
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:52:58.554: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 24 14:53:01.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 --namespace=crd-publish-openapi-8324 create -f -'
Mar 24 14:53:02.418: INFO: stderr: ""
Mar 24 14:53:02.418: INFO: stdout: "e2e-test-crd-publish-openapi-4403-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 24 14:53:02.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 --namespace=crd-publish-openapi-8324 delete e2e-test-crd-publish-openapi-4403-crds test-foo'
Mar 24 14:53:02.508: INFO: stderr: ""
Mar 24 14:53:02.508: INFO: stdout: "e2e-test-crd-publish-openapi-4403-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 24 14:53:02.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 --namespace=crd-publish-openapi-8324 apply -f -'
Mar 24 14:53:02.845: INFO: stderr: ""
Mar 24 14:53:02.845: INFO: stdout: "e2e-test-crd-publish-openapi-4403-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 24 14:53:02.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 --namespace=crd-publish-openapi-8324 delete e2e-test-crd-publish-openapi-4403-crds test-foo'
Mar 24 14:53:02.984: INFO: stderr: ""
Mar 24 14:53:02.984: INFO: stdout: "e2e-test-crd-publish-openapi-4403-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 24 14:53:02.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 --namespace=crd-publish-openapi-8324 create -f -'
Mar 24 14:53:03.240: INFO: rc: 1
Mar 24 14:53:03.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 --namespace=crd-publish-openapi-8324 apply -f -'
Mar 24 14:53:03.567: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 24 14:53:03.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 --namespace=crd-publish-openapi-8324 create -f -'
Mar 24 14:53:03.937: INFO: rc: 1
Mar 24 14:53:03.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 --namespace=crd-publish-openapi-8324 apply -f -'
Mar 24 14:53:04.288: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 24 14:53:04.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 explain e2e-test-crd-publish-openapi-4403-crds'
Mar 24 14:53:04.554: INFO: stderr: ""
Mar 24 14:53:04.554: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 24 14:53:04.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 explain e2e-test-crd-publish-openapi-4403-crds.metadata'
Mar 24 14:53:04.962: INFO: stderr: ""
Mar 24 14:53:04.962: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 24 14:53:04.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 explain e2e-test-crd-publish-openapi-4403-crds.spec'
Mar 24 14:53:05.350: INFO: stderr: ""
Mar 24 14:53:05.350: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 24 14:53:05.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 explain e2e-test-crd-publish-openapi-4403-crds.spec.bars'
Mar 24 14:53:05.688: INFO: stderr: ""
Mar 24 14:53:05.688: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 24 14:53:05.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-8324 explain e2e-test-crd-publish-openapi-4403-crds.spec.bars2'
Mar 24 14:53:06.065: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:53:08.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8324" for this suite.

• [SLOW TEST:10.614 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":149,"skipped":2598,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:53:08.998: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 14:53:09.265: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b574a63-5348-4092-b388-4fd947264865" in namespace "projected-9881" to be "Succeeded or Failed"
Mar 24 14:53:09.270: INFO: Pod "downwardapi-volume-2b574a63-5348-4092-b388-4fd947264865": Phase="Pending", Reason="", readiness=false. Elapsed: 4.492875ms
Mar 24 14:53:11.282: INFO: Pod "downwardapi-volume-2b574a63-5348-4092-b388-4fd947264865": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016598796s
STEP: Saw pod success
Mar 24 14:53:11.282: INFO: Pod "downwardapi-volume-2b574a63-5348-4092-b388-4fd947264865" satisfied condition "Succeeded or Failed"
Mar 24 14:53:11.286: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-2b574a63-5348-4092-b388-4fd947264865 container client-container: <nil>
STEP: delete the pod
Mar 24 14:53:11.338: INFO: Waiting for pod downwardapi-volume-2b574a63-5348-4092-b388-4fd947264865 to disappear
Mar 24 14:53:11.342: INFO: Pod downwardapi-volume-2b574a63-5348-4092-b388-4fd947264865 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:53:11.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9881" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2615,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:53:11.364: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 24 14:53:11.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3622 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Mar 24 14:53:11.741: INFO: stderr: ""
Mar 24 14:53:11.741: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Mar 24 14:53:11.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-3622 delete pods e2e-test-httpd-pod'
Mar 24 14:53:26.011: INFO: stderr: ""
Mar 24 14:53:26.011: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:53:26.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3622" for this suite.

• [SLOW TEST:14.711 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":151,"skipped":2632,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:53:26.076: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:53:26.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1374" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":152,"skipped":2645,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:53:26.352: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 14:53:27.025: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 14:53:30.132: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:53:40.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1053" for this suite.
STEP: Destroying namespace "webhook-1053-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.182 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":153,"skipped":2655,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:53:40.533: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6981
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6981
STEP: creating replication controller externalsvc in namespace services-6981
I0324 14:53:40.882540      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6981, replica count: 2
I0324 14:53:43.932839      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 24 14:53:44.024: INFO: Creating new exec pod
Mar 24 14:53:46.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-6981 exec execpodqqkv8 -- /bin/sh -x -c nslookup nodeport-service.services-6981.svc.cluster.local'
Mar 24 14:53:46.239: INFO: stderr: "+ nslookup nodeport-service.services-6981.svc.cluster.local\n"
Mar 24 14:53:46.239: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-6981.svc.cluster.local\tcanonical name = externalsvc.services-6981.svc.cluster.local.\nName:\texternalsvc.services-6981.svc.cluster.local\nAddress: 10.111.115.102\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6981, will wait for the garbage collector to delete the pods
Mar 24 14:53:46.309: INFO: Deleting ReplicationController externalsvc took: 15.453855ms
Mar 24 14:53:46.409: INFO: Terminating ReplicationController externalsvc pods took: 100.15906ms
Mar 24 14:53:50.584: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:53:50.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6981" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:10.116 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":154,"skipped":2656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:53:50.651: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar 24 14:53:50.849: INFO: Waiting up to 5m0s for pod "downward-api-836a7172-e437-4604-9023-c37920e05463" in namespace "downward-api-773" to be "Succeeded or Failed"
Mar 24 14:53:50.853: INFO: Pod "downward-api-836a7172-e437-4604-9023-c37920e05463": Phase="Pending", Reason="", readiness=false. Elapsed: 3.771649ms
Mar 24 14:53:52.865: INFO: Pod "downward-api-836a7172-e437-4604-9023-c37920e05463": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016109529s
STEP: Saw pod success
Mar 24 14:53:52.865: INFO: Pod "downward-api-836a7172-e437-4604-9023-c37920e05463" satisfied condition "Succeeded or Failed"
Mar 24 14:53:52.869: INFO: Trying to get logs from node talos-default-worker-1 pod downward-api-836a7172-e437-4604-9023-c37920e05463 container dapi-container: <nil>
STEP: delete the pod
Mar 24 14:53:52.906: INFO: Waiting for pod downward-api-836a7172-e437-4604-9023-c37920e05463 to disappear
Mar 24 14:53:52.910: INFO: Pod downward-api-836a7172-e437-4604-9023-c37920e05463 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:53:52.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-773" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2678,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:53:52.928: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-7222ee88-a1cd-4806-b239-0417f3db4d23
STEP: Creating a pod to test consume secrets
Mar 24 14:53:53.156: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7f283ac-4ab9-47cc-ad79-a180b874537e" in namespace "projected-2890" to be "Succeeded or Failed"
Mar 24 14:53:53.161: INFO: Pod "pod-projected-secrets-e7f283ac-4ab9-47cc-ad79-a180b874537e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278622ms
Mar 24 14:53:55.173: INFO: Pod "pod-projected-secrets-e7f283ac-4ab9-47cc-ad79-a180b874537e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016465451s
STEP: Saw pod success
Mar 24 14:53:55.173: INFO: Pod "pod-projected-secrets-e7f283ac-4ab9-47cc-ad79-a180b874537e" satisfied condition "Succeeded or Failed"
Mar 24 14:53:55.176: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-secrets-e7f283ac-4ab9-47cc-ad79-a180b874537e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 14:53:55.216: INFO: Waiting for pod pod-projected-secrets-e7f283ac-4ab9-47cc-ad79-a180b874537e to disappear
Mar 24 14:53:55.220: INFO: Pod pod-projected-secrets-e7f283ac-4ab9-47cc-ad79-a180b874537e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:53:55.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2890" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":156,"skipped":2678,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:53:55.243: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:53:55.434: INFO: Creating deployment "webserver-deployment"
Mar 24 14:53:55.443: INFO: Waiting for observed generation 1
Mar 24 14:53:57.464: INFO: Waiting for all required pods to come up
Mar 24 14:53:57.479: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 24 14:53:59.503: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 24 14:53:59.511: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 24 14:53:59.540: INFO: Updating deployment webserver-deployment
Mar 24 14:53:59.540: INFO: Waiting for observed generation 2
Mar 24 14:54:01.600: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 24 14:54:01.604: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 24 14:54:01.608: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 24 14:54:01.619: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 24 14:54:01.619: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 24 14:54:01.623: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 24 14:54:01.630: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 24 14:54:01.630: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 24 14:54:01.647: INFO: Updating deployment webserver-deployment
Mar 24 14:54:01.647: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 24 14:54:01.661: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 24 14:54:01.666: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 14:54:03.711: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4319  6ade48cc-c5f5-4926-96d6-c75f57b3b88e 16038 3 2021-03-24 14:53:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-24 14:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 14:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004347148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-24 14:54:01 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-03-24 14:54:01 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 24 14:54:03.716: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-4319  50b90bc8-b2d7-41e4-b843-f36de48a7478 16035 3 2021-03-24 14:53:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6ade48cc-c5f5-4926-96d6-c75f57b3b88e 0xc0042f4ae7 0xc0042f4ae8}] []  [{kube-controller-manager Update apps/v1 2021-03-24 14:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ade48cc-c5f5-4926-96d6-c75f57b3b88e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042f4b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 14:54:03.716: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 24 14:54:03.716: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-4319  810a1418-3a54-4b96-ac26-02bd9b5b37e6 16019 3 2021-03-24 14:53:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6ade48cc-c5f5-4926-96d6-c75f57b3b88e 0xc0042f4c07 0xc0042f4c08}] []  [{kube-controller-manager Update apps/v1 2021-03-24 14:53:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ade48cc-c5f5-4926-96d6-c75f57b3b88e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042f4d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 24 14:54:03.730: INFO: Pod "webserver-deployment-795d758f88-22fvk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-22fvk webserver-deployment-795d758f88- deployment-4319  6be85dc8-ccd6-4a80-9144-f0a2ac45f282 16013 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc004347767 0xc004347768}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.730: INFO: Pod "webserver-deployment-795d758f88-67zqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-67zqz webserver-deployment-795d758f88- deployment-4319  444bd356-f586-4e3d-a7e2-5aa28e62f126 15938 0 2021-03-24 14:53:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc0043479a0 0xc0043479a1}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:10.244.2.175,StartTime:2021-03-24 14:53:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.730: INFO: Pod "webserver-deployment-795d758f88-6ggl6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6ggl6 webserver-deployment-795d758f88- deployment-4319  22d2855a-6a1a-4802-9fe4-766545f72152 16044 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc004347cb0 0xc004347cb1}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.731: INFO: Pod "webserver-deployment-795d758f88-9b6qh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9b6qh webserver-deployment-795d758f88- deployment-4319  cfce8860-5f5c-4b8a-ac88-0b1726b3e36d 15948 0 2021-03-24 14:53:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc004347e90 0xc004347e91}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:10.244.3.58,StartTime:2021-03-24 14:53:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.731: INFO: Pod "webserver-deployment-795d758f88-9qtq8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9qtq8 webserver-deployment-795d758f88- deployment-4319  8868a0f0-a95d-4ced-b41b-9304d22e85c6 16015 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc0043a20f0 0xc0043a20f1}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.731: INFO: Pod "webserver-deployment-795d758f88-c2m4s" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c2m4s webserver-deployment-795d758f88- deployment-4319  3f90c17c-09a6-4ef0-9d47-d9c4b5c624ac 15953 0 2021-03-24 14:53:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc0043a2290 0xc0043a2291}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:10.244.3.59,StartTime:2021-03-24 14:54:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.731: INFO: Pod "webserver-deployment-795d758f88-fcs9c" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fcs9c webserver-deployment-795d758f88- deployment-4319  712abab6-f7a2-428c-b0a9-f8106ea32379 16016 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc0043a2540 0xc0043a2541}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.732: INFO: Pod "webserver-deployment-795d758f88-fzxt5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fzxt5 webserver-deployment-795d758f88- deployment-4319  28495fa6-c14f-420c-bf22-2107830aa118 15941 0 2021-03-24 14:53:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc0043a26d0 0xc0043a26d1}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:10.244.2.174,StartTime:2021-03-24 14:53:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.732: INFO: Pod "webserver-deployment-795d758f88-hfcm4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hfcm4 webserver-deployment-795d758f88- deployment-4319  1b426f22-7997-4ed2-a9cf-48e6b633e4b6 16082 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc0043a2a10 0xc0043a2a11}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.733: INFO: Pod "webserver-deployment-795d758f88-j5xrv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-j5xrv webserver-deployment-795d758f88- deployment-4319  fc56a145-b4fd-435d-a68d-75d1ccb08b8b 15951 0 2021-03-24 14:53:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc0043a2c00 0xc0043a2c01}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:10.244.3.57,StartTime:2021-03-24 14:53:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.734: INFO: Pod "webserver-deployment-795d758f88-lvcx5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lvcx5 webserver-deployment-795d758f88- deployment-4319  e987a0ba-a0ac-4e4b-8cce-86274c1440e9 16062 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc0043a2e30 0xc0043a2e31}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.734: INFO: Pod "webserver-deployment-795d758f88-m8n6d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m8n6d webserver-deployment-795d758f88- deployment-4319  029c363f-21d9-4c58-a7d7-a7c4587df992 16030 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc0043a30b0 0xc0043a30b1}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.734: INFO: Pod "webserver-deployment-795d758f88-qmwcl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qmwcl webserver-deployment-795d758f88- deployment-4319  a1d7833d-18d9-47b6-ae74-a516738976be 16025 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 50b90bc8-b2d7-41e4-b843-f36de48a7478 0xc0043a32c0 0xc0043a32c1}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50b90bc8-b2d7-41e4-b843-f36de48a7478\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.735: INFO: Pod "webserver-deployment-dd94f59b7-295l5" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-295l5 webserver-deployment-dd94f59b7- deployment-4319  574a5c57-7f33-4c7c-b0ea-d6efb5c5919f 16042 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043a34a0 0xc0043a34a1}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.735: INFO: Pod "webserver-deployment-dd94f59b7-46hnw" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-46hnw webserver-deployment-dd94f59b7- deployment-4319  c851c429-e765-4b4c-95b4-b01a33725a5a 15828 0 2021-03-24 14:53:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043a3740 0xc0043a3741}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:53:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:10.244.3.55,StartTime:2021-03-24 14:53:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:53:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://711e5740f3a685baad82eac954affc24933de6cb52d784ea544c441cceb3e516,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.735: INFO: Pod "webserver-deployment-dd94f59b7-4gmhb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4gmhb webserver-deployment-dd94f59b7- deployment-4319  b969858d-b1a8-4fde-a54b-5bd11d254b50 16071 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043a3a80 0xc0043a3a81}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.736: INFO: Pod "webserver-deployment-dd94f59b7-6ddtl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6ddtl webserver-deployment-dd94f59b7- deployment-4319  5d51a9cd-a1df-4a9a-9d9d-25f82cf2c9b1 15807 0 2021-03-24 14:53:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043a3d30 0xc0043a3d31}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:53:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:10.244.2.170,StartTime:2021-03-24 14:53:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:53:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e9d615122cb6b291d0f6d1bf87d96db98c871e657674503c54e57b3db4151f5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.736: INFO: Pod "webserver-deployment-dd94f59b7-6smvh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6smvh webserver-deployment-dd94f59b7- deployment-4319  38021834-495f-4724-92fa-436f72c6140a 16079 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043a3f90 0xc0043a3f91}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.737: INFO: Pod "webserver-deployment-dd94f59b7-9zzqf" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9zzqf webserver-deployment-dd94f59b7- deployment-4319  fff56c6e-1078-4655-b531-1e8223ac38d1 15818 0 2021-03-24 14:53:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043ca1b0 0xc0043ca1b1}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:53:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:10.244.3.52,StartTime:2021-03-24 14:53:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:53:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ace8845a30b530dbcb9d5e2f60c66fbd8d9d174765b6889473e0e9d5c126f038,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.737: INFO: Pod "webserver-deployment-dd94f59b7-bvt6l" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bvt6l webserver-deployment-dd94f59b7- deployment-4319  fbdbf26d-7ec5-442c-b2f4-6f84fe729215 15975 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043ca420 0xc0043ca421}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.738: INFO: Pod "webserver-deployment-dd94f59b7-ct8sf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ct8sf webserver-deployment-dd94f59b7- deployment-4319  ca496c4b-ce43-482b-8f82-869b000b19e4 16048 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043ca670 0xc0043ca671}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.738: INFO: Pod "webserver-deployment-dd94f59b7-d79st" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d79st webserver-deployment-dd94f59b7- deployment-4319  78eeaad0-04c2-4013-8881-aa1899ce43ab 15835 0 2021-03-24 14:53:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043ca860 0xc0043ca861}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:53:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:10.244.2.172,StartTime:2021-03-24 14:53:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:53:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://89e9788864c6956bd87e87fc1ea92533f384dc9c38d2d6ecabd6d8001ee853e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.738: INFO: Pod "webserver-deployment-dd94f59b7-d7zq8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d7zq8 webserver-deployment-dd94f59b7- deployment-4319  bc3c01f4-ee47-4c82-9c51-9413154cb683 16064 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043caa10 0xc0043caa11}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.749: INFO: Pod "webserver-deployment-dd94f59b7-jtx2s" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jtx2s webserver-deployment-dd94f59b7- deployment-4319  ed854db3-5978-47fd-89fc-e555efd1bfcd 15821 0 2021-03-24 14:53:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043cac30 0xc0043cac31}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:53:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:10.244.2.173,StartTime:2021-03-24 14:53:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:53:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ee97e08082128ecf163a263c727edb3fb4440a6613e54492ae581c47a5235596,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.749: INFO: Pod "webserver-deployment-dd94f59b7-kjtsw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kjtsw webserver-deployment-dd94f59b7- deployment-4319  ea05100e-76a8-49a3-9cf5-17ca314a1e0b 16017 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043caea0 0xc0043caea1}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.749: INFO: Pod "webserver-deployment-dd94f59b7-mq5mg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mq5mg webserver-deployment-dd94f59b7- deployment-4319  73ef3f71-ccfb-488f-8ab4-5d8c0ab99548 16040 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043cb080 0xc0043cb081}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.749: INFO: Pod "webserver-deployment-dd94f59b7-q7dsc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-q7dsc webserver-deployment-dd94f59b7- deployment-4319  630ef7fa-63bf-4fcc-8ea7-b7c2c71c8b04 15838 0 2021-03-24 14:53:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043cb300 0xc0043cb301}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:53:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:10.244.3.56,StartTime:2021-03-24 14:53:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:53:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://470ff0aed5acd1b05d40de473dbc731f6afe3987b224f3c2f661301ff8f321b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.750: INFO: Pod "webserver-deployment-dd94f59b7-r8c2g" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-r8c2g webserver-deployment-dd94f59b7- deployment-4319  c071341e-19c4-4611-baef-546ea2f209fb 15848 0 2021-03-24 14:53:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043cb580 0xc0043cb581}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:53:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:10.244.2.171,StartTime:2021-03-24 14:53:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:53:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a8ddbd245f72d6e6a14c38a2493551931e9310ebb8219f10748de19b6636d213,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.750: INFO: Pod "webserver-deployment-dd94f59b7-rlz8g" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rlz8g webserver-deployment-dd94f59b7- deployment-4319  5515bf5b-e8c3-44d9-89e3-3cbc9445f784 15809 0 2021-03-24 14:53:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043cb820 0xc0043cb821}] []  [{kube-controller-manager Update v1 2021-03-24 14:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:53:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:53:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:10.244.2.169,StartTime:2021-03-24 14:53:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 14:53:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ca961e73fa943f9ad972811ef98500b4ce111212f1497c68f51cdd8ee507d475,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.750: INFO: Pod "webserver-deployment-dd94f59b7-skh74" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-skh74 webserver-deployment-dd94f59b7- deployment-4319  e027fb2b-d074-4676-9549-c7f5fde92f47 16036 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043cba60 0xc0043cba61}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.750: INFO: Pod "webserver-deployment-dd94f59b7-vgtpw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vgtpw webserver-deployment-dd94f59b7- deployment-4319  4854ad5c-3e8b-4f20-a1af-737ed90a5036 16039 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043cbc90 0xc0043cbc91}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.750: INFO: Pod "webserver-deployment-dd94f59b7-xbzzn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xbzzn webserver-deployment-dd94f59b7- deployment-4319  b20bbe29-0e51-4638-8eea-5944cc431f77 16072 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043cbea0 0xc0043cbea1}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 14:54:03.750: INFO: Pod "webserver-deployment-dd94f59b7-zqvkv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zqvkv webserver-deployment-dd94f59b7- deployment-4319  b23c55ee-8a0c-4ee7-92b1-99089144b898 16000 0 2021-03-24 14:54:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 810a1418-3a54-4b96-ac26-02bd9b5b37e6 0xc0043f8140 0xc0043f8141}] []  [{kube-controller-manager Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"810a1418-3a54-4b96-ac26-02bd9b5b37e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 14:54:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2fx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2fx49,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2fx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 14:54:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.6,PodIP:,StartTime:2021-03-24 14:54:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:54:03.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4319" for this suite.

• [SLOW TEST:8.534 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":157,"skipped":2727,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:54:03.777: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8398
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-054e7c90-5164-40c5-8e9e-3f32ee6f2701
STEP: Creating a pod to test consume configMaps
Mar 24 14:54:04.026: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b6a9f3ac-3955-48fb-b61e-f0406183e31e" in namespace "projected-8398" to be "Succeeded or Failed"
Mar 24 14:54:04.030: INFO: Pod "pod-projected-configmaps-b6a9f3ac-3955-48fb-b61e-f0406183e31e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034336ms
Mar 24 14:54:06.043: INFO: Pod "pod-projected-configmaps-b6a9f3ac-3955-48fb-b61e-f0406183e31e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017769959s
Mar 24 14:54:08.053: INFO: Pod "pod-projected-configmaps-b6a9f3ac-3955-48fb-b61e-f0406183e31e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027193878s
Mar 24 14:54:10.064: INFO: Pod "pod-projected-configmaps-b6a9f3ac-3955-48fb-b61e-f0406183e31e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038371587s
STEP: Saw pod success
Mar 24 14:54:10.064: INFO: Pod "pod-projected-configmaps-b6a9f3ac-3955-48fb-b61e-f0406183e31e" satisfied condition "Succeeded or Failed"
Mar 24 14:54:10.067: INFO: Trying to get logs from node talos-default-worker-2 pod pod-projected-configmaps-b6a9f3ac-3955-48fb-b61e-f0406183e31e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 14:54:10.148: INFO: Waiting for pod pod-projected-configmaps-b6a9f3ac-3955-48fb-b61e-f0406183e31e to disappear
Mar 24 14:54:10.161: INFO: Pod pod-projected-configmaps-b6a9f3ac-3955-48fb-b61e-f0406183e31e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:54:10.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8398" for this suite.

• [SLOW TEST:6.406 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":2736,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:54:10.184: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7918
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 24 14:54:10.794: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 24 14:54:12.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194450, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194450, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194450, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194450, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 14:54:15.943: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:54:15.954: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:54:17.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7918" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.145 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":159,"skipped":2762,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:54:17.331: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 24 14:54:17.694: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:17.695: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:17.695: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:17.719: INFO: Number of nodes with available pods: 0
Mar 24 14:54:17.719: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:54:18.742: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:18.742: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:18.742: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:18.752: INFO: Number of nodes with available pods: 0
Mar 24 14:54:18.752: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:54:19.728: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:19.728: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:19.728: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:19.733: INFO: Number of nodes with available pods: 2
Mar 24 14:54:19.733: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 24 14:54:19.779: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:19.779: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:19.779: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:19.783: INFO: Number of nodes with available pods: 1
Mar 24 14:54:19.783: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:54:20.791: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:20.791: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:20.791: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:20.796: INFO: Number of nodes with available pods: 1
Mar 24 14:54:20.796: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:54:21.792: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:21.792: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:21.792: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:21.798: INFO: Number of nodes with available pods: 1
Mar 24 14:54:21.798: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:54:22.793: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:22.793: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:22.793: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:22.802: INFO: Number of nodes with available pods: 1
Mar 24 14:54:22.802: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:54:23.801: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:23.801: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:23.801: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:23.807: INFO: Number of nodes with available pods: 1
Mar 24 14:54:23.807: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:54:24.799: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:24.799: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:24.799: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:24.802: INFO: Number of nodes with available pods: 1
Mar 24 14:54:24.802: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:54:25.792: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:25.792: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:25.793: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:25.797: INFO: Number of nodes with available pods: 1
Mar 24 14:54:25.797: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:54:26.795: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:26.795: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:26.795: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:26.802: INFO: Number of nodes with available pods: 1
Mar 24 14:54:26.802: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 14:54:27.793: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:27.793: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:27.793: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 14:54:27.797: INFO: Number of nodes with available pods: 2
Mar 24 14:54:27.798: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9128, will wait for the garbage collector to delete the pods
Mar 24 14:54:27.977: INFO: Deleting DaemonSet.extensions daemon-set took: 29.544076ms
Mar 24 14:54:28.077: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.259785ms
Mar 24 14:54:37.974: INFO: Number of nodes with available pods: 0
Mar 24 14:54:37.974: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 14:54:37.977: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16590"},"items":null}

Mar 24 14:54:37.981: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16590"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:54:37.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9128" for this suite.

• [SLOW TEST:20.688 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":160,"skipped":2783,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:54:38.021: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-6903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar 24 14:54:38.211: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 14:55:38.237: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:55:38.241: INFO: Starting informer...
STEP: Starting pods...
Mar 24 14:55:38.491: INFO: Pod1 is running on talos-default-worker-1. Tainting Node
Mar 24 14:55:40.731: INFO: Pod2 is running on talos-default-worker-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 24 14:55:56.033: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 24 14:56:16.020: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:56:16.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6903" for this suite.

• [SLOW TEST:98.076 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":161,"skipped":2798,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:56:16.099: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5511
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5511
STEP: Creating statefulset with conflicting port in namespace statefulset-5511
STEP: Waiting until pod test-pod will start running in namespace statefulset-5511
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5511
Mar 24 14:56:18.446: INFO: Observed stateful pod in namespace: statefulset-5511, name: ss-0, uid: a0875e78-fb22-42a6-8735-b92f54161b47, status phase: Pending. Waiting for statefulset controller to delete.
Mar 24 14:56:18.984: INFO: Observed stateful pod in namespace: statefulset-5511, name: ss-0, uid: a0875e78-fb22-42a6-8735-b92f54161b47, status phase: Failed. Waiting for statefulset controller to delete.
Mar 24 14:56:19.014: INFO: Observed stateful pod in namespace: statefulset-5511, name: ss-0, uid: a0875e78-fb22-42a6-8735-b92f54161b47, status phase: Failed. Waiting for statefulset controller to delete.
Mar 24 14:56:19.027: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5511
STEP: Removing pod with conflicting port in namespace statefulset-5511
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5511 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 14:56:21.129: INFO: Deleting all statefulset in ns statefulset-5511
Mar 24 14:56:21.133: INFO: Scaling statefulset ss to 0
Mar 24 14:56:31.174: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 14:56:31.179: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:56:31.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5511" for this suite.

• [SLOW TEST:15.126 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":162,"skipped":2801,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:56:31.226: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5308
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:56:48.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5308" for this suite.

• [SLOW TEST:17.329 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":163,"skipped":2819,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:56:48.561: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-8784
STEP: creating service affinity-nodeport in namespace services-8784
STEP: creating replication controller affinity-nodeport in namespace services-8784
I0324 14:56:48.871407      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-8784, replica count: 3
I0324 14:56:51.922265      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 14:56:51.943: INFO: Creating new exec pod
Mar 24 14:56:54.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-8784 exec execpod-affinityz2hcp -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Mar 24 14:56:55.170: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 24 14:56:55.170: INFO: stdout: ""
Mar 24 14:56:55.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-8784 exec execpod-affinityz2hcp -- /bin/sh -x -c nc -zv -t -w 2 10.99.188.84 80'
Mar 24 14:56:55.314: INFO: stderr: "+ nc -zv -t -w 2 10.99.188.84 80\nConnection to 10.99.188.84 80 port [tcp/http] succeeded!\n"
Mar 24 14:56:55.314: INFO: stdout: ""
Mar 24 14:56:55.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-8784 exec execpod-affinityz2hcp -- /bin/sh -x -c nc -zv -t -w 2 10.5.0.5 30693'
Mar 24 14:56:55.545: INFO: stderr: "+ nc -zv -t -w 2 10.5.0.5 30693\nConnection to 10.5.0.5 30693 port [tcp/30693] succeeded!\n"
Mar 24 14:56:55.545: INFO: stdout: ""
Mar 24 14:56:55.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-8784 exec execpod-affinityz2hcp -- /bin/sh -x -c nc -zv -t -w 2 10.5.0.6 30693'
Mar 24 14:56:55.702: INFO: stderr: "+ nc -zv -t -w 2 10.5.0.6 30693\nConnection to 10.5.0.6 30693 port [tcp/30693] succeeded!\n"
Mar 24 14:56:55.702: INFO: stdout: ""
Mar 24 14:56:55.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-8784 exec execpod-affinityz2hcp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.5.0.5:30693/ ; done'
Mar 24 14:56:55.942: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.5.0.5:30693/\n"
Mar 24 14:56:55.942: INFO: stdout: "\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67\naffinity-nodeport-khn67"
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Received response from host: affinity-nodeport-khn67
Mar 24 14:56:55.942: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8784, will wait for the garbage collector to delete the pods
Mar 24 14:56:56.046: INFO: Deleting ReplicationController affinity-nodeport took: 11.024875ms
Mar 24 14:56:56.646: INFO: Terminating ReplicationController affinity-nodeport pods took: 600.1559ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:57:00.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8784" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:12.280 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":164,"skipped":2822,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:57:00.844: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-37ae8f54-873f-4f0f-9c98-e407620303f0
STEP: Creating a pod to test consume secrets
Mar 24 14:57:01.068: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ec205635-e090-4330-8a63-a16a4faae764" in namespace "projected-8158" to be "Succeeded or Failed"
Mar 24 14:57:01.073: INFO: Pod "pod-projected-secrets-ec205635-e090-4330-8a63-a16a4faae764": Phase="Pending", Reason="", readiness=false. Elapsed: 4.680428ms
Mar 24 14:57:03.087: INFO: Pod "pod-projected-secrets-ec205635-e090-4330-8a63-a16a4faae764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018670224s
STEP: Saw pod success
Mar 24 14:57:03.087: INFO: Pod "pod-projected-secrets-ec205635-e090-4330-8a63-a16a4faae764" satisfied condition "Succeeded or Failed"
Mar 24 14:57:03.091: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-secrets-ec205635-e090-4330-8a63-a16a4faae764 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 14:57:03.133: INFO: Waiting for pod pod-projected-secrets-ec205635-e090-4330-8a63-a16a4faae764 to disappear
Mar 24 14:57:03.136: INFO: Pod pod-projected-secrets-ec205635-e090-4330-8a63-a16a4faae764 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:57:03.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8158" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":165,"skipped":2875,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:57:03.152: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:57:05.352: INFO: Deleting pod "var-expansion-c2f1c98c-ba45-4670-9761-e95fce2257e3" in namespace "var-expansion-8768"
Mar 24 14:57:05.375: INFO: Wait up to 5m0s for pod "var-expansion-c2f1c98c-ba45-4670-9761-e95fce2257e3" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:57:07.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8768" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":166,"skipped":2876,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:57:07.416: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-btnp
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 14:57:07.635: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-btnp" in namespace "subpath-8519" to be "Succeeded or Failed"
Mar 24 14:57:07.639: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.717665ms
Mar 24 14:57:09.649: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Running", Reason="", readiness=true. Elapsed: 2.014622137s
Mar 24 14:57:11.661: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Running", Reason="", readiness=true. Elapsed: 4.026404636s
Mar 24 14:57:13.675: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Running", Reason="", readiness=true. Elapsed: 6.039966401s
Mar 24 14:57:15.689: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Running", Reason="", readiness=true. Elapsed: 8.054214933s
Mar 24 14:57:17.702: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Running", Reason="", readiness=true. Elapsed: 10.067516861s
Mar 24 14:57:19.711: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Running", Reason="", readiness=true. Elapsed: 12.075940817s
Mar 24 14:57:21.722: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Running", Reason="", readiness=true. Elapsed: 14.087652242s
Mar 24 14:57:23.733: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Running", Reason="", readiness=true. Elapsed: 16.098376538s
Mar 24 14:57:25.746: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Running", Reason="", readiness=true. Elapsed: 18.111100999s
Mar 24 14:57:27.758: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Running", Reason="", readiness=true. Elapsed: 20.123298616s
Mar 24 14:57:29.767: INFO: Pod "pod-subpath-test-configmap-btnp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.132594415s
STEP: Saw pod success
Mar 24 14:57:29.767: INFO: Pod "pod-subpath-test-configmap-btnp" satisfied condition "Succeeded or Failed"
Mar 24 14:57:29.771: INFO: Trying to get logs from node talos-default-worker-1 pod pod-subpath-test-configmap-btnp container test-container-subpath-configmap-btnp: <nil>
STEP: delete the pod
Mar 24 14:57:29.826: INFO: Waiting for pod pod-subpath-test-configmap-btnp to disappear
Mar 24 14:57:29.841: INFO: Pod pod-subpath-test-configmap-btnp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-btnp
Mar 24 14:57:29.841: INFO: Deleting pod "pod-subpath-test-configmap-btnp" in namespace "subpath-8519"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:57:29.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8519" for this suite.

• [SLOW TEST:22.449 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":167,"skipped":2881,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:57:29.865: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0324 14:57:31.656879      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 14:57:33.682: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:57:33.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2252" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":168,"skipped":2885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:57:33.712: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 14:57:34.174: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 24 14:57:36.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194654, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194654, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194654, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752194653, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 14:57:39.246: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:57:51.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3009" for this suite.
STEP: Destroying namespace "webhook-3009-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.955 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":169,"skipped":2936,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:57:51.668: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar 24 14:57:54.517: INFO: Successfully updated pod "labelsupdatec3c1933f-5b77-4294-95ca-cb056a992c3d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:57:58.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5754" for this suite.

• [SLOW TEST:6.904 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":170,"skipped":2951,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:57:58.577: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:57:58.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2416 create -f -'
Mar 24 14:57:59.280: INFO: stderr: ""
Mar 24 14:57:59.280: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 24 14:57:59.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2416 create -f -'
Mar 24 14:57:59.677: INFO: stderr: ""
Mar 24 14:57:59.677: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 24 14:58:00.688: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 14:58:00.688: INFO: Found 1 / 1
Mar 24 14:58:00.688: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 24 14:58:00.693: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 14:58:00.693: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 24 14:58:00.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2416 describe pod agnhost-primary-h24dm'
Mar 24 14:58:00.780: INFO: stderr: ""
Mar 24 14:58:00.780: INFO: stdout: "Name:         agnhost-primary-h24dm\nNamespace:    kubectl-2416\nPriority:     0\nNode:         talos-default-worker-1/10.5.0.5\nStart Time:   Wed, 24 Mar 2021 14:57:59 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.244.2.200\nIPs:\n  IP:           10.244.2.200\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://0cf264309727bbf2d33d6fda4f88b792e901f32355eb36bfa9a37e052fc042dc\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 24 Mar 2021 14:58:00 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n29s7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-n29s7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-n29s7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2416/agnhost-primary-h24dm to talos-default-worker-1\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Mar 24 14:58:00.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2416 describe rc agnhost-primary'
Mar 24 14:58:00.906: INFO: stderr: ""
Mar 24 14:58:00.906: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2416\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-h24dm\n"
Mar 24 14:58:00.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2416 describe service agnhost-primary'
Mar 24 14:58:01.002: INFO: stderr: ""
Mar 24 14:58:01.002: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2416\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.109.113.100\nIPs:               10.109.113.100\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.2.200:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 24 14:58:01.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2416 describe node talos-default-master-1'
Mar 24 14:58:01.120: INFO: stderr: ""
Mar 24 14:58:01.120: INFO: stdout: "Name:               talos-default-master-1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=talos-default-master-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"ba:88:6b:e6:78:57\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.5.0.2\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 24 Mar 2021 14:10:42 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  talos-default-master-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 24 Mar 2021 14:57:58 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 24 Mar 2021 14:11:42 +0000   Wed, 24 Mar 2021 14:11:42 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 24 Mar 2021 14:56:58 +0000   Wed, 24 Mar 2021 14:10:42 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 24 Mar 2021 14:56:58 +0000   Wed, 24 Mar 2021 14:10:42 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 24 Mar 2021 14:56:58 +0000   Wed, 24 Mar 2021 14:10:42 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 24 Mar 2021 14:56:58 +0000   Wed, 24 Mar 2021 14:11:33 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.5.0.2\n  Hostname:    talos-default-master-1\nCapacity:\n  cpu:                3\n  ephemeral-storage:  5767132Ki\n  hugepages-2Mi:      0\n  memory:             2979916Ki\n  pods:               110\nAllocatable:\n  cpu:                3\n  ephemeral-storage:  5314988843\n  hugepages-2Mi:      0\n  memory:             2877516Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 a8eb6cac33e701ae867269db5ce80e7f\n  System UUID:                0cc8a260-e3ee-40f4-bf87-d5eacc1a03de\n  Boot ID:                    67828725-46b8-44dc-9a87-4055b624a8ec\n  Kernel Version:             5.10.23-talos\n  OS Image:                   Talos (v0.9.0)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.4\n  Kubelet Version:            v1.20.5\n  Kube-Proxy Version:         v1.20.5\nPodCIDR:                      10.244.4.0/24\nPodCIDRs:                     10.244.4.0/24\nNon-terminated Pods:          (5 in total)\n  Namespace                   Name                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                              ------------  ----------  ---------------  -------------  ---\n  kube-system                 kube-apiserver-talos-default-master-1             0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  kube-system                 kube-controller-manager-talos-default-master-1    0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  kube-system                 kube-flannel-69trb                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\n  kube-system                 kube-proxy-wpm2f                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\n  kube-system                 kube-scheduler-talos-default-master-1             0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\n  hugepages-2Mi      0 (0%)    0 (0%)\nEvents:\n  Type    Reason    Age   From        Message\n  ----    ------    ----  ----        -------\n  Normal  Starting  46m   kube-proxy  Starting kube-proxy.\n"
Mar 24 14:58:01.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2416 describe namespace kubectl-2416'
Mar 24 14:58:01.207: INFO: stderr: ""
Mar 24 14:58:01.207: INFO: stdout: "Name:         kubectl-2416\nLabels:       e2e-framework=kubectl\n              e2e-run=4da38307-b270-4025-b811-f34998ca1f3a\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:58:01.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2416" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":171,"skipped":2962,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:58:01.228: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:58:03.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7077" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":172,"skipped":2966,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:58:03.587: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-397
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 14:58:03.778: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Creating first CR 
Mar 24 14:58:04.407: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T14:58:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T14:58:04Z]] name:name1 resourceVersion:17669 uid:ab0624a5-9265-43cd-8dfc-644c1cadd260] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 24 14:58:14.434: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T14:58:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T14:58:14Z]] name:name2 resourceVersion:17726 uid:af70aadd-d238-40c8-8f5a-a0b960d172f2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 24 14:58:24.461: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T14:58:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T14:58:24Z]] name:name1 resourceVersion:17748 uid:ab0624a5-9265-43cd-8dfc-644c1cadd260] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 24 14:58:34.494: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T14:58:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T14:58:34Z]] name:name2 resourceVersion:17767 uid:af70aadd-d238-40c8-8f5a-a0b960d172f2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 24 14:58:44.525: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T14:58:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T14:58:24Z]] name:name1 resourceVersion:17786 uid:ab0624a5-9265-43cd-8dfc-644c1cadd260] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 24 14:58:54.553: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T14:58:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T14:58:34Z]] name:name2 resourceVersion:17805 uid:af70aadd-d238-40c8-8f5a-a0b960d172f2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 14:59:05.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-397" for this suite.

• [SLOW TEST:61.545 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":173,"skipped":2969,"failed":0}
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 14:59:05.132: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4715
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-153c51b4-e461-4ad1-b676-63940e1c6b23 in namespace container-probe-4715
Mar 24 14:59:07.480: INFO: Started pod busybox-153c51b4-e461-4ad1-b676-63940e1c6b23 in namespace container-probe-4715
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 14:59:07.484: INFO: Initial restart count of pod busybox-153c51b4-e461-4ad1-b676-63940e1c6b23 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:03:09.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4715" for this suite.

• [SLOW TEST:243.921 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":174,"skipped":2969,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:03:09.054: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Mar 24 15:03:09.266: INFO: Waiting up to 5m0s for pod "var-expansion-6c709ad4-5b5a-4142-be61-b2f4b7cb813e" in namespace "var-expansion-2442" to be "Succeeded or Failed"
Mar 24 15:03:09.271: INFO: Pod "var-expansion-6c709ad4-5b5a-4142-be61-b2f4b7cb813e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.851314ms
Mar 24 15:03:11.307: INFO: Pod "var-expansion-6c709ad4-5b5a-4142-be61-b2f4b7cb813e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041525047s
STEP: Saw pod success
Mar 24 15:03:11.307: INFO: Pod "var-expansion-6c709ad4-5b5a-4142-be61-b2f4b7cb813e" satisfied condition "Succeeded or Failed"
Mar 24 15:03:11.311: INFO: Trying to get logs from node talos-default-worker-1 pod var-expansion-6c709ad4-5b5a-4142-be61-b2f4b7cb813e container dapi-container: <nil>
STEP: delete the pod
Mar 24 15:03:11.352: INFO: Waiting for pod var-expansion-6c709ad4-5b5a-4142-be61-b2f4b7cb813e to disappear
Mar 24 15:03:11.356: INFO: Pod var-expansion-6c709ad4-5b5a-4142-be61-b2f4b7cb813e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:03:11.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2442" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":175,"skipped":2972,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:03:11.380: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8010
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 24 15:03:14.123: INFO: Successfully updated pod "adopt-release-jdzwp"
STEP: Checking that the Job readopts the Pod
Mar 24 15:03:14.124: INFO: Waiting up to 15m0s for pod "adopt-release-jdzwp" in namespace "job-8010" to be "adopted"
Mar 24 15:03:14.128: INFO: Pod "adopt-release-jdzwp": Phase="Running", Reason="", readiness=true. Elapsed: 4.468948ms
Mar 24 15:03:16.141: INFO: Pod "adopt-release-jdzwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.017873906s
Mar 24 15:03:16.142: INFO: Pod "adopt-release-jdzwp" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 24 15:03:16.670: INFO: Successfully updated pod "adopt-release-jdzwp"
STEP: Checking that the Job releases the Pod
Mar 24 15:03:16.670: INFO: Waiting up to 15m0s for pod "adopt-release-jdzwp" in namespace "job-8010" to be "released"
Mar 24 15:03:16.680: INFO: Pod "adopt-release-jdzwp": Phase="Running", Reason="", readiness=true. Elapsed: 10.138187ms
Mar 24 15:03:16.680: INFO: Pod "adopt-release-jdzwp" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:03:16.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8010" for this suite.

• [SLOW TEST:5.362 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":176,"skipped":2974,"failed":0}
SSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:03:16.743: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 24 15:03:17.346: INFO: starting watch
STEP: patching
STEP: updating
Mar 24 15:03:17.373: INFO: waiting for watch events with expected annotations
Mar 24 15:03:17.373: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:03:17.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-562" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":177,"skipped":2977,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:03:17.528: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8817
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-48371257-23b8-4220-8060-d4d527343555
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-48371257-23b8-4220-8060-d4d527343555
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:03:21.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8817" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":178,"skipped":2980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:03:21.882: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-698
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Mar 24 15:03:22.171: INFO: Found 0 stateful pods, waiting for 3
Mar 24 15:03:32.180: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 15:03:32.180: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 15:03:32.180: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 24 15:03:32.219: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 24 15:03:42.272: INFO: Updating stateful set ss2
Mar 24 15:03:42.297: INFO: Waiting for Pod statefulset-698/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 24 15:03:52.407: INFO: Found 1 stateful pods, waiting for 3
Mar 24 15:04:02.415: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 15:04:02.415: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 15:04:02.415: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 24 15:04:02.449: INFO: Updating stateful set ss2
Mar 24 15:04:02.467: INFO: Waiting for Pod statefulset-698/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 24 15:04:12.506: INFO: Updating stateful set ss2
Mar 24 15:04:12.522: INFO: Waiting for StatefulSet statefulset-698/ss2 to complete update
Mar 24 15:04:12.522: INFO: Waiting for Pod statefulset-698/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 15:04:22.536: INFO: Deleting all statefulset in ns statefulset-698
Mar 24 15:04:22.541: INFO: Scaling statefulset ss2 to 0
Mar 24 15:04:52.569: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 15:04:52.574: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:04:52.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-698" for this suite.

• [SLOW TEST:90.726 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":179,"skipped":3047,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:04:52.609: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4480
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 15:04:52.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ae12ef6-c4d1-4ba5-9ab0-aad49cb26b46" in namespace "projected-4480" to be "Succeeded or Failed"
Mar 24 15:04:52.899: INFO: Pod "downwardapi-volume-3ae12ef6-c4d1-4ba5-9ab0-aad49cb26b46": Phase="Pending", Reason="", readiness=false. Elapsed: 5.645411ms
Mar 24 15:04:54.911: INFO: Pod "downwardapi-volume-3ae12ef6-c4d1-4ba5-9ab0-aad49cb26b46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017678867s
STEP: Saw pod success
Mar 24 15:04:54.911: INFO: Pod "downwardapi-volume-3ae12ef6-c4d1-4ba5-9ab0-aad49cb26b46" satisfied condition "Succeeded or Failed"
Mar 24 15:04:54.915: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-3ae12ef6-c4d1-4ba5-9ab0-aad49cb26b46 container client-container: <nil>
STEP: delete the pod
Mar 24 15:04:55.005: INFO: Waiting for pod downwardapi-volume-3ae12ef6-c4d1-4ba5-9ab0-aad49cb26b46 to disappear
Mar 24 15:04:55.009: INFO: Pod downwardapi-volume-3ae12ef6-c4d1-4ba5-9ab0-aad49cb26b46 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:04:55.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4480" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":180,"skipped":3057,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:04:55.021: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6510
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 24 15:04:55.212: INFO: Waiting up to 5m0s for pod "pod-1153ea28-0dc0-4c54-ab2a-7b0e4b326e60" in namespace "emptydir-6510" to be "Succeeded or Failed"
Mar 24 15:04:55.218: INFO: Pod "pod-1153ea28-0dc0-4c54-ab2a-7b0e4b326e60": Phase="Pending", Reason="", readiness=false. Elapsed: 6.559097ms
Mar 24 15:04:57.231: INFO: Pod "pod-1153ea28-0dc0-4c54-ab2a-7b0e4b326e60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019713259s
STEP: Saw pod success
Mar 24 15:04:57.231: INFO: Pod "pod-1153ea28-0dc0-4c54-ab2a-7b0e4b326e60" satisfied condition "Succeeded or Failed"
Mar 24 15:04:57.235: INFO: Trying to get logs from node talos-default-worker-1 pod pod-1153ea28-0dc0-4c54-ab2a-7b0e4b326e60 container test-container: <nil>
STEP: delete the pod
Mar 24 15:04:57.266: INFO: Waiting for pod pod-1153ea28-0dc0-4c54-ab2a-7b0e4b326e60 to disappear
Mar 24 15:04:57.270: INFO: Pod pod-1153ea28-0dc0-4c54-ab2a-7b0e4b326e60 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:04:57.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6510" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3057,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:04:57.287: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3696
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:04:57.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3696" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":182,"skipped":3065,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:04:57.651: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:04:57.884: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1f8e7000-af85-4997-af5e-c926529f3777" in namespace "security-context-test-2917" to be "Succeeded or Failed"
Mar 24 15:04:57.888: INFO: Pod "alpine-nnp-false-1f8e7000-af85-4997-af5e-c926529f3777": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868353ms
Mar 24 15:04:59.898: INFO: Pod "alpine-nnp-false-1f8e7000-af85-4997-af5e-c926529f3777": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014023471s
Mar 24 15:05:01.910: INFO: Pod "alpine-nnp-false-1f8e7000-af85-4997-af5e-c926529f3777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025748325s
Mar 24 15:05:01.910: INFO: Pod "alpine-nnp-false-1f8e7000-af85-4997-af5e-c926529f3777" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:01.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2917" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":183,"skipped":3074,"failed":0}
SSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:01.934: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-4082
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 24 15:05:02.243: INFO: starting watch
STEP: patching
STEP: updating
Mar 24 15:05:02.269: INFO: waiting for watch events with expected annotations
Mar 24 15:05:02.269: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:02.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4082" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":184,"skipped":3080,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:02.319: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5862
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Mar 24 15:05:02.582: INFO: Waiting up to 5m0s for pod "var-expansion-9ec1ed55-f4ab-4279-bb06-db456c2e2121" in namespace "var-expansion-5862" to be "Succeeded or Failed"
Mar 24 15:05:02.586: INFO: Pod "var-expansion-9ec1ed55-f4ab-4279-bb06-db456c2e2121": Phase="Pending", Reason="", readiness=false. Elapsed: 4.411004ms
Mar 24 15:05:04.599: INFO: Pod "var-expansion-9ec1ed55-f4ab-4279-bb06-db456c2e2121": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017078141s
STEP: Saw pod success
Mar 24 15:05:04.599: INFO: Pod "var-expansion-9ec1ed55-f4ab-4279-bb06-db456c2e2121" satisfied condition "Succeeded or Failed"
Mar 24 15:05:04.602: INFO: Trying to get logs from node talos-default-worker-1 pod var-expansion-9ec1ed55-f4ab-4279-bb06-db456c2e2121 container dapi-container: <nil>
STEP: delete the pod
Mar 24 15:05:04.653: INFO: Waiting for pod var-expansion-9ec1ed55-f4ab-4279-bb06-db456c2e2121 to disappear
Mar 24 15:05:04.658: INFO: Pod var-expansion-9ec1ed55-f4ab-4279-bb06-db456c2e2121 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:04.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5862" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":185,"skipped":3093,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:04.674: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:04.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8521" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":186,"skipped":3094,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:04.868: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:09.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5633" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3107,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:09.107: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1470
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Mar 24 15:05:09.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-1470 create -f -'
Mar 24 15:05:10.169: INFO: stderr: ""
Mar 24 15:05:10.169: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Mar 24 15:05:10.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-1470 diff -f -'
Mar 24 15:05:10.725: INFO: rc: 1
Mar 24 15:05:10.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-1470 delete -f -'
Mar 24 15:05:10.813: INFO: stderr: ""
Mar 24 15:05:10.813: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:10.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1470" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":188,"skipped":3117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:10.830: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4f89n in namespace proxy-5953
I0324 15:05:11.075373      22 runners.go:190] Created replication controller with name: proxy-service-4f89n, namespace: proxy-5953, replica count: 1
I0324 15:05:12.125937      22 runners.go:190] proxy-service-4f89n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 15:05:13.126272      22 runners.go:190] proxy-service-4f89n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 15:05:14.126743      22 runners.go:190] proxy-service-4f89n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 15:05:15.126925      22 runners.go:190] proxy-service-4f89n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 15:05:16.127268      22 runners.go:190] proxy-service-4f89n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 15:05:17.127621      22 runners.go:190] proxy-service-4f89n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 15:05:18.127847      22 runners.go:190] proxy-service-4f89n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 15:05:19.128235      22 runners.go:190] proxy-service-4f89n Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 15:05:19.154: INFO: setup took 8.155976912s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 24 15:05:19.164: INFO: (0) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 9.752142ms)
Mar 24 15:05:19.165: INFO: (0) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 10.633801ms)
Mar 24 15:05:19.167: INFO: (0) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 12.721447ms)
Mar 24 15:05:19.168: INFO: (0) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 13.461401ms)
Mar 24 15:05:19.168: INFO: (0) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 13.697418ms)
Mar 24 15:05:19.169: INFO: (0) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 14.205141ms)
Mar 24 15:05:19.170: INFO: (0) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 14.64946ms)
Mar 24 15:05:19.173: INFO: (0) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 18.433673ms)
Mar 24 15:05:19.173: INFO: (0) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 18.256399ms)
Mar 24 15:05:19.173: INFO: (0) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 18.042908ms)
Mar 24 15:05:19.173: INFO: (0) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 18.574759ms)
Mar 24 15:05:19.173: INFO: (0) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 18.544752ms)
Mar 24 15:05:19.173: INFO: (0) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 18.34591ms)
Mar 24 15:05:19.173: INFO: (0) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 18.682239ms)
Mar 24 15:05:19.176: INFO: (0) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 21.435185ms)
Mar 24 15:05:19.176: INFO: (0) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 21.295857ms)
Mar 24 15:05:19.182: INFO: (1) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 5.655909ms)
Mar 24 15:05:19.182: INFO: (1) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 5.811034ms)
Mar 24 15:05:19.182: INFO: (1) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 5.65142ms)
Mar 24 15:05:19.183: INFO: (1) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 6.74247ms)
Mar 24 15:05:19.185: INFO: (1) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 9.119261ms)
Mar 24 15:05:19.186: INFO: (1) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 9.164956ms)
Mar 24 15:05:19.186: INFO: (1) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 9.243489ms)
Mar 24 15:05:19.186: INFO: (1) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 9.344739ms)
Mar 24 15:05:19.186: INFO: (1) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 9.683698ms)
Mar 24 15:05:19.186: INFO: (1) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 9.801227ms)
Mar 24 15:05:19.188: INFO: (1) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 11.519139ms)
Mar 24 15:05:19.189: INFO: (1) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 12.201107ms)
Mar 24 15:05:19.189: INFO: (1) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 12.390499ms)
Mar 24 15:05:19.191: INFO: (1) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 14.245049ms)
Mar 24 15:05:19.191: INFO: (1) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 14.990459ms)
Mar 24 15:05:19.192: INFO: (1) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 14.97843ms)
Mar 24 15:05:19.196: INFO: (2) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 4.654951ms)
Mar 24 15:05:19.202: INFO: (2) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 8.860884ms)
Mar 24 15:05:19.202: INFO: (2) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 8.971984ms)
Mar 24 15:05:19.202: INFO: (2) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 8.835007ms)
Mar 24 15:05:19.202: INFO: (2) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 8.886131ms)
Mar 24 15:05:19.202: INFO: (2) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 9.440769ms)
Mar 24 15:05:19.202: INFO: (2) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 9.455168ms)
Mar 24 15:05:19.202: INFO: (2) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 9.615224ms)
Mar 24 15:05:19.202: INFO: (2) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 9.897667ms)
Mar 24 15:05:19.203: INFO: (2) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 10.617052ms)
Mar 24 15:05:19.203: INFO: (2) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 10.273654ms)
Mar 24 15:05:19.203: INFO: (2) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 10.776938ms)
Mar 24 15:05:19.205: INFO: (2) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 11.982796ms)
Mar 24 15:05:19.205: INFO: (2) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 12.196666ms)
Mar 24 15:05:19.205: INFO: (2) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 12.531227ms)
Mar 24 15:05:19.206: INFO: (2) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 13.543853ms)
Mar 24 15:05:19.212: INFO: (3) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 4.918547ms)
Mar 24 15:05:19.213: INFO: (3) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 6.328687ms)
Mar 24 15:05:19.213: INFO: (3) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 6.251274ms)
Mar 24 15:05:19.213: INFO: (3) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 6.375014ms)
Mar 24 15:05:19.213: INFO: (3) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 6.350875ms)
Mar 24 15:05:19.213: INFO: (3) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 6.209087ms)
Mar 24 15:05:19.213: INFO: (3) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 6.389741ms)
Mar 24 15:05:19.213: INFO: (3) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 6.328277ms)
Mar 24 15:05:19.214: INFO: (3) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 7.043911ms)
Mar 24 15:05:19.214: INFO: (3) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 7.326506ms)
Mar 24 15:05:19.217: INFO: (3) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 9.912637ms)
Mar 24 15:05:19.218: INFO: (3) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 10.985979ms)
Mar 24 15:05:19.218: INFO: (3) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 10.903236ms)
Mar 24 15:05:19.218: INFO: (3) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 11.188949ms)
Mar 24 15:05:19.218: INFO: (3) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 11.161532ms)
Mar 24 15:05:19.218: INFO: (3) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 11.735189ms)
Mar 24 15:05:19.224: INFO: (4) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 5.615393ms)
Mar 24 15:05:19.226: INFO: (4) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 7.138923ms)
Mar 24 15:05:19.226: INFO: (4) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 7.217315ms)
Mar 24 15:05:19.226: INFO: (4) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 7.606259ms)
Mar 24 15:05:19.227: INFO: (4) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 8.620607ms)
Mar 24 15:05:19.229: INFO: (4) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 10.723082ms)
Mar 24 15:05:19.230: INFO: (4) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 11.489941ms)
Mar 24 15:05:19.230: INFO: (4) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 11.999415ms)
Mar 24 15:05:19.231: INFO: (4) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 11.840939ms)
Mar 24 15:05:19.231: INFO: (4) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 11.968877ms)
Mar 24 15:05:19.231: INFO: (4) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 12.196596ms)
Mar 24 15:05:19.231: INFO: (4) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 12.444654ms)
Mar 24 15:05:19.232: INFO: (4) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 13.320243ms)
Mar 24 15:05:19.232: INFO: (4) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 13.436713ms)
Mar 24 15:05:19.232: INFO: (4) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 13.385107ms)
Mar 24 15:05:19.232: INFO: (4) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 13.35446ms)
Mar 24 15:05:19.240: INFO: (5) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 7.405379ms)
Mar 24 15:05:19.240: INFO: (5) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 7.287559ms)
Mar 24 15:05:19.242: INFO: (5) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 9.237509ms)
Mar 24 15:05:19.242: INFO: (5) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 9.089943ms)
Mar 24 15:05:19.243: INFO: (5) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 10.440779ms)
Mar 24 15:05:19.243: INFO: (5) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 10.875639ms)
Mar 24 15:05:19.244: INFO: (5) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 11.371823ms)
Mar 24 15:05:19.244: INFO: (5) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 11.445826ms)
Mar 24 15:05:19.244: INFO: (5) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 11.427557ms)
Mar 24 15:05:19.244: INFO: (5) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 11.870556ms)
Mar 24 15:05:19.245: INFO: (5) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 12.398848ms)
Mar 24 15:05:19.245: INFO: (5) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 12.864445ms)
Mar 24 15:05:19.245: INFO: (5) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 12.821659ms)
Mar 24 15:05:19.246: INFO: (5) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 13.321253ms)
Mar 24 15:05:19.246: INFO: (5) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 13.146418ms)
Mar 24 15:05:19.246: INFO: (5) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 13.431764ms)
Mar 24 15:05:19.253: INFO: (6) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 7.074538ms)
Mar 24 15:05:19.253: INFO: (6) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 6.333446ms)
Mar 24 15:05:19.254: INFO: (6) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 7.077697ms)
Mar 24 15:05:19.256: INFO: (6) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 9.33059ms)
Mar 24 15:05:19.256: INFO: (6) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 9.535911ms)
Mar 24 15:05:19.256: INFO: (6) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 9.305352ms)
Mar 24 15:05:19.257: INFO: (6) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 9.927774ms)
Mar 24 15:05:19.258: INFO: (6) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 11.159152ms)
Mar 24 15:05:19.258: INFO: (6) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 11.104237ms)
Mar 24 15:05:19.258: INFO: (6) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 11.299869ms)
Mar 24 15:05:19.258: INFO: (6) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 11.415838ms)
Mar 24 15:05:19.260: INFO: (6) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 13.054677ms)
Mar 24 15:05:19.261: INFO: (6) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 14.221139ms)
Mar 24 15:05:19.261: INFO: (6) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 14.364757ms)
Mar 24 15:05:19.261: INFO: (6) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 14.456307ms)
Mar 24 15:05:19.261: INFO: (6) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 14.855242ms)
Mar 24 15:05:19.267: INFO: (7) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 5.613493ms)
Mar 24 15:05:19.270: INFO: (7) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 6.619331ms)
Mar 24 15:05:19.270: INFO: (7) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 6.754878ms)
Mar 24 15:05:19.270: INFO: (7) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 6.317398ms)
Mar 24 15:05:19.271: INFO: (7) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 7.285849ms)
Mar 24 15:05:19.273: INFO: (7) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 10.271105ms)
Mar 24 15:05:19.273: INFO: (7) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 9.964153ms)
Mar 24 15:05:19.273: INFO: (7) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 11.105827ms)
Mar 24 15:05:19.273: INFO: (7) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 10.516341ms)
Mar 24 15:05:19.273: INFO: (7) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 9.840174ms)
Mar 24 15:05:19.273: INFO: (7) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 10.062834ms)
Mar 24 15:05:19.275: INFO: (7) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 12.05362ms)
Mar 24 15:05:19.275: INFO: (7) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 12.636825ms)
Mar 24 15:05:19.275: INFO: (7) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 12.754865ms)
Mar 24 15:05:19.275: INFO: (7) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 13.321524ms)
Mar 24 15:05:19.275: INFO: (7) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 12.195737ms)
Mar 24 15:05:19.281: INFO: (8) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 5.225828ms)
Mar 24 15:05:19.283: INFO: (8) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 7.341943ms)
Mar 24 15:05:19.283: INFO: (8) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 6.62025ms)
Mar 24 15:05:19.283: INFO: (8) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 7.233014ms)
Mar 24 15:05:19.283: INFO: (8) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 6.630279ms)
Mar 24 15:05:19.284: INFO: (8) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 8.114774ms)
Mar 24 15:05:19.284: INFO: (8) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 7.70474ms)
Mar 24 15:05:19.284: INFO: (8) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 7.625219ms)
Mar 24 15:05:19.284: INFO: (8) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 8.440024ms)
Mar 24 15:05:19.284: INFO: (8) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 8.763273ms)
Mar 24 15:05:19.287: INFO: (8) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 11.138234ms)
Mar 24 15:05:19.288: INFO: (8) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 12.977114ms)
Mar 24 15:05:19.289: INFO: (8) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 11.912683ms)
Mar 24 15:05:19.289: INFO: (8) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 12.127602ms)
Mar 24 15:05:19.289: INFO: (8) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 12.040201ms)
Mar 24 15:05:19.289: INFO: (8) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 12.899031ms)
Mar 24 15:05:19.296: INFO: (9) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 5.906736ms)
Mar 24 15:05:19.296: INFO: (9) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 6.766587ms)
Mar 24 15:05:19.296: INFO: (9) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 6.536088ms)
Mar 24 15:05:19.296: INFO: (9) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 7.133953ms)
Mar 24 15:05:19.298: INFO: (9) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 9.482947ms)
Mar 24 15:05:19.298: INFO: (9) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 8.966445ms)
Mar 24 15:05:19.299: INFO: (9) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 9.926287ms)
Mar 24 15:05:19.299: INFO: (9) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 9.724405ms)
Mar 24 15:05:19.299: INFO: (9) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 10.075732ms)
Mar 24 15:05:19.300: INFO: (9) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 10.20932ms)
Mar 24 15:05:19.300: INFO: (9) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 10.072553ms)
Mar 24 15:05:19.300: INFO: (9) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 10.158775ms)
Mar 24 15:05:19.303: INFO: (9) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 14.130948ms)
Mar 24 15:05:19.303: INFO: (9) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 13.584749ms)
Mar 24 15:05:19.303: INFO: (9) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 13.745104ms)
Mar 24 15:05:19.303: INFO: (9) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 13.601518ms)
Mar 24 15:05:19.308: INFO: (10) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 4.596236ms)
Mar 24 15:05:19.309: INFO: (10) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 5.22395ms)
Mar 24 15:05:19.311: INFO: (10) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 6.895614ms)
Mar 24 15:05:19.311: INFO: (10) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 6.929301ms)
Mar 24 15:05:19.311: INFO: (10) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 7.772053ms)
Mar 24 15:05:19.311: INFO: (10) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 7.521847ms)
Mar 24 15:05:19.312: INFO: (10) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 7.676123ms)
Mar 24 15:05:19.312: INFO: (10) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 8.230151ms)
Mar 24 15:05:19.312: INFO: (10) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 8.013671ms)
Mar 24 15:05:19.312: INFO: (10) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 8.339532ms)
Mar 24 15:05:19.314: INFO: (10) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 9.972722ms)
Mar 24 15:05:19.315: INFO: (10) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 11.255573ms)
Mar 24 15:05:19.317: INFO: (10) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 13.587079ms)
Mar 24 15:05:19.317: INFO: (10) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 13.597188ms)
Mar 24 15:05:19.318: INFO: (10) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 14.11809ms)
Mar 24 15:05:19.317: INFO: (10) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 13.643323ms)
Mar 24 15:05:19.330: INFO: (11) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 11.672935ms)
Mar 24 15:05:19.331: INFO: (11) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 12.58627ms)
Mar 24 15:05:19.331: INFO: (11) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 12.848387ms)
Mar 24 15:05:19.331: INFO: (11) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 12.890803ms)
Mar 24 15:05:19.331: INFO: (11) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 12.831558ms)
Mar 24 15:05:19.332: INFO: (11) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 14.029488ms)
Mar 24 15:05:19.332: INFO: (11) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 14.420112ms)
Mar 24 15:05:19.332: INFO: (11) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 14.66215ms)
Mar 24 15:05:19.332: INFO: (11) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 14.176325ms)
Mar 24 15:05:19.333: INFO: (11) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 14.680748ms)
Mar 24 15:05:19.333: INFO: (11) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 15.031476ms)
Mar 24 15:05:19.333: INFO: (11) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 15.074922ms)
Mar 24 15:05:19.334: INFO: (11) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 15.9506ms)
Mar 24 15:05:19.334: INFO: (11) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 16.352693ms)
Mar 24 15:05:19.334: INFO: (11) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 16.040402ms)
Mar 24 15:05:19.334: INFO: (11) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 16.194728ms)
Mar 24 15:05:19.340: INFO: (12) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 5.639091ms)
Mar 24 15:05:19.341: INFO: (12) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 6.762887ms)
Mar 24 15:05:19.342: INFO: (12) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 6.961509ms)
Mar 24 15:05:19.342: INFO: (12) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 6.892074ms)
Mar 24 15:05:19.342: INFO: (12) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 7.396358ms)
Mar 24 15:05:19.342: INFO: (12) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 7.50998ms)
Mar 24 15:05:19.342: INFO: (12) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 7.538286ms)
Mar 24 15:05:19.342: INFO: (12) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 7.932889ms)
Mar 24 15:05:19.343: INFO: (12) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 8.505557ms)
Mar 24 15:05:19.343: INFO: (12) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 8.704399ms)
Mar 24 15:05:19.346: INFO: (12) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 11.017036ms)
Mar 24 15:05:19.348: INFO: (12) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 13.056738ms)
Mar 24 15:05:19.348: INFO: (12) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 13.269208ms)
Mar 24 15:05:19.348: INFO: (12) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 13.865424ms)
Mar 24 15:05:19.349: INFO: (12) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 14.255537ms)
Mar 24 15:05:19.349: INFO: (12) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 14.483045ms)
Mar 24 15:05:19.355: INFO: (13) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 5.500124ms)
Mar 24 15:05:19.356: INFO: (13) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 6.046674ms)
Mar 24 15:05:19.356: INFO: (13) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 6.936841ms)
Mar 24 15:05:19.356: INFO: (13) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 6.61386ms)
Mar 24 15:05:19.356: INFO: (13) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 6.914012ms)
Mar 24 15:05:19.358: INFO: (13) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 8.335971ms)
Mar 24 15:05:19.359: INFO: (13) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 9.01262ms)
Mar 24 15:05:19.360: INFO: (13) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 11.013957ms)
Mar 24 15:05:19.361: INFO: (13) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 11.044951ms)
Mar 24 15:05:19.361: INFO: (13) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 11.334067ms)
Mar 24 15:05:19.361: INFO: (13) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 11.18365ms)
Mar 24 15:05:19.361: INFO: (13) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 11.16832ms)
Mar 24 15:05:19.363: INFO: (13) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 13.0163ms)
Mar 24 15:05:19.363: INFO: (13) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 13.223332ms)
Mar 24 15:05:19.363: INFO: (13) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 13.165867ms)
Mar 24 15:05:19.364: INFO: (13) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 14.345019ms)
Mar 24 15:05:19.369: INFO: (14) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 5.224189ms)
Mar 24 15:05:19.369: INFO: (14) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 5.180993ms)
Mar 24 15:05:19.369: INFO: (14) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 5.227259ms)
Mar 24 15:05:19.371: INFO: (14) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 7.340214ms)
Mar 24 15:05:19.371: INFO: (14) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 7.16304ms)
Mar 24 15:05:19.371: INFO: (14) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 6.894824ms)
Mar 24 15:05:19.371: INFO: (14) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 7.26602ms)
Mar 24 15:05:19.371: INFO: (14) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 7.196037ms)
Mar 24 15:05:19.371: INFO: (14) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 6.925442ms)
Mar 24 15:05:19.371: INFO: (14) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 7.025311ms)
Mar 24 15:05:19.373: INFO: (14) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 8.453712ms)
Mar 24 15:05:19.373: INFO: (14) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 8.984992ms)
Mar 24 15:05:19.375: INFO: (14) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 10.41921ms)
Mar 24 15:05:19.376: INFO: (14) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 11.645977ms)
Mar 24 15:05:19.376: INFO: (14) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 11.785314ms)
Mar 24 15:05:19.376: INFO: (14) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 11.784434ms)
Mar 24 15:05:19.382: INFO: (15) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 5.475036ms)
Mar 24 15:05:19.384: INFO: (15) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 7.834419ms)
Mar 24 15:05:19.384: INFO: (15) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 8.129752ms)
Mar 24 15:05:19.384: INFO: (15) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 7.841898ms)
Mar 24 15:05:19.385: INFO: (15) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 8.550852ms)
Mar 24 15:05:19.385: INFO: (15) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 8.747954ms)
Mar 24 15:05:19.385: INFO: (15) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 9.042197ms)
Mar 24 15:05:19.386: INFO: (15) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 10.120098ms)
Mar 24 15:05:19.387: INFO: (15) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 10.834042ms)
Mar 24 15:05:19.387: INFO: (15) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 11.200129ms)
Mar 24 15:05:19.387: INFO: (15) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 10.952781ms)
Mar 24 15:05:19.388: INFO: (15) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 11.869916ms)
Mar 24 15:05:19.389: INFO: (15) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 12.26329ms)
Mar 24 15:05:19.389: INFO: (15) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 12.689271ms)
Mar 24 15:05:19.389: INFO: (15) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 12.898811ms)
Mar 24 15:05:19.389: INFO: (15) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 13.168927ms)
Mar 24 15:05:19.399: INFO: (16) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 9.290305ms)
Mar 24 15:05:19.399: INFO: (16) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 9.133209ms)
Mar 24 15:05:19.399: INFO: (16) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 8.388128ms)
Mar 24 15:05:19.400: INFO: (16) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 9.712925ms)
Mar 24 15:05:19.400: INFO: (16) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 9.387326ms)
Mar 24 15:05:19.400: INFO: (16) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 8.869063ms)
Mar 24 15:05:19.400: INFO: (16) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 9.827664ms)
Mar 24 15:05:19.400: INFO: (16) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 8.559351ms)
Mar 24 15:05:19.400: INFO: (16) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 8.79172ms)
Mar 24 15:05:19.400: INFO: (16) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 9.64924ms)
Mar 24 15:05:19.400: INFO: (16) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 9.986709ms)
Mar 24 15:05:19.402: INFO: (16) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 10.729592ms)
Mar 24 15:05:19.403: INFO: (16) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 12.419947ms)
Mar 24 15:05:19.403: INFO: (16) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 12.165ms)
Mar 24 15:05:19.403: INFO: (16) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 11.930291ms)
Mar 24 15:05:19.404: INFO: (16) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 13.067587ms)
Mar 24 15:05:19.408: INFO: (17) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 4.34205ms)
Mar 24 15:05:19.409: INFO: (17) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 4.825236ms)
Mar 24 15:05:19.409: INFO: (17) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 4.888541ms)
Mar 24 15:05:19.409: INFO: (17) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 5.306161ms)
Mar 24 15:05:19.413: INFO: (17) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 7.80469ms)
Mar 24 15:05:19.413: INFO: (17) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 9.13095ms)
Mar 24 15:05:19.414: INFO: (17) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 9.303824ms)
Mar 24 15:05:19.414: INFO: (17) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 10.351547ms)
Mar 24 15:05:19.414: INFO: (17) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 9.952274ms)
Mar 24 15:05:19.414: INFO: (17) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 9.610115ms)
Mar 24 15:05:19.415: INFO: (17) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 10.741921ms)
Mar 24 15:05:19.416: INFO: (17) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 11.684564ms)
Mar 24 15:05:19.416: INFO: (17) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 11.471714ms)
Mar 24 15:05:19.416: INFO: (17) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 12.510788ms)
Mar 24 15:05:19.417: INFO: (17) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 11.831082ms)
Mar 24 15:05:19.416: INFO: (17) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 11.897555ms)
Mar 24 15:05:19.424: INFO: (18) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 7.230954ms)
Mar 24 15:05:19.424: INFO: (18) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 7.133083ms)
Mar 24 15:05:19.424: INFO: (18) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 7.023403ms)
Mar 24 15:05:19.424: INFO: (18) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 7.551933ms)
Mar 24 15:05:19.425: INFO: (18) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 8.620466ms)
Mar 24 15:05:19.427: INFO: (18) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 9.673428ms)
Mar 24 15:05:19.427: INFO: (18) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 9.797577ms)
Mar 24 15:05:19.427: INFO: (18) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 9.832294ms)
Mar 24 15:05:19.429: INFO: (18) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 11.377152ms)
Mar 24 15:05:19.429: INFO: (18) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 11.445245ms)
Mar 24 15:05:19.429: INFO: (18) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 11.583433ms)
Mar 24 15:05:19.430: INFO: (18) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 12.800671ms)
Mar 24 15:05:19.431: INFO: (18) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 14.241329ms)
Mar 24 15:05:19.432: INFO: (18) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 15.388233ms)
Mar 24 15:05:19.432: INFO: (18) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 15.140945ms)
Mar 24 15:05:19.432: INFO: (18) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 15.438549ms)
Mar 24 15:05:19.441: INFO: (19) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:160/proxy/: foo (200; 8.158649ms)
Mar 24 15:05:19.443: INFO: (19) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:160/proxy/: foo (200; 10.307561ms)
Mar 24 15:05:19.443: INFO: (19) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:460/proxy/: tls baz (200; 10.570417ms)
Mar 24 15:05:19.443: INFO: (19) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:1080/proxy/rewriteme">test<... (200; 10.482605ms)
Mar 24 15:05:19.443: INFO: (19) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls:162/proxy/: bar (200; 10.567947ms)
Mar 24 15:05:19.444: INFO: (19) /api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/proxy-service-4f89n-89gls/proxy/rewriteme">test</a> (200; 11.030465ms)
Mar 24 15:05:19.444: INFO: (19) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:162/proxy/: bar (200; 11.637798ms)
Mar 24 15:05:19.444: INFO: (19) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:462/proxy/: tls qux (200; 11.540668ms)
Mar 24 15:05:19.444: INFO: (19) /api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/http:proxy-service-4f89n-89gls:1080/proxy/rewriteme">... (200; 11.668796ms)
Mar 24 15:05:19.444: INFO: (19) /api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/: <a href="/api/v1/namespaces/proxy-5953/pods/https:proxy-service-4f89n-89gls:443/proxy/tlsrewritem... (200; 11.922362ms)
Mar 24 15:05:19.446: INFO: (19) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname2/proxy/: tls qux (200; 13.487967ms)
Mar 24 15:05:19.446: INFO: (19) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname1/proxy/: foo (200; 13.567181ms)
Mar 24 15:05:19.448: INFO: (19) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname1/proxy/: foo (200; 15.96309ms)
Mar 24 15:05:19.448: INFO: (19) /api/v1/namespaces/proxy-5953/services/http:proxy-service-4f89n:portname2/proxy/: bar (200; 15.896176ms)
Mar 24 15:05:19.448: INFO: (19) /api/v1/namespaces/proxy-5953/services/proxy-service-4f89n:portname2/proxy/: bar (200; 15.845901ms)
Mar 24 15:05:19.449: INFO: (19) /api/v1/namespaces/proxy-5953/services/https:proxy-service-4f89n:tlsportname1/proxy/: tls baz (200; 15.955741ms)
STEP: deleting ReplicationController proxy-service-4f89n in namespace proxy-5953, will wait for the garbage collector to delete the pods
Mar 24 15:05:19.534: INFO: Deleting ReplicationController proxy-service-4f89n took: 30.434886ms
Mar 24 15:05:19.634: INFO: Terminating ReplicationController proxy-service-4f89n pods took: 100.229349ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:26.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5953" for this suite.

• [SLOW TEST:15.342 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":189,"skipped":3161,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:26.173: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Mar 24 15:05:26.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2640 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 24 15:05:26.454: INFO: stderr: ""
Mar 24 15:05:26.454: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Mar 24 15:05:26.454: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 24 15:05:26.454: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2640" to be "running and ready, or succeeded"
Mar 24 15:05:26.460: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.351246ms
Mar 24 15:05:28.471: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.016912047s
Mar 24 15:05:28.471: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 24 15:05:28.471: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 24 15:05:28.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2640 logs logs-generator logs-generator'
Mar 24 15:05:28.549: INFO: stderr: ""
Mar 24 15:05:28.549: INFO: stdout: "I0324 15:05:27.271940       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/k58 296\nI0324 15:05:27.472078       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/mnw 262\nI0324 15:05:27.672068       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jh6 540\nI0324 15:05:27.872043       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/vjp 276\nI0324 15:05:28.072188       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/wxz 233\nI0324 15:05:28.272061       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/s9t 348\nI0324 15:05:28.472086       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/vxm 585\n"
STEP: limiting log lines
Mar 24 15:05:28.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2640 logs logs-generator logs-generator --tail=1'
Mar 24 15:05:28.629: INFO: stderr: ""
Mar 24 15:05:28.629: INFO: stdout: "I0324 15:05:28.472086       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/vxm 585\n"
Mar 24 15:05:28.630: INFO: got output "I0324 15:05:28.472086       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/vxm 585\n"
STEP: limiting log bytes
Mar 24 15:05:28.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2640 logs logs-generator logs-generator --limit-bytes=1'
Mar 24 15:05:28.734: INFO: stderr: ""
Mar 24 15:05:28.734: INFO: stdout: "I"
Mar 24 15:05:28.734: INFO: got output "I"
STEP: exposing timestamps
Mar 24 15:05:28.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2640 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 24 15:05:28.837: INFO: stderr: ""
Mar 24 15:05:28.837: INFO: stdout: "2021-03-24T15:05:28.672199223Z I0324 15:05:28.672067       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/njg5 475\n"
Mar 24 15:05:28.837: INFO: got output "2021-03-24T15:05:28.672199223Z I0324 15:05:28.672067       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/njg5 475\n"
STEP: restricting to a time range
Mar 24 15:05:31.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2640 logs logs-generator logs-generator --since=1s'
Mar 24 15:05:31.437: INFO: stderr: ""
Mar 24 15:05:31.437: INFO: stdout: "I0324 15:05:30.471985       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/t5pb 269\nI0324 15:05:30.672060       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/9x2 350\nI0324 15:05:30.872042       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/vqz 250\nI0324 15:05:31.072040       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/vbh 424\nI0324 15:05:31.272047       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/9rvz 247\n"
Mar 24 15:05:31.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2640 logs logs-generator logs-generator --since=24h'
Mar 24 15:05:31.551: INFO: stderr: ""
Mar 24 15:05:31.551: INFO: stdout: "I0324 15:05:27.271940       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/k58 296\nI0324 15:05:27.472078       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/mnw 262\nI0324 15:05:27.672068       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jh6 540\nI0324 15:05:27.872043       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/vjp 276\nI0324 15:05:28.072188       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/wxz 233\nI0324 15:05:28.272061       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/s9t 348\nI0324 15:05:28.472086       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/vxm 585\nI0324 15:05:28.672067       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/njg5 475\nI0324 15:05:28.872044       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/h8k 521\nI0324 15:05:29.072049       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/rcnr 382\nI0324 15:05:29.272105       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/wjd 402\nI0324 15:05:29.472072       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/kdv 373\nI0324 15:05:29.672056       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/csd 293\nI0324 15:05:29.872055       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/n4ts 295\nI0324 15:05:30.072086       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/5wx 500\nI0324 15:05:30.272082       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/wp5 227\nI0324 15:05:30.471985       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/t5pb 269\nI0324 15:05:30.672060       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/9x2 350\nI0324 15:05:30.872042       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/vqz 250\nI0324 15:05:31.072040       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/vbh 424\nI0324 15:05:31.272047       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/9rvz 247\nI0324 15:05:31.472035       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/kqt 406\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Mar 24 15:05:31.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2640 delete pod logs-generator'
Mar 24 15:05:36.055: INFO: stderr: ""
Mar 24 15:05:36.055: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:36.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2640" for this suite.

• [SLOW TEST:9.921 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":190,"skipped":3165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:36.096: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 24 15:05:40.393: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 24 15:05:40.398: INFO: Pod pod-with-prestop-http-hook still exists
Mar 24 15:05:42.398: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 24 15:05:42.408: INFO: Pod pod-with-prestop-http-hook still exists
Mar 24 15:05:44.398: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 24 15:05:44.410: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:44.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-723" for this suite.

• [SLOW TEST:8.343 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3223,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:44.439: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2282
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:44.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2282" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":192,"skipped":3237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:44.705: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:44.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8921" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":193,"skipped":3285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:45.010: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:48.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6529" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":194,"skipped":3345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:48.880: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6297
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:05:56.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6297" for this suite.

• [SLOW TEST:7.258 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":195,"skipped":3369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:05:56.141: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 15:05:56.748: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 24 15:05:58.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195156, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195156, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195156, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195156, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 15:06:01.818: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:06:02.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6478" for this suite.
STEP: Destroying namespace "webhook-6478-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.320 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":196,"skipped":3391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:06:02.462: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-4670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Mar 24 15:06:02.704: INFO: created test-podtemplate-1
Mar 24 15:06:02.721: INFO: created test-podtemplate-2
Mar 24 15:06:02.731: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Mar 24 15:06:02.736: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Mar 24 15:06:02.779: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:06:02.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4670" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":197,"skipped":3432,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:06:02.803: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:06:03.030: INFO: Create a RollingUpdate DaemonSet
Mar 24 15:06:03.037: INFO: Check that daemon pods launch on every node of the cluster
Mar 24 15:06:03.042: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:03.043: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:03.043: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:03.054: INFO: Number of nodes with available pods: 0
Mar 24 15:06:03.054: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 15:06:04.064: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:04.064: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:04.064: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:04.068: INFO: Number of nodes with available pods: 0
Mar 24 15:06:04.068: INFO: Node talos-default-worker-1 is running more than one daemon pod
Mar 24 15:06:05.063: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:05.063: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:05.063: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:05.068: INFO: Number of nodes with available pods: 2
Mar 24 15:06:05.068: INFO: Number of running nodes: 2, number of available pods: 2
Mar 24 15:06:05.068: INFO: Update the DaemonSet to trigger a rollout
Mar 24 15:06:05.097: INFO: Updating DaemonSet daemon-set
Mar 24 15:06:09.118: INFO: Roll back the DaemonSet before rollout is complete
Mar 24 15:06:09.141: INFO: Updating DaemonSet daemon-set
Mar 24 15:06:09.141: INFO: Make sure DaemonSet rollback is complete
Mar 24 15:06:09.146: INFO: Wrong image for pod: daemon-set-xdhfj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 24 15:06:09.146: INFO: Pod daemon-set-xdhfj is not available
Mar 24 15:06:09.152: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:09.152: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:09.152: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:10.160: INFO: Wrong image for pod: daemon-set-xdhfj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 24 15:06:10.160: INFO: Pod daemon-set-xdhfj is not available
Mar 24 15:06:10.165: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:10.165: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:10.165: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:11.161: INFO: Pod daemon-set-wb57f is not available
Mar 24 15:06:11.166: INFO: DaemonSet pods can't tolerate node talos-default-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:11.166: INFO: DaemonSet pods can't tolerate node talos-default-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 15:06:11.166: INFO: DaemonSet pods can't tolerate node talos-default-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9433, will wait for the garbage collector to delete the pods
Mar 24 15:06:11.242: INFO: Deleting DaemonSet.extensions daemon-set took: 12.085406ms
Mar 24 15:06:11.742: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.17958ms
Mar 24 15:06:17.962: INFO: Number of nodes with available pods: 0
Mar 24 15:06:17.962: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 15:06:17.965: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19811"},"items":null}

Mar 24 15:06:17.967: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19811"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:06:17.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9433" for this suite.

• [SLOW TEST:15.209 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":198,"skipped":3444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:06:18.016: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 24 15:06:18.214: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 24 15:06:23.274: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:06:24.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3338" for this suite.

• [SLOW TEST:6.332 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":199,"skipped":3491,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:06:24.353: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9996
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 24 15:06:24.558: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 15:06:26.414: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:06:34.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9996" for this suite.

• [SLOW TEST:10.199 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":200,"skipped":3496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:06:34.554: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-9767
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Mar 24 15:06:34.775: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Mar 24 15:06:34.841: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:06:34.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9767" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":201,"skipped":3531,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:06:34.895: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-e66b082e-c8bc-4785-9612-a8a4e8712d3f
STEP: Creating a pod to test consume configMaps
Mar 24 15:06:35.106: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-97c6d1a0-3191-42fc-8a5b-c2f66b57af9d" in namespace "projected-4082" to be "Succeeded or Failed"
Mar 24 15:06:35.110: INFO: Pod "pod-projected-configmaps-97c6d1a0-3191-42fc-8a5b-c2f66b57af9d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.662072ms
Mar 24 15:06:37.123: INFO: Pod "pod-projected-configmaps-97c6d1a0-3191-42fc-8a5b-c2f66b57af9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017131755s
STEP: Saw pod success
Mar 24 15:06:37.124: INFO: Pod "pod-projected-configmaps-97c6d1a0-3191-42fc-8a5b-c2f66b57af9d" satisfied condition "Succeeded or Failed"
Mar 24 15:06:37.128: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-configmaps-97c6d1a0-3191-42fc-8a5b-c2f66b57af9d container agnhost-container: <nil>
STEP: delete the pod
Mar 24 15:06:37.188: INFO: Waiting for pod pod-projected-configmaps-97c6d1a0-3191-42fc-8a5b-c2f66b57af9d to disappear
Mar 24 15:06:37.192: INFO: Pod pod-projected-configmaps-97c6d1a0-3191-42fc-8a5b-c2f66b57af9d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:06:37.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4082" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":202,"skipped":3532,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:06:37.207: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-d48849c4-b49a-4431-bb0d-23f2d3966b92
STEP: Creating a pod to test consume configMaps
Mar 24 15:06:37.410: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-04db0190-0965-4a8e-be9d-41548e48db37" in namespace "projected-412" to be "Succeeded or Failed"
Mar 24 15:06:37.414: INFO: Pod "pod-projected-configmaps-04db0190-0965-4a8e-be9d-41548e48db37": Phase="Pending", Reason="", readiness=false. Elapsed: 3.737183ms
Mar 24 15:06:39.426: INFO: Pod "pod-projected-configmaps-04db0190-0965-4a8e-be9d-41548e48db37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015512101s
STEP: Saw pod success
Mar 24 15:06:39.426: INFO: Pod "pod-projected-configmaps-04db0190-0965-4a8e-be9d-41548e48db37" satisfied condition "Succeeded or Failed"
Mar 24 15:06:39.429: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-configmaps-04db0190-0965-4a8e-be9d-41548e48db37 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 15:06:39.469: INFO: Waiting for pod pod-projected-configmaps-04db0190-0965-4a8e-be9d-41548e48db37 to disappear
Mar 24 15:06:39.473: INFO: Pod pod-projected-configmaps-04db0190-0965-4a8e-be9d-41548e48db37 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:06:39.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-412" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":203,"skipped":3536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:06:39.499: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-290
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 15:06:39.728: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d78dfacd-ebd7-459b-bad3-54f98d671fcf" in namespace "downward-api-290" to be "Succeeded or Failed"
Mar 24 15:06:39.739: INFO: Pod "downwardapi-volume-d78dfacd-ebd7-459b-bad3-54f98d671fcf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.715753ms
Mar 24 15:06:41.749: INFO: Pod "downwardapi-volume-d78dfacd-ebd7-459b-bad3-54f98d671fcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021182276s
STEP: Saw pod success
Mar 24 15:06:41.749: INFO: Pod "downwardapi-volume-d78dfacd-ebd7-459b-bad3-54f98d671fcf" satisfied condition "Succeeded or Failed"
Mar 24 15:06:41.753: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-d78dfacd-ebd7-459b-bad3-54f98d671fcf container client-container: <nil>
STEP: delete the pod
Mar 24 15:06:41.792: INFO: Waiting for pod downwardapi-volume-d78dfacd-ebd7-459b-bad3-54f98d671fcf to disappear
Mar 24 15:06:41.796: INFO: Pod downwardapi-volume-d78dfacd-ebd7-459b-bad3-54f98d671fcf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:06:41.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-290" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":204,"skipped":3584,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:06:41.819: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6985
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 24 15:06:42.000: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:06:56.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6985" for this suite.

• [SLOW TEST:14.373 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":205,"skipped":3601,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:06:56.194: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 24 15:06:56.389: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  9d038057-b303-4db6-860f-1cf6b03bea51 20134 0 2021-03-24 15:06:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 15:06:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 15:06:56.389: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  9d038057-b303-4db6-860f-1cf6b03bea51 20134 0 2021-03-24 15:06:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 15:06:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 24 15:07:06.424: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  9d038057-b303-4db6-860f-1cf6b03bea51 20166 0 2021-03-24 15:06:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 15:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 15:07:06.425: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  9d038057-b303-4db6-860f-1cf6b03bea51 20166 0 2021-03-24 15:06:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 15:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 24 15:07:16.470: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  9d038057-b303-4db6-860f-1cf6b03bea51 20186 0 2021-03-24 15:06:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 15:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 15:07:16.470: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  9d038057-b303-4db6-860f-1cf6b03bea51 20186 0 2021-03-24 15:06:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 15:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 24 15:07:26.500: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  9d038057-b303-4db6-860f-1cf6b03bea51 20205 0 2021-03-24 15:06:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 15:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 15:07:26.500: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  9d038057-b303-4db6-860f-1cf6b03bea51 20205 0 2021-03-24 15:06:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 15:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 24 15:07:36.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1576  f05beccd-c0e2-4126-8b86-0c59afabd2c7 20224 0 2021-03-24 15:07:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-24 15:07:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 15:07:36.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1576  f05beccd-c0e2-4126-8b86-0c59afabd2c7 20224 0 2021-03-24 15:07:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-24 15:07:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 24 15:07:46.565: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1576  f05beccd-c0e2-4126-8b86-0c59afabd2c7 20243 0 2021-03-24 15:07:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-24 15:07:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 15:07:46.565: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1576  f05beccd-c0e2-4126-8b86-0c59afabd2c7 20243 0 2021-03-24 15:07:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-24 15:07:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:07:56.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1576" for this suite.

• [SLOW TEST:60.411 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":206,"skipped":3604,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:07:56.606: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-907485c9-673e-41c0-adb6-3ab2cfc126d8
STEP: Creating a pod to test consume secrets
Mar 24 15:07:56.830: INFO: Waiting up to 5m0s for pod "pod-secrets-83a2f13a-9271-4ea7-8a6e-16776cba91b2" in namespace "secrets-3975" to be "Succeeded or Failed"
Mar 24 15:07:56.846: INFO: Pod "pod-secrets-83a2f13a-9271-4ea7-8a6e-16776cba91b2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.128503ms
Mar 24 15:07:58.859: INFO: Pod "pod-secrets-83a2f13a-9271-4ea7-8a6e-16776cba91b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029168949s
STEP: Saw pod success
Mar 24 15:07:58.859: INFO: Pod "pod-secrets-83a2f13a-9271-4ea7-8a6e-16776cba91b2" satisfied condition "Succeeded or Failed"
Mar 24 15:07:58.863: INFO: Trying to get logs from node talos-default-worker-1 pod pod-secrets-83a2f13a-9271-4ea7-8a6e-16776cba91b2 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 15:07:58.981: INFO: Waiting for pod pod-secrets-83a2f13a-9271-4ea7-8a6e-16776cba91b2 to disappear
Mar 24 15:07:58.990: INFO: Pod pod-secrets-83a2f13a-9271-4ea7-8a6e-16776cba91b2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:07:58.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3975" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3622,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:07:59.008: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5434
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 24 15:08:01.228: INFO: &Pod{ObjectMeta:{send-events-70316d50-b90f-4c98-bc77-f28f0ee5458b  events-5434  0a11ad40-7fbd-475d-9818-0ed553ca4df0 20302 0 2021-03-24 15:07:59 +0000 UTC <nil> <nil> map[name:foo time:190582346] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-03-24 15:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 15:08:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xgqbq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xgqbq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xgqbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:07:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:08:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:08:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:07:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:10.244.2.230,StartTime:2021-03-24 15:07:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 15:07:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://f94d4e90611f0f9a1944f26811c953ee39db20a6c313f0c35b2fcf45117ee60f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 24 15:08:03.243: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 24 15:08:05.254: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:08:05.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5434" for this suite.

• [SLOW TEST:6.321 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":208,"skipped":3650,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:08:05.331: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Mar 24 15:08:05.522: INFO: observed Pod pod-test in namespace pods-7307 in phase Pending conditions []
Mar 24 15:08:05.537: INFO: observed Pod pod-test in namespace pods-7307 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 15:08:05 +0000 UTC  }]
Mar 24 15:08:05.578: INFO: observed Pod pod-test in namespace pods-7307 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 15:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 15:08:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 15:08:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 15:08:05 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Mar 24 15:08:06.496: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Mar 24 15:08:06.575: INFO: observed event type ADDED
Mar 24 15:08:06.575: INFO: observed event type MODIFIED
Mar 24 15:08:06.576: INFO: observed event type MODIFIED
Mar 24 15:08:06.576: INFO: observed event type MODIFIED
Mar 24 15:08:06.576: INFO: observed event type MODIFIED
Mar 24 15:08:06.576: INFO: observed event type MODIFIED
Mar 24 15:08:06.578: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:08:06.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7307" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":209,"skipped":3669,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:08:06.597: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2904
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 24 15:08:06.813: INFO: Waiting up to 5m0s for pod "pod-80e921fc-21d4-41a2-8501-80b40acdef01" in namespace "emptydir-2904" to be "Succeeded or Failed"
Mar 24 15:08:06.819: INFO: Pod "pod-80e921fc-21d4-41a2-8501-80b40acdef01": Phase="Pending", Reason="", readiness=false. Elapsed: 6.351352ms
Mar 24 15:08:08.835: INFO: Pod "pod-80e921fc-21d4-41a2-8501-80b40acdef01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021720761s
STEP: Saw pod success
Mar 24 15:08:08.835: INFO: Pod "pod-80e921fc-21d4-41a2-8501-80b40acdef01" satisfied condition "Succeeded or Failed"
Mar 24 15:08:08.838: INFO: Trying to get logs from node talos-default-worker-1 pod pod-80e921fc-21d4-41a2-8501-80b40acdef01 container test-container: <nil>
STEP: delete the pod
Mar 24 15:08:08.881: INFO: Waiting for pod pod-80e921fc-21d4-41a2-8501-80b40acdef01 to disappear
Mar 24 15:08:08.885: INFO: Pod pod-80e921fc-21d4-41a2-8501-80b40acdef01 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:08:08.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2904" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3690,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:08:08.901: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 15:08:09.467: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 15:08:12.524: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:08:12.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4481" for this suite.
STEP: Destroying namespace "webhook-4481-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":211,"skipped":3706,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:08:12.748: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7616
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-7616
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 15:08:12.939: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 24 15:08:12.992: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 24 15:08:15.003: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:08:17.005: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:08:19.005: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:08:21.004: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:08:23.005: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:08:25.002: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:08:27.005: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:08:29.005: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:08:31.004: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 24 15:08:31.011: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 24 15:08:33.075: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 24 15:08:33.075: INFO: Going to poll 10.244.2.235 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Mar 24 15:08:33.078: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.235 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7616 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:08:33.078: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 15:08:34.150: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 24 15:08:34.150: INFO: Going to poll 10.244.3.87 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Mar 24 15:08:34.159: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.87 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7616 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:08:34.159: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 15:08:35.222: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:08:35.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7616" for this suite.

• [SLOW TEST:22.515 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3715,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:08:35.264: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar 24 15:08:35.435: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 15:08:35.444: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 15:08:35.448: INFO: 
Logging pods the apiserver thinks is on node talos-default-worker-1 before test
Mar 24 15:08:35.453: INFO: send-events-70316d50-b90f-4c98-bc77-f28f0ee5458b from events-5434 started at 2021-03-24 15:07:59 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.453: INFO: 	Container p ready: true, restart count 0
Mar 24 15:08:35.453: INFO: kube-flannel-9nwp5 from kube-system started at 2021-03-24 14:11:00 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.453: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 15:08:35.453: INFO: kube-proxy-vzljq from kube-system started at 2021-03-24 14:11:00 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.453: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 15:08:35.453: INFO: netserver-0 from pod-network-test-7616 started at 2021-03-24 15:08:12 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.453: INFO: 	Container webserver ready: true, restart count 0
Mar 24 15:08:35.453: INFO: test-container-pod from pod-network-test-7616 started at 2021-03-24 15:08:30 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.454: INFO: 	Container webserver ready: true, restart count 0
Mar 24 15:08:35.454: INFO: sonobuoy from sonobuoy started at 2021-03-24 14:20:14 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.455: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 24 15:08:35.455: INFO: 
Logging pods the apiserver thinks is on node talos-default-worker-2 before test
Mar 24 15:08:35.462: INFO: coredns-6bd7f94d9b-sq6kb from kube-system started at 2021-03-24 14:11:20 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.462: INFO: 	Container coredns ready: true, restart count 0
Mar 24 15:08:35.462: INFO: kube-flannel-fnwwj from kube-system started at 2021-03-24 14:10:52 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.462: INFO: 	Container kube-flannel ready: true, restart count 1
Mar 24 15:08:35.462: INFO: kube-proxy-swlm2 from kube-system started at 2021-03-24 14:10:52 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.462: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 15:08:35.462: INFO: host-test-container-pod from pod-network-test-7616 started at 2021-03-24 15:08:31 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.463: INFO: 	Container agnhost-container ready: true, restart count 0
Mar 24 15:08:35.463: INFO: netserver-1 from pod-network-test-7616 started at 2021-03-24 15:08:13 +0000 UTC (1 container statuses recorded)
Mar 24 15:08:35.463: INFO: 	Container webserver ready: true, restart count 0
Mar 24 15:08:35.463: INFO: sonobuoy-e2e-job-865e6f320bce471a from sonobuoy started at 2021-03-24 14:20:18 +0000 UTC (2 container statuses recorded)
Mar 24 15:08:35.463: INFO: 	Container e2e ready: true, restart count 0
Mar 24 15:08:35.463: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4a1628a7-28de-43dd-b013-aa5f509e29be 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-4a1628a7-28de-43dd-b013-aa5f509e29be off the node talos-default-worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4a1628a7-28de-43dd-b013-aa5f509e29be
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:08:39.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8264" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":213,"skipped":3730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:08:39.640: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3134
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Mar 24 15:08:39.932: INFO: Waiting up to 5m0s for pod "client-containers-029c677c-4906-4151-98bf-0454eb7f8df2" in namespace "containers-3134" to be "Succeeded or Failed"
Mar 24 15:08:39.941: INFO: Pod "client-containers-029c677c-4906-4151-98bf-0454eb7f8df2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.801649ms
Mar 24 15:08:41.953: INFO: Pod "client-containers-029c677c-4906-4151-98bf-0454eb7f8df2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020558796s
STEP: Saw pod success
Mar 24 15:08:41.953: INFO: Pod "client-containers-029c677c-4906-4151-98bf-0454eb7f8df2" satisfied condition "Succeeded or Failed"
Mar 24 15:08:41.956: INFO: Trying to get logs from node talos-default-worker-1 pod client-containers-029c677c-4906-4151-98bf-0454eb7f8df2 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 15:08:41.997: INFO: Waiting for pod client-containers-029c677c-4906-4151-98bf-0454eb7f8df2 to disappear
Mar 24 15:08:42.001: INFO: Pod client-containers-029c677c-4906-4151-98bf-0454eb7f8df2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:08:42.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3134" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":214,"skipped":3760,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:08:42.015: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1627
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0324 15:08:52.432997      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 15:08:54.459: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 24 15:08:54.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-2w9ts" in namespace "gc-1627"
Mar 24 15:08:54.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mxzz" in namespace "gc-1627"
Mar 24 15:08:54.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-9swms" in namespace "gc-1627"
Mar 24 15:08:54.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-dm5nw" in namespace "gc-1627"
Mar 24 15:08:54.632: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5pv9" in namespace "gc-1627"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:08:54.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1627" for this suite.

• [SLOW TEST:12.702 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":215,"skipped":3763,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:08:54.716: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-5606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:08:54.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5606" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":216,"skipped":3768,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:08:54.946: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-50dad8c2-a47f-4d84-8898-12886c363059
STEP: Creating a pod to test consume secrets
Mar 24 15:08:55.161: INFO: Waiting up to 5m0s for pod "pod-secrets-8b288f08-ad5b-4794-b218-877377160d80" in namespace "secrets-9075" to be "Succeeded or Failed"
Mar 24 15:08:55.166: INFO: Pod "pod-secrets-8b288f08-ad5b-4794-b218-877377160d80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.609406ms
Mar 24 15:08:57.179: INFO: Pod "pod-secrets-8b288f08-ad5b-4794-b218-877377160d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017157568s
STEP: Saw pod success
Mar 24 15:08:57.179: INFO: Pod "pod-secrets-8b288f08-ad5b-4794-b218-877377160d80" satisfied condition "Succeeded or Failed"
Mar 24 15:08:57.182: INFO: Trying to get logs from node talos-default-worker-1 pod pod-secrets-8b288f08-ad5b-4794-b218-877377160d80 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 15:08:57.223: INFO: Waiting for pod pod-secrets-8b288f08-ad5b-4794-b218-877377160d80 to disappear
Mar 24 15:08:57.226: INFO: Pod pod-secrets-8b288f08-ad5b-4794-b218-877377160d80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:08:57.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9075" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3775,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:08:57.253: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-1541
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1541
STEP: Deleting pre-stop pod
Mar 24 15:09:06.538: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:09:06.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1541" for this suite.

• [SLOW TEST:9.374 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":218,"skipped":3789,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:09:06.629: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1601
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar 24 15:09:06.819: INFO: PodSpec: initContainers in spec.initContainers
Mar 24 15:09:54.791: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-acce7dd8-02d0-409f-afae-04108764cfdf", GenerateName:"", Namespace:"init-container-1601", SelfLink:"", UID:"6bace9c5-a2cf-413b-b8cd-10f7abca88b5", ResourceVersion:"21189", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63752195346, loc:(*time.Location)(0x797fe80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"819036364"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001d52f40), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001d52f60)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001d52f80), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001d52fa0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-27skj", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003229100), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-27skj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-27skj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-27skj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00405fc88), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"talos-default-worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00314af50), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00405fd60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00405fdc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00405fdc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00405fdcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0060b20b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195346, loc:(*time.Location)(0x797fe80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195346, loc:(*time.Location)(0x797fe80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195346, loc:(*time.Location)(0x797fe80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195346, loc:(*time.Location)(0x797fe80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.5.0.5", PodIP:"10.244.2.248", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.2.248"}}, StartTime:(*v1.Time)(0xc001d52fc0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00314b030)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00314b0a0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://4d0fe6ab97091c5e8d1426cd5d3df15a581f1dd247cb223913f826daf4cc837d", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001d53000), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001d52fe0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00405fe5c)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:09:54.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1601" for this suite.

• [SLOW TEST:48.269 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":219,"skipped":3795,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:09:54.899: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1580
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-1580
Mar 24 15:09:55.138: INFO: Found 0 stateful pods, waiting for 1
Mar 24 15:10:05.148: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 15:10:05.190: INFO: Deleting all statefulset in ns statefulset-1580
Mar 24 15:10:05.194: INFO: Scaling statefulset ss to 0
Mar 24 15:10:35.265: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 15:10:35.268: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:10:35.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1580" for this suite.

• [SLOW TEST:40.423 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":220,"skipped":3803,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:10:35.325: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6528
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-a3b2eb6a-f241-4d39-b2c8-cd0426715391
STEP: Creating a pod to test consume secrets
Mar 24 15:10:35.533: INFO: Waiting up to 5m0s for pod "pod-secrets-efa49c34-4f61-488a-86b0-e9b619add708" in namespace "secrets-6528" to be "Succeeded or Failed"
Mar 24 15:10:35.541: INFO: Pod "pod-secrets-efa49c34-4f61-488a-86b0-e9b619add708": Phase="Pending", Reason="", readiness=false. Elapsed: 8.1595ms
Mar 24 15:10:37.554: INFO: Pod "pod-secrets-efa49c34-4f61-488a-86b0-e9b619add708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020591861s
STEP: Saw pod success
Mar 24 15:10:37.554: INFO: Pod "pod-secrets-efa49c34-4f61-488a-86b0-e9b619add708" satisfied condition "Succeeded or Failed"
Mar 24 15:10:37.557: INFO: Trying to get logs from node talos-default-worker-1 pod pod-secrets-efa49c34-4f61-488a-86b0-e9b619add708 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 15:10:37.611: INFO: Waiting for pod pod-secrets-efa49c34-4f61-488a-86b0-e9b619add708 to disappear
Mar 24 15:10:37.615: INFO: Pod pod-secrets-efa49c34-4f61-488a-86b0-e9b619add708 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:10:37.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6528" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3820,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:10:37.632: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-5bd8473a-738f-4b2b-af78-8c2fc77d4552
STEP: Creating a pod to test consume secrets
Mar 24 15:10:37.830: INFO: Waiting up to 5m0s for pod "pod-secrets-e86b09d9-3b05-4ed8-a76f-b3c264356751" in namespace "secrets-1423" to be "Succeeded or Failed"
Mar 24 15:10:37.835: INFO: Pod "pod-secrets-e86b09d9-3b05-4ed8-a76f-b3c264356751": Phase="Pending", Reason="", readiness=false. Elapsed: 3.999419ms
Mar 24 15:10:39.844: INFO: Pod "pod-secrets-e86b09d9-3b05-4ed8-a76f-b3c264356751": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013128797s
STEP: Saw pod success
Mar 24 15:10:39.844: INFO: Pod "pod-secrets-e86b09d9-3b05-4ed8-a76f-b3c264356751" satisfied condition "Succeeded or Failed"
Mar 24 15:10:39.847: INFO: Trying to get logs from node talos-default-worker-1 pod pod-secrets-e86b09d9-3b05-4ed8-a76f-b3c264356751 container secret-env-test: <nil>
STEP: delete the pod
Mar 24 15:10:39.884: INFO: Waiting for pod pod-secrets-e86b09d9-3b05-4ed8-a76f-b3c264356751 to disappear
Mar 24 15:10:39.889: INFO: Pod pod-secrets-e86b09d9-3b05-4ed8-a76f-b3c264356751 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:10:39.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1423" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3832,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:10:39.904: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 24 15:10:40.152: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8611  1e45863f-c165-40ef-8daf-e2a6c6fef21a 21404 0 2021-03-24 15:10:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 15:10:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 15:10:40.153: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8611  1e45863f-c165-40ef-8daf-e2a6c6fef21a 21405 0 2021-03-24 15:10:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 15:10:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 15:10:40.154: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8611  1e45863f-c165-40ef-8daf-e2a6c6fef21a 21406 0 2021-03-24 15:10:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 15:10:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 24 15:10:50.262: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8611  1e45863f-c165-40ef-8daf-e2a6c6fef21a 21475 0 2021-03-24 15:10:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 15:10:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 15:10:50.263: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8611  1e45863f-c165-40ef-8daf-e2a6c6fef21a 21476 0 2021-03-24 15:10:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 15:10:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 15:10:50.264: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8611  1e45863f-c165-40ef-8daf-e2a6c6fef21a 21477 0 2021-03-24 15:10:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 15:10:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:10:50.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8611" for this suite.

• [SLOW TEST:10.374 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":223,"skipped":3841,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:10:50.279: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4384
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 15:10:50.827: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 15:10:53.972: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:10:54.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4384" for this suite.
STEP: Destroying namespace "webhook-4384-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":224,"skipped":3848,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:10:54.282: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5525
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Mar 24 15:10:54.494: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:11:10.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5525" for this suite.

• [SLOW TEST:16.716 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":225,"skipped":3875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:11:11.000: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8121
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0324 15:11:17.255367      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 15:11:19.280: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:11:19.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8121" for this suite.

• [SLOW TEST:8.304 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":226,"skipped":3911,"failed":0}
S
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:11:19.305: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:11:25.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4212" for this suite.

• [SLOW TEST:6.383 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":227,"skipped":3912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:11:25.694: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5887
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-b032f084-4d42-4086-80bf-5f2dc884475f in namespace container-probe-5887
Mar 24 15:11:27.899: INFO: Started pod test-webserver-b032f084-4d42-4086-80bf-5f2dc884475f in namespace container-probe-5887
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 15:11:27.902: INFO: Initial restart count of pod test-webserver-b032f084-4d42-4086-80bf-5f2dc884475f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:15:29.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5887" for this suite.

• [SLOW TEST:243.824 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":3943,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:15:29.365: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Mar 24 15:15:29.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-1641 create -f -'
Mar 24 15:15:30.359: INFO: stderr: ""
Mar 24 15:15:30.359: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 24 15:15:31.370: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 15:15:31.370: INFO: Found 0 / 1
Mar 24 15:15:32.368: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 15:15:32.368: INFO: Found 1 / 1
Mar 24 15:15:32.368: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 24 15:15:32.372: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 15:15:32.372: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 24 15:15:32.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-1641 patch pod agnhost-primary-dh4x6 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 24 15:15:32.479: INFO: stderr: ""
Mar 24 15:15:32.479: INFO: stdout: "pod/agnhost-primary-dh4x6 patched\n"
STEP: checking annotations
Mar 24 15:15:32.483: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 15:15:32.483: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:15:32.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1641" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":229,"skipped":3962,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:15:32.500: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar 24 15:15:32.676: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 15:15:32.685: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 15:15:32.688: INFO: 
Logging pods the apiserver thinks is on node talos-default-worker-1 before test
Mar 24 15:15:32.693: INFO: kube-flannel-9nwp5 from kube-system started at 2021-03-24 14:11:00 +0000 UTC (1 container statuses recorded)
Mar 24 15:15:32.694: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 15:15:32.694: INFO: kube-proxy-vzljq from kube-system started at 2021-03-24 14:11:00 +0000 UTC (1 container statuses recorded)
Mar 24 15:15:32.694: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 15:15:32.694: INFO: agnhost-primary-dh4x6 from kubectl-1641 started at 2021-03-24 15:15:30 +0000 UTC (1 container statuses recorded)
Mar 24 15:15:32.694: INFO: 	Container agnhost-primary ready: true, restart count 0
Mar 24 15:15:32.694: INFO: sonobuoy from sonobuoy started at 2021-03-24 14:20:14 +0000 UTC (1 container statuses recorded)
Mar 24 15:15:32.694: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 24 15:15:32.694: INFO: 
Logging pods the apiserver thinks is on node talos-default-worker-2 before test
Mar 24 15:15:32.708: INFO: coredns-6bd7f94d9b-sq6kb from kube-system started at 2021-03-24 14:11:20 +0000 UTC (1 container statuses recorded)
Mar 24 15:15:32.708: INFO: 	Container coredns ready: true, restart count 0
Mar 24 15:15:32.708: INFO: kube-flannel-fnwwj from kube-system started at 2021-03-24 14:10:52 +0000 UTC (1 container statuses recorded)
Mar 24 15:15:32.708: INFO: 	Container kube-flannel ready: true, restart count 1
Mar 24 15:15:32.708: INFO: kube-proxy-swlm2 from kube-system started at 2021-03-24 14:10:52 +0000 UTC (1 container statuses recorded)
Mar 24 15:15:32.708: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 15:15:32.708: INFO: sonobuoy-e2e-job-865e6f320bce471a from sonobuoy started at 2021-03-24 14:20:18 +0000 UTC (2 container statuses recorded)
Mar 24 15:15:32.709: INFO: 	Container e2e ready: true, restart count 0
Mar 24 15:15:32.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b71edf8e-b5b8-4fcc-83c2-2794f97c0e3b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.5.0.5 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.5.0.5 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar 24 15:15:45.013: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.5.0.5 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:15:45.013: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.5.0.5, port: 54321
Mar 24 15:15:45.095: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.5.0.5:54321/hostname] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:15:45.095: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.5.0.5, port: 54321 UDP
Mar 24 15:15:45.180: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.5.0.5 54321] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:15:45.181: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar 24 15:15:50.248: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.5.0.5 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:15:50.248: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.5.0.5, port: 54321
Mar 24 15:15:50.319: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.5.0.5:54321/hostname] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:15:50.319: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.5.0.5, port: 54321 UDP
Mar 24 15:15:50.408: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.5.0.5 54321] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:15:50.408: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar 24 15:15:55.468: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.5.0.5 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:15:55.469: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.5.0.5, port: 54321
Mar 24 15:15:55.556: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.5.0.5:54321/hostname] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:15:55.556: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.5.0.5, port: 54321 UDP
Mar 24 15:15:55.629: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.5.0.5 54321] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:15:55.629: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar 24 15:16:00.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.5.0.5 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:16:00.694: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.5.0.5, port: 54321
Mar 24 15:16:00.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.5.0.5:54321/hostname] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:16:00.764: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.5.0.5, port: 54321 UDP
Mar 24 15:16:00.835: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.5.0.5 54321] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:16:00.835: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar 24 15:16:05.901: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.5.0.5 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:16:05.901: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.5.0.5, port: 54321
Mar 24 15:16:05.967: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.5.0.5:54321/hostname] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:16:05.967: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.5.0.5, port: 54321 UDP
Mar 24 15:16:06.045: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.5.0.5 54321] Namespace:sched-pred-4888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:16:06.045: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: removing the label kubernetes.io/e2e-b71edf8e-b5b8-4fcc-83c2-2794f97c0e3b off the node talos-default-worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b71edf8e-b5b8-4fcc-83c2-2794f97c0e3b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:16:11.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4888" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:38.680 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":230,"skipped":3979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:16:11.182: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:16:22.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4415" for this suite.

• [SLOW TEST:11.341 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":231,"skipped":4007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:16:22.523: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 24 15:16:22.729: INFO: Waiting up to 5m0s for pod "pod-dae39590-d116-49ea-a06d-00a6267afd8f" in namespace "emptydir-8085" to be "Succeeded or Failed"
Mar 24 15:16:22.734: INFO: Pod "pod-dae39590-d116-49ea-a06d-00a6267afd8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773728ms
Mar 24 15:16:24.747: INFO: Pod "pod-dae39590-d116-49ea-a06d-00a6267afd8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01838209s
STEP: Saw pod success
Mar 24 15:16:24.747: INFO: Pod "pod-dae39590-d116-49ea-a06d-00a6267afd8f" satisfied condition "Succeeded or Failed"
Mar 24 15:16:24.752: INFO: Trying to get logs from node talos-default-worker-2 pod pod-dae39590-d116-49ea-a06d-00a6267afd8f container test-container: <nil>
STEP: delete the pod
Mar 24 15:16:24.801: INFO: Waiting for pod pod-dae39590-d116-49ea-a06d-00a6267afd8f to disappear
Mar 24 15:16:24.804: INFO: Pod pod-dae39590-d116-49ea-a06d-00a6267afd8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:16:24.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8085" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4033,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:16:24.819: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2152
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Mar 24 15:16:25.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 create -f -'
Mar 24 15:16:25.363: INFO: stderr: ""
Mar 24 15:16:25.363: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 15:16:25.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 15:16:25.495: INFO: stderr: ""
Mar 24 15:16:25.495: INFO: stdout: "update-demo-nautilus-8bct8 update-demo-nautilus-q6kth "
Mar 24 15:16:25.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods update-demo-nautilus-8bct8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 15:16:25.570: INFO: stderr: ""
Mar 24 15:16:25.570: INFO: stdout: ""
Mar 24 15:16:25.570: INFO: update-demo-nautilus-8bct8 is created but not running
Mar 24 15:16:30.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 15:16:30.673: INFO: stderr: ""
Mar 24 15:16:30.673: INFO: stdout: "update-demo-nautilus-8bct8 update-demo-nautilus-q6kth "
Mar 24 15:16:30.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods update-demo-nautilus-8bct8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 15:16:30.758: INFO: stderr: ""
Mar 24 15:16:30.758: INFO: stdout: ""
Mar 24 15:16:30.758: INFO: update-demo-nautilus-8bct8 is created but not running
Mar 24 15:16:35.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 15:16:35.851: INFO: stderr: ""
Mar 24 15:16:35.851: INFO: stdout: "update-demo-nautilus-8bct8 update-demo-nautilus-q6kth "
Mar 24 15:16:35.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods update-demo-nautilus-8bct8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 15:16:35.936: INFO: stderr: ""
Mar 24 15:16:35.936: INFO: stdout: "true"
Mar 24 15:16:35.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods update-demo-nautilus-8bct8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 15:16:36.004: INFO: stderr: ""
Mar 24 15:16:36.004: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 15:16:36.004: INFO: validating pod update-demo-nautilus-8bct8
Mar 24 15:16:36.012: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 15:16:36.012: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 15:16:36.012: INFO: update-demo-nautilus-8bct8 is verified up and running
Mar 24 15:16:36.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods update-demo-nautilus-q6kth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 15:16:36.093: INFO: stderr: ""
Mar 24 15:16:36.093: INFO: stdout: "true"
Mar 24 15:16:36.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods update-demo-nautilus-q6kth -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 15:16:36.176: INFO: stderr: ""
Mar 24 15:16:36.176: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 15:16:36.176: INFO: validating pod update-demo-nautilus-q6kth
Mar 24 15:16:36.188: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 15:16:36.188: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 15:16:36.188: INFO: update-demo-nautilus-q6kth is verified up and running
STEP: using delete to clean up resources
Mar 24 15:16:36.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 delete --grace-period=0 --force -f -'
Mar 24 15:16:36.300: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 15:16:36.300: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 24 15:16:36.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get rc,svc -l name=update-demo --no-headers'
Mar 24 15:16:36.385: INFO: stderr: "No resources found in kubectl-2152 namespace.\n"
Mar 24 15:16:36.385: INFO: stdout: ""
Mar 24 15:16:36.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 15:16:36.468: INFO: stderr: ""
Mar 24 15:16:36.468: INFO: stdout: "update-demo-nautilus-8bct8\nupdate-demo-nautilus-q6kth\n"
Mar 24 15:16:36.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get rc,svc -l name=update-demo --no-headers'
Mar 24 15:16:37.079: INFO: stderr: "No resources found in kubectl-2152 namespace.\n"
Mar 24 15:16:37.079: INFO: stdout: ""
Mar 24 15:16:37.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-2152 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 15:16:37.176: INFO: stderr: ""
Mar 24 15:16:37.176: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:16:37.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2152" for this suite.

• [SLOW TEST:12.384 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":233,"skipped":4035,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:16:37.203: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 24 15:16:39.936: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-883 pod-service-account-0491a410-a430-45d2-919e-71e532a8d692 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 24 15:16:40.094: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-883 pod-service-account-0491a410-a430-45d2-919e-71e532a8d692 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 24 15:16:40.256: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-883 pod-service-account-0491a410-a430-45d2-919e-71e532a8d692 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:16:40.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-883" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":234,"skipped":4038,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:16:40.449: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Mar 24 15:16:41.315: INFO: created pod pod-service-account-defaultsa
Mar 24 15:16:41.315: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 24 15:16:41.344: INFO: created pod pod-service-account-mountsa
Mar 24 15:16:41.344: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 24 15:16:41.362: INFO: created pod pod-service-account-nomountsa
Mar 24 15:16:41.362: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 24 15:16:41.388: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 24 15:16:41.388: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 24 15:16:41.420: INFO: created pod pod-service-account-mountsa-mountspec
Mar 24 15:16:41.420: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 24 15:16:41.481: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 24 15:16:41.481: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 24 15:16:41.501: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 24 15:16:41.502: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 24 15:16:41.540: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 24 15:16:41.540: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 24 15:16:41.618: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 24 15:16:41.618: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:16:41.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5480" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":235,"skipped":4040,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:16:41.693: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 15:16:42.261: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 24 15:16:44.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195802, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195802, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195802, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752195802, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 15:16:47.337: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:16:47.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-293" for this suite.
STEP: Destroying namespace "webhook-293-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.011 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":236,"skipped":4055,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:16:47.704: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:16:47.961: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Pending, waiting for it to be Running (with Ready = true)
Mar 24 15:16:49.971: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:16:51.971: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:16:53.976: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:16:55.972: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:16:57.970: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:16:59.970: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:17:01.974: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:17:03.974: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:17:05.974: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:17:07.970: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:17:09.981: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = false)
Mar 24 15:17:11.975: INFO: The status of Pod test-webserver-fef95d58-5970-4a51-98e8-2dfff319cf9d is Running (Ready = true)
Mar 24 15:17:11.979: INFO: Container started at 2021-03-24 15:16:48 +0000 UTC, pod became ready at 2021-03-24 15:17:11 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:17:11.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7849" for this suite.

• [SLOW TEST:24.297 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":4058,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:17:12.003: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8175
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2449
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:17:18.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2108" for this suite.
STEP: Destroying namespace "nsdeletetest-8175" for this suite.
Mar 24 15:17:18.841: INFO: Namespace nsdeletetest-8175 was already deleted
STEP: Destroying namespace "nsdeletetest-2449" for this suite.

• [SLOW TEST:6.850 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":238,"skipped":4063,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:17:18.853: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 24 15:17:19.095: INFO: Waiting up to 5m0s for pod "pod-2880fe9f-458c-403b-a7e7-b4dd3e0c795f" in namespace "emptydir-9456" to be "Succeeded or Failed"
Mar 24 15:17:19.100: INFO: Pod "pod-2880fe9f-458c-403b-a7e7-b4dd3e0c795f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.456439ms
Mar 24 15:17:21.110: INFO: Pod "pod-2880fe9f-458c-403b-a7e7-b4dd3e0c795f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014454792s
STEP: Saw pod success
Mar 24 15:17:21.110: INFO: Pod "pod-2880fe9f-458c-403b-a7e7-b4dd3e0c795f" satisfied condition "Succeeded or Failed"
Mar 24 15:17:21.114: INFO: Trying to get logs from node talos-default-worker-1 pod pod-2880fe9f-458c-403b-a7e7-b4dd3e0c795f container test-container: <nil>
STEP: delete the pod
Mar 24 15:17:21.193: INFO: Waiting for pod pod-2880fe9f-458c-403b-a7e7-b4dd3e0c795f to disappear
Mar 24 15:17:21.198: INFO: Pod pod-2880fe9f-458c-403b-a7e7-b4dd3e0c795f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:17:21.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9456" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":239,"skipped":4064,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:17:21.212: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3450
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:17:21.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3450" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":240,"skipped":4083,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:17:21.461: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-2k9d
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 15:17:21.682: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-2k9d" in namespace "subpath-6620" to be "Succeeded or Failed"
Mar 24 15:17:21.686: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.400701ms
Mar 24 15:17:23.700: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.018662822s
Mar 24 15:17:25.710: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Running", Reason="", readiness=true. Elapsed: 4.028680633s
Mar 24 15:17:27.734: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Running", Reason="", readiness=true. Elapsed: 6.052098315s
Mar 24 15:17:29.743: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Running", Reason="", readiness=true. Elapsed: 8.061430314s
Mar 24 15:17:31.756: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Running", Reason="", readiness=true. Elapsed: 10.074238764s
Mar 24 15:17:33.769: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Running", Reason="", readiness=true. Elapsed: 12.087277663s
Mar 24 15:17:35.781: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Running", Reason="", readiness=true. Elapsed: 14.099177112s
Mar 24 15:17:37.796: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Running", Reason="", readiness=true. Elapsed: 16.114223888s
Mar 24 15:17:39.807: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Running", Reason="", readiness=true. Elapsed: 18.125629142s
Mar 24 15:17:41.816: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Running", Reason="", readiness=true. Elapsed: 20.134219468s
Mar 24 15:17:43.827: INFO: Pod "pod-subpath-test-secret-2k9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.14503284s
STEP: Saw pod success
Mar 24 15:17:43.827: INFO: Pod "pod-subpath-test-secret-2k9d" satisfied condition "Succeeded or Failed"
Mar 24 15:17:43.831: INFO: Trying to get logs from node talos-default-worker-1 pod pod-subpath-test-secret-2k9d container test-container-subpath-secret-2k9d: <nil>
STEP: delete the pod
Mar 24 15:17:43.872: INFO: Waiting for pod pod-subpath-test-secret-2k9d to disappear
Mar 24 15:17:43.875: INFO: Pod pod-subpath-test-secret-2k9d no longer exists
STEP: Deleting pod pod-subpath-test-secret-2k9d
Mar 24 15:17:43.876: INFO: Deleting pod "pod-subpath-test-secret-2k9d" in namespace "subpath-6620"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:17:43.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6620" for this suite.

• [SLOW TEST:22.432 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":241,"skipped":4095,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:17:43.894: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7167
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Mar 24 15:17:44.083: INFO: created test-event-1
Mar 24 15:17:44.111: INFO: created test-event-2
Mar 24 15:17:44.122: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Mar 24 15:17:44.126: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Mar 24 15:17:44.163: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:17:44.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7167" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":242,"skipped":4109,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:17:44.197: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-v9c6
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 15:17:44.397: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-v9c6" in namespace "subpath-660" to be "Succeeded or Failed"
Mar 24 15:17:44.402: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.871452ms
Mar 24 15:17:46.415: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017390362s
Mar 24 15:17:48.423: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Running", Reason="", readiness=true. Elapsed: 4.025108353s
Mar 24 15:17:50.434: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Running", Reason="", readiness=true. Elapsed: 6.036498498s
Mar 24 15:17:52.449: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Running", Reason="", readiness=true. Elapsed: 8.052045172s
Mar 24 15:17:54.461: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Running", Reason="", readiness=true. Elapsed: 10.063123408s
Mar 24 15:17:56.474: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Running", Reason="", readiness=true. Elapsed: 12.076960356s
Mar 24 15:17:58.485: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Running", Reason="", readiness=true. Elapsed: 14.08792512s
Mar 24 15:18:00.499: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Running", Reason="", readiness=true. Elapsed: 16.101943515s
Mar 24 15:18:02.511: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Running", Reason="", readiness=true. Elapsed: 18.113808795s
Mar 24 15:18:04.524: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Running", Reason="", readiness=true. Elapsed: 20.126806083s
Mar 24 15:18:06.538: INFO: Pod "pod-subpath-test-downwardapi-v9c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.140352748s
STEP: Saw pod success
Mar 24 15:18:06.538: INFO: Pod "pod-subpath-test-downwardapi-v9c6" satisfied condition "Succeeded or Failed"
Mar 24 15:18:06.542: INFO: Trying to get logs from node talos-default-worker-1 pod pod-subpath-test-downwardapi-v9c6 container test-container-subpath-downwardapi-v9c6: <nil>
STEP: delete the pod
Mar 24 15:18:06.584: INFO: Waiting for pod pod-subpath-test-downwardapi-v9c6 to disappear
Mar 24 15:18:06.589: INFO: Pod pod-subpath-test-downwardapi-v9c6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-v9c6
Mar 24 15:18:06.589: INFO: Deleting pod "pod-subpath-test-downwardapi-v9c6" in namespace "subpath-660"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:18:06.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-660" for this suite.

• [SLOW TEST:22.409 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":243,"skipped":4117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:18:06.607: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:18:06.810: INFO: Creating deployment "test-recreate-deployment"
Mar 24 15:18:06.823: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 24 15:18:06.835: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 24 15:18:08.849: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 24 15:18:08.852: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 24 15:18:08.882: INFO: Updating deployment test-recreate-deployment
Mar 24 15:18:08.882: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 15:18:09.202: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-617  d654bc9d-1444-4768-9e15-662b31ef178c 23494 2 2021-03-24 15:18:06 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-24 15:18:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 15:18:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039b4198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-24 15:18:09 +0000 UTC,LastTransitionTime:2021-03-24 15:18:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-03-24 15:18:09 +0000 UTC,LastTransitionTime:2021-03-24 15:18:06 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 24 15:18:09.207: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-617  dcd058b7-1c26-46e2-a758-43cbb8f8a5b6 23491 1 2021-03-24 15:18:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d654bc9d-1444-4768-9e15-662b31ef178c 0xc0039b4ca0 0xc0039b4ca1}] []  [{kube-controller-manager Update apps/v1 2021-03-24 15:18:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d654bc9d-1444-4768-9e15-662b31ef178c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039b4dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 15:18:09.207: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 24 15:18:09.207: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-617  764e24c6-abbb-4017-a8c2-7c9ef2537b7a 23481 2 2021-03-24 15:18:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d654bc9d-1444-4768-9e15-662b31ef178c 0xc0039b49d7 0xc0039b49d8}] []  [{kube-controller-manager Update apps/v1 2021-03-24 15:18:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d654bc9d-1444-4768-9e15-662b31ef178c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039b4b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 15:18:09.213: INFO: Pod "test-recreate-deployment-f79dd4667-xj9jc" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-xj9jc test-recreate-deployment-f79dd4667- deployment-617  a6a00e5a-3138-49a3-9300-7c0decb1f6d2 23495 0 2021-03-24 15:18:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 dcd058b7-1c26-46e2-a758-43cbb8f8a5b6 0xc0039b5700 0xc0039b5701}] []  [{kube-controller-manager Update v1 2021-03-24 15:18:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd058b7-1c26-46e2-a758-43cbb8f8a5b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 15:18:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fjcd5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fjcd5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fjcd5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:18:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:18:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:18:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:18:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:,StartTime:2021-03-24 15:18:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:18:09.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-617" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":244,"skipped":4142,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:18:09.235: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Mar 24 15:18:09.427: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 24 15:18:09.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 create -f -'
Mar 24 15:18:09.846: INFO: stderr: ""
Mar 24 15:18:09.846: INFO: stdout: "service/agnhost-replica created\n"
Mar 24 15:18:09.846: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 24 15:18:09.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 create -f -'
Mar 24 15:18:10.281: INFO: stderr: ""
Mar 24 15:18:10.281: INFO: stdout: "service/agnhost-primary created\n"
Mar 24 15:18:10.281: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 24 15:18:10.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 create -f -'
Mar 24 15:18:10.623: INFO: stderr: ""
Mar 24 15:18:10.623: INFO: stdout: "service/frontend created\n"
Mar 24 15:18:10.623: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 24 15:18:10.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 create -f -'
Mar 24 15:18:10.965: INFO: stderr: ""
Mar 24 15:18:10.965: INFO: stdout: "deployment.apps/frontend created\n"
Mar 24 15:18:10.965: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 24 15:18:10.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 create -f -'
Mar 24 15:18:11.281: INFO: stderr: ""
Mar 24 15:18:11.282: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 24 15:18:11.282: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 24 15:18:11.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 create -f -'
Mar 24 15:18:11.727: INFO: stderr: ""
Mar 24 15:18:11.727: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Mar 24 15:18:11.727: INFO: Waiting for all frontend pods to be Running.
Mar 24 15:18:16.777: INFO: Waiting for frontend to serve content.
Mar 24 15:18:16.788: INFO: Trying to add a new entry to the guestbook.
Mar 24 15:18:16.800: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 24 15:18:16.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 delete --grace-period=0 --force -f -'
Mar 24 15:18:16.960: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 15:18:16.960: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 15:18:16.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 delete --grace-period=0 --force -f -'
Mar 24 15:18:17.135: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 15:18:17.135: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 15:18:17.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 delete --grace-period=0 --force -f -'
Mar 24 15:18:17.323: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 15:18:17.323: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 15:18:17.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 delete --grace-period=0 --force -f -'
Mar 24 15:18:17.415: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 15:18:17.415: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 15:18:17.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 delete --grace-period=0 --force -f -'
Mar 24 15:18:17.528: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 15:18:17.528: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 15:18:17.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=kubectl-9509 delete --grace-period=0 --force -f -'
Mar 24 15:18:17.636: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 15:18:17.636: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:18:17.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9509" for this suite.

• [SLOW TEST:8.440 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":245,"skipped":4157,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:18:17.675: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-72
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Mar 24 15:18:17.917: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:18:31.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-72" for this suite.

• [SLOW TEST:13.966 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":246,"skipped":4169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:18:31.643: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:18:31.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4529" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":247,"skipped":4208,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:18:31.939: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1532
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:18:48.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1532" for this suite.

• [SLOW TEST:16.488 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":248,"skipped":4213,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:18:48.426: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-59bf
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 15:18:48.724: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-59bf" in namespace "subpath-3269" to be "Succeeded or Failed"
Mar 24 15:18:48.729: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.352842ms
Mar 24 15:18:50.741: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.016389703s
Mar 24 15:18:52.751: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Running", Reason="", readiness=true. Elapsed: 4.027079603s
Mar 24 15:18:54.765: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Running", Reason="", readiness=true. Elapsed: 6.041017623s
Mar 24 15:18:56.775: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Running", Reason="", readiness=true. Elapsed: 8.051208297s
Mar 24 15:18:58.783: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Running", Reason="", readiness=true. Elapsed: 10.058765925s
Mar 24 15:19:00.795: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Running", Reason="", readiness=true. Elapsed: 12.071255764s
Mar 24 15:19:02.808: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Running", Reason="", readiness=true. Elapsed: 14.083854471s
Mar 24 15:19:04.820: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Running", Reason="", readiness=true. Elapsed: 16.095941563s
Mar 24 15:19:06.835: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Running", Reason="", readiness=true. Elapsed: 18.110388391s
Mar 24 15:19:08.844: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Running", Reason="", readiness=true. Elapsed: 20.120126678s
Mar 24 15:19:10.855: INFO: Pod "pod-subpath-test-projected-59bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.13092179s
STEP: Saw pod success
Mar 24 15:19:10.855: INFO: Pod "pod-subpath-test-projected-59bf" satisfied condition "Succeeded or Failed"
Mar 24 15:19:10.858: INFO: Trying to get logs from node talos-default-worker-1 pod pod-subpath-test-projected-59bf container test-container-subpath-projected-59bf: <nil>
STEP: delete the pod
Mar 24 15:19:10.912: INFO: Waiting for pod pod-subpath-test-projected-59bf to disappear
Mar 24 15:19:10.916: INFO: Pod pod-subpath-test-projected-59bf no longer exists
STEP: Deleting pod pod-subpath-test-projected-59bf
Mar 24 15:19:10.916: INFO: Deleting pod "pod-subpath-test-projected-59bf" in namespace "subpath-3269"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:19:10.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3269" for this suite.

• [SLOW TEST:22.524 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":249,"skipped":4217,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:19:10.950: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-2285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 24 15:19:11.156: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 15:20:11.189: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Mar 24 15:20:11.376: INFO: Created pod: pod0-sched-preemption-low-priority
Mar 24 15:20:11.428: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:20:27.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2285" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.675 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":250,"skipped":4219,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:20:27.629: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:20:27.930: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"26508a50-0f86-41dd-913c-d70d4644f539", Controller:(*bool)(0xc00492e7f6), BlockOwnerDeletion:(*bool)(0xc00492e7f7)}}
Mar 24 15:20:27.977: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d46913af-0a8e-429a-949b-361fb9397a99", Controller:(*bool)(0xc004a36e2e), BlockOwnerDeletion:(*bool)(0xc004a36e2f)}}
Mar 24 15:20:28.001: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4d3c5a0b-8fec-4d58-9517-2d16df40db20", Controller:(*bool)(0xc00492ea4e), BlockOwnerDeletion:(*bool)(0xc00492ea4f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:20:33.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7382" for this suite.

• [SLOW TEST:5.542 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":251,"skipped":4227,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:20:33.171: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 15:20:33.491: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ceb6580-cfd1-4e9c-8614-5f79834de6e1" in namespace "downward-api-5916" to be "Succeeded or Failed"
Mar 24 15:20:33.502: INFO: Pod "downwardapi-volume-9ceb6580-cfd1-4e9c-8614-5f79834de6e1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.115197ms
Mar 24 15:20:35.513: INFO: Pod "downwardapi-volume-9ceb6580-cfd1-4e9c-8614-5f79834de6e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022025092s
STEP: Saw pod success
Mar 24 15:20:35.513: INFO: Pod "downwardapi-volume-9ceb6580-cfd1-4e9c-8614-5f79834de6e1" satisfied condition "Succeeded or Failed"
Mar 24 15:20:35.517: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-9ceb6580-cfd1-4e9c-8614-5f79834de6e1 container client-container: <nil>
STEP: delete the pod
Mar 24 15:20:35.559: INFO: Waiting for pod downwardapi-volume-9ceb6580-cfd1-4e9c-8614-5f79834de6e1 to disappear
Mar 24 15:20:35.562: INFO: Pod downwardapi-volume-9ceb6580-cfd1-4e9c-8614-5f79834de6e1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:20:35.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5916" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":252,"skipped":4232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:20:35.578: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:20:37.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3328" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":253,"skipped":4254,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:20:37.910: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:20:38.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3442" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":254,"skipped":4259,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:20:38.291: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-9891d5ab-defd-4b82-8bec-0799589f0286
STEP: Creating a pod to test consume secrets
Mar 24 15:20:38.528: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9e48de99-0ccf-4510-a7e8-d90df3dae77f" in namespace "projected-3583" to be "Succeeded or Failed"
Mar 24 15:20:38.539: INFO: Pod "pod-projected-secrets-9e48de99-0ccf-4510-a7e8-d90df3dae77f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.790465ms
Mar 24 15:20:40.546: INFO: Pod "pod-projected-secrets-9e48de99-0ccf-4510-a7e8-d90df3dae77f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017796511s
STEP: Saw pod success
Mar 24 15:20:40.546: INFO: Pod "pod-projected-secrets-9e48de99-0ccf-4510-a7e8-d90df3dae77f" satisfied condition "Succeeded or Failed"
Mar 24 15:20:40.550: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-secrets-9e48de99-0ccf-4510-a7e8-d90df3dae77f container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 15:20:40.589: INFO: Waiting for pod pod-projected-secrets-9e48de99-0ccf-4510-a7e8-d90df3dae77f to disappear
Mar 24 15:20:40.593: INFO: Pod pod-projected-secrets-9e48de99-0ccf-4510-a7e8-d90df3dae77f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:20:40.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3583" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":255,"skipped":4261,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:20:40.606: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7751
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-7751
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7751 to expose endpoints map[]
Mar 24 15:20:40.872: INFO: successfully validated that service multi-endpoint-test in namespace services-7751 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7751
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7751 to expose endpoints map[pod1:[100]]
Mar 24 15:20:42.941: INFO: successfully validated that service multi-endpoint-test in namespace services-7751 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7751
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7751 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 24 15:20:44.981: INFO: successfully validated that service multi-endpoint-test in namespace services-7751 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7751
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7751 to expose endpoints map[pod2:[101]]
Mar 24 15:20:45.050: INFO: successfully validated that service multi-endpoint-test in namespace services-7751 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7751
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7751 to expose endpoints map[]
Mar 24 15:20:45.115: INFO: successfully validated that service multi-endpoint-test in namespace services-7751 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:20:45.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7751" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":256,"skipped":4275,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:20:45.172: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-1cab4f12-6417-47a5-9bb8-f2c27c209e0a in namespace container-probe-7366
Mar 24 15:20:47.371: INFO: Started pod liveness-1cab4f12-6417-47a5-9bb8-f2c27c209e0a in namespace container-probe-7366
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 15:20:47.375: INFO: Initial restart count of pod liveness-1cab4f12-6417-47a5-9bb8-f2c27c209e0a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:24:48.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7366" for this suite.

• [SLOW TEST:243.805 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":257,"skipped":4292,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:24:48.979: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:24:49.232: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-ea8a0858-e831-4767-9c08-27a2116610d1" in namespace "security-context-test-7441" to be "Succeeded or Failed"
Mar 24 15:24:49.247: INFO: Pod "busybox-privileged-false-ea8a0858-e831-4767-9c08-27a2116610d1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.605829ms
Mar 24 15:24:51.257: INFO: Pod "busybox-privileged-false-ea8a0858-e831-4767-9c08-27a2116610d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025121928s
Mar 24 15:24:51.257: INFO: Pod "busybox-privileged-false-ea8a0858-e831-4767-9c08-27a2116610d1" satisfied condition "Succeeded or Failed"
Mar 24 15:24:51.276: INFO: Got logs for pod "busybox-privileged-false-ea8a0858-e831-4767-9c08-27a2116610d1": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:24:51.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7441" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":258,"skipped":4302,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:24:51.296: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9136
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:25:19.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9136" for this suite.

• [SLOW TEST:28.337 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":259,"skipped":4306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:25:19.633: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6895
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-44f892c7-581a-4981-9ea7-d0a2736a5b08
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:25:22.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6895" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4356,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:25:22.024: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar 24 15:25:22.212: INFO: Waiting up to 5m0s for pod "downward-api-7354c201-3a9d-4580-b624-481d145fe6db" in namespace "downward-api-9166" to be "Succeeded or Failed"
Mar 24 15:25:22.215: INFO: Pod "downward-api-7354c201-3a9d-4580-b624-481d145fe6db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.65092ms
Mar 24 15:25:24.226: INFO: Pod "downward-api-7354c201-3a9d-4580-b624-481d145fe6db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013897493s
STEP: Saw pod success
Mar 24 15:25:24.226: INFO: Pod "downward-api-7354c201-3a9d-4580-b624-481d145fe6db" satisfied condition "Succeeded or Failed"
Mar 24 15:25:24.229: INFO: Trying to get logs from node talos-default-worker-2 pod downward-api-7354c201-3a9d-4580-b624-481d145fe6db container dapi-container: <nil>
STEP: delete the pod
Mar 24 15:25:24.305: INFO: Waiting for pod downward-api-7354c201-3a9d-4580-b624-481d145fe6db to disappear
Mar 24 15:25:24.309: INFO: Pod downward-api-7354c201-3a9d-4580-b624-481d145fe6db no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:25:24.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9166" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":261,"skipped":4422,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:25:24.327: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4953
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3313
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:25:54.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-686" for this suite.
STEP: Destroying namespace "nsdeletetest-4953" for this suite.
Mar 24 15:25:54.157: INFO: Namespace nsdeletetest-4953 was already deleted
STEP: Destroying namespace "nsdeletetest-3313" for this suite.

• [SLOW TEST:29.862 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":262,"skipped":4442,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:25:54.189: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9360
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9360
STEP: creating service affinity-clusterip-transition in namespace services-9360
STEP: creating replication controller affinity-clusterip-transition in namespace services-9360
I0324 15:25:54.472277      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-9360, replica count: 3
I0324 15:25:57.523312      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 15:25:57.540: INFO: Creating new exec pod
Mar 24 15:26:00.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-9360 exec execpod-affinitytjkcm -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Mar 24 15:26:01.279: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 24 15:26:01.279: INFO: stdout: ""
Mar 24 15:26:01.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-9360 exec execpod-affinitytjkcm -- /bin/sh -x -c nc -zv -t -w 2 10.103.199.227 80'
Mar 24 15:26:01.452: INFO: stderr: "+ nc -zv -t -w 2 10.103.199.227 80\nConnection to 10.103.199.227 80 port [tcp/http] succeeded!\n"
Mar 24 15:26:01.452: INFO: stdout: ""
Mar 24 15:26:01.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-9360 exec execpod-affinitytjkcm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.199.227:80/ ; done'
Mar 24 15:26:01.718: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n"
Mar 24 15:26:01.718: INFO: stdout: "\naffinity-clusterip-transition-vtpkx\naffinity-clusterip-transition-vtpkx\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-mnjxm\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-mnjxm\naffinity-clusterip-transition-vtpkx\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-vtpkx\naffinity-clusterip-transition-vtpkx\naffinity-clusterip-transition-mnjxm\naffinity-clusterip-transition-mnjxm"
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-vtpkx
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-vtpkx
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-mnjxm
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-mnjxm
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-vtpkx
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-vtpkx
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-vtpkx
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-mnjxm
Mar 24 15:26:01.718: INFO: Received response from host: affinity-clusterip-transition-mnjxm
Mar 24 15:26:01.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=services-9360 exec execpod-affinitytjkcm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.199.227:80/ ; done'
Mar 24 15:26:01.961: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.199.227:80/\n"
Mar 24 15:26:01.961: INFO: stdout: "\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb\naffinity-clusterip-transition-pxzfb"
Mar 24 15:26:01.961: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.961: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.961: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.961: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.961: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.961: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Received response from host: affinity-clusterip-transition-pxzfb
Mar 24 15:26:01.962: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9360, will wait for the garbage collector to delete the pods
Mar 24 15:26:02.097: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.781055ms
Mar 24 15:26:02.697: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 600.163126ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:26:17.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9360" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:23.721 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":263,"skipped":4447,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:26:17.909: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4359
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 24 15:26:18.108: INFO: Waiting up to 5m0s for pod "pod-75d33469-b5e2-4ac5-8c0e-bed7e3b76767" in namespace "emptydir-4359" to be "Succeeded or Failed"
Mar 24 15:26:18.112: INFO: Pod "pod-75d33469-b5e2-4ac5-8c0e-bed7e3b76767": Phase="Pending", Reason="", readiness=false. Elapsed: 3.495503ms
Mar 24 15:26:20.124: INFO: Pod "pod-75d33469-b5e2-4ac5-8c0e-bed7e3b76767": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016260855s
STEP: Saw pod success
Mar 24 15:26:20.125: INFO: Pod "pod-75d33469-b5e2-4ac5-8c0e-bed7e3b76767" satisfied condition "Succeeded or Failed"
Mar 24 15:26:20.129: INFO: Trying to get logs from node talos-default-worker-1 pod pod-75d33469-b5e2-4ac5-8c0e-bed7e3b76767 container test-container: <nil>
STEP: delete the pod
Mar 24 15:26:20.187: INFO: Waiting for pod pod-75d33469-b5e2-4ac5-8c0e-bed7e3b76767 to disappear
Mar 24 15:26:20.191: INFO: Pod pod-75d33469-b5e2-4ac5-8c0e-bed7e3b76767 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:26:20.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4359" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":264,"skipped":4459,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:26:20.216: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:26:20.420: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 24 15:26:25.440: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 24 15:26:25.440: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 24 15:26:27.452: INFO: Creating deployment "test-rollover-deployment"
Mar 24 15:26:27.473: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 24 15:26:29.490: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 24 15:26:29.497: INFO: Ensure that both replica sets have 1 created replica
Mar 24 15:26:29.503: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 24 15:26:29.529: INFO: Updating deployment test-rollover-deployment
Mar 24 15:26:29.529: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 24 15:26:31.542: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 24 15:26:31.549: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 24 15:26:31.556: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 15:26:31.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196391, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:33.570: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 15:26:33.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196391, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:35.573: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 15:26:35.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196391, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:37.570: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 15:26:37.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196391, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:39.573: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 15:26:39.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196391, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:41.625: INFO: 
Mar 24 15:26:41.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196401, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196387, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:43.570: INFO: 
Mar 24 15:26:43.570: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 15:26:43.579: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7611  d6c5c0c1-3e47-4385-8e59-1dcf307ad438 25570 2 2021-03-24 15:26:27 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-24 15:26:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 15:26:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a36b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-24 15:26:27 +0000 UTC,LastTransitionTime:2021-03-24 15:26:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-03-24 15:26:42 +0000 UTC,LastTransitionTime:2021-03-24 15:26:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 24 15:26:43.584: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-7611  93af7921-76b9-420b-8762-0a61933dbba8 25559 2 2021-03-24 15:26:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d6c5c0c1-3e47-4385-8e59-1dcf307ad438 0xc003ea7447 0xc003ea7448}] []  [{kube-controller-manager Update apps/v1 2021-03-24 15:26:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6c5c0c1-3e47-4385-8e59-1dcf307ad438\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ea74d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 24 15:26:43.584: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 24 15:26:43.584: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7611  8b8a3345-6277-4303-a2fc-33a2b9b58dba 25569 2 2021-03-24 15:26:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d6c5c0c1-3e47-4385-8e59-1dcf307ad438 0xc003ea7337 0xc003ea7338}] []  [{e2e.test Update apps/v1 2021-03-24 15:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 15:26:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6c5c0c1-3e47-4385-8e59-1dcf307ad438\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ea73d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 15:26:43.585: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-7611  35404aac-85e6-462d-a4b0-9a569e34b997 25527 2 2021-03-24 15:26:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d6c5c0c1-3e47-4385-8e59-1dcf307ad438 0xc003ea7547 0xc003ea7548}] []  [{kube-controller-manager Update apps/v1 2021-03-24 15:26:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6c5c0c1-3e47-4385-8e59-1dcf307ad438\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ea75d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 15:26:43.590: INFO: Pod "test-rollover-deployment-668db69979-k44lp" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-k44lp test-rollover-deployment-668db69979- deployment-7611  0c6b94ca-44a9-4c4e-95d5-85e630f9f139 25538 0 2021-03-24 15:26:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 93af7921-76b9-420b-8762-0a61933dbba8 0xc004a36ec7 0xc004a36ec8}] []  [{kube-controller-manager Update v1 2021-03-24 15:26:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93af7921-76b9-420b-8762-0a61933dbba8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 15:26:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f4tz4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f4tz4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f4tz4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-default-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:26:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:26:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:26:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 15:26:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.5.0.5,PodIP:10.244.2.52,StartTime:2021-03-24 15:26:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 15:26:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://c79f5845b24638bef273baaf57e09429338e3c01ef89416122977881e0edaa72,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:26:43.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7611" for this suite.

• [SLOW TEST:23.398 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":265,"skipped":4470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:26:43.615: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8070
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar 24 15:26:43.778: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Mar 24 15:26:44.493: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 24 15:26:46.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:48.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:51.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:52.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:54.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:57.526: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196404, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 15:26:59.739: INFO: Waited 739.667094ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:27:00.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8070" for this suite.

• [SLOW TEST:16.900 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":266,"skipped":4506,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:27:00.516: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:27:02.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9383" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4508,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:27:02.793: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:27:03.017: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b8bc0dc8-6dfe-4f75-b0ba-c132ed3c330f" in namespace "security-context-test-623" to be "Succeeded or Failed"
Mar 24 15:27:03.022: INFO: Pod "busybox-user-65534-b8bc0dc8-6dfe-4f75-b0ba-c132ed3c330f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.892488ms
Mar 24 15:27:05.035: INFO: Pod "busybox-user-65534-b8bc0dc8-6dfe-4f75-b0ba-c132ed3c330f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01729849s
Mar 24 15:27:05.035: INFO: Pod "busybox-user-65534-b8bc0dc8-6dfe-4f75-b0ba-c132ed3c330f" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:27:05.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-623" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":268,"skipped":4526,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:27:05.055: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7593
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7593, will wait for the garbage collector to delete the pods
Mar 24 15:27:07.384: INFO: Deleting Job.batch foo took: 88.795458ms
Mar 24 15:27:07.885: INFO: Terminating Job.batch foo pods took: 500.237385ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:27:47.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7593" for this suite.

• [SLOW TEST:42.776 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":269,"skipped":4540,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:27:47.831: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5204
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-f154f699-8f81-482b-bc38-b87015fd5685
STEP: Creating a pod to test consume configMaps
Mar 24 15:27:48.038: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-316bacb3-0733-4657-9206-ca5c728a26b4" in namespace "projected-5204" to be "Succeeded or Failed"
Mar 24 15:27:48.042: INFO: Pod "pod-projected-configmaps-316bacb3-0733-4657-9206-ca5c728a26b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884011ms
Mar 24 15:27:50.052: INFO: Pod "pod-projected-configmaps-316bacb3-0733-4657-9206-ca5c728a26b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013881097s
STEP: Saw pod success
Mar 24 15:27:50.052: INFO: Pod "pod-projected-configmaps-316bacb3-0733-4657-9206-ca5c728a26b4" satisfied condition "Succeeded or Failed"
Mar 24 15:27:50.057: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-configmaps-316bacb3-0733-4657-9206-ca5c728a26b4 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 15:27:50.099: INFO: Waiting for pod pod-projected-configmaps-316bacb3-0733-4657-9206-ca5c728a26b4 to disappear
Mar 24 15:27:50.102: INFO: Pod pod-projected-configmaps-316bacb3-0733-4657-9206-ca5c728a26b4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:27:50.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5204" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":270,"skipped":4540,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:27:50.122: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-48fb921b-95d3-4ad8-b3c4-ebdd5eb69017
STEP: Creating a pod to test consume configMaps
Mar 24 15:27:50.335: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ee5d9bb-237c-4c03-87c3-fd7ec63ed04f" in namespace "configmap-8885" to be "Succeeded or Failed"
Mar 24 15:27:50.352: INFO: Pod "pod-configmaps-0ee5d9bb-237c-4c03-87c3-fd7ec63ed04f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.118562ms
Mar 24 15:27:52.407: INFO: Pod "pod-configmaps-0ee5d9bb-237c-4c03-87c3-fd7ec63ed04f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.071179297s
STEP: Saw pod success
Mar 24 15:27:52.407: INFO: Pod "pod-configmaps-0ee5d9bb-237c-4c03-87c3-fd7ec63ed04f" satisfied condition "Succeeded or Failed"
Mar 24 15:27:52.411: INFO: Trying to get logs from node talos-default-worker-1 pod pod-configmaps-0ee5d9bb-237c-4c03-87c3-fd7ec63ed04f container agnhost-container: <nil>
STEP: delete the pod
Mar 24 15:27:52.438: INFO: Waiting for pod pod-configmaps-0ee5d9bb-237c-4c03-87c3-fd7ec63ed04f to disappear
Mar 24 15:27:52.442: INFO: Pod pod-configmaps-0ee5d9bb-237c-4c03-87c3-fd7ec63ed04f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:27:52.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8885" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4559,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:27:52.463: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-70
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-70
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Mar 24 15:27:52.698: INFO: Found 0 stateful pods, waiting for 3
Mar 24 15:28:02.707: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 15:28:02.707: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 15:28:02.707: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 15:28:02.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-70 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 15:28:02.887: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 15:28:02.887: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 15:28:02.887: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 24 15:28:12.938: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 24 15:28:22.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-70 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 15:28:23.129: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 15:28:23.129: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 15:28:23.129: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 15:28:33.159: INFO: Waiting for StatefulSet statefulset-70/ss2 to complete update
Mar 24 15:28:33.159: INFO: Waiting for Pod statefulset-70/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Mar 24 15:28:43.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-70 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 15:28:43.324: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 15:28:43.324: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 15:28:43.324: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 15:28:53.382: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 24 15:29:03.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=statefulset-70 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 15:29:03.574: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 15:29:03.574: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 15:29:03.574: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 15:29:23.605: INFO: Waiting for StatefulSet statefulset-70/ss2 to complete update
Mar 24 15:29:23.605: INFO: Waiting for Pod statefulset-70/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 15:29:33.620: INFO: Deleting all statefulset in ns statefulset-70
Mar 24 15:29:33.624: INFO: Scaling statefulset ss2 to 0
Mar 24 15:29:43.663: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 15:29:43.667: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:29:43.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-70" for this suite.

• [SLOW TEST:111.244 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":272,"skipped":4601,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:29:43.707: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1309
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-7f9fb38c-7d6a-4189-abf1-b245c965c8bf
STEP: Creating configMap with name cm-test-opt-upd-86c6da0d-3c85-4f79-8877-8b86827500ce
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7f9fb38c-7d6a-4189-abf1-b245c965c8bf
STEP: Updating configmap cm-test-opt-upd-86c6da0d-3c85-4f79-8877-8b86827500ce
STEP: Creating configMap with name cm-test-opt-create-eb7caeb2-a333-43b0-9811-18ecc6819dc4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:29:48.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1309" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:29:48.150: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 15:29:48.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 15:29:51.668: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:29:51.674: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-678-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:29:52.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9632" for this suite.
STEP: Destroying namespace "webhook-9632-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":274,"skipped":4632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:29:52.990: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-5142270c-e87b-4a56-b342-cc6e9e5bb46d
STEP: Creating a pod to test consume configMaps
Mar 24 15:29:53.228: INFO: Waiting up to 5m0s for pod "pod-configmaps-952aad39-afd0-48e0-b763-46cf2b394c63" in namespace "configmap-3438" to be "Succeeded or Failed"
Mar 24 15:29:53.233: INFO: Pod "pod-configmaps-952aad39-afd0-48e0-b763-46cf2b394c63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.73779ms
Mar 24 15:29:55.244: INFO: Pod "pod-configmaps-952aad39-afd0-48e0-b763-46cf2b394c63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01670184s
STEP: Saw pod success
Mar 24 15:29:55.245: INFO: Pod "pod-configmaps-952aad39-afd0-48e0-b763-46cf2b394c63" satisfied condition "Succeeded or Failed"
Mar 24 15:29:55.248: INFO: Trying to get logs from node talos-default-worker-2 pod pod-configmaps-952aad39-afd0-48e0-b763-46cf2b394c63 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 15:29:55.296: INFO: Waiting for pod pod-configmaps-952aad39-afd0-48e0-b763-46cf2b394c63 to disappear
Mar 24 15:29:55.301: INFO: Pod pod-configmaps-952aad39-afd0-48e0-b763-46cf2b394c63 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:29:55.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3438" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4689,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:29:55.333: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8104
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8104.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8104.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 15:29:57.635: INFO: DNS probes using dns-8104/dns-test-93ddeccc-3772-4997-999c-b7d376814c01 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:29:57.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8104" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":276,"skipped":4707,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:29:57.680: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 15:29:58.414: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 15:30:01.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:01.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6970" for this suite.
STEP: Destroying namespace "webhook-6970-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":277,"skipped":4711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:01.886: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7140
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-674ddcec-5ff3-44ab-bc3e-daf7e392a77f
STEP: Creating configMap with name cm-test-opt-upd-a1515664-228c-4451-b85c-5b9b62ed1adb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-674ddcec-5ff3-44ab-bc3e-daf7e392a77f
STEP: Updating configmap cm-test-opt-upd-a1515664-228c-4451-b85c-5b9b62ed1adb
STEP: Creating configMap with name cm-test-opt-create-570a75cc-3e52-46fc-95fe-2715e6a44740
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:06.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7140" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":278,"skipped":4762,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:06.361: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-5847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Mar 24 15:30:06.577: INFO: Major version: 1
STEP: Confirm minor version
Mar 24 15:30:06.577: INFO: cleanMinorVersion: 20
Mar 24 15:30:06.577: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:06.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5847" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":279,"skipped":4775,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:06.591: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2459
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar 24 15:30:06.805: INFO: Waiting up to 5m0s for pod "downward-api-b9af3bb5-1752-49bb-a8c2-68b900d8c880" in namespace "downward-api-2459" to be "Succeeded or Failed"
Mar 24 15:30:06.808: INFO: Pod "downward-api-b9af3bb5-1752-49bb-a8c2-68b900d8c880": Phase="Pending", Reason="", readiness=false. Elapsed: 3.503741ms
Mar 24 15:30:08.822: INFO: Pod "downward-api-b9af3bb5-1752-49bb-a8c2-68b900d8c880": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01708615s
STEP: Saw pod success
Mar 24 15:30:08.822: INFO: Pod "downward-api-b9af3bb5-1752-49bb-a8c2-68b900d8c880" satisfied condition "Succeeded or Failed"
Mar 24 15:30:08.826: INFO: Trying to get logs from node talos-default-worker-2 pod downward-api-b9af3bb5-1752-49bb-a8c2-68b900d8c880 container dapi-container: <nil>
STEP: delete the pod
Mar 24 15:30:08.863: INFO: Waiting for pod downward-api-b9af3bb5-1752-49bb-a8c2-68b900d8c880 to disappear
Mar 24 15:30:08.867: INFO: Pod downward-api-b9af3bb5-1752-49bb-a8c2-68b900d8c880 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:08.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2459" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":280,"skipped":4777,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:08.884: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 24 15:30:11.692: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1224afd0-9969-4ef5-997e-1ac0a8f8612f"
Mar 24 15:30:11.692: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1224afd0-9969-4ef5-997e-1ac0a8f8612f" in namespace "pods-7061" to be "terminated due to deadline exceeded"
Mar 24 15:30:11.696: INFO: Pod "pod-update-activedeadlineseconds-1224afd0-9969-4ef5-997e-1ac0a8f8612f": Phase="Running", Reason="", readiness=true. Elapsed: 4.751798ms
Mar 24 15:30:13.704: INFO: Pod "pod-update-activedeadlineseconds-1224afd0-9969-4ef5-997e-1ac0a8f8612f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012546661s
Mar 24 15:30:15.717: INFO: Pod "pod-update-activedeadlineseconds-1224afd0-9969-4ef5-997e-1ac0a8f8612f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.024875507s
Mar 24 15:30:15.717: INFO: Pod "pod-update-activedeadlineseconds-1224afd0-9969-4ef5-997e-1ac0a8f8612f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:15.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7061" for this suite.

• [SLOW TEST:6.854 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4785,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:15.738: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 24 15:30:20.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 15:30:20.045: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 15:30:22.046: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 15:30:22.058: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 15:30:24.045: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 15:30:24.061: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:24.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9852" for this suite.

• [SLOW TEST:8.340 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":282,"skipped":4786,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:24.079: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-850
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-55aa66fa-fb2d-48f1-84c4-f8b9d36c105a
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:24.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-850" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":283,"skipped":4793,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:24.273: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9221
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:24.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9221" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":284,"skipped":4813,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:24.531: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1457
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 15:30:25.029: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 24 15:30:27.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196625, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196625, loc:(*time.Location)(0x797fe80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196625, loc:(*time.Location)(0x797fe80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752196625, loc:(*time.Location)(0x797fe80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 15:30:30.114: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 24 15:30:30.151: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:30.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1457" for this suite.
STEP: Destroying namespace "webhook-1457-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.769 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":285,"skipped":4816,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:30.303: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-6599cf21-4865-40fa-b4bf-20c2a1fe3f9f
STEP: Creating a pod to test consume secrets
Mar 24 15:30:30.520: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e528912c-676b-4f69-866d-997df9bb0be5" in namespace "projected-3922" to be "Succeeded or Failed"
Mar 24 15:30:30.523: INFO: Pod "pod-projected-secrets-e528912c-676b-4f69-866d-997df9bb0be5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.430757ms
Mar 24 15:30:32.534: INFO: Pod "pod-projected-secrets-e528912c-676b-4f69-866d-997df9bb0be5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013713923s
STEP: Saw pod success
Mar 24 15:30:32.534: INFO: Pod "pod-projected-secrets-e528912c-676b-4f69-866d-997df9bb0be5" satisfied condition "Succeeded or Failed"
Mar 24 15:30:32.537: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-secrets-e528912c-676b-4f69-866d-997df9bb0be5 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 15:30:32.569: INFO: Waiting for pod pod-projected-secrets-e528912c-676b-4f69-866d-997df9bb0be5 to disappear
Mar 24 15:30:32.572: INFO: Pod pod-projected-secrets-e528912c-676b-4f69-866d-997df9bb0be5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:32.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3922" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":286,"skipped":4821,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:32.584: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 15:30:32.804: INFO: Waiting up to 5m0s for pod "downwardapi-volume-650f8bdd-033e-450f-913a-90684fd61f76" in namespace "downward-api-6578" to be "Succeeded or Failed"
Mar 24 15:30:32.808: INFO: Pod "downwardapi-volume-650f8bdd-033e-450f-913a-90684fd61f76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.594303ms
Mar 24 15:30:34.820: INFO: Pod "downwardapi-volume-650f8bdd-033e-450f-913a-90684fd61f76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016140709s
STEP: Saw pod success
Mar 24 15:30:34.820: INFO: Pod "downwardapi-volume-650f8bdd-033e-450f-913a-90684fd61f76" satisfied condition "Succeeded or Failed"
Mar 24 15:30:34.824: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-650f8bdd-033e-450f-913a-90684fd61f76 container client-container: <nil>
STEP: delete the pod
Mar 24 15:30:34.865: INFO: Waiting for pod downwardapi-volume-650f8bdd-033e-450f-913a-90684fd61f76 to disappear
Mar 24 15:30:34.868: INFO: Pod downwardapi-volume-650f8bdd-033e-450f-913a-90684fd61f76 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:34.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6578" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":4827,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:34.881: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9106
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-ec2628b5-dfc2-4b18-b818-9a570d09a8d7
STEP: Creating a pod to test consume configMaps
Mar 24 15:30:35.084: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-72e341a5-6437-4f30-b993-cc83cb80fc01" in namespace "projected-9106" to be "Succeeded or Failed"
Mar 24 15:30:35.097: INFO: Pod "pod-projected-configmaps-72e341a5-6437-4f30-b993-cc83cb80fc01": Phase="Pending", Reason="", readiness=false. Elapsed: 12.630567ms
Mar 24 15:30:37.110: INFO: Pod "pod-projected-configmaps-72e341a5-6437-4f30-b993-cc83cb80fc01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025599699s
STEP: Saw pod success
Mar 24 15:30:37.110: INFO: Pod "pod-projected-configmaps-72e341a5-6437-4f30-b993-cc83cb80fc01" satisfied condition "Succeeded or Failed"
Mar 24 15:30:37.113: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-configmaps-72e341a5-6437-4f30-b993-cc83cb80fc01 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 15:30:37.167: INFO: Waiting for pod pod-projected-configmaps-72e341a5-6437-4f30-b993-cc83cb80fc01 to disappear
Mar 24 15:30:37.171: INFO: Pod pod-projected-configmaps-72e341a5-6437-4f30-b993-cc83cb80fc01 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:37.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9106" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":288,"skipped":4844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:37.188: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-7320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:37.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7320" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":289,"skipped":4867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:37.444: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5262
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:30:39.775: INFO: Waiting up to 5m0s for pod "client-envvars-8430c342-715b-4647-828a-6ecc79aba459" in namespace "pods-5262" to be "Succeeded or Failed"
Mar 24 15:30:39.779: INFO: Pod "client-envvars-8430c342-715b-4647-828a-6ecc79aba459": Phase="Pending", Reason="", readiness=false. Elapsed: 3.263721ms
Mar 24 15:30:41.792: INFO: Pod "client-envvars-8430c342-715b-4647-828a-6ecc79aba459": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016017166s
STEP: Saw pod success
Mar 24 15:30:41.792: INFO: Pod "client-envvars-8430c342-715b-4647-828a-6ecc79aba459" satisfied condition "Succeeded or Failed"
Mar 24 15:30:41.795: INFO: Trying to get logs from node talos-default-worker-1 pod client-envvars-8430c342-715b-4647-828a-6ecc79aba459 container env3cont: <nil>
STEP: delete the pod
Mar 24 15:30:41.820: INFO: Waiting for pod client-envvars-8430c342-715b-4647-828a-6ecc79aba459 to disappear
Mar 24 15:30:41.824: INFO: Pod client-envvars-8430c342-715b-4647-828a-6ecc79aba459 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:41.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5262" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":290,"skipped":4892,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:41.843: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2044
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-a1c4ad99-13bc-4542-8f08-8e124f4c3b5d-7899
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:42.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2044" for this suite.
STEP: Destroying namespace "nspatchtest-a1c4ad99-13bc-4542-8f08-8e124f4c3b5d-7899" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":291,"skipped":4898,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:42.268: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2020
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:30:42.499: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:43.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2020" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":292,"skipped":4903,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:43.715: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:55.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-408" for this suite.

• [SLOW TEST:11.385 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":293,"skipped":4914,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:55.103: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8629
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 24 15:30:55.284: INFO: Waiting up to 5m0s for pod "pod-53763dbf-ef49-43ab-ad85-db6ce30c19b4" in namespace "emptydir-8629" to be "Succeeded or Failed"
Mar 24 15:30:55.290: INFO: Pod "pod-53763dbf-ef49-43ab-ad85-db6ce30c19b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.660266ms
Mar 24 15:30:57.303: INFO: Pod "pod-53763dbf-ef49-43ab-ad85-db6ce30c19b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01880829s
STEP: Saw pod success
Mar 24 15:30:57.303: INFO: Pod "pod-53763dbf-ef49-43ab-ad85-db6ce30c19b4" satisfied condition "Succeeded or Failed"
Mar 24 15:30:57.307: INFO: Trying to get logs from node talos-default-worker-1 pod pod-53763dbf-ef49-43ab-ad85-db6ce30c19b4 container test-container: <nil>
STEP: delete the pod
Mar 24 15:30:57.351: INFO: Waiting for pod pod-53763dbf-ef49-43ab-ad85-db6ce30c19b4 to disappear
Mar 24 15:30:57.355: INFO: Pod pod-53763dbf-ef49-43ab-ad85-db6ce30c19b4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:57.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8629" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":294,"skipped":4933,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:57.375: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2658
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-2d41a879-a883-4d0d-841e-28c586d83d01
STEP: Creating a pod to test consume secrets
Mar 24 15:30:57.676: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a0f1c5c6-2b98-4fbc-8abe-0cdc46cb234f" in namespace "projected-2658" to be "Succeeded or Failed"
Mar 24 15:30:57.713: INFO: Pod "pod-projected-secrets-a0f1c5c6-2b98-4fbc-8abe-0cdc46cb234f": Phase="Pending", Reason="", readiness=false. Elapsed: 37.685684ms
Mar 24 15:30:59.723: INFO: Pod "pod-projected-secrets-a0f1c5c6-2b98-4fbc-8abe-0cdc46cb234f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046820393s
STEP: Saw pod success
Mar 24 15:30:59.723: INFO: Pod "pod-projected-secrets-a0f1c5c6-2b98-4fbc-8abe-0cdc46cb234f" satisfied condition "Succeeded or Failed"
Mar 24 15:30:59.727: INFO: Trying to get logs from node talos-default-worker-1 pod pod-projected-secrets-a0f1c5c6-2b98-4fbc-8abe-0cdc46cb234f container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 15:30:59.762: INFO: Waiting for pod pod-projected-secrets-a0f1c5c6-2b98-4fbc-8abe-0cdc46cb234f to disappear
Mar 24 15:30:59.766: INFO: Pod pod-projected-secrets-a0f1c5c6-2b98-4fbc-8abe-0cdc46cb234f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:30:59.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2658" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":295,"skipped":4944,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:30:59.788: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 15:31:00.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f10c2e17-316a-428b-a0c6-2b6df02fba93" in namespace "downward-api-1878" to be "Succeeded or Failed"
Mar 24 15:31:00.073: INFO: Pod "downwardapi-volume-f10c2e17-316a-428b-a0c6-2b6df02fba93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.203394ms
Mar 24 15:31:02.085: INFO: Pod "downwardapi-volume-f10c2e17-316a-428b-a0c6-2b6df02fba93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016709189s
STEP: Saw pod success
Mar 24 15:31:02.085: INFO: Pod "downwardapi-volume-f10c2e17-316a-428b-a0c6-2b6df02fba93" satisfied condition "Succeeded or Failed"
Mar 24 15:31:02.089: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-f10c2e17-316a-428b-a0c6-2b6df02fba93 container client-container: <nil>
STEP: delete the pod
Mar 24 15:31:02.125: INFO: Waiting for pod downwardapi-volume-f10c2e17-316a-428b-a0c6-2b6df02fba93 to disappear
Mar 24 15:31:02.128: INFO: Pod downwardapi-volume-f10c2e17-316a-428b-a0c6-2b6df02fba93 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:31:02.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1878" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":296,"skipped":4950,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:31:02.151: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 15:31:02.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-46c61cee-d537-4c28-93e9-38dc712040b9" in namespace "downward-api-9081" to be "Succeeded or Failed"
Mar 24 15:31:02.355: INFO: Pod "downwardapi-volume-46c61cee-d537-4c28-93e9-38dc712040b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.370429ms
Mar 24 15:31:04.363: INFO: Pod "downwardapi-volume-46c61cee-d537-4c28-93e9-38dc712040b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011532371s
STEP: Saw pod success
Mar 24 15:31:04.363: INFO: Pod "downwardapi-volume-46c61cee-d537-4c28-93e9-38dc712040b9" satisfied condition "Succeeded or Failed"
Mar 24 15:31:04.367: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-46c61cee-d537-4c28-93e9-38dc712040b9 container client-container: <nil>
STEP: delete the pod
Mar 24 15:31:04.412: INFO: Waiting for pod downwardapi-volume-46c61cee-d537-4c28-93e9-38dc712040b9 to disappear
Mar 24 15:31:04.416: INFO: Pod downwardapi-volume-46c61cee-d537-4c28-93e9-38dc712040b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:31:04.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9081" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":4950,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:31:04.428: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 15:31:04.634: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bad0a39b-5dd4-4507-904a-6c029329bad0" in namespace "downward-api-4444" to be "Succeeded or Failed"
Mar 24 15:31:04.638: INFO: Pod "downwardapi-volume-bad0a39b-5dd4-4507-904a-6c029329bad0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.567159ms
Mar 24 15:31:06.650: INFO: Pod "downwardapi-volume-bad0a39b-5dd4-4507-904a-6c029329bad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015969265s
STEP: Saw pod success
Mar 24 15:31:06.650: INFO: Pod "downwardapi-volume-bad0a39b-5dd4-4507-904a-6c029329bad0" satisfied condition "Succeeded or Failed"
Mar 24 15:31:06.654: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-bad0a39b-5dd4-4507-904a-6c029329bad0 container client-container: <nil>
STEP: delete the pod
Mar 24 15:31:06.687: INFO: Waiting for pod downwardapi-volume-bad0a39b-5dd4-4507-904a-6c029329bad0 to disappear
Mar 24 15:31:06.691: INFO: Pod downwardapi-volume-bad0a39b-5dd4-4507-904a-6c029329bad0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:31:06.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4444" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":298,"skipped":4954,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:31:06.713: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-3186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar 24 15:31:06.911: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 15:32:07.159: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:32:07.162: INFO: Starting informer...
STEP: Starting pod...
Mar 24 15:32:07.394: INFO: Pod is running on talos-default-worker-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 24 15:32:07.419: INFO: Pod wasn't evicted. Proceeding
Mar 24 15:32:07.419: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 24 15:33:22.444: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:33:22.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3186" for this suite.

• [SLOW TEST:135.552 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":299,"skipped":5042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:33:22.490: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9001
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:33:22.694: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 24 15:33:25.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-9001 --namespace=crd-publish-openapi-9001 create -f -'
Mar 24 15:33:26.432: INFO: stderr: ""
Mar 24 15:33:26.432: INFO: stdout: "e2e-test-crd-publish-openapi-8119-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 24 15:33:26.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-9001 --namespace=crd-publish-openapi-9001 delete e2e-test-crd-publish-openapi-8119-crds test-cr'
Mar 24 15:33:26.549: INFO: stderr: ""
Mar 24 15:33:26.549: INFO: stdout: "e2e-test-crd-publish-openapi-8119-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 24 15:33:26.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-9001 --namespace=crd-publish-openapi-9001 apply -f -'
Mar 24 15:33:26.930: INFO: stderr: ""
Mar 24 15:33:26.930: INFO: stdout: "e2e-test-crd-publish-openapi-8119-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 24 15:33:26.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-9001 --namespace=crd-publish-openapi-9001 delete e2e-test-crd-publish-openapi-8119-crds test-cr'
Mar 24 15:33:27.027: INFO: stderr: ""
Mar 24 15:33:27.027: INFO: stdout: "e2e-test-crd-publish-openapi-8119-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 24 15:33:27.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-088565179 --namespace=crd-publish-openapi-9001 explain e2e-test-crd-publish-openapi-8119-crds'
Mar 24 15:33:27.293: INFO: stderr: ""
Mar 24 15:33:27.293: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8119-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:33:29.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9001" for this suite.

• [SLOW TEST:6.742 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":300,"skipped":5088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:33:29.239: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4943
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 15:33:29.433: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2d1da11-3677-40f4-a6e4-21e0eccbd37f" in namespace "projected-4943" to be "Succeeded or Failed"
Mar 24 15:33:29.437: INFO: Pod "downwardapi-volume-d2d1da11-3677-40f4-a6e4-21e0eccbd37f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.79364ms
Mar 24 15:33:31.450: INFO: Pod "downwardapi-volume-d2d1da11-3677-40f4-a6e4-21e0eccbd37f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0165321s
STEP: Saw pod success
Mar 24 15:33:31.450: INFO: Pod "downwardapi-volume-d2d1da11-3677-40f4-a6e4-21e0eccbd37f" satisfied condition "Succeeded or Failed"
Mar 24 15:33:31.455: INFO: Trying to get logs from node talos-default-worker-1 pod downwardapi-volume-d2d1da11-3677-40f4-a6e4-21e0eccbd37f container client-container: <nil>
STEP: delete the pod
Mar 24 15:33:31.509: INFO: Waiting for pod downwardapi-volume-d2d1da11-3677-40f4-a6e4-21e0eccbd37f to disappear
Mar 24 15:33:31.512: INFO: Pod downwardapi-volume-d2d1da11-3677-40f4-a6e4-21e0eccbd37f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:33:31.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4943" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":301,"skipped":5134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:33:31.533: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8656
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 24 15:33:31.733: INFO: Waiting up to 5m0s for pod "pod-df369b7a-c0ef-4eb5-85b5-fb79b743e538" in namespace "emptydir-8656" to be "Succeeded or Failed"
Mar 24 15:33:31.741: INFO: Pod "pod-df369b7a-c0ef-4eb5-85b5-fb79b743e538": Phase="Pending", Reason="", readiness=false. Elapsed: 7.520225ms
Mar 24 15:33:33.753: INFO: Pod "pod-df369b7a-c0ef-4eb5-85b5-fb79b743e538": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019854889s
STEP: Saw pod success
Mar 24 15:33:33.753: INFO: Pod "pod-df369b7a-c0ef-4eb5-85b5-fb79b743e538" satisfied condition "Succeeded or Failed"
Mar 24 15:33:33.756: INFO: Trying to get logs from node talos-default-worker-1 pod pod-df369b7a-c0ef-4eb5-85b5-fb79b743e538 container test-container: <nil>
STEP: delete the pod
Mar 24 15:33:33.793: INFO: Waiting for pod pod-df369b7a-c0ef-4eb5-85b5-fb79b743e538 to disappear
Mar 24 15:33:33.797: INFO: Pod pod-df369b7a-c0ef-4eb5-85b5-fb79b743e538 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:33:33.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8656" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":302,"skipped":5163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:33:33.816: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 24 15:33:36.566: INFO: Successfully updated pod "pod-update-e783f783-e271-4944-9bb4-ad5b6c2bce92"
STEP: verifying the updated pod is in kubernetes
Mar 24 15:33:36.577: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:33:36.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3087" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5190,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:33:36.592: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar 24 15:33:36.774: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 15:33:36.870: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 15:33:36.877: INFO: 
Logging pods the apiserver thinks is on node talos-default-worker-1 before test
Mar 24 15:33:36.882: INFO: kube-flannel-9nwp5 from kube-system started at 2021-03-24 14:11:00 +0000 UTC (1 container statuses recorded)
Mar 24 15:33:36.882: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 15:33:36.882: INFO: kube-proxy-vzljq from kube-system started at 2021-03-24 14:11:00 +0000 UTC (1 container statuses recorded)
Mar 24 15:33:36.882: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 15:33:36.882: INFO: pod-update-e783f783-e271-4944-9bb4-ad5b6c2bce92 from pods-3087 started at 2021-03-24 15:33:34 +0000 UTC (1 container statuses recorded)
Mar 24 15:33:36.882: INFO: 	Container nginx ready: true, restart count 0
Mar 24 15:33:36.882: INFO: sonobuoy from sonobuoy started at 2021-03-24 14:20:14 +0000 UTC (1 container statuses recorded)
Mar 24 15:33:36.882: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 24 15:33:36.882: INFO: 
Logging pods the apiserver thinks is on node talos-default-worker-2 before test
Mar 24 15:33:36.888: INFO: coredns-6bd7f94d9b-sq6kb from kube-system started at 2021-03-24 14:11:20 +0000 UTC (1 container statuses recorded)
Mar 24 15:33:36.888: INFO: 	Container coredns ready: true, restart count 0
Mar 24 15:33:36.888: INFO: kube-flannel-fnwwj from kube-system started at 2021-03-24 14:10:52 +0000 UTC (1 container statuses recorded)
Mar 24 15:33:36.888: INFO: 	Container kube-flannel ready: true, restart count 1
Mar 24 15:33:36.888: INFO: kube-proxy-swlm2 from kube-system started at 2021-03-24 14:10:52 +0000 UTC (1 container statuses recorded)
Mar 24 15:33:36.888: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 15:33:36.888: INFO: sonobuoy-e2e-job-865e6f320bce471a from sonobuoy started at 2021-03-24 14:20:18 +0000 UTC (2 container statuses recorded)
Mar 24 15:33:36.889: INFO: 	Container e2e ready: true, restart count 0
Mar 24 15:33:36.889: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node talos-default-worker-1
STEP: verifying the node has the label node talos-default-worker-2
Mar 24 15:33:36.952: INFO: Pod coredns-6bd7f94d9b-sq6kb requesting resource cpu=100m on Node talos-default-worker-2
Mar 24 15:33:36.953: INFO: Pod kube-flannel-9nwp5 requesting resource cpu=0m on Node talos-default-worker-1
Mar 24 15:33:36.953: INFO: Pod kube-flannel-fnwwj requesting resource cpu=0m on Node talos-default-worker-2
Mar 24 15:33:36.953: INFO: Pod kube-proxy-swlm2 requesting resource cpu=0m on Node talos-default-worker-2
Mar 24 15:33:36.953: INFO: Pod kube-proxy-vzljq requesting resource cpu=0m on Node talos-default-worker-1
Mar 24 15:33:36.953: INFO: Pod pod-update-e783f783-e271-4944-9bb4-ad5b6c2bce92 requesting resource cpu=0m on Node talos-default-worker-1
Mar 24 15:33:36.954: INFO: Pod sonobuoy requesting resource cpu=0m on Node talos-default-worker-1
Mar 24 15:33:36.954: INFO: Pod sonobuoy-e2e-job-865e6f320bce471a requesting resource cpu=0m on Node talos-default-worker-2
STEP: Starting Pods to consume most of the cluster CPU.
Mar 24 15:33:36.954: INFO: Creating a pod which consumes cpu=2100m on Node talos-default-worker-1
Mar 24 15:33:36.978: INFO: Creating a pod which consumes cpu=2030m on Node talos-default-worker-2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ad736fe-1051-4606-90d0-066ea284a5a0.166f512682755bd3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6309/filler-pod-7ad736fe-1051-4606-90d0-066ea284a5a0 to talos-default-worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ad736fe-1051-4606-90d0-066ea284a5a0.166f5126a6584e65], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ad736fe-1051-4606-90d0-066ea284a5a0.166f5126ad0089e1], Reason = [Created], Message = [Created container filler-pod-7ad736fe-1051-4606-90d0-066ea284a5a0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ad736fe-1051-4606-90d0-066ea284a5a0.166f5126b0a19f51], Reason = [Started], Message = [Started container filler-pod-7ad736fe-1051-4606-90d0-066ea284a5a0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a5371dfd-33a6-4f68-a071-9ed1005a1206.166f512680376296], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6309/filler-pod-a5371dfd-33a6-4f68-a071-9ed1005a1206 to talos-default-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a5371dfd-33a6-4f68-a071-9ed1005a1206.166f5126acfc87ba], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a5371dfd-33a6-4f68-a071-9ed1005a1206.166f5126b108e9fe], Reason = [Created], Message = [Created container filler-pod-a5371dfd-33a6-4f68-a071-9ed1005a1206]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a5371dfd-33a6-4f68-a071-9ed1005a1206.166f5126b6f8aca1], Reason = [Started], Message = [Started container filler-pod-a5371dfd-33a6-4f68-a071-9ed1005a1206]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.166f5126fd1a3835], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.166f5126ffeda649], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node talos-default-worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node talos-default-worker-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:33:40.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6309" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":304,"skipped":5198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:33:40.178: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-6673
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 15:33:40.490: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 24 15:33:40.538: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 24 15:33:42.551: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:33:44.552: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:33:46.549: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:33:48.549: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:33:50.552: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:33:52.550: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 15:33:54.550: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 24 15:33:54.556: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 24 15:33:56.566: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 24 15:33:58.569: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 24 15:34:00.675: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 24 15:34:00.675: INFO: Going to poll 10.244.2.87 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Mar 24 15:34:00.678: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.87:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6673 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:34:00.678: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 15:34:00.748: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 24 15:34:00.748: INFO: Going to poll 10.244.3.124 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Mar 24 15:34:00.752: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.124:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6673 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 15:34:00.753: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
Mar 24 15:34:00.834: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:34:00.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6673" for this suite.

• [SLOW TEST:20.673 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5227,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:34:00.855: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar 24 15:34:01.035: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:34:06.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3895" for this suite.

• [SLOW TEST:5.761 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":306,"skipped":5247,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:34:06.616: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar 24 15:34:06.792: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 15:34:06.801: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 15:34:06.806: INFO: 
Logging pods the apiserver thinks is on node talos-default-worker-1 before test
Mar 24 15:34:06.812: INFO: kube-flannel-9nwp5 from kube-system started at 2021-03-24 14:11:00 +0000 UTC (1 container statuses recorded)
Mar 24 15:34:06.812: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 15:34:06.812: INFO: kube-proxy-vzljq from kube-system started at 2021-03-24 14:11:00 +0000 UTC (1 container statuses recorded)
Mar 24 15:34:06.812: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 15:34:06.812: INFO: sonobuoy from sonobuoy started at 2021-03-24 14:20:14 +0000 UTC (1 container statuses recorded)
Mar 24 15:34:06.812: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 24 15:34:06.812: INFO: 
Logging pods the apiserver thinks is on node talos-default-worker-2 before test
Mar 24 15:34:06.817: INFO: pod-init-09617ea5-a41b-4714-afeb-2b16a6a091ce from init-container-3895 started at 2021-03-24 15:34:01 +0000 UTC (1 container statuses recorded)
Mar 24 15:34:06.818: INFO: 	Container run1 ready: true, restart count 0
Mar 24 15:34:06.818: INFO: coredns-6bd7f94d9b-sq6kb from kube-system started at 2021-03-24 14:11:20 +0000 UTC (1 container statuses recorded)
Mar 24 15:34:06.818: INFO: 	Container coredns ready: true, restart count 0
Mar 24 15:34:06.818: INFO: kube-flannel-fnwwj from kube-system started at 2021-03-24 14:10:52 +0000 UTC (1 container statuses recorded)
Mar 24 15:34:06.818: INFO: 	Container kube-flannel ready: true, restart count 1
Mar 24 15:34:06.818: INFO: kube-proxy-swlm2 from kube-system started at 2021-03-24 14:10:52 +0000 UTC (1 container statuses recorded)
Mar 24 15:34:06.818: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 15:34:06.818: INFO: sonobuoy-e2e-job-865e6f320bce471a from sonobuoy started at 2021-03-24 14:20:18 +0000 UTC (2 container statuses recorded)
Mar 24 15:34:06.818: INFO: 	Container e2e ready: true, restart count 0
Mar 24 15:34:06.818: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8c7a2230-6028-4341-9302-72e417c66f1c 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.5.0.5 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-8c7a2230-6028-4341-9302-72e417c66f1c off the node talos-default-worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8c7a2230-6028-4341-9302-72e417c66f1c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:39:13.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7529" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:306.412 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":307,"skipped":5265,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:39:13.028: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5372
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 15:39:13.708: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 15:39:16.854: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:39:16.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5372" for this suite.
STEP: Destroying namespace "webhook-5372-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":308,"skipped":5270,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:39:17.070: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-9759
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 24 15:39:17.648: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 15:39:20.711: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 15:39:20.746: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:39:22.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9759" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:5.209 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":309,"skipped":5273,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:39:22.279: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Mar 24 15:39:22.530: INFO: created test-pod-1
Mar 24 15:39:22.550: INFO: created test-pod-2
Mar 24 15:39:22.576: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:39:22.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9583" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":310,"skipped":5278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 15:39:22.763: INFO: >>> kubeConfig: /tmp/kubeconfig-088565179
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 24 15:39:23.017: INFO: Waiting up to 5m0s for pod "pod-4603cbb8-9466-47fc-8f81-902bc08c8911" in namespace "emptydir-5943" to be "Succeeded or Failed"
Mar 24 15:39:23.021: INFO: Pod "pod-4603cbb8-9466-47fc-8f81-902bc08c8911": Phase="Pending", Reason="", readiness=false. Elapsed: 3.279798ms
Mar 24 15:39:25.032: INFO: Pod "pod-4603cbb8-9466-47fc-8f81-902bc08c8911": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015177251s
STEP: Saw pod success
Mar 24 15:39:25.033: INFO: Pod "pod-4603cbb8-9466-47fc-8f81-902bc08c8911" satisfied condition "Succeeded or Failed"
Mar 24 15:39:25.036: INFO: Trying to get logs from node talos-default-worker-1 pod pod-4603cbb8-9466-47fc-8f81-902bc08c8911 container test-container: <nil>
STEP: delete the pod
Mar 24 15:39:25.094: INFO: Waiting for pod pod-4603cbb8-9466-47fc-8f81-902bc08c8911 to disappear
Mar 24 15:39:25.098: INFO: Pod pod-4603cbb8-9466-47fc-8f81-902bc08c8911 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 15:39:25.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5943" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":311,"skipped":5306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSMar 24 15:39:25.120: INFO: Running AfterSuite actions on all nodes
Mar 24 15:39:25.120: INFO: Running AfterSuite actions on node 1
Mar 24 15:39:25.120: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 4736.972 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h18m58.118204025s
Test Suite Passed
